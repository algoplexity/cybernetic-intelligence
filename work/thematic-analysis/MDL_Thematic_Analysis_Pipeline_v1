{"cells":[{"cell_type":"markdown","metadata":{"id":"8VT-aVewtbcF"},"source":["Refer to requirements on wiki"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4857,"status":"ok","timestamp":1756826259207,"user":{"displayName":"Mak éº¥Yeu Wen è€€æ¦®","userId":"12147101794364230563"},"user_tz":-600},"id":"lxX6RNzBo6ES","outputId":"ecd9b739-8a5a-4088-817e-4999f3439a9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting repo2text\n","  Downloading repo2text-0.1.1-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: gitpython>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from repo2text) (3.1.45)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython>=3.1.0->repo2text) (4.0.12)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.0->repo2text) (5.0.2)\n","Downloading repo2text-0.1.1-py3-none-any.whl (4.1 kB)\n","Installing collected packages: repo2text\n","Successfully installed repo2text-0.1.1\n"]},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _xla_gc_callback at 0x78ea18d84900>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","    \n","KeyboardInterrupt: \n"]}],"source":["pip install repo2text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pFK1FXUjpaHR","outputId":"911e6504-f838-4770-e0f5-a508d425731b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning repository from https://github.com/BerriAI/litellm...\n"]}],"source":["!repo2text .py https://github.com/BerriAI/litellm"]},{"cell_type":"markdown","source":["#3rd Sept"],"metadata":{"id":"_qCkGQRgziCF"}},{"cell_type":"code","source":["# ==============================================================================\n","# @title      CELL 1 (v2.5): One-Time Install (Run ONCE per session)\n","# ==============================================================================\n","print(\"--- Installing all necessary libraries... ---\")\n","# Uninstall existing DSPy to avoid conflicts\n","!pip uninstall -q -y dspy dspy-ai\n","# Install DSPy from GitHub (latest main branch)\n","!pip install -q -U git+https://github.com/stanfordnlp/dspy.git\n","# Install pinned dependencies for compatibility\n","!pip install -q -U transformers==4.44.2 bitsandbytes==0.43.3 accelerate==0.33.0 torch==2.4.0 huggingface_hub==0.24.6 pydantic==2.8.2 soundfile==0.12.1\n","print(\"\\nâœ… Installation complete.\")\n","# Verify DSPy version\n","import dspy\n","print(f\"âœ… DSPy version: {dspy.__version__}\")\n","print(\"ðŸ”´ IMPORTANT: Please restart the runtime now. Go to 'Runtime' -> 'Restart session'.\")"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":342},"id":"3NUrXZaSDdE-","executionInfo":{"status":"error","timestamp":1756827120403,"user_tz":-600,"elapsed":34152,"user":{"displayName":"Mak éº¥Yeu Wen è€€æ¦®","userId":"12147101794364230563"}},"outputId":"5a8af17d-ed3a-454b-868a-7e3eaa331c46"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Installing all necessary libraries... ---\n","\u001b[33mWARNING: Skipping dspy as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for dspy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\n","âœ… Installation complete.\n"]},{"output_type":"error","ename":"AttributeError","evalue":"module 'dspy' has no attribute '__version__'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3567970775.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Verify DSPy version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdspy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… DSPy version: {dspy.__version__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸ”´ IMPORTANT: Please restart the runtime now. Go to 'Runtime' -> 'Restart session'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'dspy' has no attribute '__version__'"]}]},{"cell_type":"code","source":["# ==============================================================================\n","# @title      CELL 1 (v2.1): One-Time Install (Run ONCE per session)\n","# ==============================================================================\n","print(\"--- Installing all necessary libraries... ---\")\n","# Uninstall any existing DSPy to avoid conflicts\n","!pip uninstall -q -y dspy dspy-ai\n","# Install pinned versions\n","!pip install -q -U dspy-ai==2.5.1 transformers==4.44.2 bitsandbytes==0.43.3 accelerate==0.33.0 torch==2.4.0 huggingface_hub==0.24.6 pydantic==2.8.2 soundfile==0.12.1\n","print(\"\\nâœ… Installation complete.\")\n","# Verify DSPy version\n","import dspy\n","print(f\"âœ… DSPy version: {dspy.__version__}\")  # Should print 2.5.1\n","print(\"ðŸ”´ IMPORTANT: Please restart the runtime now. Go to 'Runtime' -> 'Restart session'.\")"],"metadata":{"id":"uN9b_Wdf-n5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# @title      CELL 3 (v5.3): Minimal Thematic Extraction for One Response\n","# ==============================================================================\n","import json\n","import dspy\n","from pydantic import BaseModel, Field\n","from huggingface_hub import login\n","from google.colab import userdata\n","\n","# --- Configuration ---\n","QUESTION_TO_TEST = 'Q5'\n","DATA_DIR = '.'  # Adjust to your data directory\n","P1_QUESTION_CORPUS_FILE = f'{DATA_DIR}/question_corpus.json'\n","QUESTION_DEFINITIONS_FILE = f'{DATA_DIR}/question_definitions.json'\n","OUTPUT_FILE = f'{DATA_DIR}/p1_thematic_dictionary_DSPy_GEMMA3_v5_3.json'\n","HF_TOKEN = userdata.get('HF_TOKEN')\n","if HF_TOKEN:\n","    login(token=HF_TOKEN)\n","else:\n","    raise ValueError(\"HF_TOKEN not found in Colab Secrets.\")\n","\n","# --- Step 1: Configure Gemma 3 4B-IT ---\n","print(f\"--- Configuring Gemma 3 4B-IT ---\")\n","try:\n","    lm = dspy.LM(\n","        model=\"google/gemma-3-4b-it\",  # Hugging Face model ID\n","        model_type=\"local\",  # Force local transformers backend\n","        torch_dtype=\"bfloat16\",\n","        load_in_4bit=True,\n","        max_tokens=2048,  # Reduced for single response\n","        trust_remote_code=True\n","    )\n","    dspy.settings.configure(lm=lm, adapter=dspy.JSONAdapter())\n","    print(\"âœ… Gemma 3 4B-IT configured with dspy.LM (local transformers backend).\")\n","except Exception as e:\n","    if \"Vertex AI\" in str(e):\n","        print(f\"WARNING: Vertex AI branch detected: {e}\")\n","        print(\"Confirming local model loading...\")\n","    else:\n","        print(f\"WARNING: Failed to configure Gemma 3 4B-IT: {e}\")\n","    print(\"Attempting fallback to Gemma 3 2B-IT...\")\n","    try:\n","        lm = dspy.LM(\n","            model=\"google/gemma-3-2b-it\",  # Fallback model\n","            model_type=\"local\",\n","            torch_dtype=\"bfloat16\",\n","            load_in_4bit=True,\n","            max_tokens=2048,\n","            trust_remote_code=True\n","        )\n","        dspy.settings.configure(lm=lm, adapter=dspy.JSONAdapter())\n","        print(\"âœ… Fallback to Gemma 3 2B-IT successful.\")\n","    except Exception as e2:\n","        print(f\"CRITICAL: Failed to configure fallback model: {e2}\")\n","        raise\n","\n","# --- Step 2: Load Data (Single Response) ---\n","print(f\"\\n--- Loading data for {QUESTION_TO_TEST} ---\")\n","try:\n","    with open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8') as f:\n","        question_corpus = json.load(f)\n","    with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f:\n","        qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","    docs = question_corpus.get(QUESTION_TO_TEST, [])\n","    if not docs:\n","        raise ValueError(f\"No responses found for {QUESTION_TO_TEST}\")\n","    single_response = docs[0]  # Process only the first response\n","    question_text = qid_to_text.get(QUESTION_TO_TEST, \"\")\n","    print(f\"âœ… Loaded 1 response for {QUESTION_TO_TEST}: {single_response['text'][:50]}...\")\n","except Exception as e:\n","    print(f\"CRITICAL: Failed to load data: {e}\")\n","    raise\n","\n","# --- Step 3: Define Typed Signatures with Pydantic ---\n","class Theme(BaseModel):\n","    theme_name: str = Field(description=\"Unique, nuanced theme name\")\n","    keywords: list[str] = Field(description=\"3-5 keywords describing the theme\", min_items=3, max_items=5)\n","\n","class ExtractThemesFromResponse(dspy.Signature):\n","    \"\"\"Analyze a single survey response to extract 1-3 nuanced, specific themes. Output a JSON list of dictionaries, each with 'theme_name' (string) and 'keywords' (list of 3-5 strings). Use double quotes, no extra fields (e.g., 'description', 'id'). Example: [{\"theme_name\": \"Theme\", \"keywords\": [\"word1\", \"word2\", \"word3\"]}]\"\"\"\n","    question_text: str = dspy.InputField()\n","    response: str = dspy.InputField()\n","    themes_json: list[Theme] = dspy.OutputField()\n","\n","# --- Step 4: Training Set (Minimal for Privacy Context) ---\n","train_set_extract = [\n","    dspy.Example(\n","        question_text=\"Views on workplace privacy regulations?\",\n","        response=\"The privacy regulator should enforce compliance and handle complaints effectively.\",\n","        themes_json=[\n","            Theme(theme_name=\"Regulator Enforcement\", keywords=[\"regulator\", \"enforcement\", \"compliance\"]),\n","            Theme(theme_name=\"Complaint Handling\", keywords=[\"complaints\", \"grievances\", \"resolution\"])\n","        ]\n","    ).with_inputs(\"question_text\", \"response\")\n","]\n","\n","print(\"âœ… Training set created (1 example).\")\n","\n","# --- Step 5: Optimize Module with Assertions ---\n","print(f\"\\n--- Optimizing Theme Extractor ---\")\n","try:\n","    optimizer_extract = dspy.MIPROv2(num_candidates=5, metric=None, max_bootstrapped_demos=1)\n","    compiled_extractor = optimizer_extract.compile(\n","        dspy.TypedChainOfThought(ExtractThemesFromResponse),\n","        trainset=train_set_extract\n","    )\n","    dspy.Assert(\n","        compiled_extractor.themes_json,\n","        lambda x: all(isinstance(t, Theme) and 3 <= len(t.keywords) <= 5 for t in x),\n","        \"Each theme must be a Theme object with 3-5 keywords.\"\n","    )\n","    print(\"âœ… Extractor compiled with MIPROv2 and TypedChainOfThought.\")\n","except Exception as e:\n","    print(f\"CRITICAL: Failed to optimize module: {e}\")\n","    raise\n","\n","# --- Step 6: Extract Themes from Single Response ---\n","print(f\"\\n--- Extracting themes from 1 response ---\")\n","try:\n","    result = compiled_extractor(question_text=question_text, response=single_response['text'])\n","    extracted_themes = result.themes_json  # Typed output ensures Pydantic structure\n","    print(f\"âœ… Extracted themes: {json.dumps([t.model_dump() for t in extracted_themes], indent=2)}\")\n","except Exception as e:\n","    print(f\"CRITICAL: Failed to extract themes: {e}\")\n","    raise\n","\n","# --- Step 7: Save Output (No Aggregation for Single Response) ---\n","print(f\"\\n--- Saving extracted themes ---\")\n","try:\n","    output = {QUESTION_TO_TEST: [t.model_dump() for t in extracted_themes]}\n","    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","        json.dump(output, f, indent=4)\n","    print(f\"\\n>>> EXTRACTED THEMES <<<\"); print(\"=\"*30); print(json.dumps(output, indent=2)); print(\"=\"*30)\n","    print(f\"\\nâœ… SUCCESS: Themes saved to {OUTPUT_FILE}\")\n","except Exception as e:\n","    print(f\"\\nCRITICAL FAILURE during saving: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":559},"cellView":"form","id":"Es5sHdv7CRD3","executionInfo":{"status":"error","timestamp":1756826792962,"user_tz":-600,"elapsed":1440,"user":{"displayName":"Mak éº¥Yeu Wen è€€æ¦®","userId":"12147101794364230563"}},"outputId":"bc16fcda-896a-4645-ae8b-18070f83bac8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n","--- Configuring Gemma 3 4B-IT ---\n","WARNING: Failed to configure Gemma 3 4B-IT: module 'dspy' has no attribute 'JSONAdapter'\n","Attempting fallback to Gemma 3 2B-IT...\n","CRITICAL: Failed to configure fallback model: module 'dspy' has no attribute 'JSONAdapter'\n"]},{"output_type":"error","ename":"AttributeError","evalue":"module 'dspy' has no attribute 'JSONAdapter'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1520445089.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     )\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdspy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdspy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONAdapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… Gemma 3 4B-IT configured with dspy.LM (local transformers backend).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'dspy' has no attribute 'JSONAdapter'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1520445089.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         )\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mdspy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdspy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONAdapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… Fallback to Gemma 3 2B-IT successful.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'dspy' has no attribute 'JSONAdapter'"]}]},{"cell_type":"code","source":["# ==============================================================================\n","# @title      CELL 3 (v4.7): Optimized Thematic Extraction with Gemma 3 4B-IT\n","# ==============================================================================\n","import json\n","import re\n","import dspy\n","import ast\n","from collections import defaultdict\n","from huggingface_hub import login\n","from google.colab import userdata\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# --- Configuration ---\n","QUESTION_TO_TEST = 'Q5'\n","OUTPUT_FILE = os.path.join(DATA_DIR, 'p1_thematic_dictionary_DSPy_GEMMA3_OPT_SUCCESS.json')\n","HF_TOKEN = userdata.get('HF_TOKEN')\n","if HF_TOKEN:\n","    login(token=HF_TOKEN)\n","else:\n","    raise ValueError(\"HF_TOKEN not found.\")\n","\n","# --- Step 1: Configure Gemma 3 4B-IT ---\n","print(f\"--- Configuring Gemma 3 4B-IT ---\")\n","try:\n","    lm = dspy.HFModel(model=\"google/gemma-3-4b-it\", torch_dtype=\"bfloat16\", load_in_4bit=True)\n","    dspy.settings.configure(lm=lm)\n","    print(\"âœ… Gemma 3 4B-IT configured.\")\n","except Exception as e:\n","    print(f\"CRITICAL: Failed to configure Gemma 3: {e}\")\n","    raise\n","\n","# --- Step 2: Load Data ---\n","print(f\"\\n--- Loading data for {QUESTION_TO_TEST} ---\")\n","with open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8') as f:\n","    question_corpus = json.load(f)\n","with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f:\n","    qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","docs = question_corpus.get(QUESTION_TO_TEST, [])\n","question_text = qid_to_text.get(QUESTION_TO_TEST, \"\")\n","print(f\"âœ… Loaded {len(docs)} documents for {QUESTION_TO_TEST}.\")\n","\n","# --- Step 3: Define Typed Signatures with Pydantic ---\n","from pydantic import BaseModel, Field\n","\n","class Theme(BaseModel):\n","    theme_name: str = Field(description=\"Nuanced theme name\")\n","    keywords: list[str] = Field(description=\"3-5 keywords\", min_items=3, max_items=5)\n","\n","class ExtractThemesFromResponse(dspy.Signature):\n","    \"\"\"Analyze a single survey response to extract 1-3 nuanced themes.\"\"\"\n","    question_text: str = dspy.InputField()\n","    response: str = dspy.InputField()\n","    themes_json: list[Theme] = dspy.OutputField()\n","\n","class AggregateThemes(dspy.Signature):\n","    \"\"\"Aggregate themes into a dictionary, merging intelligently.\"\"\"\n","    question_text: str = dspy.InputField()\n","    all_extracted_themes: str = dspy.InputField()\n","    thematic_dictionary_json: dict[str, list[str]] = dspy.OutputField()\n","\n","# --- Step 4: Training Sets (Expanded for Gemma 3) ---\n","train_set_extract = [\n","    # ... (keep previous examples, add one for privacy context)\n","    dspy.Example(\n","        question_text=\"Views on privacy regulator roles?\",\n","        response=\"Regulator should enforce obligations and develop codes.\",\n","        themes_json=[Theme(theme_name=\"Enforcement Role\", keywords=[\"enforce\", \"obligations\", \"compliance\"]),\n","                     Theme(theme_name=\"Code Development\", keywords=[\"develop\", \"codes\", \"standards\"])]\n","    ).with_inputs(\"question_text\", \"response\")\n","]\n","\n","train_set_aggregate = [\n","    # ... (keep previous)\n","]\n","\n","print(\"âœ… Training sets created.\")\n","\n","# --- Step 5: Optimize Modules with Assertions ---\n","print(f\"\\n--- Optimizing Modules ---\")\n","optimizer_extract = dspy.MIPROv2(num_candidates=10, metric=None, max_bootstrapped_demos=4)\n","compiled_extractor = optimizer_extract.compile(dspy.ChainOfThought(ExtractThemesFromResponse), trainset=train_set_extract)\n","dspy.Assert(compiled_extractor.themes_json, lambda x: all(3 <= len(t.keywords) <= 5 for t in x), \"Keywords must be 3-5 per theme.\")\n","print(\"âœ… Extractor compiled with MIPROv2, ChainOfThought, and Assertions.\")\n","\n","optimizer_aggregate = dspy.BootstrapFewShot(metric=None, max_bootstrapped_demos=2)\n","compiled_aggregator = optimizer_aggregate.compile(dspy.Predict(AggregateThemes), trainset=train_set_aggregate)\n","print(\"âœ… Aggregator compiled.\")\n","\n","# --- Step 6: Parallel Per-Response Extraction with Tool ---\n","print(f\"\\n--- Extracting themes from each of {len(docs)} responses (batched) ---\")\n","extracted_themes_list = []\n","failed_responses = []\n","\n","def extract_batch(batch_docs):\n","    batch_results = []\n","    for item in batch_docs:\n","        try:\n","            result = compiled_extractor(question_text=question_text, response=item['text'])\n","            parsed_list = result.themes_json  # Typed output ensures structure\n","            batch_results.append(parsed_list)\n","        except Exception as e:\n","            failed_responses.append((item, str(e)))\n","    return batch_results\n","\n","# Batch into groups of 10 for parallel processing\n","batches = [docs[i:i+10] for i in range(0, len(docs), 10)]\n","with ThreadPoolExecutor(max_workers=5) as executor:\n","    results = list(executor.map(extract_batch, batches))\n","extracted_themes_list = [item for sublist in results for item in sublist]\n","\n","print(f\"âœ… Extracted themes from {len(extracted_themes_list)} responses. Failed: {len(failed_responses)}\")\n","if failed_responses:\n","    print(\"\\n--- Failed Responses ---\")\n","    for idx, error in enumerate(failed_responses[:5]):\n","        print(f\"Failed {idx+1}: {error}\")\n","\n","# --- Step 7: Aggregate All Extracted Themes ---\n","print(f\"\\n--- Aggregating all extracted themes ---\")\n","all_extracted_str = json.dumps([t.model_dump() for themes in extracted_themes_list for t in themes])\n","try:\n","    result = compiled_aggregator(question_text=question_text, all_extracted_themes=all_extracted_str)\n","    raw_output = result.thematic_dictionary_json\n","    print(\"\\n>>> RAW AGGREGATED OUTPUT <<<\"); print(\"=\"*30); print(raw_output); print(\"=\"*30)\n","\n","    parsed_json = ast.literal_eval(raw_output) if isinstance(raw_output, str) else raw_output\n","\n","    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","        json.dump({QUESTION_TO_TEST: parsed_json}, f, indent=4)\n","    print(f\"\\nâœ… SUCCESS: Thematic dictionary saved to {OUTPUT_FILE}\")\n","except Exception as e:\n","    print(f\"\\nCRITICAL FAILURE during aggregation: {e}\")"],"metadata":{"cellView":"form","id":"KStzU0Cn84Py"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# @title      CELL 3 (v4.4): Enhanced Thematic Extraction with Per-Response Analysis\n","# ==============================================================================\n","import json\n","import re\n","import dspy\n","import ast  # For safe evaluation of Python dict literals\n","from collections import defaultdict\n","\n","# --- Configuration ---\n","QUESTION_TO_TEST = 'Q5'\n","OUTPUT_FILE = os.path.join(DATA_DIR, 'p1_thematic_dictionary_DSPy_ENHANCED_SUCCESS.json')\n","\n","# --- Step 1: Load Data ---\n","print(f\"--- Loading data for {QUESTION_TO_TEST} ---\")\n","with open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8') as f: question_corpus = json.load(f)\n","with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f: qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","docs = question_corpus.get(QUESTION_TO_TEST, [])\n","question_text = qid_to_text.get(QUESTION_TO_TEST, \"\")\n","print(f\"âœ… Loaded {len(docs)} documents for {QUESTION_TO_TEST}.\")\n","\n","# --- Step 2: Define Signatures ---\n","print(f\"\\n--- Defining Signatures for Per-Response Extraction and Aggregation ---\")\n","class ExtractThemesFromResponse(dspy.Signature):\n","    \"\"\"You are an expert data analyst. Analyze a single survey response and extract recurring themes as a JSON list of dictionaries, each with 'theme_name' and 'keywords' (list of 3-5 strings). Focus on nuanced, specific themes unique to this response.\n","    Response MUST be ONLY the JSON list. Use double quotes.\"\"\"\n","    question_text: str = dspy.InputField()\n","    response: str = dspy.InputField()\n","    themes_json: str = dspy.OutputField()\n","\n","class AggregateThemes(dspy.Signature):\n","    \"\"\"You are an expert data analyst. Aggregate extracted themes from multiple responses into a clean thematic dictionary. Output a JSON object where keys are consolidated theme names and values are lists of associated keywords. Merge similar themes intelligently to avoid collapse, preserving nuance.\n","    Response MUST be ONLY the JSON object. All keys and strings in double quotes.\"\"\"\n","    question_text: str = dspy.InputField()\n","    all_extracted_themes: str = dspy.InputField()\n","    thematic_dictionary_json: str = dspy.OutputField()\n","\n","# --- Step 3: Training Sets ---\n","train_set_extract = [\n","    dspy.Example(\n","        question_text=\"Thoughts on public parks?\",\n","        response=\"The playground is old. Benches are needed.\",\n","        themes_json=json.dumps([{\"theme_name\": \"Park Maintenance\", \"keywords\": [\"playground\", \"old\", \"repairs\"]}, {\"theme_name\": \"Park Amenities\", \"keywords\": [\"benches\", \"seating\", \"facilities\"]}])\n","    ).with_inputs(\"question_text\", \"response\"),\n","    dspy.Example(\n","        question_text=\"How to improve local transport?\",\n","        response=\"I can never find parking. The bus is always late.\",\n","        themes_json=json.dumps([{\"theme_name\": \"Parking Availability\", \"keywords\": [\"parking\", \"car parks\", \"finding a spot\"]}, {\"theme_name\": \"Public Transport Reliability\", \"keywords\": [\"bus\", \"late\", \"punctuality\"]}])\n","    ).with_inputs(\"question_text\", \"response\")\n","]\n","\n","train_set_aggregate = [\n","    dspy.Example(\n","        question_text=\"Thoughts on public parks?\",\n","        all_extracted_themes=json.dumps([\n","            [{\"theme_name\": \"Park Maintenance\", \"keywords\": [\"playground\", \"old\", \"repairs\"]}, {\"theme_name\": \"Park Amenities\", \"keywords\": [\"benches\", \"seating\", \"facilities\"]}],\n","            [{\"theme_name\": \"Park Maintenance\", \"keywords\": [\"grass\", \"mowing\", \"upkeep\"]}, {\"theme_name\": \"Park Amenities\", \"keywords\": [\"picnic tables\", \"shade\", \"areas\"]}]\n","        ]),\n","        thematic_dictionary_json=json.dumps({\"Park Maintenance\": [\"playground\", \"old\", \"repairs\", \"grass\", \"mowing\", \"upkeep\"], \"Park Amenities\": [\"benches\", \"seating\", \"facilities\", \"picnic tables\", \"shade\", \"areas\"]})\n","    ).with_inputs(\"question_text\", \"all_extracted_themes\"),\n","    dspy.Example(\n","        question_text=\"How to improve local transport?\",\n","        all_extracted_themes=json.dumps([\n","            [{\"theme_name\": \"Parking Availability\", \"keywords\": [\"parking\", \"car parks\", \"finding a spot\"]}, {\"theme_name\": \"Public Transport Reliability\", \"keywords\": [\"bus\", \"late\", \"punctuality\"]}],\n","            [{\"theme_name\": \"Parking Availability\", \"keywords\": [\"spaces\", \"availability\", \"downtown\"]}, {\"theme_name\": \"Public Transport Reliability\", \"keywords\": [\"train\", \"delays\", \"schedule\"]}]\n","        ]),\n","        thematic_dictionary_json=json.dumps({\"Parking Availability\": [\"parking\", \"car parks\", \"finding a spot\", \"spaces\", \"availability\", \"downtown\"], \"Public Transport Reliability\": [\"bus\", \"late\", \"punctuality\", \"train\", \"delays\", \"schedule\"]})\n","    ).with_inputs(\"question_text\", \"all_extracted_themes\")\n","]\n","\n","print(\"âœ… Training sets created.\")\n","\n","# --- Step 4: Optimize Modules ---\n","optimizer_extract = dspy.BootstrapFewShot(metric=None, max_bootstrapped_demos=2)\n","compiled_extractor = optimizer_extract.compile(dspy.Predict(ExtractThemesFromResponse), trainset=train_set_extract)\n","print(\"âœ… Extractor module compiled.\")\n","\n","optimizer_aggregate = dspy.BootstrapFewShot(metric=None, max_bootstrapped_demos=2)\n","compiled_aggregator = optimizer_aggregate.compile(dspy.Predict(AggregateThemes), trainset=train_set_aggregate)\n","print(\"âœ… Aggregator module compiled.\")\n","\n","# --- Step 5: Execute Per-Response Extraction ---\n","print(f\"\\n--- Extracting themes from each of {len(docs)} responses ---\")\n","extracted_themes_list = []\n","for i, item in enumerate(docs):\n","    try:\n","        result = compiled_extractor(question_text=question_text, response=item['text'])\n","        raw_output = result.themes_json.strip()\n","        if not raw_output.startswith('['):\n","            print(f\"Warning: Invalid list output for response {i+1}. Skipping.\")\n","            continue\n","        json_compatible = raw_output.replace(\"'\", '\"')\n","        parsed_list = json.loads(json_compatible)\n","        extracted_themes_list.append(parsed_list)\n","    except Exception as e:\n","        print(f\"Error extracting from response {i+1}: {e}\")\n","\n","print(f\"âœ… Extracted themes from {len(extracted_themes_list)} responses.\")\n","\n","# --- Step 6: Aggregate All Extracted Themes ---\n","print(f\"\\n--- Aggregating all extracted themes ---\")\n","all_extracted_str = json.dumps(extracted_themes_list)  # Concatenated as JSON string\n","try:\n","    result = compiled_aggregator(question_text=question_text, all_extracted_themes=all_extracted_str)\n","    raw_output = result.thematic_dictionary_json.strip()\n","    print(\"\\n>>> RAW AGGREGATED OUTPUT <<<\"); print(\"=\"*30); print(raw_output); print(\"=\"*30)\n","\n","    if not raw_output.startswith('{'):\n","        raise ValueError(\"Aggregator did not return a dictionary-like string.\")\n","\n","    # Robust parsing with ast.literal_eval\n","    parsed_json = ast.literal_eval(raw_output)\n","\n","    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","        json.dump({QUESTION_TO_TEST: parsed_json}, f, indent=4)\n","    print(f\"\\nâœ… SUCCESS: Enhanced thematic dictionary generated and saved to {OUTPUT_FILE}\")\n","except Exception as e:\n","    print(f\"\\nCRITICAL FAILURE during aggregation: {e}\")"],"metadata":{"cellView":"form","id":"jT5FsCMvzlBh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2nd Sept"],"metadata":{"id":"JU4YzoVY7R2C"}},{"cell_type":"code","source":["# ==============================================================================\n","# @title      CELL 1: One-Time Install (Run ONCE per session)\n","# ==============================================================================\n","print(\"--- Installing all necessary libraries... ---\")\n","!pip install -q -U git+https://github.com/stanfordnlp/dspy.git\n","!pip install -q transformers bitsandbytes accelerate torch huggingface_hub\n","\n","print(\"\\nâœ… Installation complete.\")\n","print(\"ðŸ”´ IMPORTANT: Please restart the runtime now. Go to 'Runtime' -> 'Restart session'.\")"],"metadata":{"id":"Jv_d7pOE7Rhx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# @title      CELL 2 (v4.1): The Definitive, Stable, & Powerful Configuration\n","# ==============================================================================\n","import json; import os; import re; import time; import torch; from google.colab import drive; import dspy; import transformers\n","\n","print(\"--- Step A: Mounting Google Drive & Authenticating ---\")\n","try:\n","    drive.mount('/content/drive', force_remount=True); print(\"âœ… Google Drive mounted.\")\n","    from google.colab import userdata; from huggingface_hub import login\n","    HF_TOKEN = userdata.get('HF_TOKEN');\n","    if HF_TOKEN: login(token=HF_TOKEN)\n","    print(\"âœ… Hugging Face session authenticated.\")\n","except Exception as e: print(f\"ERROR during setup: {e}\"); raise e\n","\n","print(\"\\n--- Step B: Loading the Proven Model with the Universal Stability Fix ---\")\n","try:\n","    model_id = \"google/gemma-2b-it\"\n","    print(f\"Loading proven model: {model_id}\")\n","\n","    model_pipeline = transformers.pipeline(\n","        \"text-generation\",\n","        model=model_id,\n","        model_kwargs={\n","            \"torch_dtype\": torch.bfloat16,\n","            # This is the UNIVERSAL stability fix for our environment.\n","            \"attn_implementation\": \"eager\"\n","        },\n","        device_map=\"auto\",\n","    )\n","\n","    class LocalHuggingFaceModel(dspy.LM):\n","        def __init__(self, pipeline, model_id, **kwargs):\n","            super().__init__(model_id)\n","            self.pipeline = pipeline\n","            self.kwargs = kwargs\n","        def __call__(self, messages, only_completed=True, return_sorted=False, **kwargs):\n","            prompt = messages[-1]['content']\n","            combined_kwargs = {**self.kwargs, **kwargs}\n","            response = self.pipeline(prompt, **combined_kwargs)\n","            return [response[0]['generated_text'][len(prompt):]]\n","\n","    llm = LocalHuggingFaceModel(model_pipeline, model_id=model_id, max_new_tokens=2048)\n","    dspy.settings.configure(lm=llm)\n","    print(f\"âœ… Model '{model_id}' loaded and DSPy configured. Ready for experiments.\")\n","except Exception as e:\n","    print(f\"CRITICAL: Failed to load model or configure DSPy. Error: {e}\"); raise e\n","\n","# --- Global Configuration (Unchanged) ---\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'; DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Data/')\n","P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json'); QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":691},"id":"Eq9w583slY9V","executionInfo":{"status":"error","timestamp":1756827205922,"user_tz":-600,"elapsed":7378,"user":{"displayName":"Mak éº¥Yeu Wen è€€æ¦®","userId":"12147101794364230563"}},"outputId":"e9c457cf-bffa-4c63-9a5c-1d7b893853e0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Step A: Mounting Google Drive & Authenticating ---\n","Mounted at /content/drive\n","âœ… Google Drive mounted.\n","The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n","âœ… Hugging Face session authenticated.\n","\n","--- Step B: Loading the Proven Model with the Universal Stability Fix ---\n","Loading proven model: google/gemma-2b-it\n","CRITICAL: Failed to load model or configure DSPy. Error: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n","operator torchvision::nms does not exist\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Failed to import transformers.pipelines because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseImageProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_processing_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageProcessingMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision::nms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0muse_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0muse_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m_register_fake\u001b[0;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_to_register\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registration_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_library/abstract_impl.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, func, source)\u001b[0m\n\u001b[1;32m     30\u001b[0m             )\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_has_kernel_for_dispatch_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             raise RuntimeError(\n","\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1806857298.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… Model '{model_id}' loaded and DSPy configured. Ready for experiments.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CRITICAL: Failed to load model or configure DSPy. Error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# --- Global Configuration (Unchanged) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1806857298.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading proven model: {model_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     model_pipeline = transformers.pipeline(\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1606\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist"]}]},{"cell_type":"code","source":["SUPERCEDED\n","# ==============================================================================\n","# @title      CELL 2: One-Time Load & Configure (Run ONCE after restart)\n","# ==============================================================================\n","import json\n","import os\n","import re\n","import time\n","import torch\n","from google.colab import drive\n","import dspy\n","import transformers\n","\n","print(\"--- Step A: Mounting Google Drive & Authenticating ---\")\n","try:\n","    drive.mount('/content/drive', force_remount=True)\n","    print(\"âœ… Google Drive mounted.\")\n","    from google.colab import userdata\n","    from huggingface_hub import login\n","    HF_TOKEN = userdata.get('HF_TOKEN')\n","    if HF_TOKEN: login(token=HF_TOKEN)\n","    print(\"âœ… Hugging Face session authenticated.\")\n","except Exception as e:\n","    print(f\"ERROR during setup: {e}\"); raise e\n","\n","print(\"\\n--- Step B: Loading Model and Configuring DSPy (This is the slow part) ---\")\n","try:\n","    model_id = \"google/gemma-3-270m-it\" # \"google/gemma-2b-it\"\n","    # This pipeline object will now live in memory for the whole session\n","    model_pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n","\n","    class LocalHuggingFaceModel(dspy.LM):\n","        def __init__(self, pipeline, model_id, **kwargs):\n","            super().__init__(model_id)\n","            self.pipeline = pipeline\n","            self.kwargs = kwargs\n","        def __call__(self, messages, only_completed=True, return_sorted=False, **kwargs):\n","            prompt = messages[-1]['content']\n","            response = self.pipeline(prompt, **self.kwargs)\n","            return [response[0]['generated_text'][len(prompt):]]\n","\n","    llm = LocalHuggingFaceModel(model_pipeline, model_id=model_id, max_new_tokens=1024)\n","    dspy.settings.configure(lm=llm)\n","    print(\"âœ… Model loaded and DSPy configured. Ready for experiments.\")\n","except Exception as e:\n","    print(f\"CRITICAL: Failed to load model or configure DSPy. Error: {e}\"); raise e\n","\n","# --- Global Configuration ---\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Data/')\n","P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json')\n","QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')"],"metadata":{"cellView":"form","id":"jsTJOWj57cc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# @title      CELL 3 (v4.1): The Final, Corrected, and Reviewed Solution\n","# ==============================================================================\n","# This cell assumes Cell 1 (Install) and Cell 2 (v4.1 - gemma-2b-it with eager) have been run.\n","\n","# --- Imports for this cell ---\n","import json\n","import re # We need this for robust parsing\n","import dspy\n","\n","# --- Configuration ---\n","QUESTION_TO_TEST = 'Q5'\n","# --- THIS IS THE CORRECTED LINE ---\n","OUTPUT_FILE = os.path.join(DATA_DIR, 'p1_thematic_dictionary_DSPy_FINAL_SUCCESS.json')\n","\n","# --- Step 1: Load Data ---\n","print(f\"--- Loading data for {QUESTION_TO_TEST} ---\")\n","with open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8') as f: question_corpus = json.load(f)\n","with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f: qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","docs = [item['text'] for item in question_corpus.get(QUESTION_TO_TEST, [])]; sample_text = \"\\n\\n---\\n\\n\".join(docs[:20]); question_text = qid_to_text.get(QUESTION_TO_TEST, \"\")\n","print(f\"âœ… Loaded and prepared a sample of {len(docs[:20])} documents.\")\n","\n","# --- Step 2: Define and Optimize ---\n","print(f\"\\n--- Defining Signature, Training Set, and Optimizing ---\")\n","\n","class GenerateThematicDictionary(dspy.Signature):\n","    \"\"\"You are an expert data analyst. Your task is to analyze survey responses and output a clean JSON object containing recurring themes.\n","    Your response MUST be ONLY the JSON object. All keys and string values MUST be enclosed in double quotes.\"\"\"\n","    question_text: str = dspy.InputField()\n","    survey_responses: str = dspy.InputField()\n","    thematic_dictionary_json: str = dspy.OutputField()\n","\n","train_set = [\n","    dspy.Example(\n","        question_text=\"Thoughts on public parks?\",\n","        survey_responses=\"The playground is old. Benches are needed.\",\n","        thematic_dictionary_json=json.dumps({\n","            \"Park Maintenance\": [\"playground\", \"old\", \"repairs\"],\n","            \"Park Amenities\": [\"benches\", \"seating\", \"facilities\"]\n","        })\n","    ).with_inputs(\"question_text\", \"survey_responses\"),\n","    dspy.Example(\n","        question_text=\"How to improve local transport?\",\n","        survey_responses=\"I can never find parking. The bus is always late.\",\n","        thematic_dictionary_json=json.dumps({\n","            \"Parking Availability\": [\"parking\", \"car parks\", \"finding a spot\"],\n","            \"Public Transport Reliability\": [\"bus\", \"late\", \"punctuality\"]\n","        })\n","    ).with_inputs(\"question_text\", \"survey_responses\")\n","]\n","print(\"âœ… Training set created.\")\n","\n","optimizer = dspy.BootstrapFewShot(metric=None, max_bootstrapped_demos=2)\n","compiled_generator = optimizer.compile(dspy.Predict(GenerateThematicDictionary), trainset=train_set)\n","print(\"âœ… Module compiled successfully.\")\n","\n","# --- Step 3: Execute and Save ---\n","print(f\"\\n--- Running thematic synthesis with the FINAL module ---\")\n","try:\n","    result = compiled_generator(question_text=question_text, survey_responses=sample_text)\n","    raw_output = result.thematic_dictionary_json\n","\n","    print(\"\\n>>> RAW FINAL DSPy MODULE OUTPUT <<<\"); print(\"=\"*30); print(raw_output); print(\"=\"*30)\n","\n","    # We are using your proven, robust parsing logic to handle \"stuttering\".\n","    json_str_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n","\n","    if json_str_match:\n","        json_str = json_str_match.group(0)\n","        parsed_json = json.loads(json_str)\n","\n","        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","            json.dump({QUESTION_TO_TEST: parsed_json}, f, indent=4)\n","\n","        print(f\"\\nâœ… SUCCESS: Final module generated a valid JSON and it was saved to {OUTPUT_FILE}\")\n","    else:\n","        print(f\"\\nâŒ FAILURE: Could not find any valid JSON object in the model's output.\")\n","\n","except Exception as e:\n","    print(f\"\\nCRITICAL FAILURE during final parsing and saving: {e}\")"],"metadata":{"cellView":"form","id":"KBYIrGqIn6MT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"JCqPOOeNt5Qp"}},{"cell_type":"markdown","source":["# 2nd Sept"],"metadata":{"id":"GGRy-68Nnmef"}},{"cell_type":"code","source":["# ==============================================================================\n","# @title      CELL 1: INSTALL LIBRARIES\n","# ==============================================================================\n","print(\"--- Installing all necessary libraries... ---\")\n","# Install the latest DSPy from the source\n","!pip install -q -U git+https://github.com/stanfordnlp/dspy.git\n","\n","# Install transformers and its dependencies\n","!pip install -q transformers bitsandbytes accelerate torch huggingface_hub\n","\n","print(\"\\nâœ… Installation complete.\")\n","print(\"ðŸ”´ IMPORTANT: Please restart the runtime now. Go to 'Runtime' -> 'Restart session' (or Ctrl+M).\")"],"metadata":{"id":"xOgkjj5XuiCM","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# @title      CELL 2: EXECUTE THE FULL PIPELINE\n","# ==============================================================================\n","import json\n","import os\n","import re\n","import time\n","import torch\n","from google.colab import drive\n","import dspy\n","import transformers\n","\n","# --- This is the full, final script, now with confidence that imports will work ---\n","def run_dspy_standalone_test_final():\n","    print(\"--- Step 1: Mounting Google Drive ---\")\n","    try:\n","        drive.mount('/content/drive', force_remount=True); print(\"âœ… Google Drive mounted.\")\n","    except Exception as e:\n","        print(f\"ERROR: Drive mount failed: {e}\"); return\n","\n","    print(\"\\n--- Step 2: Authenticating ---\")\n","    try:\n","        from google.colab import userdata\n","        from huggingface_hub import login\n","        HF_TOKEN = userdata.get('HF_TOKEN')\n","        if HF_TOKEN: login(token=HF_TOKEN)\n","        print(\"âœ… Hugging Face session authenticated.\")\n","    except Exception as e:\n","        print(f\"WARNING: Authentication failed: {e}\")\n","\n","    # --- Configuration ---\n","    BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","    DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Data/')\n","    P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json')\n","    QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","    QUESTION_TO_TEST = 'Q5'\n","    OUTPUT_FILE = os.path.join(DATA_DIR, 'p1_thematic_dictionary_DSPy_SUCCESS.json')\n","\n","    # --- Step 3: Load Model and Configure DSPy ---\n","    print(\"\\n--- Step 3: Loading Stable Model and Configuring DSPy ---\")\n","    try:\n","        model_id = \"google/gemma-2b-it\"\n","        model_pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n","        print(\"âœ… Transformers pipeline loaded successfully.\")\n","\n","        class LocalHuggingFaceModel(dspy.LM):\n","            def __init__(self, pipeline, model_id, **kwargs):\n","                super().__init__(model_id)\n","                self.pipeline = pipeline\n","                self.kwargs = kwargs\n","            def __call__(self, messages, only_completed=True, return_sorted=False, **kwargs):\n","                prompt = messages[-1]['content']\n","                response = self.pipeline(prompt, **self.kwargs)\n","                return [response[0]['generated_text'][len(prompt):]]\n","\n","        llm = LocalHuggingFaceModel(model_pipeline, model_id=model_id, max_new_tokens=1024)\n","        dspy.settings.configure(lm=llm)\n","        print(\"âœ… DSPy configured successfully with local model.\")\n","    except Exception as e:\n","        print(f\"CRITICAL: Failed to load model or configure DSPy. Error: {e}\"); return\n","\n","    # --- Step 4: Load Data ---\n","    print(f\"\\n--- Step 4: Loading data for {QUESTION_TO_TEST} ---\")\n","    try:\n","        with open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8') as f:\n","            question_corpus = json.load(f)\n","        with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f:\n","            qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","        docs = [item['text'] for item in question_corpus.get(QUESTION_TO_TEST, [])]\n","        sample_text = \"\\n\\n---\\n\\n\".join(docs[:20])\n","        question_text = qid_to_text.get(QUESTION_TO_TEST, \"\")\n","        print(f\"âœ… Loaded and prepared a sample of {len(docs[:20])} documents.\")\n","    except Exception as e:\n","        print(f\"CRITICAL: Failed to load data. Error: {e}\"); return\n","\n","    # --- Step 5: Generate Themes with a DSPy Signature ---\n","    print(f\"\\n--- Step 5: Running thematic synthesis with a DSPy Signature ---\")\n","    try:\n","        class GenerateThematicDictionary(dspy.Signature):\n","            \"\"\"You are an expert data analyst. Your task is to analyze survey responses and output a clean JSON object.\n","            Identify the 5 to 7 most important, recurring themes from the text.\n","            For each theme, provide a short, descriptive name and a list of 3-5 key concepts or phrases that define it.\n","            Your response MUST be a single, valid JSON object. The keys should be the theme names and the values should be the list of key concepts.\"\"\"\n","            question_text = dspy.InputField(desc=\"The survey question that was asked.\")\n","            survey_responses = dspy.InputField(desc=\"A sample of survey responses to analyze.\")\n","            thematic_dictionary_json = dspy.OutputField(desc=\"A single, clean JSON object string.\")\n","\n","        theme_generator = dspy.Predict(GenerateThematicDictionary)\n","        result = theme_generator(question_text=question_text, survey_responses=sample_text)\n","        raw_output = result.thematic_dictionary_json\n","\n","        print(\"\\n>>> RAW DSPy MODULE OUTPUT <<<\"); print(\"=\"*30); print(raw_output); print(\"=\"*30)\n","\n","        # --- Step 6: Final Check and Save ---\n","        print(\"\\n--- Step 6: Final Check and Save ---\")\n","        json_str_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n","        if json_str_match:\n","            json_str = json_str_match.group(0)\n","            parsed_json = json.loads(json_str)\n","            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","                json.dump({QUESTION_TO_TEST: parsed_json}, f, indent=4)\n","            print(f\"\\nâœ… SUCCESS: DSPy generated a valid JSON and it was saved to {OUTPUT_FILE}\")\n","        else:\n","            print(\"\\nâŒ FAILURE: DSPy generated a response, but it did not contain a valid JSON object.\")\n","    except Exception as e:\n","        print(f\"\\nCRITICAL FAILURE during generation or parsing: {e}\")\n","\n","# --- Execute the Final Test ---\n","run_dspy_standalone_test_final()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418,"referenced_widgets":["1355e01a44f24c25a8dd270ad0dd302d","91a375b095844345937fe4f364545ab3","69547a5938364762a056a8b190b107ba","06c6242a2b3843fd8403dd299e7a5fe6","447406aba33d4eaaa9e221d6a6d045c3","e44ce7d2609542e2bfc000870e913fd5","04db27acf6f04a779b72a821fccbe921","d676651d47e543159652391d386c2c02","7cec0a2fbf5b4811be6e8d40049e0dd5","84bf459b020449dc87dc46751e046dfe","10e76bb57df044b5a8b91cbd9053f54a"]},"cellView":"form","id":"qIRdeWvvujbe","executionInfo":{"status":"error","timestamp":1756827183101,"user_tz":-600,"elapsed":12669,"user":{"displayName":"Mak éº¥Yeu Wen è€€æ¦®","userId":"12147101794364230563"}},"outputId":"68e7cde0-6965-4511-a00d-80ba06933204"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1355e01a44f24c25a8dd270ad0dd302d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--- Step 1: Mounting Google Drive ---\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2554588204.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# --- Execute the Final Test ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mrun_dspy_standalone_test_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-2554588204.py\u001b[0m in \u001b[0;36mrun_dspy_standalone_test_final\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Step 1: Mounting Google Drive ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… Google Drive mounted.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ERROR: Drive mount failed: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"TGU4CAF7r_2N"}},{"cell_type":"markdown","metadata":{"id":"tplD111nwvay"},"source":["# 1st Sept - LLM powered"]},{"cell_type":"markdown","metadata":{"id":"r8fg45LryaOo"},"source":["## Test cells"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hyc3X8oDuNyJ"},"outputs":[],"source":["# @title Dummy\n","# 1. Install DSPy\n","!pip install -U -q git+https://github.com/stanfordnlp/dspy.git\n","\n","# 2. Import dspy\n","import dspy\n","\n","# 3. Set up a Dummy Language Model (LM)\n","# This LM will return \"Paris\" for the first call, \"dummy\" for subsequent calls.\n","# You can customize the list of dictionaries to simulate various responses.\n","responses = [{\"answer\": \"Paris\"}] + [{\"answer\": \"dummy\"} for _ in range(10)]\n","lm = dspy.utils.DummyLM(responses)\n","\n","# Configure DSPy to use this dummy language model\n","dspy.settings.configure(lm=lm) #\n","\n","# 4. Define a simple DSPy program using `dspy.Predict`\n","# A signature defines the input and output fields of the language model call.\n","class BasicQA(dspy.Signature):\n","    \"\"\"Answer questions with short factoid answers.\"\"\"\n","    question = dspy.InputField()\n","    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n","\n","# Create an instance of the Predict module with your signature\n","predict = dspy.Predict(BasicQA) #\n","\n","# 5. Call the program with a question\n","question_input = \"What is the capital of France?\"\n","result = predict(question=question_input)\n","\n","# 6. Print the result\n","print(f\"Question: {question_input}\")\n","print(f\"Answer: {result.answer}\")\n","\n","# Example of a second call to see the next dummy response\n","question_input_2 = \"What is 1+1?\"\n","result_2 = predict(question=question_input_2)\n","print(f\"\\nQuestion: {question_input_2}\")\n","print(f\"Answer: {result_2.answer}\")\n"]},{"cell_type":"code","source":["# ==============================================================================\n","# @title      Final Fix v2: Updating the __call__ Signature for Compatibility\n","# ==============================================================================\n","\n","# 1. Install necessary libraries\n","!pip install -q -U git+https://github.com/stanfordnlp/dspy.git\n","!pip install -q transformers bitsandbytes accelerate torch huggingface_hub\n","\n","# 2. Import necessary modules\n","import dspy\n","import torch\n","import transformers\n","from google.colab import userdata\n","from huggingface_hub import login\n","\n","print(\"\\n--- Step 1: Authenticating & Explicitly Loading LLM ---\")\n","try:\n","    HF_TOKEN = userdata.get('HF_TOKEN')\n","    if not HF_TOKEN:\n","        raise ValueError(\"HF_TOKEN not found in Colab secrets. Please add it.\")\n","    login(token=HF_TOKEN)\n","    print(\"âœ… Hugging Face login successful.\")\n","\n","    model_id = \"google/gemma-2b-it\"\n","\n","    print(f\"Loading model '{model_id}' locally...\")\n","    model_pipeline = transformers.pipeline(\n","        \"text-generation\",\n","        model=model_id,\n","        model_kwargs={\"torch_dtype\": torch.bfloat16},\n","        device_map=\"auto\",\n","    )\n","    print(\"âœ… Model loaded successfully onto device.\")\n","\n","    # --- THIS IS THE CORRECTED WRAPPER ---\n","    class LocalHuggingFaceModel(dspy.LM):\n","        def __init__(self, pipeline, model_id, **kwargs):\n","            super().__init__(model_id)\n","            self.pipeline = pipeline\n","            self.kwargs = kwargs\n","            print(f\"Initialized custom wrapper for model: {model_id}\")\n","\n","        def basic_request(self, prompt, **kwargs):\n","            # This method is likely for simpler, direct calls. It still uses 'prompt'.\n","            response = self.pipeline(prompt, **self.kwargs)\n","            return response[0]['generated_text']\n","\n","        # --- THIS IS THE CRITICAL FIX ---\n","        # The DSPy Predict module now calls the LM with a 'messages' argument.\n","        def __call__(self, messages, only_completed=True, return_sorted=False, **kwargs):\n","            # Extract the actual prompt string from the 'messages' list.\n","            # The user's prompt is the content of the last message.\n","            prompt = messages[-1]['content']\n","\n","            response = self.pipeline(prompt, **self.kwargs)\n","            # Return just the completion, not the original prompt.\n","            return [response[0]['generated_text'][len(prompt):]]\n","\n","    # Instantiate our wrapper\n","    llm = LocalHuggingFaceModel(\n","        model_pipeline,\n","        model_id=model_id,\n","        max_new_tokens=150\n","    )\n","\n","    # Configure DSPy to use this new local language model\n","    dspy.settings.configure(lm=llm)\n","    print(\"âœ… DSPy configured to use the local Gemma model.\")\n","\n","except Exception as e:\n","    print(f\"CRITICAL: Failed during model setup. Error: {e}\")\n","    raise\n","\n","# --- UNCHANGED FROM HERE ---\n","print(\"\\n--- Step 2: Defining the DSPy Program (Unchanged) ---\")\n","class BasicQA(dspy.Signature):\n","    \"\"\"Answer questions with short factoid answers.\"\"\"\n","    question = dspy.InputField()\n","    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n","\n","predict = dspy.Predict(BasicQA)\n","\n","print(\"\\n--- Step 3: Executing the Program ---\")\n","question_input = \"What is the capital of France?\"\n","result = predict(question=question_input)\n","\n","print(f\"\\nQuestion: {question_input}\")\n","print(f\"Answer: {result.answer}\")"],"metadata":{"id":"pbZGDyjS4vc-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JEMFtwWo4pa"},"outputs":[],"source":["# ==============================================================================\n","#     @title      DSPy Standalone Test (v13) - Forcing Local Execution\n","#\n","# Purpose: To bypass the remote Inference API default by using dspy.HFClient,\n","#          which explicitly loads and runs the model locally.\n","# ==============================================================================\n","\n","def run_dspy_local_client_test():\n","    print(\"--- Step 1: Installing/Updating Libraries ---\")\n","    try:\n","        from IPython.utils import io\n","        with io.capture_output() as captured:\n","            !pip install -q -U git+https://github.com/stanfordnlp/dspy.git\n","            !pip install -q transformers bitsandbytes accelerate torch huggingface_hub\n","        print(\"Libraries installed successfully.\")\n","    except Exception as e:\n","        print(f\"ERROR: Library install failed: {e}\"); return\n","\n","    import dspy\n","    import torch\n","    from google.colab import userdata\n","    from huggingface_hub import login\n","\n","    print(\"\\n--- Step 2: Authenticating & Configuring LLM for LOCAL Execution ---\")\n","    try:\n","        HF_TOKEN = userdata.get('HF_TOKEN')\n","        if HF_TOKEN:\n","            login(token=HF_TOKEN)\n","            print(\"Hugging Face login successful.\")\n","        else:\n","            raise ValueError(\"HF_TOKEN not found in Colab secrets.\")\n","\n","        # ### --- THE FINAL EXECUTION FIX (v13) --- ###\n","        # We use dspy.HFClient to explicitly load the model locally.\n","        # This bypasses the dspy.LM wrapper which was defaulting to a remote API call.\n","        # This class takes the raw model ID, not the prefixed one.\n","        llm = dspy.HFClient(\n","            model=\"google/gemma-2b-it\",\n","            # We don't need the api_key here because the huggingface_hub.login()\n","            # call has already configured the environment's authentication.\n","            model_kwargs={\n","                \"torch_dtype\": torch.bfloat16,\n","                \"device_map\": \"auto\", # Automatically use the GPU\n","            }\n","        )\n","        dspy.configure(lm=llm)\n","        print(\"--- LLM configured successfully using LOCAL dspy.HFClient ---\")\n","\n","    except Exception as e:\n","        print(f\"CRITICAL: Failed to configure LLM. Error: {e}\"); return\n","\n","    # --- Step 3: Define a DSPy Program ---\n","    print(\"\\n--- Step 3: Defining a simple Program ---\")\n","    generate_answer = dspy.Predict(\"question -> answer\")\n","    print(\"--- Program defined ---\")\n","\n","    # --- Step 4: Execute the Program ---\n","    print(\"\\n--- Step 4: Executing the program ---\")\n","    try:\n","        my_question = \"What was the key finding of the Milgram experiment?\"\n","        prediction = generate_answer(question=my_question)\n","\n","        print(\"\\n>>> DSPy Program Execution Complete <<<\")\n","        print(\"=========================================\")\n","        print(f\"Question: {my_question}\")\n","        print(f\"Predicted Answer:\\n{prediction.answer}\")\n","        print(\"=========================================\")\n","\n","        if prediction.answer:\n","            print(\"\\nSUCCESS: DSPy successfully generated a response.\")\n","        else:\n","            print(\"\\nWARNING: DSPy ran but did not produce an answer.\")\n","\n","    except Exception as e:\n","        print(f\"\\nCRITICAL FAILURE during DSPy program execution: {e}\")\n","\n","# --- Execute the Local Client DSPy Test ---\n","run_dspy_local_client_test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBQZkH9XHJW8"},"outputs":[],"source":["# @title Cell 1: Installation\n","# Install the very latest version directly from the official GitHub repository.\n","# Then, restart the runtime.\n","!pip install -U -q git+https://github.com/stanfordnlp/dspy.git\n","!pip install -q transformers bitsandbytes accelerate torch\n","print(\"âœ… Installation complete. Please restart the runtime now.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"axvVtOpgQt3Y"},"outputs":[],"source":["# @title Cell 1: Install libraries and log in to Hugging Face\n","# !pip install -U -q git+https://github.com/stanfordnlp/dspy.git\n","# !pip install -q transformers bitsandbytes accelerate torch huggingface_hub\n","\n","from google.colab import userdata\n","\n","HF_TOKEN = userdata.get('HF_TOKEN')\n","if not HF_TOKEN:\n","    raise ValueError(\"HF_TOKEN not found in Colab secrets. Please add it.\")\n","\n","# Use the command-line tool to perform a persistent login.\n","# This is the most reliable method.\n","!huggingface-cli login --token $HF_TOKEN --add-to-git-credential\n","\n","print(\"âœ… Environment setup complete. Please restart the runtime now.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8w7QfbYaRkWS"},"outputs":[],"source":["# @title Cell 2: Run DSPy with Direct Token Injection\n","import dspy\n","import torch\n","from google.colab import userdata\n","\n","print(\"\\n--- Step 3: Running DSPy Program ---\")\n","HF_TOKEN = userdata.get('HF_TOKEN') # Retrieve the token again\n","\n","try:\n","    print(\"Configuring DSPy...\")\n","\n","    # ### --- THE DIRECT INJECTION FIX --- ###\n","    # We are now passing the HF_TOKEN directly to the dspy.LM constructor\n","    # using the `api_key` parameter. This forces LiteLLM to use this\n","    # specific token for the Hugging Face API call, bypassing any\n","    # environment-based credential lookup issues.\n","    llm = dspy.LM(\n","        \"huggingface/google/gemma-3-270m-it\",\n","        api_key=HF_TOKEN, # Direct Token Injection\n","        model_kwargs={\n","            \"torch_dtype\": \"auto\",\n","            \"load_in_4bit\": True,\n","        }\n","    )\n","    dspy.configure(lm=llm)\n","    print(\"LLM configured successfully.\")\n","\n","    print(\"\\nDefining and executing program...\")\n","    predict = dspy.Predict(\"question -> answer\")\n","    my_question = \"What was the key finding of the Milgram experiment?\"\n","    prediction = predict(question=my_question)\n","\n","    print(\"\\n>>> DSPy Program Execution Complete <<<\")\n","    print(\"=========================================\")\n","    print(f\"Question: {my_question}\")\n","    print(f\"Predicted Answer:\\n{prediction.answer}\")\n","    print(\"=========================================\")\n","\n","    if prediction.answer:\n","        print(\"\\nSUCCESS: DSPy successfully generated a response.\")\n","    else:\n","        print(\"\\nWARNING: DSPy ran but did not produce an answer.\")\n","\n","except Exception as e:\n","    print(f\"\\nCRITICAL FAILURE during DSPy program execution: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15apbPK2RqK6"},"outputs":[],"source":["# @title Cell 2: Run this AFTER restarting the runtime\n","\n","import litellm\n","import torch\n","\n","print(\"--- Step 1: Configuring LiteLLM ---\")\n","# Set a higher timeout as downloading the model can be slow\n","litellm.set_verbose = True\n","litellm.timeout = 300 # 5 minutes\n","\n","model_name = \"huggingface/google/gemma-3-270m-it\"\n","messages = [{\"role\": \"user\", \"content\": \"What was the key finding of the Milgram experiment?\"}]\n","\n","print(f\"\\n--- Step 2: Calling litellm.completion for model: {model_name} ---\")\n","\n","try:\n","    # This is the direct call to the underlying library dspy uses.\n","    # We are not passing any API keys or tokens here, because the `transformers`\n","    # library (which litellm uses for Hugging Face) should find the credential\n","    # that was saved to disk by the CLI login in Cell 1.\n","    response = litellm.completion(\n","        model=model_name,\n","        messages=messages,\n","        max_tokens=250,\n","        temperature=0.1,\n","        # The model_kwargs are passed to the underlying Hugging Face pipeline\n","        model_kwargs={\n","            \"torch_dtype\": \"auto\",\n","            \"load_in_4bit\": True,\n","        }\n","    )\n","\n","    print(\"\\n>>> LiteLLM Execution Complete <<<\")\n","    print(\"====================================\")\n","    print(response)\n","    print(\"====================================\")\n","\n","    answer = response.choices[0].message.content\n","    if answer:\n","        print(\"\\nSUCCESS: LiteLLM successfully generated a response.\")\n","        print(f\"Answer: {answer}\")\n","    else:\n","        print(\"\\nWARNING: LiteLLM ran but did not produce an answer.\")\n","\n","except Exception as e:\n","    print(f\"\\nCRITICAL FAILURE during LiteLLM execution: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvoufcySQ54_"},"outputs":[],"source":["# @title Cell 2: Run this AFTER restarting the runtime\n","\n","import dspy\n","import torch\n","\n","print(\"--- Step 1: Configuring DSPy ---\")\n","try:\n","    # We no longer need any login code here. The 'transformers' library\n","    # will automatically find the token saved by the CLI login.\n","\n","    llm = dspy.LM(\n","        \"huggingface/google/gemma-3-270m-it\",\n","        model_kwargs={\n","            \"torch_dtype\": \"auto\",\n","            \"load_in_4bit\": True,\n","        }\n","    )\n","    dspy.configure(lm=llm)\n","    print(\"--- LLM configured successfully ---\")\n","\n","except Exception as e:\n","    print(f\"CRITICAL: Failed to configure LLM. Error: {e}\")\n","    raise e\n","\n","# --- Step 2: Define and Execute a simple Program ---\n","print(\"\\n--- Step 2: Defining and Executing a simple Program ---\")\n","try:\n","    predict = dspy.Predict(\"question -> answer\")\n","    my_question = \"What was the key finding of the Milgram experiment?\"\n","    prediction = predict(question=my_question)\n","\n","    print(\"\\n>>> DSPy Program Execution Complete <<<\")\n","    print(\"=========================================\")\n","    print(f\"Question: {my_question}\")\n","    print(f\"Predicted Answer:\\n{prediction.answer}\")\n","    print(\"=========================================\")\n","\n","    if prediction.answer:\n","        print(\"\\nSUCCESS: DSPy successfully generated a response.\")\n","    else:\n","        print(\"\\nWARNING: DSPy ran but did not produce an answer.\")\n","\n","except Exception as e:\n","    print(f\"\\nCRITICAL FAILURE during DSPy program execution: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"eE2_8NGykIEg"},"source":["## summary of what we were trying to achieve with our final standalone test (v9.16).\n","\n","---\n","\n","### **Baseline Summary: Hybrid LLM + K*-Means Thematic Analysis Pipeline**\n","\n","**Objective:**\n","The goal was to develop a robust, automated pipeline to perform thematic analysis on survey responses. The desired output was a consolidated list of core themes for each question, quantified by prevalence.\n","\n","**Final Proposed Architecture (v9.16):**\n","After determining that purely unsupervised, bottom-up clustering methods were insufficient, we designed a more advanced, two-stage hybrid architecture:\n","\n","1.  **Stage 1: Granular Theme Generation (LLM-based):**\n","    *   **Methodology:** The pipeline would iterate through each survey response for a given question individually. A powerful Large Language Model (`google/gemma-2b-it`) would be used to analyze the text of each response and generate a small list of 1-3 \"granular themes\" that were present in that specific document.\n","    *   **Goal:** This would transform the unstructured text into a large, semi-structured list of thematic \"tags\" representing the initial, fine-grained points made by all respondents.\n","\n","2.  **Stage 2: Core Theme Consolidation (K*-Means Clustering):**\n","    *   **Methodology:** The large set of granular themes generated in Stage 1 would then be consolidated. First, each unique theme name would be converted into a rich, semantic vector using a Sentence-Transformer model (`all-MiniLM-L6-v2`). Then, the principled, parameter-free `KStarMeans` (k*-means) algorithm would be used to cluster these semantic vectors.\n","    *   **Goal:** `KStarMeans` would automatically determine the optimal number of \"core themes\" (`k*`) by grouping semantically similar granular themes together, thus producing the final, high-level summary for the question.\n","\n","**Point of Failure & Key Lesson Learned:**\n","Although the individual components (the LLM's analytical capability, the K*-Means algorithm) were sound, the pipeline failed at the **interface between them.**\n","\n","*   **The Failure:** The LLM, while intelligent, proved to be an unreliable data formatting tool. When tasked with generating the granular themes in Stage 1, its output was inconsistent and frequently malformed (e.g., producing invalid JSON, non-list structures, or conversational text).\n","*   **The Consequence:** The custom-written parsing function in the pipeline was too brittle to handle this messy, unpredictable output. It consistently failed to extract the themes, resulting in an empty input for the K*-Means algorithm, which then correctly reported that there were no themes to cluster.\n","*   **The Lesson:** This established a clear baseline: **A robust, production-grade pipeline cannot be built on fragile, ad-hoc parsing of creative LLM outputs.** A more principled and structured method for ensuring reliable, structured data from the LLM is required. This conclusion leads directly to the decision to explore frameworks like DSPy, which are specifically designed to solve this problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ZnThRxBVYr1"},"outputs":[],"source":["# ==============================================================================\n","# @title          FINAL STANDALONE TEST: Hybrid LLM + K*-Means\n","#           VERSION 9.16 -- Robust Parsing & Simplified Prompt\n","#\n","# Purpose: This self-contained script validates our hybrid architecture.\n","#\n","# Change Log (v9.16):\n","# - Replaced brittle regex parsing with a robust function to handle messy LLM output.\n","# - Simplified the LLM prompt to ask for a comma-separated list, which is\n","#   easier for the model to generate correctly than formal JSON.\n","# ==============================================================================\n","\n","import json\n","import os\n","import re\n","import time\n","import torch\n","import numpy as np\n","import itertools\n","from collections import defaultdict\n","from google.colab import drive\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import euclidean_distances\n","\n","def run_final_hybrid_standalone_test_v9_16():\n","    print(\"--- Step 1: Mounting, Installing, Authenticating ---\")\n","    try:\n","        drive.mount('/content/drive', force_remount=True)\n","        from IPython.utils import io\n","        with io.capture_output() as captured:\n","            !pip install -q transformers bitsandbytes accelerate sentence-transformers\n","        from google.colab import userdata\n","        from huggingface_hub import login\n","        HF_TOKEN = userdata.get('HF_TOKEN')\n","        if HF_TOKEN: login(token=HF_TOKEN)\n","        print(\"Setup complete.\")\n","    except Exception as e:\n","        print(f\"ERROR during setup: {e}\"); return\n","\n","    # --- Configuration ---\n","    BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","    DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Data/')\n","    P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json')\n","    QUESTION_TO_TEST = 'Q5'\n","    BATCH_SIZE = 8\n","\n","    # --- Load Models ---\n","    print(\"\\n--- Step 2: Loading All Models ---\")\n","    try:\n","        llm_model_id = \"google/gemma-2b-it\"\n","        tokenizer = AutoTokenizer.from_pretrained(llm_model_id)\n","        llm_model = AutoModelForCausalLM.from_pretrained(\n","            llm_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\",\n","        )\n","        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n","        print(\"--- All models loaded successfully ---\")\n","    except Exception as e:\n","        print(f\"CRITICAL: Failed to load models. Error: {e}\"); return\n","\n","    # --- Load Data ---\n","    print(f\"\\n--- Step 3: Loading a batch of real documents for {QUESTION_TO_TEST} ---\")\n","    try:\n","        with open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8') as f:\n","            question_corpus = json.load(f)\n","        docs = question_corpus.get(QUESTION_TO_TEST, [])\n","        doc_batch = docs[:BATCH_SIZE]\n","        print(f\"Loaded a batch of {len(doc_batch)} real documents.\")\n","    except Exception as e:\n","        print(f\"CRITICAL: Failed to load data. Error: {e}\"); return\n","\n","    # ### --- CRITICAL FIX (v9.16) --- ###\n","    # New robust parsing function\n","    def parse_themes_from_llm_output(llm_text):\n","        # Find content within quotes\n","        themes = re.findall(r'\"([^\"]+)\"', llm_text)\n","        # Clean up any extra characters or empty strings\n","        cleaned_themes = [t.strip() for t in themes if t.strip()]\n","        return cleaned_themes\n","\n","    # --- Phase 1: Generate Granular Themes ---\n","    print(f\"\\n--- Step 4: Generating granular themes for each document ---\")\n","    try:\n","        prompts = []\n","        for doc in doc_batch:\n","            doc_text_snippet = doc.get('text', '')[:3000]\n","            # ### --- CRITICAL FIX (v9.16) --- ###\n","            # Simplified prompt\n","            prompt_content = f\"\"\"You are an expert analyst. Read the survey response and identify the 1 to 3 most important themes.\n","Respond with a simple comma-separated list of short theme names. For example: \"Theme A\", \"Theme B\", \"Theme C\".\n","Do not use JSON or any other formatting.\n","\n","### Survey Response:\n","{doc_text_snippet}\n","\n","### Comma-Separated List of Theme Names:\"\"\"\n","            prompts.append(f\"<start_of_turn>user\\n{prompt_content}<end_of_turn>\\n<start_of_turn>model\\n\")\n","\n","        tokenizer.pad_token = tokenizer.eos_token\n","        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).to(\"cuda\")\n","        outputs = llm_model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)\n","        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","        all_granular_themes = []\n","        print(\"\\n>>> GRANULAR THEME OUTPUT <<<\")\n","        print(\"==============================\")\n","        for i, full_output in enumerate(decoded_outputs):\n","            model_response_start = full_output.find('<start_of_turn>model\\n') + len('<start_of_turn>model\\n')\n","            llm_output_str = full_output[model_response_start:].strip()\n","            print(f\"--- Doc {i+1} Raw Output ---\\n{llm_output_str}\\n\")\n","            # Use the new robust parsing function\n","            themes = parse_themes_from_llm_output(llm_output_str)\n","            print(f\"--- Doc {i+1} Parsed Themes --- \\n{themes}\\n\")\n","            all_granular_themes.extend(themes)\n","        print(\"==============================\")\n","\n","        unique_granular_themes = sorted(list(set(all_granular_themes)))\n","        print(f\"\\nSUCCESS: Generated a total of {len(unique_granular_themes)} unique granular themes from the batch.\")\n","        print(\"Generated Themes:\", unique_granular_themes)\n","\n","    except Exception as e:\n","        print(f\"\\nCRITICAL FAILURE during granular theme generation: {e}\"); return\n","\n","    # --- Phase 2: K*-Means Consolidation ---\n","    print(f\"\\n--- Step 5: Consolidating themes with K*-Means ---\")\n","    if len(unique_granular_themes) < 2:\n","        print(\"Not enough unique themes to cluster. Test complete.\")\n","        return\n","\n","    try:\n","        class KStarMeans:\n","            # ... (Full KStarMeans class code is unchanged and goes here)\n","            def __init__(self, patience=10, max_iter=300, n_init_split=5):\n","                self.patience = patience; self.max_iter = max_iter; self.n_init_split = n_init_split\n","                self.centroids_, self.labels_ = None, None; self.m_precision = 64\n","            def _mdl_cost(self, X, centroids, assignments):\n","                N, d = X.shape; k = len(centroids)\n","                if k == 0 or (N > 0 and len(np.unique(assignments)) != k): return np.inf\n","                modelcost = k * d * self.m_precision\n","                idxcost = N * np.log(k) if k > 1 else 0.0\n","                sse = np.sum((X - centroids[assignments])**2) if N > 0 else 0.0\n","                residualcost = (N * d * np.log(2 * np.pi) + sse) / 2.0\n","                return modelcost + idxcost + residualcost\n","            def _init_subcentroids(self, X_cluster):\n","                if len(X_cluster) < 2: return np.array([X_cluster[0], X_cluster[0]]) if len(X_cluster) == 1 else np.empty((2, X_cluster.shape[1]))\n","                sub_centroids = np.empty((2, X_cluster.shape[1]))\n","                idx1 = np.random.choice(len(X_cluster)); sub_centroids[0] = X_cluster[idx1]\n","                dist_sq = euclidean_distances(X_cluster, sub_centroids[0:1]).flatten()**2\n","                idx2 = np.random.choice(len(X_cluster), p=dist_sq/np.sum(dist_sq)) if np.sum(dist_sq) > 0 else (idx1 + 1) % len(X_cluster)\n","                sub_centroids[1] = X_cluster[idx2]; return sub_centroids\n","            def _kmeans_step(self, X, centroids):\n","                if len(centroids) == 0: return centroids, np.zeros(X.shape[0], dtype=int)\n","                assignments = np.argmin(euclidean_distances(X, centroids), axis=1)\n","                unique_labels = np.unique(assignments)\n","                new_centroids = np.array([X[assignments == i].mean(axis=0) for i in unique_labels])\n","                if len(unique_labels) < len(centroids):\n","                    map_labels = {old: new for new, old in enumerate(unique_labels)}\n","                    assignments = np.array([map_labels[i] for i in assignments])\n","                return new_centroids, assignments\n","            def _find_best_split(self, X, centroids, assignments):\n","                N, d = X.shape; k = len(centroids); best_delta_mdl, best_split_candidate = 0, None\n","                for i in range(k):\n","                    cluster_points = X[assignments == i]\n","                    if len(cluster_points) < 2: continue\n","                    best_sse_reduction, best_sub_centroids = -1, None\n","                    for _ in range(self.n_init_split):\n","                        sub_centroids_init = self._init_subcentroids(cluster_points)\n","                        sse_before = np.sum((cluster_points - centroids[i])**2)\n","                        sub_assign = np.argmin(euclidean_distances(cluster_points, sub_centroids_init), axis=1)\n","                        sse_after = sum(np.sum((cluster_points[sub_assign == l] - sub_centroids_init[l])**2) for l in range(2) if np.any(sub_assign == l))\n","                        if (sse_before - sse_after) > best_sse_reduction:\n","                            best_sse_reduction = sse_before - sse_after; best_sub_centroids = sub_centroids_init\n","                    delta_mdl = (d * self.m_precision) + (N * np.log((k + 1)/k)) - (best_sse_reduction / 2.0)\n","                    if delta_mdl < best_delta_mdl:\n","                        best_delta_mdl = delta_mdl; temp_centroids = np.delete(centroids, i, axis=0)\n","                        best_split_candidate = np.vstack([temp_centroids, best_sub_centroids])\n","                return best_split_candidate\n","            def _find_best_merge(self, X, centroids, assignments):\n","                N, d = X.shape; k = len(centroids); best_delta_mdl, best_merge_candidate = 0, None\n","                if k < 2: return None\n","                for idx1, idx2 in itertools.combinations(range(k), 2):\n","                    p1 = X[assignments == idx1]; p2 = X[assignments == idx2]\n","                    if len(p1) == 0 or len(p2) == 0: continue\n","                    sse_before = np.sum((p1 - centroids[idx1])**2) + np.sum((p2 - centroids[idx2])**2)\n","                    merged_points = np.vstack([p1, p2]); merged_centroid = np.mean(merged_points, axis=0)\n","                    sse_after = np.sum((merged_points - merged_centroid)**2)\n","                    delta_mdl = (-d * self.m_precision) + (N * np.log((k - 1)/k)) + ((sse_after - sse_before) / 2.0)\n","                    if delta_mdl < best_delta_mdl:\n","                        best_delta_mdl = delta_mdl; temp_centroids = np.delete(centroids, sorted([idx1, idx2]), axis=0)\n","                        best_merge_candidate = np.vstack([temp_centroids, [merged_centroid]])\n","                return best_merge_candidate\n","            def fit(self, X):\n","                if X.shape[0] == 0: self.centroids_, self.labels_ = np.array([]), np.array([]); return self\n","                centroids, assignments = self._kmeans_step(X, np.array([X.mean(axis=0)]))\n","                best_cost = self._mdl_cost(X, centroids, assignments)\n","                best_centroids, best_assignments = np.copy(centroids), np.copy(assignments)\n","                unimproved_count = 0\n","                for i in range(self.max_iter):\n","                    current_centroids, current_assignments = np.copy(best_centroids), np.copy(best_assignments)\n","                    split_candidate = self._find_best_split(X, current_centroids, current_assignments)\n","                    merge_candidate = self._find_best_merge(X, current_centroids, current_assignments)\n","                    split_cost = self._mdl_cost(X, *self._kmeans_step(X, split_candidate)) if split_candidate is not None else np.inf\n","                    merge_cost = self._mdl_cost(X, *self._kmeans_step(X, merge_candidate)) if merge_candidate is not None else np.inf\n","                    kmeans_cost = self._mdl_cost(X, *self._kmeans_step(X, current_centroids))\n","                    costs = [split_cost, merge_cost, kmeans_cost]; candidates = [split_candidate, merge_candidate, current_centroids]\n","                    best_action_idx = np.argmin(costs)\n","                    if candidates[best_action_idx] is None:\n","                        unimproved_count += 1\n","                        if unimproved_count >= self.patience: break\n","                        continue\n","                    next_centroids, next_assignments = self._kmeans_step(X, candidates[best_action_idx])\n","                    new_cost = self._mdl_cost(X, next_centroids, next_assignments)\n","                    if new_cost < best_cost:\n","                        best_cost, best_centroids, best_assignments = new_cost, np.copy(next_centroids), np.copy(next_assignments)\n","                        unimproved_count = 0\n","                    else:\n","                        unimproved_count += 1\n","                        if unimproved_count >= self.patience: break\n","                self.centroids_, self.labels_ = best_centroids, best_assignments; return self\n","\n","        theme_vectors = embedding_model.encode(unique_granular_themes)\n","        k_star_model = KStarMeans().fit(theme_vectors)\n","        final_assignments = k_star_model.labels_\n","        optimal_k = len(k_star_model.centroids_)\n","\n","        print(f\"\\n>>> K*-MEANS RESULT <<<\")\n","        print(f\"==============================\")\n","        print(f\"K*-Means consolidated {len(unique_granular_themes)} granular themes into {optimal_k} core themes.\")\n","\n","        final_clusters = defaultdict(list)\n","        for theme_idx, cluster_idx in enumerate(final_assignments):\n","            final_clusters[cluster_idx].append(unique_granular_themes[theme_idx])\n","\n","        for i, (cluster_id, themes) in enumerate(final_clusters.items()):\n","            print(f\"\\n--- Core Theme {i+1} ---\")\n","            for theme in themes:\n","                print(f\"  - {theme}\")\n","        print(\"==============================\")\n","        print(\"\\nSUCCESS: Hybrid analysis complete.\")\n","\n","    except Exception as e:\n","        print(f\"\\nCRITICAL FAILURE during K*-Means consolidation: {e}\")\n","\n","# --- Execute the Final Standalone Test ---\n","run_final_hybrid_standalone_test_v9_16()"]},{"cell_type":"markdown","metadata":{"id":"51Kfj_PdxENt"},"source":["## Cell 2: Configuration and Phase 1 (Theme Generation)\n","(This cell is now configured to run on our new, faster pilot questions: Q28 and Q3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FF77A1-XxDLJ"},"outputs":[],"source":["# ==============================================================================\n","#              PIPELINE V10 - CELL 2: CONFIG & PHASE 1 (FINAL PILOT)\n","# ==============================================================================\n","import time\n","import json\n","import re\n","\n","# --- Configuration ---\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Data/')\n","REPORTS_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Reports/')\n","os.makedirs(REPORTS_DIR, exist_ok=True)\n","P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json')\n","P1_THEMATIC_DICTIONARY_FILE = os.path.join(DATA_DIR, 'p1_thematic_dictionary_v10_final.json')\n","P2_CLASSIFICATION_OUTPUT_FILE = os.path.join(DATA_DIR, 'p2_classification_output_v10_final.json')\n","P3_FINAL_REPORT_FILE = os.path.join(REPORTS_DIR, f'Stakeholder_Report_v10_final_{time.strftime(\"%Y%m%d\")}.txt')\n","QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","\n","# ### --- FINAL PILOT CONFIGURATION --- ###\n","PILOT_QUESTIONS = ['Q28'] # , 'Q3']\n","\n","print(f\"Loading corpus...\"); full_corpus = json.load(open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8')); print(\"Corpus loaded.\")\n","with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f: qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","\n","def generate_llm_response(prompt_content):\n","    prompt = f\"<start_of_turn>user\\n{prompt_content}<end_of_turn>\\n<start_of_turn>model\\n\"\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=8190).to(\"cuda\")\n","    outputs = model.generate(**inputs, max_new_tokens=1024, pad_token_id=tokenizer.eos_token_id)\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    model_response_start = response.find('<start_of_turn>model\\n') + len('<start_of_turn>model\\n')\n","    return response[model_response_start:].strip()\n","\n","# ==============================================================================\n","#                                  RUN PHASE 1\n","# ==============================================================================\n","thematic_dictionary = {}\n","if os.path.exists(P1_THEMATIC_DICTIONARY_FILE):\n","    print(f\"Loading existing thematic dictionary...\"); thematic_dictionary = json.load(open(P1_THEMATIC_DICTIONARY_FILE, 'r'))\n","else:\n","    print(\"\\n--- Starting Phase 1: Candidate Theme Generation ---\")\n","    for qid in PILOT_QUESTIONS:\n","        print(f\"\\nGenerating themes for QID: {qid}...\")\n","        docs = [item['text'] for item in full_corpus.get(qid, [])]\n","        if not docs: print(f\"  - No documents for {qid}.\"); continue\n","        sample_text = \"\\n\\n---\\n\\n\".join(docs[:20])\n","        question_text = qid_to_text.get(qid, \"\")\n","        prompt = f\"\"\"You are an expert data analyst. Your task is to analyze survey responses and output a clean JSON object.\n","Read the following survey responses for the question: \"{question_text}\"\n","Identify the 5 to 7 most important, recurring themes from the text.\n","For each theme, provide a short, descriptive name and a list of 3-5 key concepts or phrases that define it.\n","Your response MUST be a single, valid JSON object. The keys should be the theme names and the values should be the list of key concepts. Do not include any extra text, explanations, or markdown formatting.\n","\n","### Survey Responses:\n","{sample_text}\n","\n","### JSON Output:\"\"\"\n","        llm_output_str = generate_llm_response(prompt)\n","        try:\n","            json_str_match = re.search(r'\\{.*\\}', llm_output_str, re.DOTALL)\n","            parsed_json = json.loads(json_str_match.group(0)) if json_str_match else {}\n","            thematic_dictionary[qid] = parsed_json\n","            print(f\"  - Successfully generated themes for {qid}.\")\n","        except Exception as e:\n","            print(f\"  - ERROR parsing themes for {qid}: {e}\\n  - Raw response: {llm_output_str}\")\n","            thematic_dictionary[qid] = {}\n","    with open(P1_THEMATIC_DICTIONARY_FILE, 'w') as f: json.dump(thematic_dictionary, f, indent=2)\n","    print(f\"\\n--- Phase 1 Complete ---\")\n","\n","print(\"\\nGenerated/Loaded Thematic Dictionary:\")\n","print(json.dumps(thematic_dictionary, indent=2))"]},{"cell_type":"markdown","metadata":{"id":"hfY4JNvDxQMW"},"source":[]},{"cell_type":"markdown","metadata":{"id":"NXHrBuQ22X50"},"source":["# 31st Aug - LLM powered"]},{"cell_type":"markdown","metadata":{"id":"Ms39--jb0_kl"},"source":["## Test Cells"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nEY3pbYo7mf"},"outputs":[],"source":["# ==============================================================================\n","# @title          FINAL STANDALONE TEST: Complex Prompt on Stable Base\n","#           VERSION 9.7 -- The Definitive Test\n","#\n","# Purpose: This self-contained script validates the core of our new LLM-based\n","#          methodology by executing one complete theme generation task.\n","# ==============================================================================\n","\n","import json\n","import os\n","import re\n","import time\n","import torch\n","from google.colab import drive\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","def run_final_standalone_test_v9_7():\n","    print(\"--- Step 1: Mounting Google Drive ---\")\n","    try:\n","        drive.mount('/content/drive', force_remount=True); print(\"Google Drive mounted.\")\n","    except Exception as e:\n","        print(f\"ERROR: Drive mount failed: {e}\"); return\n","\n","    print(\"\\n--- Step 2: Installing Libraries ---\")\n","    try:\n","        from IPython.utils import io\n","        with io.capture_output() as captured:\n","            !pip install -q transformers bitsandbytes accelerate\n","        print(\"Libraries installed successfully.\")\n","    except Exception as e:\n","        print(f\"ERROR: Library install failed: {e}\"); return\n","\n","    print(\"\\n--- Step 3: Authenticating ---\")\n","    try:\n","        from google.colab import userdata\n","        from huggingface_hub import login\n","        HF_TOKEN = userdata.get('HF_TOKEN')\n","        if HF_TOKEN: login(token=HF_TOKEN)\n","        print(\"Hugging Face session authenticated.\")\n","    except (ImportError, KeyError):\n","        print(\"WARNING: Could not access Colab Secrets.\")\n","\n","    # --- Configuration ---\n","    BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","    DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Data/')\n","    P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json')\n","    QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","    QUESTION_TO_TEST = 'Q5'\n","    OUTPUT_FILE = os.path.join(DATA_DIR, 'p1_thematic_dictionary_STANDALONE_SUCCESS.json')\n","\n","    # --- Load Model (Using the proven stable model) ---\n","    model, tokenizer = None, None\n","    print(\"\\n--- Step 4: Loading Stable Model (google/gemma-2b-it) ---\")\n","    try:\n","        model_id = \"google/gemma-2b-it\"\n","        tokenizer = AutoTokenizer.from_pretrained(model_id)\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_id,\n","            torch_dtype=torch.bfloat16,\n","            device_map=\"auto\",\n","        )\n","        print(\"--- Model loaded successfully ---\")\n","    except Exception as e:\n","        print(f\"CRITICAL: Failed to load model. Error: {e}\"); return\n","\n","    # --- Load Data ---\n","    print(f\"\\n--- Step 5: Loading data for {QUESTION_TO_TEST} ---\")\n","    try:\n","        with open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8') as f:\n","            question_corpus = json.load(f)\n","        with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f:\n","            qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","        docs = question_corpus.get(QUESTION_TO_TEST, [])\n","\n","        sample_docs = docs[:20]\n","        sample_text = \"\\n\\n---\\n\\n\".join([doc['text'] for doc in sample_docs])\n","        question_text = qid_to_text.get(QUESTION_TO_TEST, \"\")\n","        print(f\"Loaded and prepared a sample of {len(sample_docs)} documents.\")\n","    except Exception as e:\n","        print(f\"CRITICAL: Failed to load data. Error: {e}\"); return\n","\n","    # --- Generate Themes with Complex Prompt ---\n","    print(f\"\\n--- Step 6: Running thematic synthesis with the full, complex prompt ---\")\n","    try:\n","        prompt_content = f\"\"\"You are an expert data analyst. Your task is to analyze survey responses and output a clean JSON object.\n","Read the following survey responses for the question: \"{question_text}\"\n","Identify the 5 to 7 most important, recurring themes from the text.\n","For each theme, provide a short, descriptive name and a list of 3-5 key concepts or phrases that define it.\n","Your response MUST be a single, valid JSON object. The keys should be the theme names and the values should be the list of key concepts. Do not include any extra text, explanations, or markdown formatting before or after the JSON object.\n","\n","### Survey Responses:\n","{sample_text}\n","\n","### JSON Output:\"\"\"\n","\n","        final_prompt = f\"<start_of_turn>user\\n{prompt_content}<end_of_turn>\\n<start_of_turn>model\\n\"\n","\n","        inputs = tokenizer(final_prompt, return_tensors=\"pt\", truncation=True, max_length=8190)\n","        input_ids = inputs.input_ids.to(\"cuda\")\n","        attention_mask = inputs.attention_mask.to(\"cuda\")\n","\n","        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=1024, pad_token_id=tokenizer.eos_token_id)\n","\n","        raw_output = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n","\n","        print(\"\\n>>> RAW LLM OUTPUT <<<\")\n","        print(\"==============================\")\n","        print(raw_output)\n","        print(\"==============================\")\n","\n","        print(\"\\n--- Step 7: Final Check and Save ---\")\n","        json_start = raw_output.find('{')\n","        json_end = raw_output.rfind('}') + 1\n","        if json_start != -1 and json_end != -1:\n","            json_str = raw_output[json_start:json_end]\n","            parsed_json = json.loads(json_str)\n","            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","                json.dump({QUESTION_TO_TEST: parsed_json}, f, indent=4)\n","            print(f\"\\nSUCCESS: Model generated a valid JSON and it was saved to {OUTPUT_FILE}\")\n","        else:\n","            print(\"\\nFAILURE: Model generated a response, but it did not contain a valid JSON object.\")\n","\n","    except Exception as e:\n","        print(f\"\\nCRITICAL FAILURE during generation or parsing: {e}\")\n","\n","# --- Execute the Final Standalone Test ---\n","run_final_standalone_test_v9_7()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFqhHMIS1pa6"},"outputs":[],"source":["# ==============================================================================\n","# @title          FINAL MINIMAL TEST: COMPLEX PROMPT ON STABLE BASE\n","#           VERSION 9.7 -- Syntax Error FIX\n","#\n","# Purpose: To combine our proven, stable model loading script with our\n","#          full, complex theme-generation prompt. This isolates the prompt\n","#          itself as the final variable to test.\n","# Change Log (v9.7):\n","# - Removed a misplaced comment that was causing a SyntaxError.\n","# ==============================================================================\n","\n","import json\n","import os\n","import re\n","import time\n","import torch\n","from google.colab import drive\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","def run_final_standalone_test_fixed():\n","    print(\"--- Step 1: Mounting Google Drive ---\")\n","    try:\n","        drive.mount('/content/drive', force_remount=True); print(\"Google Drive mounted.\")\n","    except Exception as e:\n","        print(f\"ERROR: Drive mount failed: {e}\"); return\n","\n","    print(\"\\n--- Step 2: Installing Libraries ---\")\n","    try:\n","        from IPython.utils import io\n","        with io.capture_output() as captured:\n","            !pip install -q transformers bitsandbytes accelerate\n","        print(\"Libraries installed successfully.\")\n","    except Exception as e:\n","        print(f\"ERROR: Library install failed: {e}\"); return\n","\n","    print(\"\\n--- Step 3: Authenticating ---\")\n","    try:\n","        from google.colab import userdata\n","        from huggingface_hub import login\n","        HF_TOKEN = userdata.get('HF_TOKEN')\n","        if HF_TOKEN: login(token=HF_TOKEN)\n","        print(\"Hugging Face session authenticated.\")\n","    except (ImportError, KeyError):\n","        print(\"WARNING: Could not access Colab Secrets.\")\n","\n","    # --- Configuration ---\n","    BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","    DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Data/')\n","    P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json')\n","    QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","    QUESTION_TO_TEST = 'Q5'\n","\n","    # --- Load Model (Using the proven stable model) ---\n","    model, tokenizer = None, None\n","    print(\"\\n--- Step 4: Loading Stable Model (google/gemma-2b-it) ---\")\n","    try:\n","        model_id = \"google/gemma-2b-it\"\n","        tokenizer = AutoTokenizer.from_pretrained(model_id)\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_id,\n","            torch_dtype=torch.bfloat16,\n","            device_map=\"auto\",\n","        )\n","        print(\"--- Model loaded successfully ---\")\n","    except Exception as e:\n","        print(f\"CRITICAL: Failed to load model. Error: {e}\"); return\n","\n","    # --- Load Data ---\n","    print(f\"\\n--- Step 5: Loading data for {QUESTION_TO_TEST} ---\")\n","    try:\n","        with open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8') as f:\n","            question_corpus = json.load(f)\n","        with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f:\n","            qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","        docs = question_corpus.get(QUESTION_TO_TEST, [])\n","\n","        sample_docs = docs[:20]\n","        sample_text = \"\\n\\n---\\n\\n\".join([doc['text'] for doc in sample_docs])\n","        question_text = qid_to_text.get(QUESTION_TO_TEST, \"\")\n","        print(f\"Loaded and prepared a sample of {len(sample_docs)} documents.\")\n","    except Exception as e:\n","        print(f\"CRITICAL: Failed to load data. Error: {e}\"); return\n","\n","    # --- Generate Themes with Complex Prompt ---\n","    print(f\"\\n--- Step 6: Running thematic synthesis with the full, complex prompt ---\")\n","    try:\n","        prompt_content = f\"\"\"You are an expert data analyst. Your task is to analyze survey responses and output a clean JSON object.\n","\n","Read the following survey responses for the question: \"{question_text}\"\n","\n","Identify the 5 to 7 most important, recurring themes from the text.\n","For each theme, provide a short, descriptive name and a list of 3-5 key concepts or phrases that define it.\n","\n","Your response MUST be a single, valid JSON object. The keys should be the theme names and the values should be the list of key concepts. Do not include any extra text, explanations, or markdown formatting before or after the JSON object.\n","\n","### Survey Responses:\n","{sample_text}\n","\n","### JSON Output:\"\"\"\n","\n","        final_prompt = f\"<start_of_turn>user\\n{prompt_content}<end_of_turn>\\n<start_of_turn>model\\n\"\n","\n","        # ### --- CRITICAL SYNTAX FIX --- ###\n","        # The misplaced comment has been removed.\n","        inputs = tokenizer(final_prompt, return_tensors=\"pt\", truncation=True, max_length=8190)\n","        input_ids = inputs.input_ids.to(\"cuda\")\n","        attention_mask = inputs.attention_mask.to(\"cuda\")\n","\n","        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=1024, pad_token_id=tokenizer.eos_token_id)\n","\n","        raw_output = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n","\n","        print(\"\\n>>> RAW LLM OUTPUT <<<\")\n","        print(\"==============================\")\n","        print(raw_output)\n","        print(\"==============================\")\n","\n","        print(\"\\n--- Step 7: Final Check ---\")\n","        if \"{\" in raw_output and \"}\" in raw_output:\n","            print(\"\\nSUCCESS: Model generated a response containing JSON-like structures without crashing.\")\n","        else:\n","            print(\"\\nWARNING: Model generated a response, but it does not appear to contain a JSON object.\")\n","\n","    except Exception as e:\n","        print(f\"\\nCRITICAL FAILURE during generation: {e}\")\n","\n","# --- Execute the Final Standalone Test ---\n","run_final_standalone_test_fixed()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vNWBOKQ2o2Ad"},"outputs":[],"source":["# ==============================================================================\n","# @title          ULTIMATE MINIMAL WORKING EXAMPLE (UNIT TEST)\n","#           VERSION 9.5 -- Direct Documentation Snippet\n","#\n","# Purpose: To test the absolute core model generation capability using code\n","#          adapted directly from the official documentation, removing all\n","#          other variables from our pipeline.\n","# ==============================================================================\n","\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","def run_ultimate_minimal_test():\n","    print(\"--- Step 1: Installing Libraries ---\")\n","    try:\n","        from IPython.utils import io\n","        with io.capture_output() as captured:\n","            !pip install -q transformers bitsandbytes accelerate\n","        print(\"Libraries installed successfully.\")\n","    except Exception as e:\n","        print(f\"ERROR: Library install failed: {e}\")\n","        return\n","\n","    print(\"\\n--- Step 2: Authenticating ---\")\n","    try:\n","        from google.colab import userdata\n","        from huggingface_hub import login\n","        HF_TOKEN = userdata.get('HF_TOKEN')\n","        if HF_TOKEN:\n","            login(token=HF_TOKEN)\n","            print(\"Hugging Face session authenticated.\")\n","    except (ImportError, KeyError):\n","        print(\"WARNING: Could not access Colab Secrets.\")\n","\n","    # --- Load Model ---\n","    model, tokenizer = None, None\n","    print(\"\\n--- Step 3: Loading Model ---\")\n","    try:\n","        # Using the base google model to avoid unsloth-specific issues for this test\n","        model_id = \"google/gemma-2b-it\" # Changed to the base instruction-tuned model\n","        tokenizer = AutoTokenizer.from_pretrained(model_id)\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_id,\n","            torch_dtype=torch.bfloat16,\n","            device_map=\"auto\",\n","        )\n","        print(\"--- Model loaded successfully ---\")\n","    except Exception as e:\n","        print(f\"CRITICAL: Failed to load model. Error: {e}\")\n","        return\n","\n","    # --- Generate Text with Simple Multi-Turn Prompt ---\n","    print(\"\\n--- Step 4: Generating text with a simple multi-turn prompt ---\")\n","    try:\n","        # The exact multi-turn structure from the documentation\n","        chat = [\n","            { \"role\": \"user\", \"content\": \"What is a good place for a family vacation?\" },\n","            { \"role\": \"model\", \"content\": \"What kind of activities do you enjoy?\" },\n","            { \"role\": \"user\", \"content\": \"We like hiking, swimming, and visiting historical sites.\" }\n","        ]\n","\n","        # Using the official chat template method, which is the most robust way\n","        prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n","\n","        inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n","\n","        # Generate the output\n","        outputs = model.generate(input_ids=inputs, max_new_tokens=150)\n","\n","        # Decode and print the result\n","        raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        print(\"\\n>>> MODEL'S FULL RESPONSE (including prompt) <<<\")\n","        print(\"======================================================\")\n","        print(raw_output)\n","        print(\"======================================================\")\n","\n","        print(\"\\nSUCCESS: Model generated a response without crashing.\")\n","\n","    except Exception as e:\n","        print(f\"\\nCRITICAL FAILURE during generation: {e}\")\n","\n","# --- Execute the Minimal Test ---\n","run_ultimate_minimal_test()"]},{"cell_type":"markdown","metadata":{"id":"XoK0roh82kz1"},"source":["## Cell 1: Setup and Model Loading\n","(This cell handles all installations, loads the Gemma-3 model into memory, and prepares it for inference. This is the most computationally intensive step.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItWaYvn-uq42"},"outputs":[],"source":["# ==============================================================================\n","#                      PIPELINE V10 - CELL 1: SETUP & MODEL LOADING\n","# ==============================================================================\n","import os\n","import torch\n","from google.colab import drive\n","\n","print(\"--- Mounting Google Drive ---\")\n","try:\n","    drive.mount('/content/drive', force_remount=True)\n","    print(\"Google Drive mounted.\")\n","except Exception as e:\n","    print(f\"ERROR: Drive mount failed: {e}\")\n","\n","print(\"\\n--- Installing Libraries ---\")\n","try:\n","    from IPython.utils import io\n","    with io.capture_output() as captured:\n","        !pip install -q transformers bitsandbytes accelerate\n","    print(\"Libraries installed successfully.\")\n","except Exception as e:\n","    print(f\"ERROR: Library installation failed: {e}\")\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import json\n","\n","print(\"\\n--- Authenticating ---\")\n","try:\n","    from google.colab import userdata\n","    from huggingface_hub import login\n","    HF_TOKEN = userdata.get('HF_TOKEN')\n","    if HF_TOKEN:\n","        login(token=HF_TOKEN)\n","        print(\"Hugging Face session authenticated.\")\n","except (ImportError, KeyError):\n","    print(\"WARNING: Could not access Colab Secrets.\")\n","\n","# --- Load the Stable Gemma Model ---\n","print(\"\\n--- Loading Gemma Model (google/gemma-2b-it) ---\")\n","\n","# ### --- CRITICAL LESSON 1 --- ###\n","# We are using the stable, official Google model, NOT the unsloth version.\n","model_id = \"google/gemma-2b-it\"\n","\n","try:\n","    tokenizer = AutoTokenizer.from_pretrained(model_id)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_id,\n","        torch_dtype=torch.bfloat16,\n","        device_map=\"auto\",\n","    )\n","    print(\"\\n--- Model Setup Complete ---\")\n","except Exception as e:\n","    print(f\"CRITICAL: Failed to load model. Error: {e}\")\n"]},{"cell_type":"markdown","metadata":{"id":"LJojPZha6PLJ"},"source":["## Cell 2: Configuration and Phase 1 - Candidate Theme Generation\n","(This cell defines our file paths and runs the first, crucial phase. It takes a sample of the data and uses the loaded LLM to generate our thematic dictionary.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wsmJXXXK8BH"},"outputs":[],"source":["# ==============================================================================\n","#      PIPELINE V10.5 - CELL 2: CONFIG & PHASE 1 (ULTRA-STRICT PROMPT)\n","# ==============================================================================\n","import time\n","import json\n","import re\n","\n","# --- Configuration ---\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Data/')\n","REPORTS_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Reports/')\n","os.makedirs(REPORTS_DIR, exist_ok=True)\n","P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json')\n","P1_THEMATIC_DICTIONARY_FILE = os.path.join(DATA_DIR, 'p1_thematic_dictionary_v10.5.json') # New version file\n","P2_CLASSIFICATION_OUTPUT_FILE = os.path.join(DATA_DIR, 'p2_classification_output_v10.5.json')\n","P3_FINAL_REPORT_FILE = os.path.join(REPORTS_DIR, f'Stakeholder_Report_v10.5_{time.strftime(\"%Y%m%d\")}.txt')\n","QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","PILOT_QUESTIONS = ['Q5', 'Q3']\n","\n","# --- Helper Function: Load Data (with debug statements) ---\n","def load_data_files():\n","    try:\n","        print(\"--- DEBUG: Loading corpus file...\")\n","        with open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8') as f:\n","            full_corpus = json.load(f)\n","        print(\"--- DEBUG: Corpus loaded successfully.\")\n","\n","        print(\"--- DEBUG: Loading question definitions file...\")\n","        with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f:\n","            qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","        print(\"--- DEBUG: Question definitions loaded successfully.\")\n","        return full_corpus, qid_to_text\n","    except Exception as e:\n","        print(f\"--- DEBUG: CRITICAL FAILURE during data loading: {e}\")\n","        return None, None\n","\n","full_corpus, qid_to_text = load_data_files()\n","\n","def generate_llm_response(prompt_content):\n","    print(\"--- DEBUG: Formatting prompt for Gemma model...\")\n","    prompt = f\"<start_of_turn>user\\n{prompt_content}<end_of_turn>\\n<start_of_turn>model\\n\"\n","\n","    print(\"--- DEBUG: Encoding prompt...\")\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=8190)\n","    input_ids = inputs.input_ids.to(\"cuda\")\n","    attention_mask = inputs.attention_mask.to(\"cuda\")\n","\n","    print(\"--- DEBUG: Sending prompt to model.generate()...\")\n","    start_time = time.time()\n","    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=1024, pad_token_id=tokenizer.eos_token_id)\n","    end_time = time.time()\n","    print(f\"--- DEBUG: Model generation took {end_time - start_time:.2f} seconds.\")\n","\n","    print(\"--- DEBUG: Decoding model output...\")\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    model_response_start = response.find('<start_of_turn>model\\n') + len('<start_of_turn>model\\n')\n","    model_response = response[model_response_start:].strip()\n","    print(\"--- DEBUG: Successfully decoded and extracted model response.\")\n","    return model_response\n","\n","# ==============================================================================\n","#                                  RUN PHASE 1\n","# ==============================================================================\n","thematic_dictionary = {}\n","if os.path.exists(P1_THEMATIC_DICTIONARY_FILE):\n","    print(f\"--- DEBUG: Loading existing thematic dictionary from {P1_THEMATIC_DICTIONARY_FILE}...\")\n","    thematic_dictionary = json.load(open(P1_THEMATIC_DICTIONARY_FILE, 'r'))\n","else:\n","    print(\"\\n--- Starting Phase 1: Candidate Theme Generation ---\")\n","    if full_corpus is None:\n","        print(\"--- DEBUG: Halting Phase 1 because corpus data could not be loaded.\")\n","    else:\n","        for qid in PILOT_QUESTIONS:\n","            print(f\"\\n>>> Generating themes for QID: {qid}...\")\n","\n","            docs = [item['text'] for item in full_corpus.get(qid, [])]\n","            if not docs:\n","                print(f\"  --- DEBUG: No documents found for {qid}. Skipping.\"); continue\n","\n","            sample_text = \"\\n\\n---\\n\\n\".join(docs[:20])\n","            question_text = qid_to_text.get(qid, \"\")\n","            print(f\"  --- DEBUG: Created a sample of {len(docs[:20])} documents.\")\n","\n","            # ### --- CRITICAL FIX (V10.5): ULTRA-STRICT PROMPT --- ###\n","            prompt = f\"\"\"You are a data formatting expert. Your task is to analyze text and produce a single, clean JSON object.\n","\n","Analyze the survey responses for the question: \"{question_text}\"\n","\n","Your single task is to identify 5 to 7 recurring themes.\n","\n","Your output MUST be a single, valid JSON object.\n","- The keys MUST be the short, descriptive theme names.\n","- The values MUST be a list of 3-5 key concepts as strings.\n","- DO NOT create nested objects.\n","- DO NOT repeat themes.\n","- DO NOT add any text or explanations before or after the JSON object.\n","\n","### Survey Responses:\n","{sample_text}\n","\n","### JSON Output:\"\"\"\n","\n","            print(\"  --- DEBUG: Sending final, ultra-strict prompt...\")\n","            llm_output_str = generate_llm_response(prompt)\n","\n","            print(\"\\n  --- DEBUG: RAW LLM OUTPUT RECEIVED ---\")\n","            print(\"  vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\")\n","            print(llm_output_str)\n","            print(\"  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n","\n","            try:\n","                print(\"\\n  --- DEBUG: Attempting to parse JSON from raw output...\")\n","                json_str_match = re.search(r'\\{.*\\}', llm_output_str, re.DOTALL)\n","\n","                if json_str_match:\n","                    parsed_json = json.loads(json_str_match.group(0))\n","                    thematic_dictionary[qid] = parsed_json\n","                    print(f\"  --- DEBUG: SUCCESS! Parsed thematic dictionary for {qid}.\")\n","                else:\n","                    print(f\"  --- DEBUG: ERROR! No valid JSON object found in response.\")\n","                    thematic_dictionary[qid] = {}\n","            except Exception as e:\n","                print(f\"  --- DEBUG: CRITICAL ERROR during JSON parsing: {e}\")\n","                thematic_dictionary[qid] = {}\n","\n","        print(f\"\\n--- DEBUG: Attempting to save thematic dictionary to {P1_THEMATIC_DICTIONARY_FILE}...\")\n","        with open(P1_THEMATIC_DICTIONARY_FILE, 'w') as f: json.dump(thematic_dictionary, f, indent=2)\n","        print(f\"--- Phase 1 Complete ---\")\n","\n","print(\"\\n--- FINAL STATE: Generated/Loaded Thematic Dictionary ---\")\n","print(json.dumps(thematic_dictionary, indent=2))"]},{"cell_type":"markdown","metadata":{"id":"iVpWmZh76pTp"},"source":["## Cell 3: Phase 2 - Zero-Shot Classification\n","(This cell iterates through all responses and uses the thematic dictionary to classify each one.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D37Zy_eBEb8n"},"outputs":[],"source":["# ==============================================================================\n","#         PIPELINE V9 - CELL 3: PHASE 2 (CLASSIFICATION WITH CHECKPOINTING)\n","# ==============================================================================\n","import time\n","import json\n","\n","classification_output = {}\n","\n","# ### --- CRITICAL FIX --- ###\n","# We now implement a robust resume/checkpointing system for this long-running phase.\n","\n","print(\"\\n--- Starting Phase 2: Zero-Shot Classification ---\")\n","\n","# Load the thematic dictionary from Phase 1\n","with open(P1_THEMATIC_DICTIONARY_FILE, 'r', encoding='utf-8') as f:\n","    thematic_dictionary = json.load(f)\n","\n","# Load existing classifications if the file exists, to resume progress\n","if os.path.exists(P2_CLASSIFICATION_OUTPUT_FILE):\n","    print(f\"Resuming from existing classification file: {P2_CLASSIFICATION_OUTPUT_FILE}\")\n","    with open(P2_CLASSIFICATION_OUTPUT_FILE, 'r', encoding='utf-8') as f:\n","        classification_output = json.load(f)\n","else:\n","    print(\"No existing classification file found. Starting from scratch.\")\n","\n","for qid in PILOT_QUESTIONS:\n","    if qid not in classification_output:\n","        classification_output[qid] = []\n","\n","    print(f\"\\nClassifying responses for QID: {qid}...\")\n","\n","    themes_for_qid = thematic_dictionary.get(qid, {})\n","    if not themes_for_qid:\n","        print(f\"  - No themes defined for {qid}. Skipping.\"); continue\n","\n","    theme_definitions_str = \"\\n\".join([f\"- \\\"{name}\\\"\" for name in themes_for_qid.keys()])\n","    docs_to_classify = full_corpus.get(qid, [])\n","\n","    # Get the set of already processed response IDs for this question\n","    processed_ids = {item['response_id'] for item in classification_output.get(qid, [])}\n","    start_index = len(processed_ids)\n","\n","    if start_index > 0:\n","        print(f\"  - Resuming. {start_index} responses already classified.\")\n","\n","    for i in range(start_index, len(docs_to_classify)):\n","        doc = docs_to_classify[i]\n","        doc_id = doc.get('response_id', f'unknown_{i}')\n","        doc_text = doc.get('text', '')\n","\n","        if not doc_text.strip() or doc_id in processed_ids: continue\n","\n","        prompt = f\"\"\"Perform a zero-shot classification task. Based on the user's response, identify which of the following predefined themes are discussed.\n","Respond with a valid JSON list containing only the names of the matching themes. If no themes match, respond with an empty list [].\n","\n","### Themes:\n","{theme_definitions_str}\n","\n","### User Response:\n","{doc_text}\n","\n","### JSON List Output:\"\"\"\n","\n","        llm_output_str = generate_llm_response(prompt)\n","\n","        try:\n","            json_str_match = re.search(r'\\[.*\\]', llm_output_str, re.DOTALL)\n","            classified_themes = json.loads(json_str_match.group(0)) if json_str_match else []\n","        except (json.JSONDecodeError, AttributeError):\n","            classified_themes = [] # Mark as unclassified on error\n","\n","        classification_output[qid].append({\n","            \"response_id\": doc_id,\n","            \"themes\": classified_themes\n","        })\n","\n","        # Checkpoint progress periodically\n","        if (i + 1) % 25 == 0:\n","            with open(P2_CLASSIFICATION_OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","                json.dump(classification_output, f, indent=2)\n","            print(f\"  - CHECKPOINT: Classified {i+1} / {len(docs_to_classify)} responses. Progress saved.\")\n","\n","    print(f\"  - Finished classification for {qid}.\")\n","\n","# Save the final complete classification output\n","with open(P2_CLASSIFICATION_OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","    json.dump(classification_output, f, indent=2)\n","print(f\"\\n--- Phase 2 Complete: Final classification output saved to {P2_CLASSIFICATION_OUTPUT_FILE} ---\")"]},{"cell_type":"markdown","metadata":{"id":"SLeaxze766yb"},"source":["## Cell 4: Phase 3 - Aggregation and Reporting\n","(This is the final, fast cell that counts the results and generates the human-readable report.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nV79UP0_faKE"},"outputs":[],"source":["# ==============================================================================\n","#                  PIPELINE V10 - CELL 4: PHASE 3 (REPORTING - CORRECTED)\n","# ==============================================================================\n","import time\n","import json\n","import os\n","from collections import Counter # ### --- CRITICAL FIX --- ### Added missing import\n","\n","print(\"\\n--- Starting Phase 3: Aggregation and Reporting ---\")\n","\n","# Load the classification results from Phase 2\n","with open(P2_CLASSIFICATION_OUTPUT_FILE, 'r', encoding='utf-8') as f:\n","    classification_output = json.load(f)\n","\n","# Load the thematic dictionary for context\n","with open(P1_THEMATIC_DICTIONARY_FILE, 'r', encoding='utf-8') as f:\n","    thematic_dictionary = json.load(f)\n","\n","# Load question definitions\n","with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f:\n","    qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","\n","with open(P3_FINAL_REPORT_FILE, 'w', encoding='utf-8') as f_out:\n","    f_out.write(f\"LLM-Based Thematic Analysis (v10 - Gemma)\\nGenerated: {time.strftime('%c')}\\n\" + \"=\"*70 + \"\\n\\n\")\n","\n","    for qid, classifications in classification_output.items():\n","        f_out.write(f\"Question {qid}: {qid_to_text.get(qid, 'N/A')}\\n\\n\")\n","\n","        total_respondents = len({item['response_id'] for item in classifications})\n","        if total_respondents == 0:\n","            f_out.write(\"  No responses were classified for this question.\\n\\n---\\n\\n\")\n","            continue\n","\n","        # Aggregate counts for each theme\n","        theme_counts = Counter()\n","        for item in classifications:\n","            # Ensure themes are valid keys before counting\n","            valid_themes = [theme for theme in item.get('themes', []) if theme in thematic_dictionary.get(qid, {})]\n","            for theme in valid_themes:\n","                theme_counts[theme] += 1\n","\n","        if not theme_counts:\n","            f_out.write(\"  No significant recurring themes were identified.\\n\\n---\\n\\n\")\n","            continue\n","\n","        f_out.write(f\"  Analysis based on {total_respondents} unique respondents.\\n\\n\")\n","        f_out.write(\"  The most common points made by respondents were:\\n\\n\")\n","\n","        # Sort themes by count, descending\n","        for i, (theme_name, count) in enumerate(theme_counts.most_common()):\n","            percentage = (count / total_respondents * 100)\n","\n","            f_out.write(f\"  {i+1}. Theme: [{theme_name}]\\n\")\n","            f_out.write(f\"     Indicated by approx {percentage:.2f}% of respondents ({count} of {total_respondents} total).\\n\")\n","\n","            defining_concepts = thematic_dictionary.get(qid, {}).get(theme_name, [])\n","            if defining_concepts:\n","                f_out.write(f\"     Defining Concepts: {defining_concepts}\\n\")\n","\n","            # Simple quote extraction\n","            quote = \"Not available.\"\n","            for item in classifications:\n","                 if theme_name in item.get('themes', []):\n","                    response_id = item['response_id']\n","                    for doc in full_corpus.get(qid, []):\n","                        if doc.get('response_id') == response_id:\n","                            quote = doc.get('text', 'Quote text not found.')\n","                            break\n","                    break\n","\n","            f_out.write(f'     Example: â€œ...{quote[:300]}...â€\\n\\n')\n","\n","        f_out.write(\"---\\n\\n\")\n","\n","print(f\"--- Phase 3 Complete: Final report saved to {P3_FINAL_REPORT_FILE} ---\")\n","print(\"\\n Please review the final report file in your Google Drive.\")"]},{"cell_type":"markdown","metadata":{"id":"_gbsNq9qHG2v"},"source":["# 31st Aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHAVOJbloqYK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kKJQMLlLoqQP"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"abCSCBlxoNdT"},"source":["## Split multiple cells"]},{"cell_type":"markdown","metadata":{"id":"GYi2GaJlog4B"},"source":["## Cell 2: Global Setup and Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OU6VZPzYog4C"},"outputs":[],"source":["# ==============================================================================\n","#                               SETUP & CONFIGURATION\n","# ==============================================================================\n","import json\n","import os\n","import re\n","import sys\n","import time\n","import pandas as pd\n","import numpy as np\n","from collections import defaultdict\n","from pathlib import Path\n","from typing import Dict\n","from google.colab import drive\n","\n","# --- Step 1: Mount Google Drive ---\n","print(\"Mounting Google Drive... Please follow the authentication prompts.\")\n","try:\n","    drive.mount('/content/drive')\n","    print(\"Google Drive mounted successfully.\")\n","except Exception as e:\n","    print(f\"ERROR: Failed to mount Google Drive. Error: {e}\")\n","\n","# --- Step 2: Install necessary libraries ---\n","!pip install -q transformers bitsandbytes accelerate sentence-transformers bertopic umap-learn hdbscan huggingface_hub\n","\n","# --- Step 3: Import all required libraries ---\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","from sentence_transformers import SentenceTransformer, util\n","from bertopic import BERTopic\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import nltk\n","import torch\n","import traceback\n","from huggingface_hub import login # <--- IMPORT THE LOGIN FUNCTION\n","\n","# --- Step 4: Download NLTK resources ---\n","print(\"Downloading NLTK resources ('punkt' and 'punkt_tab')...\")\n","nltk.download('punkt', quiet=True)\n","nltk.download('punkt_tab', quiet=True)\n","print(\"NLTK resources are ready.\")\n","\n","# ==============================================================================\n","#                     STEP 5: AUTHENTICATION (REVISED)\n","# ==============================================================================\n","# This section handles the Hugging Face token for accessing models.\n","# It loads the token from Colab Secrets and uses it to log in.\n","\n","try:\n","    from google.colab import userdata\n","    HF_TOKEN = userdata.get('HF_TOKEN')\n","\n","    if HF_TOKEN:\n","        # --- NEW AUTHENTICATION STEP ---\n","        # This explicitly logs the session in, ensuring all libraries can authenticate.\n","        login(token=HF_TOKEN)\n","        print(\"Hugging Face token loaded and session authenticated successfully.\")\n","    else:\n","        print(\"WARNING: Hugging Face token not found in Colab Secrets.\")\n","\n","except (ImportError, KeyError):\n","    print(\"WARNING: Could not access Colab Secrets.\")\n","    print(\"         Public models will work, but private/gated models may fail.\")\n","    HF_TOKEN = None\n","\n","# ==============================================================================\n","#                      STEP 6: Define FileLogger Class\n","# ==============================================================================\n","class FileLogger:\n","    # (The FileLogger class definition remains the same as the previous corrected version)\n","    def __init__(self, log_file_path: str):\n","        self.log_file_path = log_file_path\n","        log_dir = os.path.dirname(log_file_path)\n","        if log_dir and not os.path.exists(log_dir):\n","            os.makedirs(log_dir, exist_ok=True)\n","        with open(self.log_file_path, 'w', encoding='utf-8') as f:\n","            f.write(f\"{self._get_timestamp()}: Starting new log at {self.log_file_path}\\n\")\n","\n","    def _get_timestamp(self) -> str:\n","        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","\n","    def log(self, message: str):\n","        print(message)\n","        try:\n","            with open(self.log_file_path, 'a', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","        except Exception as e:\n","            print(f\"!!! LOGGER FAILED: {e} !!!\\n{self._get_timestamp()}: {message}\")\n","\n","    def _get_serializer(self):\n","        def default_serializer(obj):\n","            if isinstance(obj, (np.integer, np.floating)): return obj.item()\n","            if isinstance(obj, np.ndarray): return obj.tolist()\n","            if isinstance(obj, set): return sorted(list(obj))\n","            try: return json.JSONEncoder().default(obj)\n","            except TypeError: return f\"UNSERIALIZABLE:{type(obj).__name__}\"\n","        return default_serializer\n","\n","    def log_json(self, data: dict, prefix: str = \"JSON_DATA\"):\n","        log_entry = {\"timestamp\": self._get_timestamp(), \"type\": prefix, \"data\": data}\n","        try:\n","            json_string = json.dumps(log_entry, default=self._get_serializer(), indent=2)\n","            print(f\"DEBUG: Logging JSON object with prefix '{prefix}' to file: {self.log_file_path}\")\n","            with open(self.log_file_path, 'a', encoding='utf-8') as f:\n","                f.write(json_string + \"\\n\")\n","        except Exception as e:\n","            self.log(f\"ERROR: log_json failed: {e}\\n{traceback.format_exc()}\")\n","\n","# ==============================================================================\n","#                            PIPELINE CONFIGURATION\n","# ==============================================================================\n","# (This section remains unchanged)\n","# --- Base Paths ---\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","LOG_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Logs/')\n","DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Data/')\n","REPORTS_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Reports/')\n","for d in [LOG_DIR, DATA_DIR, REPORTS_DIR]:\n","    os.makedirs(d, exist_ok=True)\n","\n","# --- Input Files ---\n","RAW_RESPONSES_JSONL = os.path.join(BASE_PROJECT_DIR, 'processed_responses.jsonl')\n","QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","\n","# --- Phase-Specific Output Files ---\n","P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json')\n","P2_GRANULAR_TOPICS_FILE = os.path.join(DATA_DIR, 'p2_granular_topics.json')\n","P3_MDL_CORE_THEMES_FILE = os.path.join(DATA_DIR, 'p3_mdl_core_themes.json')\n","P4_FINAL_ANALYSIS_FILE = os.path.join(DATA_DIR, 'p4_final_analysis_report.json')\n","P5_STAKEHOLDER_REPORT_FILE = os.path.join(REPORTS_DIR, f'Stakeholder_Report_{time.strftime(\"%Y%m%d\")}.txt')\n","\n","# --- Model & Algorithm Parameters ---\n","P1_EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n","P1_SIMILARITY_SCORE = 0.5\n","P2_BERTOPIC_MIN_TOPIC_SIZE = 3\n","P3_MDL_PENALTY_FACTOR = 1.5\n","P4_NUM_QUOTES_PER_THEME = 3\n","\n","# --- Initialize Master Logger ---\n","master_logger = FileLogger(os.path.join(LOG_DIR, f'pipeline_master_{time.strftime(\"%Y%m%d_%H%M%S\")}.log'))\n","ENABLE_DEBUG_LOGGING = True\n","print(\"Setup and configuration complete.\")"]},{"cell_type":"markdown","metadata":{"id":"Z4Wstkbdog4E"},"source":["## Cell 3: Phase 1 - UDS Generation (Question-Centric Corpus)\n","**Goal:** Process all raw survey responses, extract text from PDFs, and use semantic search to collate all text snippets relevant to each specific question.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIHDQ6n6Iouw"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"FV1zGOaXog4G"},"source":["## Cell 4: Phase 2 - Granular Topic Discovery (BERTopic)\n","* **Goal:** For each question, analyze its collected text corpus to discover a large set of granular, fine-grained topics. This creates the foundational data that the MDL algorithm will cluster.\n","* **Goal:** For each question, analyze its collected text corpus to discover a large set of granular, fine-grained topics. This creates the foundational data that the MDL algorithm will cluster.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7z79PxKog4H"},"outputs":[],"source":["def run_phase2_discover_granular_topics(corpus_file, output_file, logger):\n","    \"\"\"\n","    For each question in the corpus, runs BERTopic to discover granular topics.\n","    This version is resumable.\n","    \"\"\"\n","    logger.log(\"--- Starting Phase 2: Granular Topic Discovery ---\")\n","    try:\n","        # 1. Load corpus and initialize model\n","        logger.log(f\"Loading question corpus from: {corpus_file}\")\n","        with open(corpus_file, 'r', encoding='utf-8') as f:\n","            question_corpus = json.load(f)\n","\n","        logger.log(\"Initializing BERTopic model...\")\n","        embedding_model = SentenceTransformer(P1_EMBEDDING_MODEL)\n","        topic_model = BERTopic(embedding_model=embedding_model, min_topic_size=P2_BERTOPIC_MIN_TOPIC_SIZE, verbose=False)\n","\n","        # --- RESUME LOGIC START ---\n","        all_granular_topics = {}\n","        if os.path.exists(output_file):\n","            try:\n","                with open(output_file, 'r', encoding='utf-8') as f_in:\n","                    all_granular_topics = json.load(f_in)\n","                logger.log(f\"Resuming Phase 2. Found {len(all_granular_topics)} previously processed questions in {output_file}.\")\n","            except json.JSONDecodeError:\n","                logger.log(f\"WARNING: Could not parse existing output file {output_file}. Starting Phase 2 from scratch.\")\n","                all_granular_topics = {}\n","        # --- RESUME LOGIC END ---\n","\n","        # 3. Loop and discover topics for each question\n","        qids = list(question_corpus.keys())\n","        for i, qid in enumerate(qids):\n","            logger.log(f\"  > Processing QID {qid} ({i+1}/{len(qids)})...\")\n","\n","            # --- SKIP IF ALREADY PROCESSED ---\n","            if qid in all_granular_topics:\n","                logger.log(f\"    - Skipping {qid}, results already exist.\")\n","                continue\n","\n","            documents = [item['text'] for item in question_corpus[qid]]\n","\n","            if len(documents) < P2_BERTOPIC_MIN_TOPIC_SIZE:\n","                logger.log(f\"    - Skipping {qid}, not enough documents ({len(documents)}).\")\n","                all_granular_topics[qid] = [] # Mark as processed but empty\n","                continue\n","\n","            topics, _ = topic_model.fit_transform(documents)\n","            topic_info = topic_model.get_topic_info()\n","            granular_topics_for_qid = topic_info[topic_info['Topic'] != -1]\n","            all_granular_topics[qid] = granular_topics_for_qid[['Name', 'Representation']].to_dict('records')\n","            logger.log(f\"    - Found {len(granular_topics_for_qid)} granular topics for {qid}.\")\n","\n","            # --- SAVE PROGRESS ---\n","            with open(output_file, 'w', encoding='utf-8') as f_out:\n","                json.dump(all_granular_topics, f_out, indent=2)\n","\n","        logger.log(\"--- Phase 2 Complete: Granular topics discovered and saved. ---\")\n","        return True\n","    except Exception as e:\n","        logger.log(f\"CRITICAL ERROR in Phase 2: {e}\\n{traceback.format_exc()}\")\n","        return False\n","\n","# Placeholder for execution\n","print(\"Phase 2 function defined with resume capability.\")"]},{"cell_type":"markdown","metadata":{"id":"MDrdy-cBog4H"},"source":["## Cell 5: Phase 3 - MDL Core Theme Clustering\n","Goal: Apply the MDL algorithm to the granular topics discovered in Phase 2. This step groups similar granular topics into a statistically optimal set of \"core themes\" for each question. This is the core analytical engine of the pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUqtqmeWog4I"},"outputs":[],"source":["# ==============================================================================\n","#                      IMPORTS FOR PHASE 3\n","# ==============================================================================\n","import json\n","import os\n","import re\n","import traceback\n","import numpy as np\n","from collections import defaultdict\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# ==============================================================================\n","#                         MDL HELPER FUNCTIONS\n","# ==============================================================================\n","# (These helpers: vectorize_topics, calculate_cluster_cost, calculate_mdl_cost remain unchanged)\n","def vectorize_topics(documents):\n","    vectorizer = TfidfVectorizer(use_idf=False, norm='l1')\n","    topic_vectors = vectorizer.fit_transform(documents).toarray()\n","    return topic_vectors\n","\n","def calculate_cluster_cost(topic_vectors_subset):\n","    if topic_vectors_subset.shape[0] < 2: return 0\n","    centroid = np.mean(topic_vectors_subset, axis=0)\n","    sse = np.sum((topic_vectors_subset - centroid)**2)\n","    return 0.5 * topic_vectors_subset.shape[0] * np.log(sse + 1e-9)\n","\n","def calculate_mdl_cost(partition, topic_vectors, penalty_factor):\n","    n_points = topic_vectors.shape[0]\n","    k_clusters = len(partition)\n","    if k_clusters == 0: return float('inf')\n","    l_model = n_points * np.log(k_clusters)\n","    l_data = sum(calculate_cluster_cost(topic_vectors[indices, :]) for indices in partition.values() if indices)\n","    return l_model + (penalty_factor * l_data)\n","\n","# --- This is the definitive version of the clustering function ---\n","def find_optimal_partition(topic_vectors, penalty_factor, granular_topic_names, qid, logger):\n","    \"\"\"\n","    Performs agglomerative clustering with definitive checkpointing and clear logging.\n","    \"\"\"\n","    state_file = os.path.join(DATA_DIR, f'p3_mdl_state_{qid}.json')\n","    partition, best_partition, best_cost, completed_iterations = None, None, float('inf'), 0\n","\n","    if os.path.exists(state_file):\n","        try:\n","            with open(state_file, 'r') as f:\n","                state = json.load(f)\n","            partition = {int(k): v for k, v in state['partition'].items()}\n","            best_partition = {int(k): v for k, v in state['best_partition'].items()}\n","            best_cost = state['best_cost']\n","            completed_iterations = state['completed_iterations']\n","            logger.log(f\"    - Resuming MDL clustering for {qid} from iteration {completed_iterations + 1}.\")\n","        except Exception as e:\n","            logger.log(f\"    - WARNING: Could not load state file {state_file}. Starting from scratch. Error: {e}\")\n","            partition = None\n","\n","    if partition is None:\n","        partition = {i: [i] for i in range(len(granular_topic_names))}\n","        cost = calculate_mdl_cost(partition, topic_vectors, penalty_factor)\n","        best_partition, best_cost = partition, cost\n","        completed_iterations = 0\n","        logger.log(f\"    - Initializing MDL clustering for {qid}. Initial Cost: {best_cost:.2f}\")\n","\n","    num_initial_topics = len(granular_topic_names)\n","    total_merges = num_initial_topics - 1\n","\n","    for i in range(completed_iterations, total_merges):\n","\n","        current_iteration_num = i + 1\n","        if len(partition) < 2: break\n","\n","        # --- Find the best pair of themes to merge ---\n","        best_merge, lowest_merge_cost = None, float('inf')\n","        themes_to_merge = list(partition.keys())\n","        for j in range(len(themes_to_merge)):\n","            for k in range(j + 1, len(themes_to_merge)):\n","                t1, t2 = themes_to_merge[j], themes_to_merge[k]\n","                temp_partition = {key: val[:] for key, val in partition.items()}\n","                temp_partition[t1].extend(temp_partition.pop(t2))\n","                merge_cost = calculate_mdl_cost(temp_partition, topic_vectors, penalty_factor)\n","                if merge_cost < lowest_merge_cost:\n","                    lowest_merge_cost, best_merge = merge_cost, (t1, t2)\n","\n","        # --- Perform the merge ---\n","        t1, t2 = best_merge\n","        partition[t1].extend(partition.pop(t2))\n","\n","        # --- Log if a new best state is found ---\n","        if lowest_merge_cost < best_cost:\n","            best_cost = lowest_merge_cost\n","            best_partition = {k: v[:] for k, v in partition.items()}\n","            logger.log(f\"    - New best state at iteration {current_iteration_num}: {len(best_partition)} themes, Cost: {best_cost:.2f}\")\n","\n","        # --- SIMPLIFIED AND RELIABLE LOGGING & CHECKPOINTING ---\n","        # Log progress at specific intervals\n","        log_intervals = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 75, 100] # Log at these specific iterations\n","        if ENABLE_DEBUG_LOGGING and (current_iteration_num in log_intervals or current_iteration_num % 50 == 0): # Then every 50\n","             logger.log(f\"      - Progress: Merge iteration {current_iteration_num}/{total_merges} complete.\")\n","\n","        # Checkpoint every 20 iterations, GUARANTEED to log.\n","        if current_iteration_num % 20 == 0 and current_iteration_num < total_merges:\n","            state_to_save = {\n","                'partition': partition, 'best_partition': best_partition,\n","                'best_cost': best_cost, 'completed_iterations': current_iteration_num\n","            }\n","            with open(state_file, 'w') as f:\n","                json.dump(state_to_save, f)\n","            # This message will now print regardless of the debug flag.\n","            logger.log(f\"      >>> CHECKPOINT SAVED at iteration {current_iteration_num} for {qid}. <<<\")\n","\n","    # Final cleanup\n","    if os.path.exists(state_file):\n","        os.remove(state_file)\n","        logger.log(f\"    - Clustering for {qid} complete. Removed temporary state file.\")\n","\n","    return best_partition\n","\n","# ==============================================================================\n","#                        MAIN PHASE 3 FUNCTION\n","# ==============================================================================\n","\n","def run_phase3_mdl_clustering(granular_topics_file, output_file, logger):\n","    logger.log(\"--- Starting Phase 3: MDL Core Theme Clustering ---\")\n","    try:\n","        logger.log(f\"Loading granular topics from: {granular_topics_file}\")\n","        with open(granular_topics_file, 'r', encoding='utf-8') as f:\n","            all_granular_topics = json.load(f)\n","\n","        all_core_themes = {}\n","        if os.path.exists(output_file):\n","            try:\n","                with open(output_file, 'r', encoding='utf-8') as f_in:\n","                    all_core_themes = json.load(f_in)\n","                logger.log(f\"Resuming Phase 3. Found {len(all_core_themes)} previously processed questions in {output_file}.\")\n","            except json.JSONDecodeError:\n","                logger.log(f\"WARNING: Could not parse {output_file}. Starting Phase 3 from scratch.\")\n","                all_core_themes = {}\n","\n","        qids = list(all_granular_topics.keys())\n","        for i, qid in enumerate(qids):\n","            logger.log(f\"  > Clustering themes for QID {qid} ({i+1}/{len(qids)})...\")\n","\n","            if qid in all_core_themes:\n","                logger.log(f\"    - Skipping {qid}, results already exist.\")\n","                continue\n","\n","            topics_for_qid = all_granular_topics[qid]\n","            if not topics_for_qid:\n","                logger.log(f\"    - Skipping {qid}, no granular topics found.\")\n","                all_core_themes[qid] = {}\n","                continue\n","\n","            granular_topic_names = [topic['Name'] for topic in topics_for_qid]\n","            topic_documents = [\" \".join(topic['Representation']) for topic in topics_for_qid]\n","\n","            topic_vectors = vectorize_topics(topic_documents)\n","\n","            # --- THIS IS THE CORRECTED FUNCTION CALL ---\n","            optimal_partition = find_optimal_partition(topic_vectors, P3_MDL_PENALTY_FACTOR, granular_topic_names, qid, logger)\n","\n","            formatted_themes = {}\n","            for theme_id, topic_indices in optimal_partition.items():\n","                theme_vectors_subset = topic_vectors[topic_indices, :]\n","                centroid = np.mean(theme_vectors_subset, axis=0)\n","                distances = np.sum((theme_vectors_subset - centroid)**2, axis=1)\n","                central_topic_idx = topic_indices[np.argmin(distances)]\n","                theme_name = granular_topic_names[central_topic_idx]\n","                constituent_topics = [granular_topic_names[idx] for idx in topic_indices]\n","                formatted_themes[theme_name] = constituent_topics\n","\n","            all_core_themes[qid] = formatted_themes\n","            logger.log(f\"    - Found {len(formatted_themes)} core themes for {qid}.\")\n","\n","            with open(output_file, 'w', encoding='utf-8') as f_out:\n","                json.dump(all_core_themes, f_out, indent=2)\n","            logger.log(f\"    - Progress saved to {output_file}.\")\n","\n","        logger.log(\"--- Phase 3 Complete: MDL core themes identified and saved. ---\")\n","        return True\n","    except Exception as e:\n","        logger.log(f\"CRITICAL ERROR in Phase 3: {e}\\n{traceback.format_exc()}\")\n","        return False\n","\n","# Placeholder for execution - real execution is in the Orchestration Cell\n","print(\"Phase 3 function defined with internal checkpointing.\")"]},{"cell_type":"markdown","metadata":{"id":"g_Gp8Vceog4J"},"source":["## Cell 6: Phase 4 - Quantification and Quote Extraction\n","\n","**Goal:** With the core themes now defined by MDL, this phase quantifies their prevalence. It searches the original text corpus for keywords associated with each theme to count unique respondents and extract supporting quotes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6aCJTN2og4J"},"outputs":[],"source":["def run_phase4_quantify_and_quote(corpus_file, mdl_themes_file, output_file, logger):\n","    \"\"\"\n","    Quantifies themes and extracts quotes based on MDL results.\n","    This version is resumable.\n","    \"\"\"\n","    logger.log(\"--- Starting Phase 4: Quantification & Quote Extraction ---\")\n","    try:\n","        # 1. Load necessary files\n","        logger.log(f\"Loading question corpus from: {corpus_file}\")\n","        with open(corpus_file, 'r', encoding='utf-8') as f:\n","            question_corpus = json.load(f)\n","\n","        logger.log(f\"Loading MDL core themes from: {mdl_themes_file}\")\n","        with open(mdl_themes_file, 'r', encoding='utf-8') as f:\n","            all_core_themes = json.load(f)\n","\n","        # --- RESUME LOGIC START ---\n","        final_analysis = {}\n","        if os.path.exists(output_file):\n","            try:\n","                with open(output_file, 'r', encoding='utf-8') as f_in:\n","                    final_analysis = json.load(f_in)\n","                logger.log(f\"Resuming Phase 4. Found {len(final_analysis)} previously processed questions in {output_file}.\")\n","            except json.JSONDecodeError:\n","                logger.log(f\"WARNING: Could not parse existing output file {output_file}. Starting Phase 4 from scratch.\")\n","                final_analysis = {}\n","        # --- RESUME LOGIC END ---\n","\n","        # 2. Process each question\n","        qids = list(all_core_themes.keys())\n","        for i, qid in enumerate(qids):\n","            logger.log(f\"  > Quantifying themes for QID {qid} ({i+1}/{len(qids)})...\")\n","\n","            # --- SKIP IF ALREADY PROCESSED ---\n","            if qid in final_analysis:\n","                logger.log(f\"    - Skipping {qid}, results already exist.\")\n","                continue\n","\n","            core_themes_for_qid = all_core_themes[qid]\n","            documents_for_qid = question_corpus.get(qid, [])\n","\n","            quantified_themes = []\n","            for core_theme_name, granular_topic_names in core_themes_for_qid.items():\n","                keywords = set(kw for name in granular_topic_names for kw in re.sub(r'^\\d+[_]', '', name).split('_'))\n","\n","                respondent_ids = set()\n","                potential_quotes = []\n","                for doc in documents_for_qid:\n","                    text_lower = doc['text'].lower()\n","                    if any(kw in text_lower for kw in keywords):\n","                        respondent_ids.add(doc['response_id'])\n","                        sentences = nltk.sent_tokenize(doc['text'])\n","                        for sentence in sentences:\n","                            if any(kw in sentence.lower() for kw in keywords):\n","                                potential_quotes.append({\"response_id\": doc['response_id'], \"quote\": sentence.strip()})\n","                                break\n","\n","                unique_quotes = [dict(t) for t in {tuple(d.items()) for d in potential_quotes}]\n","\n","                quantified_themes.append({\n","                    'core_theme_name': core_theme_name,\n","                    'constituent_topics': granular_topic_names,\n","                    'keywords': sorted(list(keywords)),\n","                    'unique_respondent_count': len(respondent_ids),\n","                    'example_quotes': unique_quotes[:P4_NUM_QUOTES_PER_THEME]\n","                })\n","\n","            final_analysis[qid] = sorted(quantified_themes, key=lambda x: x['unique_respondent_count'], reverse=True)\n","\n","            # --- SAVE PROGRESS ---\n","            with open(output_file, 'w', encoding='utf-8') as f_out:\n","                json.dump(final_analysis, f_out, indent=2)\n","\n","        logger.log(\"--- Phase 4 Complete: Final analysis report data generated. ---\")\n","        return True\n","    except Exception as e:\n","        logger.log(f\"CRITICAL ERROR in Phase 4: {e}\\n{traceback.format_exc()}\")\n","        return False\n","\n","# Placeholder for execution\n","print(\"Phase 4 function defined with resume capability.\")"]},{"cell_type":"markdown","metadata":{"id":"cWhOVIDvog4K"},"source":["## Cell 7: Phase 5 - Stakeholder Report Generation\n","**Goal:** Take the final structured JSON data from Phase 4 and format it into a clear, human-readable text report for stakeholders.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dX8nidw6og4L"},"outputs":[],"source":["\n","def run_phase5_generate_report(final_analysis_file, output_txt_file, qid_map_file, logger):\n","    \"\"\"\n","    Generates the final human-readable text report.\n","    \"\"\"\n","    logger.log(\"--- Starting Phase 5: Stakeholder Report Generation ---\")\n","    try:\n","        # 1. Load data\n","        logger.log(f\"Loading final analysis from: {final_analysis_file}\")\n","        with open(final_analysis_file, 'r', encoding='utf-8') as f:\n","            final_analysis = json.load(f)\n","\n","        with open(qid_map_file, 'r', encoding='utf-8') as f:\n","            qid_data = json.load(f)\n","        qid_to_text = {qid: data['question_text'] for qid, data in qid_data.get(\"question_data\", {}).items()}\n","\n","        total_respondents = len({\n","            quote['response_id']\n","            for qid in final_analysis\n","            for theme in final_analysis[qid]\n","            for quote in theme['example_quotes']\n","        })\n","        logger.log(f\"Total unique respondents found in analysis: {total_respondents}\")\n","\n","        # 2. Write report\n","        with open(output_txt_file, 'w', encoding='utf-8') as f_out:\n","            f_out.write(\"MDL-Based Thematic Analysis of Consultation Responses\\n\")\n","            f_out.write(f\"Generated: {time.strftime('%c')}\\n\")\n","            f_out.write(f\"Source Analysis File: {os.path.basename(final_analysis_file)}\\n\")\n","            f_out.write(\"=\"*70 + \"\\n\\n\")\n","\n","            sorted_qids = sorted(final_analysis.keys(), key=lambda q: int(re.sub(r'\\D', '', q)))\n","\n","            for qid in sorted_qids:\n","                question_text = qid_to_text.get(qid, f\"Text for {qid} not found.\")\n","                f_out.write(f\"Question {qid}: {question_text}\\n\\n\")\n","                f_out.write(\"  The most common points made by respondents were:\\n\\n\")\n","\n","                themes = final_analysis[qid]\n","                for i, theme in enumerate(themes):\n","                    theme_name_clean = re.sub(r'^\\d+[_]', '', theme['core_theme_name']).replace('_', ' ').title()\n","                    count = theme['unique_respondent_count']\n","                    percentage = (count / total_respondents * 100) if total_respondents > 0 else 0\n","\n","                    f_out.write(f\"  {i+1}. Theme: [{theme_name_clean}]\\n\")\n","                    f_out.write(f\"     This point was indicated by approx {percentage:.2f}% of respondents (in {count} unique responses).\\n\")\n","\n","                    if theme['example_quotes']:\n","                        f_out.write(\"     Examples include:\\n\")\n","                        for quote_info in theme['example_quotes']:\n","                            f_out.write(f'       â€œ...{quote_info[\"quote\"]}...â€\\n')\n","                    f_out.write(\"\\n\")\n","\n","                f_out.write(\"--- End of Question ---\\n\\n\")\n","\n","        logger.log(f\"--- Phase 5 Complete: Stakeholder report saved to {output_txt_file} ---\")\n","        return True\n","    except Exception as e:\n","        logger.log(f\"CRITICAL ERROR in Phase 5: {e}\\n{traceback.format_exc()}\")\n","        return False\n","\n","# Placeholder for execution\n","print(\"Phase 5 function defined.\")"]},{"cell_type":"markdown","metadata":{"id":"SRSOh3pRog4L"},"source":["\n","## Cell 8: Main Pipeline Orchestration"]},{"cell_type":"markdown","metadata":{"id":"CSJ_7RRkHPrN"},"source":["## Consolidated Workflow Pilot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSOyCGhhoIU9"},"outputs":[],"source":["# ==============================================================================\n","#                 MDL-BASED THEMATIC ANALYSIS PIPELINE (PILOT RUN)\n","#               (All code consolidated into a single executable cell)\n","#                 *** VERSION 8 - FINAL BUG FIX & SEMANTIC VECTORS ***\n","# ==============================================================================\n","# Purpose: This script implements the final, most robust version of the pipeline.\n","#\n","# Change Log (v8):\n","#   - CRITICAL BUG FIX (Phase 2): Fixed an AttributeError by removing an\n","#     unnecessary '.tolist()' call on a variable that was already a list.\n","#     This resolves the crash identified in the last run.\n","# ==============================================================================\n","\n","def run_consolidated_pipeline_v8_final():\n","    # ==============================================================================\n","    #                               SETUP & CONFIGURATION\n","    # ==============================================================================\n","    import json, os, re, sys, time, pandas as pd, numpy as np, torch, traceback, itertools\n","    from collections import defaultdict, Counter\n","    from google.colab import drive\n","    print(\"--- Step 1: Mounting Google Drive ---\")\n","    try: drive.mount('/content/drive', force_remount=True); print(\"Google Drive mounted.\")\n","    except Exception as e: print(f\"ERROR: Drive mount failed: {e}\"); return\n","    print(\"\\n--- Step 2: Installing libraries ---\")\n","    try:\n","        from IPython.utils import io\n","        with io.capture_output() as captured:\n","            !pip install -q transformers bitsandbytes accelerate sentence-transformers bertopic umap-learn hdbscan huggingface_hub scikit-learn nltk\n","        print(\"Libraries installed successfully.\")\n","    except Exception as e: print(f\"ERROR: Library install failed: {e}\"); return\n","    from sentence_transformers import SentenceTransformer\n","    from bertopic import BERTopic\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    from sklearn.metrics.pairwise import euclidean_distances\n","    import nltk\n","    from huggingface_hub import login\n","    from nltk.corpus import stopwords\n","    from nltk.stem import WordNetLemmatizer\n","\n","    print(\"\\n--- Step 3: Downloading NLTK resources ---\")\n","    with io.capture_output() as captured:\n","        nltk.download('punkt', quiet=True); nltk.download('stopwords', quiet=True); nltk.download('wordnet', quiet=True)\n","    print(\"NLTK resources are ready.\")\n","    print(\"\\n--- Step 4: Authenticating ---\")\n","    try:\n","        from google.colab import userdata\n","        HF_TOKEN = userdata.get('HF_TOKEN')\n","        if HF_TOKEN: login(token=HF_TOKEN)\n","        print(\"Hugging Face session authenticated.\")\n","    except: print(\"WARNING: Could not access Colab Secrets.\")\n","\n","    class FileLogger:\n","        def __init__(self, log_file_path: str):\n","            self.log_file_path = log_file_path; log_dir = os.path.dirname(log_file_path)\n","            if log_dir and not os.path.exists(log_dir): os.makedirs(log_dir, exist_ok=True)\n","            with open(self.log_file_path, 'w', encoding='utf-8') as f: f.write(f\"{self._get_timestamp()}: Starting new log\\n\")\n","        def _get_timestamp(self) -> str: return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","        def log(self, message: str):\n","            print(message)\n","            try:\n","                with open(self.log_file_path, 'a', encoding='utf-8') as f: f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","            except Exception as e: print(f\"!!! LOGGER FAILED: {e} !!!\")\n","\n","    BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","    LOG_DIR, DATA_DIR, REPORTS_DIR = [os.path.join(BASE_PROJECT_DIR, d) for d in ['Pipeline_Logs/', 'Pipeline_Data/', 'Pipeline_Reports/']]\n","    for d in [LOG_DIR, DATA_DIR, REPORTS_DIR]: os.makedirs(d, exist_ok=True)\n","    P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json')\n","    P2_GRANULAR_TOPICS_FILE = os.path.join(DATA_DIR, 'p2_granular_topics_FinalPilot_v8.json')\n","    P3_MDL_CORE_THEMES_FILE = os.path.join(DATA_DIR, 'p3_mdl_core_themes_FinalPilot_v8.json')\n","    P4_FINAL_ANALYSIS_FILE = os.path.join(DATA_DIR, 'p4_final_analysis_report_FinalPilot_v8.json')\n","    P5_STAKEHOLDER_REPORT_FILE = os.path.join(REPORTS_DIR, f'Stakeholder_Report_FinalPilot_v8_{time.strftime(\"%Y%m%d\")}.txt')\n","    QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","    P2_BERTOPIC_MIN_TOPIC_SIZE = 3\n","    P4_NUM_QUOTES_PER_THEME = 3\n","    P4_NUM_KEYWORDS_PER_THEME = 7\n","    PILOT_MODE_QUESTIONS = ['Q5', 'Q3']\n","    QUESTIONS_TO_EXCLUDE = ['Q32', 'Q33', 'Q34', 'Q35', 'Q36']\n","    master_logger = FileLogger(os.path.join(LOG_DIR, f'pipeline_master_FinalPilot_v8_{time.strftime(\"%Y%m%d_%H%M%S\")}.log'))\n","    print(\"\\n--- Step 5: Setup and configuration complete ---\")\n","\n","    def preprocess_text(text, custom_stopwords):\n","        text = text.lower(); text = re.sub(r'[^\\w\\s]', '', text); tokens = text.split()\n","        stop_words = set(stopwords.words('english'))\n","        tokens = [t for t in tokens if t not in stop_words and t not in custom_stopwords]\n","        lemmatizer = WordNetLemmatizer()\n","        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n","        return \" \".join(tokens)\n","\n","    def run_phase2_v8(corpus_file, output_file, logger):\n","        logger.log(\"--- Starting Phase 2: Granular Topic Discovery ---\")\n","        try:\n","            with open(corpus_file, 'r', encoding='utf-8') as f: question_corpus = json.load(f)\n","            model = BERTopic(embedding_model=SentenceTransformer('all-MiniLM-L6-v2'), min_topic_size=P2_BERTOPIC_MIN_TOPIC_SIZE, verbose=False)\n","            custom_stopwords = set(['proposal', 'act', 'privacy', 'data', 'information', 'rights', 'submission', 'response'])\n","            output_data = {}\n","            if os.path.exists(output_file):\n","                logger.log(f\"Output file {output_file} already exists. To re-run, please delete it first.\"); return True\n","            for qid, docs_data in question_corpus.items():\n","                logger.log(f\"  > Processing QID {qid}...\")\n","                docs = [item['text'] for item in docs_data]\n","                if len(docs) < P2_BERTOPIC_MIN_TOPIC_SIZE:\n","                    output_data[qid] = {'topics_info': [], 'doc_assignments': []}; continue\n","                processed_docs = [preprocess_text(doc, custom_stopwords) for doc in docs]\n","                doc_assignments, _ = model.fit_transform(processed_docs)\n","                topic_info = model.get_topic_info()\n","                output_data[qid] = {\n","                    'topics_info': topic_info[topic_info['Topic'] != -1].to_dict('records'),\n","                    ### --- BUG FIX (v8) --- ###\n","                    # doc_assignments is already a list, so the unnecessary .tolist() call is removed.\n","                    'doc_assignments': doc_assignments\n","                }\n","                logger.log(f\"    - Found {len(output_data[qid]['topics_info'])} granular topics for {qid}.\")\n","            with open(output_file, 'w', encoding='utf-8') as f: json.dump(output_data, f, indent=2)\n","            logger.log(\"--- Phase 2 Complete ---\"); return True\n","        except Exception as e:\n","            logger.log(f\"CRITICAL ERROR in Phase 2: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    class KStarMeans:\n","        def __init__(self, patience=10, max_iter=300, n_init_split=5):\n","            self.patience = patience; self.max_iter = max_iter; self.n_init_split = n_init_split\n","            self.centroids_, self.labels_ = None, None; self.m_precision = 64\n","        def _mdl_cost(self, X, centroids, assignments):\n","            N, d = X.shape; k = len(centroids)\n","            if k == 0 or (N > 0 and len(np.unique(assignments)) != k): return np.inf\n","            modelcost = k * d * self.m_precision\n","            idxcost = N * np.log(k) if k > 1 else 0.0\n","            sse = np.sum((X - centroids[assignments])**2) if N > 0 else 0.0\n","            residualcost = (N * d * np.log(2 * np.pi) + sse) / 2.0\n","            return modelcost + idxcost + residualcost\n","        def _init_subcentroids(self, X_cluster):\n","            if len(X_cluster) < 2: return np.array([X_cluster[0], X_cluster[0]]) if len(X_cluster) == 1 else np.empty((2, X_cluster.shape[1]))\n","            sub_centroids = np.empty((2, X_cluster.shape[1]))\n","            idx1 = np.random.choice(len(X_cluster)); sub_centroids[0] = X_cluster[idx1]\n","            dist_sq = euclidean_distances(X_cluster, sub_centroids[0:1]).flatten()**2\n","            idx2 = np.random.choice(len(X_cluster), p=dist_sq/np.sum(dist_sq)) if np.sum(dist_sq) > 0 else (idx1 + 1) % len(X_cluster)\n","            sub_centroids[1] = X_cluster[idx2]; return sub_centroids\n","        def _kmeans_step(self, X, centroids):\n","            if len(centroids) == 0: return centroids, np.zeros(X.shape[0], dtype=int)\n","            assignments = np.argmin(euclidean_distances(X, centroids), axis=1)\n","            unique_labels = np.unique(assignments)\n","            new_centroids = np.array([X[assignments == i].mean(axis=0) for i in unique_labels])\n","            if len(unique_labels) < len(centroids):\n","                map_labels = {old: new for new, old in enumerate(unique_labels)}\n","                assignments = np.array([map_labels[i] for i in assignments])\n","            return new_centroids, assignments\n","        def _find_best_split(self, X, centroids, assignments):\n","            N, d = X.shape; k = len(centroids); best_delta_mdl, best_split_candidate = 0, None\n","            for i in range(k):\n","                cluster_points = X[assignments == i]\n","                if len(cluster_points) < 2: continue\n","                best_sse_reduction, best_sub_centroids = -1, None\n","                for _ in range(self.n_init_split):\n","                    sub_centroids_init = self._init_subcentroids(cluster_points)\n","                    sse_before = np.sum((cluster_points - centroids[i])**2)\n","                    sub_assign = np.argmin(euclidean_distances(cluster_points, sub_centroids_init), axis=1)\n","                    sse_after = sum(np.sum((cluster_points[sub_assign == l] - sub_centroids_init[l])**2) for l in range(2) if np.any(sub_assign == l))\n","                    if (sse_before - sse_after) > best_sse_reduction:\n","                        best_sse_reduction = sse_before - sse_after; best_sub_centroids = sub_centroids_init\n","                delta_mdl = (d * self.m_precision) + (N * np.log((k + 1)/k)) - (best_sse_reduction / 2.0)\n","                if delta_mdl < best_delta_mdl:\n","                    best_delta_mdl = delta_mdl; temp_centroids = np.delete(centroids, i, axis=0)\n","                    best_split_candidate = np.vstack([temp_centroids, best_sub_centroids])\n","            return best_split_candidate\n","        def _find_best_merge(self, X, centroids, assignments):\n","            N, d = X.shape; k = len(centroids); best_delta_mdl, best_merge_candidate = 0, None\n","            if k < 2: return None\n","            for idx1, idx2 in itertools.combinations(range(k), 2):\n","                p1 = X[assignments == idx1]; p2 = X[assignments == idx2]\n","                if len(p1) == 0 or len(p2) == 0: continue\n","                sse_before = np.sum((p1 - centroids[idx1])**2) + np.sum((p2 - centroids[idx2])**2)\n","                merged_points = np.vstack([p1, p2]); merged_centroid = np.mean(merged_points, axis=0)\n","                sse_after = np.sum((merged_points - merged_centroid)**2)\n","                delta_mdl = (-d * self.m_precision) + (N * np.log((k - 1)/k)) + ((sse_after - sse_before) / 2.0)\n","                if delta_mdl < best_delta_mdl:\n","                    best_delta_mdl = delta_mdl; temp_centroids = np.delete(centroids, sorted([idx1, idx2]), axis=0)\n","                    best_merge_candidate = np.vstack([temp_centroids, [merged_centroid]])\n","            return best_merge_candidate\n","        def fit(self, X):\n","            if X.shape[0] == 0: self.centroids_, self.labels_ = np.array([]), np.array([]); return self\n","            centroids, assignments = self._kmeans_step(X, np.array([X.mean(axis=0)]))\n","            best_cost = self._mdl_cost(X, centroids, assignments)\n","            best_centroids, best_assignments = np.copy(centroids), np.copy(assignments)\n","            unimproved_count = 0\n","            for i in range(self.max_iter):\n","                current_centroids, current_assignments = np.copy(best_centroids), np.copy(best_assignments)\n","                split_candidate = self._find_best_split(X, current_centroids, current_assignments)\n","                merge_candidate = self._find_best_merge(X, current_centroids, current_assignments)\n","                split_cost = self._mdl_cost(X, *self._kmeans_step(X, split_candidate)) if split_candidate is not None else np.inf\n","                merge_cost = self._mdl_cost(X, *self._kmeans_step(X, merge_candidate)) if merge_candidate is not None else np.inf\n","                kmeans_cost = self._mdl_cost(X, *self._kmeans_step(X, current_centroids))\n","                costs = [split_cost, merge_cost, kmeans_cost]; candidates = [split_candidate, merge_candidate, current_centroids]\n","                best_action_idx = np.argmin(costs)\n","                if candidates[best_action_idx] is None:\n","                    unimproved_count += 1\n","                    if unimproved_count >= self.patience: break\n","                    continue\n","                next_centroids, next_assignments = self._kmeans_step(X, candidates[best_action_idx])\n","                new_cost = self._mdl_cost(X, next_centroids, next_assignments)\n","                if new_cost < best_cost:\n","                    best_cost, best_centroids, best_assignments = new_cost, np.copy(next_centroids), np.copy(next_assignments)\n","                    unimproved_count = 0\n","                else:\n","                    unimproved_count += 1\n","                    if unimproved_count >= self.patience: break\n","            self.centroids_, self.labels_ = best_centroids, best_assignments; return self\n","\n","    def run_phase3_v7_semantic(corpus_file, granular_topics_file, output_file, logger):\n","        logger.log(\"--- Starting Phase 3 (v7): K*-Means on Semantic Vectors ---\")\n","        try:\n","            with open(corpus_file, 'r', encoding='utf-8') as f: question_corpus = json.load(f)\n","            with open(granular_topics_file, 'r', encoding='utf-8') as f: granular_topics_data = json.load(f)\n","            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n","            all_core_themes = {}\n","            if os.path.exists(output_file):\n","                logger.log(f\"Output file {output_file} already exists. To re-run, please delete it first.\"); return True\n","            for qid, data in granular_topics_data.items():\n","                logger.log(f\"  > Processing QID {qid}...\")\n","                topics_info, doc_assignments = data['topics_info'], data['doc_assignments']\n","                if not topics_info: all_core_themes[qid] = {}; continue\n","                original_docs = [item['text'] for item in question_corpus.get(qid, [])]\n","                logger.log(f\"    - Generating embeddings for {len(original_docs)} documents...\")\n","                doc_embeddings = embedding_model.encode(original_docs, show_progress_bar=True)\n","                logger.log(\"    - Calculating theme vectors from document embeddings...\")\n","                topic_ids = [info['Topic'] for info in topics_info]\n","                theme_vectors, valid_topic_info = [], []\n","                for topic_id, topic_info_item in zip(topic_ids, topics_info):\n","                    doc_indices = [i for i, assigned_topic in enumerate(doc_assignments) if assigned_topic == topic_id]\n","                    if doc_indices:\n","                        theme_vectors.append(np.mean(doc_embeddings[doc_indices], axis=0))\n","                        valid_topic_info.append(topic_info_item)\n","                if not theme_vectors: all_core_themes[qid] = {}; continue\n","                theme_vectors = np.array(theme_vectors)\n","                logger.log(f\"    - Running K*-Means on {len(theme_vectors)} theme vectors...\")\n","                k_star_model = KStarMeans().fit(theme_vectors)\n","                final_assignments, optimal_k = k_star_model.labels_, len(k_star_model.centroids_)\n","                logger.log(f\"    - K*-Means found an optimal k* of {optimal_k} themes.\")\n","                final_partition = defaultdict(list)\n","                for point_idx, cluster_idx in enumerate(final_assignments): final_partition[cluster_idx].append(point_idx)\n","                formatted_themes = {}\n","                for _, inds in final_partition.items():\n","                    if not inds: continue\n","                    cluster_centroid = np.mean(theme_vectors[inds], axis=0)\n","                    central_topic_idx_in_inds = np.argmin(euclidean_distances(theme_vectors[inds], [cluster_centroid]))\n","                    original_topic_idx = inds[central_topic_idx_in_inds]\n","                    theme_name = valid_topic_info[original_topic_idx]['Name']\n","                    formatted_themes[theme_name] = [valid_topic_info[i] for i in inds]\n","                all_core_themes[qid] = formatted_themes\n","            with open(output_file, 'w', encoding='utf-8') as f: json.dump(all_core_themes, f, indent=2)\n","            logger.log(\"--- Phase 3 Complete ---\"); return True\n","        except Exception as e:\n","            logger.log(f\"CRITICAL ERROR in Phase 3: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    def run_phase4(corpus_file, mdl_themes_file, output_file, logger):\n","        logger.log(\"--- Starting Phase 4: Quantification (v4) ---\")\n","        try:\n","            with open(corpus_file, 'r', encoding='utf-8') as f: question_corpus = json.load(f)\n","            with open(mdl_themes_file, 'r', encoding='utf-8') as f: all_core_themes = json.load(f)\n","            final_analysis = {}\n","            if os.path.exists(output_file):\n","                logger.log(f\"Output file {output_file} already exists. To re-run, please delete it first.\"); return True\n","            for qid, core_themes in all_core_themes.items():\n","                logger.log(f\"  > Quantifying themes for QID {qid}...\")\n","                docs_for_qid = question_corpus.get(qid, [])\n","                quant_themes = []\n","                for core_theme_name, topics_data in core_themes.items():\n","                    all_words = [word for topic in topics_data for word in topic['Representation']]\n","                    keywords = {word for word, count in Counter(all_words).most_common(P4_NUM_KEYWORDS_PER_THEME)}\n","                    if not keywords: continue\n","                    resp_ids, quotes = set(), []\n","                    for doc in docs_for_qid:\n","                        text_lower = ' ' + doc['text'].lower() + ' '\n","                        if any(f' {kw} ' in text_lower for kw in keywords):\n","                            resp_ids.add(doc['response_id'])\n","                            for sent in nltk.sent_tokenize(doc['text']):\n","                                if any(kw in sent.lower() for kw in keywords):\n","                                    quotes.append({\"response_id\": doc['response_id'], \"quote\": sent.strip()}); break\n","                    unique_quotes = [dict(t) for t in {tuple(d.items()) for d in quotes}]\n","                    quant_themes.append({'core_theme_name': core_theme_name, 'keywords': sorted(list(keywords)), 'unique_respondent_count': len(resp_ids), 'example_quotes': unique_quotes[:P4_NUM_QUOTES_PER_THEME]})\n","                final_analysis[qid] = sorted(quant_themes, key=lambda x: x['unique_respondent_count'], reverse=True)\n","            with open(output_file, 'w', encoding='utf-8') as f: json.dump(final_analysis, f, indent=2)\n","            logger.log(\"--- Phase 4 Complete ---\"); return True\n","        except Exception as e:\n","            logger.log(f\"CRITICAL ERROR in Phase 4: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    def run_phase5(final_analysis_file, corpus_file, output_txt_file, qid_map_file, logger):\n","        logger.log(\"--- Starting Phase 5: Stakeholder Report Generation ---\")\n","        try:\n","            with open(final_analysis_file, 'r', encoding='utf-8') as f: final_analysis = json.load(f)\n","            with open(corpus_file, 'r', encoding='utf-8') as f: question_corpus = json.load(f)\n","            with open(qid_map_file, 'r', encoding='utf-8') as f: qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","            with open(output_txt_file, 'w', encoding='utf-8') as f_out:\n","                f_out.write(f\"MDL-Based Thematic Analysis (Final Pilot Run v8 - Semantic K*-Means)\\nGenerated: {time.strftime('%c')}\\n\" + \"=\"*70 + \"\\n\\n\")\n","                for qid in sorted(final_analysis.keys(), key=lambda q: int(re.sub(r'\\D', '', q))):\n","                    f_out.write(f\"Question {qid}: {qid_to_text.get(qid, 'N/A')}\\n\\n\")\n","                    total_resp = len({doc['response_id'] for doc in question_corpus.get(qid, [])})\n","                    logger.log(f\"  - QID {qid}: Found {total_resp} unique respondents.\")\n","                    themes = final_analysis.get(qid, [])\n","                    if not themes: f_out.write(\"  No significant recurring themes were identified.\\n\\n\"); continue\n","                    f_out.write(\"  The most common points made by respondents were:\\n\\n\")\n","                    for i, theme in enumerate(themes):\n","                        name = re.sub(r'^\\d+[_]', '', theme['core_theme_name']).replace('_', ' ').title()\n","                        count = theme['unique_respondent_count']\n","                        perc = (count / total_resp * 100) if total_resp > 0 else 0\n","                        f_out.write(f\"  {i+1}. Theme: [{name}]\\n\")\n","                        f_out.write(f\"     Indicated by approx {perc:.2f}% of respondents ({count} of {total_resp} total).\\n\")\n","                        f_out.write(f\"     Keywords: {theme.get('keywords', [])}\\n\")\n","                        if theme.get('example_quotes'):\n","                            f_out.write(\"     Examples include:\\n\")\n","                            for q_info in theme['example_quotes']: f_out.write(f'       â€œ...{q_info[\"quote\"]}...â€\\n')\n","                        f_out.write(\"\\n\")\n","                    f_out.write(\"---\\n\\n\")\n","            logger.log(\"--- Phase 5 Complete ---\"); return True\n","        except Exception as e:\n","            logger.log(f\"CRITICAL ERROR in Phase 5: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    master_logger.log(\"======== STARTING MDL THEMATIC ANALYSIS PIPELINE (FINAL PILOT V8) ========\")\n","    try:\n","        with open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8') as f: full_corpus = json.load(f)\n","    except FileNotFoundError:\n","        master_logger.log(f\"CRITICAL: Main corpus file ({P1_QUESTION_CORPUS_FILE}) is missing.\"); return\n","\n","    qids_to_process = [qid for qid in full_corpus.keys() if qid not in QUESTIONS_TO_EXCLUDE and qid in PILOT_MODE_QUESTIONS]\n","    if not qids_to_process:\n","        master_logger.log(\"No questions to process for the pilot. Halting.\"); return\n","    master_logger.log(f\"Pilot will process: {qids_to_process}\")\n","    pilot_corpus = {qid: full_corpus[qid] for qid in qids_to_process}\n","    TEMP_PILOT_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_temp_pilot_corpus.json')\n","    with open(TEMP_PILOT_CORPUS_FILE, 'w', encoding='utf-8') as f: json.dump(pilot_corpus, f, indent=2)\n","\n","    if not run_phase2_v8(TEMP_PILOT_CORPUS_FILE, P2_GRANULAR_TOPICS_FILE, master_logger):\n","        master_logger.log(\"CRITICAL: Phase 2 failed.\"); return\n","    if not run_phase3_v7_semantic(TEMP_PILOT_CORPUS_FILE, P2_GRANULAR_TOPICS_FILE, P3_MDL_CORE_THEMES_FILE, master_logger):\n","        master_logger.log(\"CRITICAL: Phase 3 failed.\"); return\n","    if not run_phase4(TEMP_PILOT_CORPUS_FILE, P3_MDL_CORE_THEMES_FILE, P4_FINAL_ANALYSIS_FILE, master_logger):\n","        master_logger.log(\"CRITICAL: Phase 4 failed.\"); return\n","    if not run_phase5(P4_FINAL_ANALYSIS_FILE, TEMP_PILOT_CORPUS_FILE, P5_STAKEHOLDER_REPORT_FILE, QUESTION_DEFINITIONS_FILE, master_logger):\n","        master_logger.log(\"CRITICAL: Phase 5 failed.\"); return\n","\n","    master_logger.log(f\"======== FINAL PILOT PIPELINE FINISHED SUCCESSFULLY ========\")\n","    master_logger.log(f\"Please review the report at: {P5_STAKEHOLDER_REPORT_FILE}\")\n","    os.remove(TEMP_PILOT_CORPUS_FILE)\n","\n","# ==============================================================================\n","#                      EXECUTE\n","# ==============================================================================\n","run_consolidated_pipeline_v8_final()"]},{"cell_type":"markdown","metadata":{"id":"G-7IMEJcxqBy"},"source":["# 30th Aug"]},{"cell_type":"markdown","metadata":{"id":"540_H-6exzH6"},"source":["## Consolidated flow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QdY5HxxrtDq"},"outputs":[],"source":["# ==============================================================================\n","#                 MDL-BASED THEMATIC ANALYSIS PIPELINE (PILOT RUN)\n","#               (All code consolidated into a single executable cell)\n","#                           *** VERSION 2 - BUG FIXED ***\n","# ==============================================================================\n","# Purpose: This script executes a pilot run of the corrected thematic analysis\n","#          pipeline on questions Q19 & Q3.\n","#\n","# Change Log (v2):\n","#   - FIXED a critical bug where the MDL checkpointing function could not access\n","#     the correct LOG_DIR, preventing the save/resume feature from working.\n","#     The LOG_DIR is now passed directly to the function.\n","# ==============================================================================\n","\n","def run_consolidated_pipeline_v2():\n","    # ==============================================================================\n","    #                               SETUP & CONFIGURATION\n","    # ==============================================================================\n","    import json\n","    import os\n","    import re\n","    import sys\n","    import time\n","    import pandas as pd\n","    import numpy as np\n","    from collections import defaultdict\n","    from pathlib import Path\n","    from typing import Dict\n","    from google.colab import drive\n","\n","    print(\"--- Step 1: Mounting Google Drive ---\")\n","    try:\n","        drive.mount('/content/drive', force_remount=True)\n","        print(\"Google Drive mounted successfully.\")\n","    except Exception as e:\n","        print(f\"ERROR: Failed to mount Google Drive. Error: {e}\")\n","        return\n","\n","    print(\"\\n--- Step 2: Installing necessary libraries ---\")\n","    try:\n","        from IPython.utils import io\n","        with io.capture_output() as captured:\n","            !pip install -q transformers bitsandbytes accelerate sentence-transformers bertopic umap-learn hdbscan huggingface_hub scikit-learn nltk\n","        print(\"Libraries installed successfully.\")\n","    except Exception as e:\n","        print(f\"ERROR: Failed to install libraries. Error: {e}\")\n","        return\n","\n","    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","    from sentence_transformers import SentenceTransformer, util\n","    from bertopic import BERTopic\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    import nltk\n","    import torch\n","    import traceback\n","    from huggingface_hub import login\n","    from nltk.corpus import stopwords\n","    from nltk.stem import WordNetLemmatizer\n","\n","    print(\"\\n--- Step 3: Downloading NLTK resources ---\")\n","    with io.capture_output() as captured:\n","        nltk.download('punkt', quiet=True)\n","        nltk.download('stopwords', quiet=True)\n","        nltk.download('wordnet', quiet=True)\n","        nltk.download('punkt_tab', quiet=True)\n","    print(\"NLTK resources are ready.\")\n","\n","    print(\"\\n--- Step 4: Authenticating with Hugging Face ---\")\n","    try:\n","        from google.colab import userdata\n","        HF_TOKEN = userdata.get('HF_TOKEN')\n","        if HF_TOKEN:\n","            login(token=HF_TOKEN)\n","            print(\"Hugging Face session authenticated successfully.\")\n","        else:\n","            print(\"WARNING: Hugging Face token not found in Colab Secrets.\")\n","    except (ImportError, KeyError):\n","        print(\"WARNING: Could not access Colab Secrets.\")\n","\n","    # ==============================================================================\n","    #                          FileLogger Class Definition\n","    # ==============================================================================\n","    class FileLogger:\n","        def __init__(self, log_file_path: str):\n","            self.log_file_path = log_file_path\n","            log_dir = os.path.dirname(log_file_path)\n","            if log_dir and not os.path.exists(log_dir):\n","                os.makedirs(log_dir, exist_ok=True)\n","            with open(self.log_file_path, 'w', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: Starting new log at {self.log_file_path}\\n\")\n","        def _get_timestamp(self) -> str:\n","            return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","        def log(self, message: str):\n","            print(message)\n","            try:\n","                with open(self.log_file_path, 'a', encoding='utf-8') as f:\n","                    f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","            except Exception as e:\n","                print(f\"!!! LOGGER FAILED: {e} !!!\\n{self._get_timestamp()}: {message}\")\n","\n","    # ==============================================================================\n","    #                            PIPELINE CONFIGURATION\n","    # ==============================================================================\n","    BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","    LOG_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Logs/')\n","    DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Data/')\n","    REPORTS_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Reports/')\n","    for d in [LOG_DIR, DATA_DIR, REPORTS_DIR]:\n","        os.makedirs(d, exist_ok=True)\n","\n","    QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","    P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json')\n","    P2_GRANULAR_TOPICS_FILE = os.path.join(DATA_DIR, 'p2_granular_topics_Pilot.json')\n","    P3_MDL_CORE_THEMES_FILE = os.path.join(DATA_DIR, 'p3_mdl_core_themes_Pilot.json')\n","    P4_FINAL_ANALYSIS_FILE = os.path.join(DATA_DIR, 'p4_final_analysis_report_Pilot.json')\n","    P5_STAKEHOLDER_REPORT_FILE = os.path.join(REPORTS_DIR, f'Stakeholder_Report_Pilot_{time.strftime(\"%Y%m%d\")}.txt')\n","\n","    P2_BERTOPIC_MIN_TOPIC_SIZE = 3\n","    P3_MDL_PENALTY_FACTOR = 1.5\n","    P4_NUM_QUOTES_PER_THEME = 3\n","    P4_NUM_KEYWORDS_PER_THEME = 5\n","\n","    PILOT_MODE_QUESTIONS = ['Q19', 'Q3']\n","    QUESTIONS_TO_EXCLUDE = ['Q32', 'Q33', 'Q34', 'Q35', 'Q36']\n","\n","    master_logger = FileLogger(os.path.join(LOG_DIR, f'pipeline_master_PILOT_{time.strftime(\"%Y%m%d_%H%M%S\")}.log'))\n","    print(\"\\n--- Step 5: Setup and configuration complete ---\")\n","\n","    # ==============================================================================\n","    #                             PHASE 2 FUNCTION (Modified)\n","    # ==============================================================================\n","    def preprocess_text(text, custom_stopwords):\n","        text = text.lower()\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","        tokens = text.split()\n","        stop_words = set(stopwords.words('english'))\n","        tokens = [token for token in tokens if token not in stop_words]\n","        tokens = [token for token in tokens if token not in custom_stopwords]\n","        lemmatizer = WordNetLemmatizer()\n","        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","        return \" \".join(tokens)\n","\n","    def run_phase2_discover_granular_topics(corpus_file, output_file, logger):\n","        logger.log(\"--- Starting Phase 2: Granular Topic Discovery ---\")\n","        try:\n","            with open(corpus_file, 'r', encoding='utf-8') as f: question_corpus = json.load(f)\n","            logger.log(\"Initializing BERTopic model...\")\n","            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n","            topic_model = BERTopic(embedding_model=embedding_model, min_topic_size=P2_BERTOPIC_MIN_TOPIC_SIZE, verbose=False)\n","            custom_stopwords = set(['proposal', 'act', 'privacy', 'data', 'information', 'rights', 'submission', 'response'])\n","            logger.log(f\"Using custom stopwords: {sorted(list(custom_stopwords))}\")\n","            all_granular_topics = {}\n","            if os.path.exists(output_file):\n","                logger.log(f\"Output file {output_file} already exists. To re-run, please delete it first.\")\n","                return True\n","            for qid, documents_data in question_corpus.items():\n","                logger.log(f\"  > Processing QID {qid}...\")\n","                documents = [item['text'] for item in documents_data]\n","                if len(documents) < P2_BERTOPIC_MIN_TOPIC_SIZE:\n","                    logger.log(f\"    - Skipping {qid}, not enough documents ({len(documents)}).\")\n","                    all_granular_topics[qid] = []\n","                    continue\n","                processed_documents = [preprocess_text(doc, custom_stopwords) for doc in documents]\n","                topics, _ = topic_model.fit_transform(processed_documents)\n","                topic_info = topic_model.get_topic_info()\n","                granular_topics_for_qid = topic_info[topic_info['Topic'] != -1]\n","                all_granular_topics[qid] = granular_topics_for_qid[['Name', 'Representation']].to_dict('records')\n","                logger.log(f\"    - Found {len(granular_topics_for_qid)} granular topics for {qid}.\")\n","            with open(output_file, 'w', encoding='utf-8') as f_out: json.dump(all_granular_topics, f_out, indent=2)\n","            logger.log(\"--- Phase 2 Complete ---\")\n","            return True\n","        except Exception as e:\n","            logger.log(f\"CRITICAL ERROR in Phase 2: {e}\\n{traceback.format_exc()}\")\n","            return False\n","\n","    # ==============================================================================\n","    #                             PHASE 3 FUNCTION (BUG FIXED)\n","    # ==============================================================================\n","    def vectorize_topics_p3(documents):\n","        vectorizer = TfidfVectorizer(use_idf=False, norm='l1')\n","        return vectorizer.fit_transform(documents).toarray()\n","    def calculate_cluster_cost_p3(topic_vectors_subset):\n","        if topic_vectors_subset.shape[0] < 2: return 0\n","        centroid = np.mean(topic_vectors_subset, axis=0)\n","        sse = np.sum((topic_vectors_subset - centroid)**2)\n","        return 0.5 * topic_vectors_subset.shape[0] * np.log(sse + 1e-9)\n","    def calculate_mdl_cost_p3(partition, topic_vectors, penalty_factor):\n","        n_points, k_clusters = topic_vectors.shape[0], len(partition)\n","        if k_clusters == 0: return float('inf')\n","        l_model = n_points * np.log(k_clusters)\n","        l_data = sum(calculate_cluster_cost_p3(topic_vectors[indices, :]) for indices in partition.values() if indices)\n","        return l_model + (penalty_factor * l_data)\n","\n","    ### --- BUG FIX --- ###\n","    # The find_optimal_partition function now explicitly accepts 'log_dir' as an argument.\n","    def find_optimal_partition_p3(topic_vectors, penalty_factor, granular_topic_names, qid, logger, log_dir):\n","        state_file = os.path.join(log_dir, f'p3_mdl_state_{qid}.json')\n","        partition, best_partition, best_cost, completed_iterations = None, None, float('inf'), 0\n","        if os.path.exists(state_file):\n","            try:\n","                with open(state_file, 'r') as f: state = json.load(f)\n","                partition = {int(k): v for k, v in state['partition'].items()}\n","                best_partition = {int(k): v for k, v in state['best_partition'].items()}\n","                best_cost, completed_iterations = state['best_cost'], state['completed_iterations']\n","                logger.log(f\"    - RESUMING MDL clustering for {qid} from iteration {completed_iterations + 1}.\")\n","            except Exception as e:\n","                logger.log(f\"    - WARNING: Could not load state file {state_file}. Starting from scratch. Error: {e}\")\n","                partition = None\n","        if partition is None:\n","            partition = {i: [i] for i in range(len(granular_topic_names))}\n","            cost = calculate_mdl_cost_p3(partition, topic_vectors, penalty_factor)\n","            best_partition, best_cost, completed_iterations = partition, cost, 0\n","            logger.log(f\"    - Initializing MDL clustering for {qid}. Initial Cost: {best_cost:.2f}, {len(partition)} themes.\")\n","\n","        total_merges = len(granular_topic_names) - 1\n","        for i in range(completed_iterations, total_merges):\n","            current_iteration_num = i + 1\n","            if len(partition) < 2: break\n","            best_merge, lowest_merge_cost = None, float('inf')\n","            themes_to_merge = list(partition.keys())\n","            for j in range(len(themes_to_merge)):\n","                for k in range(j + 1, len(themes_to_merge)):\n","                    t1, t2 = themes_to_merge[j], themes_to_merge[k]\n","                    temp_partition = {key: val[:] for key, val in partition.items()}\n","                    temp_partition[t1].extend(temp_partition.pop(t2))\n","                    merge_cost = calculate_mdl_cost_p3(temp_partition, topic_vectors, penalty_factor)\n","                    if merge_cost < lowest_merge_cost:\n","                        lowest_merge_cost, best_merge = merge_cost, (t1, t2)\n","            if best_merge:\n","                t1, t2 = best_merge\n","                partition[t1].extend(partition.pop(t2))\n","                if lowest_merge_cost < best_cost:\n","                    best_cost = lowest_merge_cost\n","                    best_partition = {key: val[:] for key, val in partition.items()}\n","                    logger.log(f\"    - New best state: {len(best_partition)} themes, Cost: {best_cost:.2f}\")\n","                if current_iteration_num % 20 == 0 and current_iteration_num < total_merges:\n","                    state_to_save = {'partition': partition, 'best_partition': best_partition, 'best_cost': best_cost, 'completed_iterations': current_iteration_num}\n","                    with open(state_file, 'w') as f: json.dump(state_to_save, f)\n","                    logger.log(f\"      >>> CHECKPOINT SAVED at iteration {current_iteration_num} for {qid}. <<<\")\n","            else: break\n","        if os.path.exists(state_file):\n","            os.remove(state_file)\n","            logger.log(f\"    - Clustering for {qid} complete. Removed temporary state file.\")\n","        return best_partition\n","\n","    def run_phase3_mdl_clustering(granular_topics_file, output_file, logger, log_dir): # Added log_dir\n","        logger.log(\"--- Starting Phase 3: MDL Core Theme Clustering ---\")\n","        try:\n","            with open(granular_topics_file, 'r', encoding='utf-8') as f: all_granular_topics = json.load(f)\n","            all_core_themes = {}\n","            if os.path.exists(output_file):\n","                logger.log(f\"Output file {output_file} already exists. To re-run, please delete it first.\")\n","                return True\n","            for qid, topics_for_qid in all_granular_topics.items():\n","                logger.log(f\"  > Clustering themes for QID {qid}...\")\n","                if not topics_for_qid:\n","                    all_core_themes[qid] = {}\n","                    continue\n","                granular_topic_names = [topic['Name'] for topic in topics_for_qid]\n","                topic_documents = [\" \".join(topic['Representation']) for topic in topics_for_qid]\n","                topic_vectors = vectorize_topics_p3(topic_documents)\n","                ### --- BUG FIX --- ###\n","                # LOG_DIR is now passed to the function call.\n","                optimal_partition = find_optimal_partition_p3(topic_vectors, P3_MDL_PENALTY_FACTOR, granular_topic_names, qid, logger, log_dir)\n","                formatted_themes = {}\n","                for theme_id, topic_indices in optimal_partition.items():\n","                    central_topic_idx = topic_indices[np.argmin(np.sum((topic_vectors[topic_indices, :] - np.mean(topic_vectors[topic_indices, :], axis=0))**2, axis=1))]\n","                    theme_name = granular_topic_names[central_topic_idx]\n","                    formatted_themes[theme_name] = [granular_topic_names[idx] for idx in topic_indices]\n","                all_core_themes[qid] = formatted_themes\n","                logger.log(f\"    - Found {len(formatted_themes)} core themes for {qid}.\")\n","            with open(output_file, 'w', encoding='utf-8') as f_out: json.dump(all_core_themes, f_out, indent=2)\n","            logger.log(\"--- Phase 3 Complete ---\")\n","            return True\n","        except Exception as e:\n","            logger.log(f\"CRITICAL ERROR in Phase 3: {e}\\n{traceback.format_exc()}\")\n","            return False\n","\n","    # ==============================================================================\n","    #                             PHASE 4 FUNCTION (Modified)\n","    # ==============================================================================\n","    def run_phase4_quantify_and_quote(corpus_file, mdl_themes_file, output_file, logger):\n","        logger.log(\"--- Starting Phase 4: Quantification & Quote Extraction ---\")\n","        try:\n","            with open(corpus_file, 'r', encoding='utf-8') as f: question_corpus = json.load(f)\n","            with open(mdl_themes_file, 'r', encoding='utf-8') as f: all_core_themes = json.load(f)\n","            final_analysis = {}\n","            if os.path.exists(output_file):\n","                logger.log(f\"Output file {output_file} already exists. To re-run, please delete it first.\")\n","                return True\n","            for qid, core_themes_for_qid in all_core_themes.items():\n","                logger.log(f\"  > Quantifying themes for QID {qid}...\")\n","                documents_for_qid = question_corpus.get(qid, [])\n","                quantified_themes = []\n","                for core_theme_name, granular_topic_names in core_themes_for_qid.items():\n","                    temp_keywords = set(kw for name in granular_topic_names for kw in re.sub(r'^\\d+[_]', '', name).split('_'))\n","                    theme_corpus = [doc['text'] for doc in documents_for_qid if any(kw in doc['text'].lower() for kw in temp_keywords)]\n","                    if not theme_corpus: continue\n","                    try:\n","                        vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=20)\n","                        tfidf_matrix = vectorizer.fit_transform(theme_corpus)\n","                        keywords = set(vectorizer.get_feature_names_out()[np.argsort(tfidf_matrix.sum(axis=0).A1)[::-1][:P4_NUM_KEYWORDS_PER_THEME]])\n","                    except ValueError: keywords = temp_keywords\n","                    respondent_ids, potential_quotes = set(), []\n","                    for doc in documents_for_qid:\n","                        if any(kw in doc['text'].lower() for kw in keywords):\n","                            respondent_ids.add(doc['response_id'])\n","                            for sentence in nltk.sent_tokenize(doc['text']):\n","                                if any(kw in sentence.lower() for kw in keywords):\n","                                    potential_quotes.append({\"response_id\": doc['response_id'], \"quote\": sentence.strip()})\n","                                    break\n","                    unique_quotes = [dict(t) for t in {tuple(d.items()) for d in potential_quotes}]\n","                    quantified_themes.append({'core_theme_name': core_theme_name, 'keywords': sorted(list(keywords)), 'unique_respondent_count': len(respondent_ids), 'example_quotes': unique_quotes[:P4_NUM_QUOTES_PER_THEME]})\n","                final_analysis[qid] = sorted(quantified_themes, key=lambda x: x['unique_respondent_count'], reverse=True)\n","            with open(output_file, 'w', encoding='utf-8') as f_out: json.dump(final_analysis, f_out, indent=2)\n","            logger.log(\"--- Phase 4 Complete ---\")\n","            return True\n","        except Exception as e:\n","            logger.log(f\"CRITICAL ERROR in Phase 4: {e}\\n{traceback.format_exc()}\")\n","            return False\n","\n","    # ==============================================================================\n","    #                             PHASE 5 FUNCTION (Modified)\n","    # ==============================================================================\n","    def run_phase5_generate_report(final_analysis_file, corpus_file, output_txt_file, qid_map_file, logger):\n","        logger.log(\"--- Starting Phase 5: Stakeholder Report Generation ---\")\n","        try:\n","            with open(final_analysis_file, 'r', encoding='utf-8') as f: final_analysis = json.load(f)\n","            with open(corpus_file, 'r', encoding='utf-8') as f: question_corpus = json.load(f)\n","            with open(qid_map_file, 'r', encoding='utf-8') as f: qid_to_text = {qid: data['question_text'] for qid, data in json.load(f).get(\"question_data\", {}).items()}\n","            with open(output_txt_file, 'w', encoding='utf-8') as f_out:\n","                f_out.write(\"MDL-Based Thematic Analysis of Consultation Responses (PILOT RUN)\\nGenerated: {time.strftime('%c')}\\n\" + \"=\"*70 + \"\\n\\n\")\n","                for qid in sorted(final_analysis.keys(), key=lambda q: int(re.sub(r'\\D', '', q))):\n","                    f_out.write(f\"Question {qid}: {qid_to_text.get(qid, 'N/A')}\\n\\n\")\n","                    docs_for_qid = question_corpus.get(qid, [])\n","                    total_respondents_for_qid = len({doc['response_id'] for doc in docs_for_qid})\n","                    logger.log(f\"  - QID {qid}: Found {total_respondents_for_qid} unique respondents.\")\n","                    if total_respondents_for_qid == 0:\n","                        f_out.write(\"  No responses found for this question.\\n\\n\")\n","                        continue\n","                    themes = final_analysis.get(qid, [])\n","                    if not themes:\n","                        f_out.write(\"  No significant recurring themes were identified.\\n\\n\")\n","                        continue\n","                    f_out.write(\"  The most common points made by respondents were:\\n\\n\")\n","                    for i, theme in enumerate(themes):\n","                        theme_name_clean = re.sub(r'^\\d+[_]', '', theme['core_theme_name']).replace('_', ' ').title()\n","                        count = theme['unique_respondent_count']\n","                        percentage = (count / total_respondents_for_qid * 100)\n","                        f_out.write(f\"  {i+1}. Theme: [{theme_name_clean}]\\n\")\n","                        f_out.write(f\"     This point was indicated by approx {percentage:.2f}% of respondents (in {count} responses out of {total_respondents_for_qid} total).\\n\")\n","                        if theme.get('example_quotes'):\n","                            f_out.write(\"     Examples include:\\n\")\n","                            for quote_info in theme['example_quotes']: f_out.write(f'       â€œ...{quote_info[\"quote\"]}...â€\\n')\n","                        f_out.write(\"\\n\")\n","                    f_out.write(\"---\\n\\n\")\n","            logger.log(\"--- Phase 5 Complete ---\")\n","            return True\n","        except Exception as e:\n","            logger.log(f\"CRITICAL ERROR in Phase 5: {e}\\n{traceback.format_exc()}\")\n","            return False\n","\n","    # ==============================================================================\n","    #                          PIPELINE ORCHESTRATION\n","    # ==============================================================================\n","    master_logger.log(\"======== STARTING MDL THEMATIC ANALYSIS PIPELINE (PILOT RUN V2) ========\")\n","    master_logger.log(\"Filtering corpus for pilot run...\")\n","    try:\n","        with open(P1_QUESTION_CORPUS_FILE, 'r', encoding='utf-8') as f:\n","            full_corpus = json.load(f)\n","    except FileNotFoundError:\n","        master_logger.log(f\"CRITICAL: The main corpus file ({P1_QUESTION_CORPUS_FILE}) is missing.\")\n","        return\n","\n","    qids_to_process = [qid for qid in full_corpus.keys() if qid not in QUESTIONS_TO_EXCLUDE and qid in PILOT_MODE_QUESTIONS]\n","    if not qids_to_process:\n","        master_logger.log(\"No questions to process for the pilot. Halting.\")\n","        return\n","    master_logger.log(f\"Pilot will process: {qids_to_process}\")\n","    pilot_corpus = {qid: full_corpus[qid] for qid in qids_to_process}\n","    TEMP_PILOT_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_temp_pilot_corpus.json')\n","    with open(TEMP_PILOT_CORPUS_FILE, 'w', encoding='utf-8') as f:\n","        json.dump(pilot_corpus, f, indent=2)\n","\n","    if not run_phase2_discover_granular_topics(TEMP_PILOT_CORPUS_FILE, P2_GRANULAR_TOPICS_FILE, master_logger):\n","        master_logger.log(\"CRITICAL: Phase 2 failed. Halting pipeline.\")\n","        return\n","\n","    ### --- BUG FIX --- ###\n","    # LOG_DIR is now passed as an argument to the Phase 3 function.\n","    if not run_phase3_mdl_clustering(P2_GRANULAR_TOPICS_FILE, P3_MDL_CORE_THEMES_FILE, master_logger, LOG_DIR):\n","        master_logger.log(\"CRITICAL: Phase 3 failed. Halting pipeline.\")\n","        return\n","\n","    if not run_phase4_quantify_and_quote(TEMP_PILOT_CORPUS_FILE, P3_MDL_CORE_THEMES_FILE, P4_FINAL_ANALYSIS_FILE, master_logger):\n","        master_logger.log(\"CRITICAL: Phase 4 failed. Halting pipeline.\")\n","        return\n","\n","    if not run_phase5_generate_report(P4_FINAL_ANALYSIS_FILE, TEMP_PILOT_CORPUS_FILE, P5_STAKEHOLDER_REPORT_FILE, QUESTION_DEFINITIONS_FILE, master_logger):\n","        master_logger.log(\"CRITICAL: Phase 5 failed.\")\n","        return\n","\n","    master_logger.log(f\"======== PILOT PIPELINE FINISHED SUCCESSFULLY ========\")\n","    master_logger.log(f\"Please review the pilot report at: {P5_STAKEHOLDER_REPORT_FILE}\")\n","    os.remove(TEMP_PILOT_CORPUS_FILE)\n","    master_logger.log(\"Temporary pilot corpus file has been removed.\")\n","\n","# ==============================================================================\n","#                      EXECUTE THE CONSOLIDATED PIPELINE\n","# ==============================================================================\n","run_consolidated_pipeline_v2()"]},{"cell_type":"markdown","metadata":{"id":"QkHNJlmlTL1-"},"source":["# 27th Aug"]},{"cell_type":"markdown","metadata":{"id":"NR1kklAiTWWY"},"source":["## Preparation\n","\n","Script 1: Analyzing processed_responses.jsonl\n","This script will read through the entire JSON Lines (.jsonl) file, checking each line to ensure it's valid JSON. It will report on the total number of responses, the common keys found across all responses, and flag any keys that are only present in some of them. This will confirm the structure we expect to parse in Phase 1 of our pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YdsFjE9gVwtl"},"outputs":[],"source":["import json\n","from collections import Counter, defaultdict\n","import os\n","from google.colab import drive\n","\n","# ==============================================================================\n","#                               STEP 1: MOUNT DRIVE\n","# ==============================================================================\n","print(\"Mounting Google Drive... Please follow the authentication prompts.\")\n","try:\n","    drive.mount('/content/drive')\n","    print(\"Google Drive mounted successfully.\")\n","except Exception as e:\n","    print(f\"ERROR: Failed to mount Google Drive. {e}\")\n","    # Exit if drive mounting fails, as subsequent steps will not work.\n","    # In a notebook, we just print the error and the user would stop.\n","\n","# ==============================================================================\n","#                      STEP 2: DEFINE ANALYSIS FUNCTIONS\n","# ==============================================================================\n","\n","def analyze_jsonl_structure(filepath):\n","    \"\"\"Analyzes the structure of a JSON Lines file.\"\"\"\n","    if not os.path.exists(filepath):\n","        print(f\"ERROR: File not found at '{filepath}'\")\n","        return None\n","\n","    total_lines = 0\n","    valid_json_lines = 0\n","    key_counter = Counter()\n","    key_presence = defaultdict(int)\n","    data_types = defaultdict(lambda: Counter())\n","\n","    print(f\"\\n--- Analyzing file: {os.path.basename(filepath)} ---\")\n","\n","    with open(filepath, 'r', encoding='utf-8') as f:\n","        for i, line in enumerate(f):\n","            total_lines += 1\n","            try:\n","                data = json.loads(line)\n","                if not isinstance(data, dict):\n","                    continue\n","\n","                valid_json_lines += 1\n","                key_counter.update(data.keys())\n","\n","                for key, value in data.items():\n","                    key_presence[key] += 1\n","                    type_name = type(value).__name__\n","                    data_types[key][type_name] += 1\n","\n","            except json.JSONDecodeError:\n","                continue\n","\n","    print(f\"Total Lines: {total_lines}\")\n","    print(f\"Valid JSON Lines: {valid_json_lines}\")\n","\n","    if valid_json_lines > 0:\n","        print(\"\\nKey Analysis:\")\n","        for key, count in key_counter.items():\n","            presence_percentage = (key_presence[key] / valid_json_lines) * 100\n","            print(f\"- Key: '{key}'\")\n","            print(f\"  > Occurrences: {count} ({presence_percentage:.2f}% of valid lines)\")\n","\n","            print(\"  > Data Types Found:\")\n","            for type_name, type_count in data_types[key].items():\n","                type_percentage = (type_count / count) * 100\n","                print(f\"    - {type_name}: {type_count} ({type_percentage:.2f}%)\")\n","\n","    return {\n","        \"total_lines\": total_lines,\n","        \"valid_json_lines\": valid_json_lines,\n","        \"key_analysis\": dict(key_counter)\n","    }\n","\n","def analyze_json_structure(filepath):\n","    \"\"\"Analyzes the structure of a standard JSON file.\"\"\"\n","    if not os.path.exists(filepath):\n","        print(f\"ERROR: File not found at '{filepath}'\")\n","        return None\n","\n","    print(f\"\\n--- Analyzing file: {os.path.basename(filepath)} ---\")\n","\n","    try:\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","\n","        if not isinstance(data, dict):\n","            print(\"Root of JSON is not a dictionary.\")\n","            return None\n","\n","        print(\"Top-Level Keys:\", list(data.keys()))\n","\n","        if \"question_data\" in data and isinstance(data[\"question_data\"], dict):\n","            question_data = data[\"question_data\"]\n","            num_questions = len(question_data)\n","            print(f\"\\nFound 'question_data' with {num_questions} questions.\")\n","\n","            if num_questions > 0:\n","                first_qid, first_q_data = next(iter(question_data.items()))\n","                print(f\"\\nStructure of a sample question ('{first_qid}'):\")\n","                for key, value in first_q_data.items():\n","                    print(f\"- Key: '{key}', Type: {type(value).__name__}\")\n","                    if isinstance(value, list) and value:\n","                        print(f\"  > First item in list has type: {type(value[0]).__name__}\")\n","\n","        return {\"top_level_keys\": list(data.keys())}\n","\n","    except json.JSONDecodeError as e:\n","        print(f\"ERROR: Invalid JSON in file: {e}\")\n","        return None\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","        return None\n","\n","\n","# ==============================================================================\n","#                               STEP 3: RUN ANALYSIS\n","# ==============================================================================\n","if __name__ == \"__main__\":\n","    # Define file paths from the notebook's configuration\n","    BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","    PROCESSED_RESPONSES_FILE = os.path.join(BASE_PROJECT_DIR, 'processed_responses.jsonl')\n","    QUESTION_CENTRIC_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","\n","    # Now that the drive is mounted, these paths will be accessible\n","    analyze_jsonl_structure(PROCESSED_RESPONSES_FILE)\n","\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","    analyze_json_structure(QUESTION_CENTRIC_FILE)"]},{"cell_type":"markdown","metadata":{"id":"1nVeh2ir0xzV"},"source":["# 25th Aug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KexZ1bvyziJ8"},"outputs":[],"source":["# @title Authenticate Environment\n","from huggingface_hub import notebook_login\n","# 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\n","notebook_login()"]},{"cell_type":"markdown","metadata":{"id":"Sm49MDby1ZeX"},"source":["## Cell 1: Introduction & Pipeline Overview"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FJBbdHv01Il"},"outputs":[],"source":["# MDL-Based Thematic Analysis Pipeline v1.0\n","\n","**Purpose:** This notebook implements a comprehensive, end-to-end pipeline to solve the core business problem:\n","\n","> To produce a report that, for each of the questions asked in a public consultation, identifies the most common points made by respondents, providing a quantified estimation of how often that point was made, as well as providing real quotations exemplifying each point.\n","\n","**Methodology:**\n","This pipeline integrates the structured approach from `Thematic_Analysis_Pipeline_v0_1.ipynb` with the powerful Minimum Description Length (MDL) clustering engine from `3_mdl_analysis_simplified.ipynb`. It replaces subjective theme generation with a principled, data-driven approach.\n","\n","**The pipeline consists of five phases:**\n","\n","1.  **UDS Generation:** Processes raw survey submissions (including PDFs) and collates all relevant text into a **question-centric corpus**.\n","2.  **Granular Topic Discovery:** For each question, uses BERTopic to discover a large number of fine-grained, granular topics from the text. This provides the raw material for MDL.\n","3.  **MDL Core Theme Clustering:** Applies the MDL algorithm to the granular topics, finding the optimal number of \"core themes\" by grouping similar topics together in a way that is statistically efficient.\n","4.  **Quantification & Quote Extraction:** For each MDL-derived core theme, searches the original text corpus to count the number of unique respondents who discussed it and extracts high-quality example quotations.\n","5.  **Report Generation:** Formats the final, quantified analysis into a human-readable stakeholder report."]},{"cell_type":"markdown","metadata":{"id":"H6WBTC1D1bFu"},"source":["## Cell 2: Global Setup and Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSuE71AqY5_E"},"outputs":[],"source":["# ==============================================================================\n","#                               SETUP & CONFIGURATION\n","# ==============================================================================\n","import json\n","import os\n","import re\n","import sys\n","import time\n","import pandas as pd\n","import numpy as np\n","from collections import defaultdict\n","from pathlib import Path\n","from typing import Dict\n","from google.colab import drive\n","\n","# --- Step 1: Mount Google Drive ---\n","print(\"Mounting Google Drive... Please follow the authentication prompts.\")\n","try:\n","    drive.mount('/content/drive')\n","    print(\"Google Drive mounted successfully.\")\n","except Exception as e:\n","    print(f\"ERROR: Failed to mount Google Drive. Error: {e}\")\n","\n","# --- Step 2: Install necessary libraries ---\n","!pip install -q transformers bitsandbytes accelerate sentence-transformers bertopic umap-learn hdbscan huggingface_hub\n","\n","# --- Step 3: Import all required libraries ---\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","from sentence_transformers import SentenceTransformer, util\n","from bertopic import BERTopic\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import nltk\n","import torch\n","import traceback\n","from huggingface_hub import login # <--- IMPORT THE LOGIN FUNCTION\n","\n","# --- Step 4: Download NLTK resources ---\n","print(\"Downloading NLTK resources ('punkt' and 'punkt_tab')...\")\n","nltk.download('punkt', quiet=True)\n","nltk.download('punkt_tab', quiet=True)\n","print(\"NLTK resources are ready.\")\n","\n","# ==============================================================================\n","#                     STEP 5: AUTHENTICATION (REVISED)\n","# ==============================================================================\n","# This section handles the Hugging Face token for accessing models.\n","# It loads the token from Colab Secrets and uses it to log in.\n","\n","try:\n","    from google.colab import userdata\n","    HF_TOKEN = userdata.get('HF_TOKEN')\n","\n","    if HF_TOKEN:\n","        # --- NEW AUTHENTICATION STEP ---\n","        # This explicitly logs the session in, ensuring all libraries can authenticate.\n","        login(token=HF_TOKEN)\n","        print(\"Hugging Face token loaded and session authenticated successfully.\")\n","    else:\n","        print(\"WARNING: Hugging Face token not found in Colab Secrets.\")\n","\n","except (ImportError, KeyError):\n","    print(\"WARNING: Could not access Colab Secrets.\")\n","    print(\"         Public models will work, but private/gated models may fail.\")\n","    HF_TOKEN = None\n","\n","# ==============================================================================\n","#                      STEP 6: Define FileLogger Class\n","# ==============================================================================\n","class FileLogger:\n","    # (The FileLogger class definition remains the same as the previous corrected version)\n","    def __init__(self, log_file_path: str):\n","        self.log_file_path = log_file_path\n","        log_dir = os.path.dirname(log_file_path)\n","        if log_dir and not os.path.exists(log_dir):\n","            os.makedirs(log_dir, exist_ok=True)\n","        with open(self.log_file_path, 'w', encoding='utf-8') as f:\n","            f.write(f\"{self._get_timestamp()}: Starting new log at {self.log_file_path}\\n\")\n","\n","    def _get_timestamp(self) -> str:\n","        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","\n","    def log(self, message: str):\n","        print(message)\n","        try:\n","            with open(self.log_file_path, 'a', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","        except Exception as e:\n","            print(f\"!!! LOGGER FAILED: {e} !!!\\n{self._get_timestamp()}: {message}\")\n","\n","    def _get_serializer(self):\n","        def default_serializer(obj):\n","            if isinstance(obj, (np.integer, np.floating)): return obj.item()\n","            if isinstance(obj, np.ndarray): return obj.tolist()\n","            if isinstance(obj, set): return sorted(list(obj))\n","            try: return json.JSONEncoder().default(obj)\n","            except TypeError: return f\"UNSERIALIZABLE:{type(obj).__name__}\"\n","        return default_serializer\n","\n","    def log_json(self, data: dict, prefix: str = \"JSON_DATA\"):\n","        log_entry = {\"timestamp\": self._get_timestamp(), \"type\": prefix, \"data\": data}\n","        try:\n","            json_string = json.dumps(log_entry, default=self._get_serializer(), indent=2)\n","            print(f\"DEBUG: Logging JSON object with prefix '{prefix}' to file: {self.log_file_path}\")\n","            with open(self.log_file_path, 'a', encoding='utf-8') as f:\n","                f.write(json_string + \"\\n\")\n","        except Exception as e:\n","            self.log(f\"ERROR: log_json failed: {e}\\n{traceback.format_exc()}\")\n","\n","# ==============================================================================\n","#                            PIPELINE CONFIGURATION\n","# ==============================================================================\n","# (This section remains unchanged)\n","# --- Base Paths ---\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","LOG_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Logs/')\n","DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Data/')\n","REPORTS_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Reports/')\n","for d in [LOG_DIR, DATA_DIR, REPORTS_DIR]:\n","    os.makedirs(d, exist_ok=True)\n","\n","# --- Input Files ---\n","RAW_RESPONSES_JSONL = os.path.join(BASE_PROJECT_DIR, 'processed_responses.jsonl')\n","QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","\n","# --- Phase-Specific Output Files ---\n","P1_QUESTION_CORPUS_FILE = os.path.join(DATA_DIR, 'p1_question_centric_corpus.json')\n","P2_GRANULAR_TOPICS_FILE = os.path.join(DATA_DIR, 'p2_granular_topics.json')\n","P3_MDL_CORE_THEMES_FILE = os.path.join(DATA_DIR, 'p3_mdl_core_themes.json')\n","P4_FINAL_ANALYSIS_FILE = os.path.join(DATA_DIR, 'p4_final_analysis_report.json')\n","P5_STAKEHOLDER_REPORT_FILE = os.path.join(REPORTS_DIR, f'Stakeholder_Report_{time.strftime(\"%Y%m%d\")}.txt')\n","\n","# --- Model & Algorithm Parameters ---\n","P1_EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n","P1_SIMILARITY_SCORE = 0.5\n","P2_BERTOPIC_MIN_TOPIC_SIZE = 3\n","P3_MDL_PENALTY_FACTOR = 1.5\n","P4_NUM_QUOTES_PER_THEME = 3\n","\n","# --- Initialize Master Logger ---\n","master_logger = FileLogger(os.path.join(LOG_DIR, f'pipeline_master_{time.strftime(\"%Y%m%d_%H%M%S\")}.log'))\n","ENABLE_DEBUG_LOGGING = True\n","print(\"Setup and configuration complete.\")"]},{"cell_type":"markdown","metadata":{"id":"MRNG-a7N1eqN"},"source":["## Cell 3: Phase 1 - UDS Generation (Question-Centric Corpus)\n","**Goal:** Process all raw survey responses, extract text from PDFs, and use semantic search to collate all text snippets relevant to each specific question.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0bl_vq8cW_H"},"outputs":[],"source":["# ==============================================================================\n","#                      IMPORTS FOR PHASE 1\n","# ==============================================================================\n","import json\n","import os\n","import re\n","import traceback # <--- FIX for traceback error\n","import torch     # <--- FIX for torch error\n","from collections import defaultdict\n","import nltk\n","from sentence_transformers import SentenceTransformer, util\n","\n","# ==============================================================================\n","\n","# markdown cell from previous response\n","# **Goal:** Process all raw survey responses, extract text from PDFs, and use semantic search to collate all text snippets relevant to each specific question.\n","\n","def run_phase1_generate_corpus(raw_file, qid_map_file, output_file, logger):\n","    \"\"\"\n","    Loads raw responses, extracts PDF text, and uses semantic search to\n","    create a question-centric corpus.\n","    \"\"\"\n","    logger.log(\"--- Starting Phase 1: UDS Generation ---\")\n","\n","    try:\n","        # 1. Load sentence transformer model\n","        logger.log(f\"Loading embedding model: {P1_EMBEDDING_MODEL}...\")\n","        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        embedding_model = SentenceTransformer(P1_EMBEDDING_MODEL, device=device)\n","\n","        # 2. Load question definitions and create embeddings\n","        logger.log(f\"Loading question definitions from: {qid_map_file}\")\n","        with open(qid_map_file, 'r', encoding='utf-8') as f:\n","            qid_data = json.load(f)\n","\n","        qid_to_text = {qid: data['question_text'] for qid, data in qid_data.get(\"question_data\", {}).items()}\n","        qids = list(qid_to_text.keys())\n","        question_texts = list(qid_to_text.values())\n","        question_embeddings = embedding_model.encode(question_texts, convert_to_tensor=True, device=device)\n","        logger.log(f\"Generated embeddings for {len(qids)} questions.\")\n","\n","        # 3. Initialize corpus\n","        question_corpus = defaultdict(list)\n","\n","        # 4. Loop through raw responses\n","        logger.log(f\"Processing raw responses from: {raw_file}\")\n","        with open(raw_file, 'r', encoding='utf-8') as f_in:\n","            for i, line in enumerate(f_in):\n","                if (i + 1) % 50 == 0:\n","                    logger.log(f\"  > Processed {i+1} responses...\")\n","\n","                response = json.loads(line)\n","                response_id = response.get('id', f'unknown_{i}')\n","                pdf_text = response.get('pdf_text', '')\n","\n","                if len(pdf_text) < 100:\n","                    continue\n","\n","                sentences = nltk.sent_tokenize(pdf_text)\n","                if not sentences:\n","                    continue\n","\n","                sentence_embeddings = embedding_model.encode(sentences, convert_to_tensor=True, device=device)\n","\n","                # 5. Find relevant sentences for each question\n","                all_similarities = util.cos_sim(question_embeddings, sentence_embeddings)\n","\n","                for q_idx, qid in enumerate(qids):\n","                    similarities = all_similarities[q_idx]\n","                    relevant_indices = torch.where(similarities > P1_SIMILARITY_SCORE)[0]\n","\n","                    added_passages = set()\n","                    for sent_idx_tensor in relevant_indices:\n","                        sent_idx = sent_idx_tensor.item()\n","                        start = max(0, sent_idx - 1)\n","                        end = min(len(sentences), sent_idx + 2)\n","                        passage = \" \".join(sentences[start:end])\n","                        if passage not in added_passages:\n","                            question_corpus[qid].append({\"response_id\": response_id, \"text\": passage})\n","                            added_passages.add(passage)\n","\n","        # 6. Save the corpus\n","        logger.log(f\"Saving question-centric corpus to: {output_file}\")\n","        with open(output_file, 'w', encoding='utf-8') as f_out:\n","            json.dump(question_corpus, f_out, indent=2)\n","\n","        logger.log(\"--- Phase 1 Complete: Question-Centric Corpus saved. ---\")\n","        return True\n","    except Exception as e:\n","        logger.log(f\"CRITICAL ERROR in Phase 1: {e}\\n{traceback.format_exc()}\")\n","        return False\n","\n","# Placeholder for execution - real execution is in the Orchestration Cell\n","print(\"Phase 1 function defined.\")"]},{"cell_type":"markdown","metadata":{"id":"bJNAfJLt1ijN"},"source":["## Cell 4: Phase 2 - Granular Topic Discovery (BERTopic)\n","* **Goal:** For each question, analyze its collected text corpus to discover a large set of granular, fine-grained topics. This creates the foundational data that the MDL algorithm will cluster.\n","* **Goal:** For each question, analyze its collected text corpus to discover a large set of granular, fine-grained topics. This creates the foundational data that the MDL algorithm will cluster.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DAvr1mf-K1z"},"outputs":[],"source":["def run_phase2_discover_granular_topics(corpus_file, output_file, logger):\n","    \"\"\"\n","    For each question in the corpus, runs BERTopic to discover granular topics.\n","    This version is resumable.\n","    \"\"\"\n","    logger.log(\"--- Starting Phase 2: Granular Topic Discovery ---\")\n","    try:\n","        # 1. Load corpus and initialize model\n","        logger.log(f\"Loading question corpus from: {corpus_file}\")\n","        with open(corpus_file, 'r', encoding='utf-8') as f:\n","            question_corpus = json.load(f)\n","\n","        logger.log(\"Initializing BERTopic model...\")\n","        embedding_model = SentenceTransformer(P1_EMBEDDING_MODEL)\n","        topic_model = BERTopic(embedding_model=embedding_model, min_topic_size=P2_BERTOPIC_MIN_TOPIC_SIZE, verbose=False)\n","\n","        # --- RESUME LOGIC START ---\n","        all_granular_topics = {}\n","        if os.path.exists(output_file):\n","            try:\n","                with open(output_file, 'r', encoding='utf-8') as f_in:\n","                    all_granular_topics = json.load(f_in)\n","                logger.log(f\"Resuming Phase 2. Found {len(all_granular_topics)} previously processed questions in {output_file}.\")\n","            except json.JSONDecodeError:\n","                logger.log(f\"WARNING: Could not parse existing output file {output_file}. Starting Phase 2 from scratch.\")\n","                all_granular_topics = {}\n","        # --- RESUME LOGIC END ---\n","\n","        # 3. Loop and discover topics for each question\n","        qids = list(question_corpus.keys())\n","        for i, qid in enumerate(qids):\n","            logger.log(f\"  > Processing QID {qid} ({i+1}/{len(qids)})...\")\n","\n","            # --- SKIP IF ALREADY PROCESSED ---\n","            if qid in all_granular_topics:\n","                logger.log(f\"    - Skipping {qid}, results already exist.\")\n","                continue\n","\n","            documents = [item['text'] for item in question_corpus[qid]]\n","\n","            if len(documents) < P2_BERTOPIC_MIN_TOPIC_SIZE:\n","                logger.log(f\"    - Skipping {qid}, not enough documents ({len(documents)}).\")\n","                all_granular_topics[qid] = [] # Mark as processed but empty\n","                continue\n","\n","            topics, _ = topic_model.fit_transform(documents)\n","            topic_info = topic_model.get_topic_info()\n","            granular_topics_for_qid = topic_info[topic_info['Topic'] != -1]\n","            all_granular_topics[qid] = granular_topics_for_qid[['Name', 'Representation']].to_dict('records')\n","            logger.log(f\"    - Found {len(granular_topics_for_qid)} granular topics for {qid}.\")\n","\n","            # --- SAVE PROGRESS ---\n","            with open(output_file, 'w', encoding='utf-8') as f_out:\n","                json.dump(all_granular_topics, f_out, indent=2)\n","\n","        logger.log(\"--- Phase 2 Complete: Granular topics discovered and saved. ---\")\n","        return True\n","    except Exception as e:\n","        logger.log(f\"CRITICAL ERROR in Phase 2: {e}\\n{traceback.format_exc()}\")\n","        return False\n","\n","# Placeholder for execution\n","print(\"Phase 2 function defined with resume capability.\")"]},{"cell_type":"markdown","metadata":{"id":"OZHEmKKj1nXh"},"source":["## Cell 5: Phase 3 - MDL Core Theme Clustering\n","Goal: Apply the MDL algorithm to the granular topics discovered in Phase 2. This step groups similar granular topics into a statistically optimal set of \"core themes\" for each question. This is the core analytical engine of the pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3R49Ofm215LF"},"outputs":[],"source":["# ==============================================================================\n","#                      IMPORTS FOR PHASE 3\n","# ==============================================================================\n","import json\n","import os\n","import re\n","import traceback\n","import numpy as np\n","from collections import defaultdict\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# ==============================================================================\n","#                         MDL HELPER FUNCTIONS\n","# ==============================================================================\n","# (These helpers: vectorize_topics, calculate_cluster_cost, calculate_mdl_cost remain unchanged)\n","def vectorize_topics(documents):\n","    vectorizer = TfidfVectorizer(use_idf=False, norm='l1')\n","    topic_vectors = vectorizer.fit_transform(documents).toarray()\n","    return topic_vectors\n","\n","def calculate_cluster_cost(topic_vectors_subset):\n","    if topic_vectors_subset.shape[0] < 2: return 0\n","    centroid = np.mean(topic_vectors_subset, axis=0)\n","    sse = np.sum((topic_vectors_subset - centroid)**2)\n","    return 0.5 * topic_vectors_subset.shape[0] * np.log(sse + 1e-9)\n","\n","def calculate_mdl_cost(partition, topic_vectors, penalty_factor):\n","    n_points = topic_vectors.shape[0]\n","    k_clusters = len(partition)\n","    if k_clusters == 0: return float('inf')\n","    l_model = n_points * np.log(k_clusters)\n","    l_data = sum(calculate_cluster_cost(topic_vectors[indices, :]) for indices in partition.values() if indices)\n","    return l_model + (penalty_factor * l_data)\n","\n","# --- This is the definitive version of the clustering function ---\n","def find_optimal_partition(topic_vectors, penalty_factor, granular_topic_names, qid, logger):\n","    \"\"\"\n","    Performs agglomerative clustering with definitive checkpointing and clear logging.\n","    \"\"\"\n","    state_file = os.path.join(DATA_DIR, f'p3_mdl_state_{qid}.json')\n","    partition, best_partition, best_cost, completed_iterations = None, None, float('inf'), 0\n","\n","    if os.path.exists(state_file):\n","        try:\n","            with open(state_file, 'r') as f:\n","                state = json.load(f)\n","            partition = {int(k): v for k, v in state['partition'].items()}\n","            best_partition = {int(k): v for k, v in state['best_partition'].items()}\n","            best_cost = state['best_cost']\n","            completed_iterations = state['completed_iterations']\n","            logger.log(f\"    - Resuming MDL clustering for {qid} from iteration {completed_iterations + 1}.\")\n","        except Exception as e:\n","            logger.log(f\"    - WARNING: Could not load state file {state_file}. Starting from scratch. Error: {e}\")\n","            partition = None\n","\n","    if partition is None:\n","        partition = {i: [i] for i in range(len(granular_topic_names))}\n","        cost = calculate_mdl_cost(partition, topic_vectors, penalty_factor)\n","        best_partition, best_cost = partition, cost\n","        completed_iterations = 0\n","        logger.log(f\"    - Initializing MDL clustering for {qid}. Initial Cost: {best_cost:.2f}\")\n","\n","    num_initial_topics = len(granular_topic_names)\n","    total_merges = num_initial_topics - 1\n","\n","    for i in range(completed_iterations, total_merges):\n","\n","        current_iteration_num = i + 1\n","        if len(partition) < 2: break\n","\n","        # --- Find the best pair of themes to merge ---\n","        best_merge, lowest_merge_cost = None, float('inf')\n","        themes_to_merge = list(partition.keys())\n","        for j in range(len(themes_to_merge)):\n","            for k in range(j + 1, len(themes_to_merge)):\n","                t1, t2 = themes_to_merge[j], themes_to_merge[k]\n","                temp_partition = {key: val[:] for key, val in partition.items()}\n","                temp_partition[t1].extend(temp_partition.pop(t2))\n","                merge_cost = calculate_mdl_cost(temp_partition, topic_vectors, penalty_factor)\n","                if merge_cost < lowest_merge_cost:\n","                    lowest_merge_cost, best_merge = merge_cost, (t1, t2)\n","\n","        # --- Perform the merge ---\n","        t1, t2 = best_merge\n","        partition[t1].extend(partition.pop(t2))\n","\n","        # --- Log if a new best state is found ---\n","        if lowest_merge_cost < best_cost:\n","            best_cost = lowest_merge_cost\n","            best_partition = {k: v[:] for k, v in partition.items()}\n","            logger.log(f\"    - New best state at iteration {current_iteration_num}: {len(best_partition)} themes, Cost: {best_cost:.2f}\")\n","\n","        # --- SIMPLIFIED AND RELIABLE LOGGING & CHECKPOINTING ---\n","        # Log progress at specific intervals\n","        log_intervals = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 75, 100] # Log at these specific iterations\n","        if ENABLE_DEBUG_LOGGING and (current_iteration_num in log_intervals or current_iteration_num % 50 == 0): # Then every 50\n","             logger.log(f\"      - Progress: Merge iteration {current_iteration_num}/{total_merges} complete.\")\n","\n","        # Checkpoint every 20 iterations, GUARANTEED to log.\n","        if current_iteration_num % 20 == 0 and current_iteration_num < total_merges:\n","            state_to_save = {\n","                'partition': partition, 'best_partition': best_partition,\n","                'best_cost': best_cost, 'completed_iterations': current_iteration_num\n","            }\n","            with open(state_file, 'w') as f:\n","                json.dump(state_to_save, f)\n","            # This message will now print regardless of the debug flag.\n","            logger.log(f\"      >>> CHECKPOINT SAVED at iteration {current_iteration_num} for {qid}. <<<\")\n","\n","    # Final cleanup\n","    if os.path.exists(state_file):\n","        os.remove(state_file)\n","        logger.log(f\"    - Clustering for {qid} complete. Removed temporary state file.\")\n","\n","    return best_partition\n","\n","# ==============================================================================\n","#                        MAIN PHASE 3 FUNCTION\n","# ==============================================================================\n","\n","def run_phase3_mdl_clustering(granular_topics_file, output_file, logger):\n","    logger.log(\"--- Starting Phase 3: MDL Core Theme Clustering ---\")\n","    try:\n","        logger.log(f\"Loading granular topics from: {granular_topics_file}\")\n","        with open(granular_topics_file, 'r', encoding='utf-8') as f:\n","            all_granular_topics = json.load(f)\n","\n","        all_core_themes = {}\n","        if os.path.exists(output_file):\n","            try:\n","                with open(output_file, 'r', encoding='utf-8') as f_in:\n","                    all_core_themes = json.load(f_in)\n","                logger.log(f\"Resuming Phase 3. Found {len(all_core_themes)} previously processed questions in {output_file}.\")\n","            except json.JSONDecodeError:\n","                logger.log(f\"WARNING: Could not parse {output_file}. Starting Phase 3 from scratch.\")\n","                all_core_themes = {}\n","\n","        qids = list(all_granular_topics.keys())\n","        for i, qid in enumerate(qids):\n","            logger.log(f\"  > Clustering themes for QID {qid} ({i+1}/{len(qids)})...\")\n","\n","            if qid in all_core_themes:\n","                logger.log(f\"    - Skipping {qid}, results already exist.\")\n","                continue\n","\n","            topics_for_qid = all_granular_topics[qid]\n","            if not topics_for_qid:\n","                logger.log(f\"    - Skipping {qid}, no granular topics found.\")\n","                all_core_themes[qid] = {}\n","                continue\n","\n","            granular_topic_names = [topic['Name'] for topic in topics_for_qid]\n","            topic_documents = [\" \".join(topic['Representation']) for topic in topics_for_qid]\n","\n","            topic_vectors = vectorize_topics(topic_documents)\n","\n","            # --- THIS IS THE CORRECTED FUNCTION CALL ---\n","            optimal_partition = find_optimal_partition(topic_vectors, P3_MDL_PENALTY_FACTOR, granular_topic_names, qid, logger)\n","\n","            formatted_themes = {}\n","            for theme_id, topic_indices in optimal_partition.items():\n","                theme_vectors_subset = topic_vectors[topic_indices, :]\n","                centroid = np.mean(theme_vectors_subset, axis=0)\n","                distances = np.sum((theme_vectors_subset - centroid)**2, axis=1)\n","                central_topic_idx = topic_indices[np.argmin(distances)]\n","                theme_name = granular_topic_names[central_topic_idx]\n","                constituent_topics = [granular_topic_names[idx] for idx in topic_indices]\n","                formatted_themes[theme_name] = constituent_topics\n","\n","            all_core_themes[qid] = formatted_themes\n","            logger.log(f\"    - Found {len(formatted_themes)} core themes for {qid}.\")\n","\n","            with open(output_file, 'w', encoding='utf-8') as f_out:\n","                json.dump(all_core_themes, f_out, indent=2)\n","            logger.log(f\"    - Progress saved to {output_file}.\")\n","\n","        logger.log(\"--- Phase 3 Complete: MDL core themes identified and saved. ---\")\n","        return True\n","    except Exception as e:\n","        logger.log(f\"CRITICAL ERROR in Phase 3: {e}\\n{traceback.format_exc()}\")\n","        return False\n","\n","# Placeholder for execution - real execution is in the Orchestration Cell\n","print(\"Phase 3 function defined with internal checkpointing.\")"]},{"cell_type":"markdown","metadata":{"id":"4Oeimbx72KkK"},"source":["## Cell 6: Phase 4 - Quantification and Quote Extraction\n","\n","**Goal:** With the core themes now defined by MDL, this phase quantifies their prevalence. It searches the original text corpus for keywords associated with each theme to count unique respondents and extract supporting quotes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tq-jl3Xa_FxS"},"outputs":[],"source":["def run_phase4_quantify_and_quote(corpus_file, mdl_themes_file, output_file, logger):\n","    \"\"\"\n","    Quantifies themes and extracts quotes based on MDL results.\n","    This version is resumable.\n","    \"\"\"\n","    logger.log(\"--- Starting Phase 4: Quantification & Quote Extraction ---\")\n","    try:\n","        # 1. Load necessary files\n","        logger.log(f\"Loading question corpus from: {corpus_file}\")\n","        with open(corpus_file, 'r', encoding='utf-8') as f:\n","            question_corpus = json.load(f)\n","\n","        logger.log(f\"Loading MDL core themes from: {mdl_themes_file}\")\n","        with open(mdl_themes_file, 'r', encoding='utf-8') as f:\n","            all_core_themes = json.load(f)\n","\n","        # --- RESUME LOGIC START ---\n","        final_analysis = {}\n","        if os.path.exists(output_file):\n","            try:\n","                with open(output_file, 'r', encoding='utf-8') as f_in:\n","                    final_analysis = json.load(f_in)\n","                logger.log(f\"Resuming Phase 4. Found {len(final_analysis)} previously processed questions in {output_file}.\")\n","            except json.JSONDecodeError:\n","                logger.log(f\"WARNING: Could not parse existing output file {output_file}. Starting Phase 4 from scratch.\")\n","                final_analysis = {}\n","        # --- RESUME LOGIC END ---\n","\n","        # 2. Process each question\n","        qids = list(all_core_themes.keys())\n","        for i, qid in enumerate(qids):\n","            logger.log(f\"  > Quantifying themes for QID {qid} ({i+1}/{len(qids)})...\")\n","\n","            # --- SKIP IF ALREADY PROCESSED ---\n","            if qid in final_analysis:\n","                logger.log(f\"    - Skipping {qid}, results already exist.\")\n","                continue\n","\n","            core_themes_for_qid = all_core_themes[qid]\n","            documents_for_qid = question_corpus.get(qid, [])\n","\n","            quantified_themes = []\n","            for core_theme_name, granular_topic_names in core_themes_for_qid.items():\n","                keywords = set(kw for name in granular_topic_names for kw in re.sub(r'^\\d+[_]', '', name).split('_'))\n","\n","                respondent_ids = set()\n","                potential_quotes = []\n","                for doc in documents_for_qid:\n","                    text_lower = doc['text'].lower()\n","                    if any(kw in text_lower for kw in keywords):\n","                        respondent_ids.add(doc['response_id'])\n","                        sentences = nltk.sent_tokenize(doc['text'])\n","                        for sentence in sentences:\n","                            if any(kw in sentence.lower() for kw in keywords):\n","                                potential_quotes.append({\"response_id\": doc['response_id'], \"quote\": sentence.strip()})\n","                                break\n","\n","                unique_quotes = [dict(t) for t in {tuple(d.items()) for d in potential_quotes}]\n","\n","                quantified_themes.append({\n","                    'core_theme_name': core_theme_name,\n","                    'constituent_topics': granular_topic_names,\n","                    'keywords': sorted(list(keywords)),\n","                    'unique_respondent_count': len(respondent_ids),\n","                    'example_quotes': unique_quotes[:P4_NUM_QUOTES_PER_THEME]\n","                })\n","\n","            final_analysis[qid] = sorted(quantified_themes, key=lambda x: x['unique_respondent_count'], reverse=True)\n","\n","            # --- SAVE PROGRESS ---\n","            with open(output_file, 'w', encoding='utf-8') as f_out:\n","                json.dump(final_analysis, f_out, indent=2)\n","\n","        logger.log(\"--- Phase 4 Complete: Final analysis report data generated. ---\")\n","        return True\n","    except Exception as e:\n","        logger.log(f\"CRITICAL ERROR in Phase 4: {e}\\n{traceback.format_exc()}\")\n","        return False\n","\n","# Placeholder for execution\n","print(\"Phase 4 function defined with resume capability.\")"]},{"cell_type":"markdown","metadata":{"id":"C-BoHGxc2Olh"},"source":["## Cell 7: Phase 5 - Stakeholder Report Generation\n","**Goal:** Take the final structured JSON data from Phase 4 and format it into a clear, human-readable text report for stakeholders.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WaoonTu_Msjc"},"outputs":[],"source":["\n","def run_phase5_generate_report(final_analysis_file, output_txt_file, qid_map_file, logger):\n","    \"\"\"\n","    Generates the final human-readable text report.\n","    \"\"\"\n","    logger.log(\"--- Starting Phase 5: Stakeholder Report Generation ---\")\n","    try:\n","        # 1. Load data\n","        logger.log(f\"Loading final analysis from: {final_analysis_file}\")\n","        with open(final_analysis_file, 'r', encoding='utf-8') as f:\n","            final_analysis = json.load(f)\n","\n","        with open(qid_map_file, 'r', encoding='utf-8') as f:\n","            qid_data = json.load(f)\n","        qid_to_text = {qid: data['question_text'] for qid, data in qid_data.get(\"question_data\", {}).items()}\n","\n","        total_respondents = len({\n","            quote['response_id']\n","            for qid in final_analysis\n","            for theme in final_analysis[qid]\n","            for quote in theme['example_quotes']\n","        })\n","        logger.log(f\"Total unique respondents found in analysis: {total_respondents}\")\n","\n","        # 2. Write report\n","        with open(output_txt_file, 'w', encoding='utf-8') as f_out:\n","            f_out.write(\"MDL-Based Thematic Analysis of Consultation Responses\\n\")\n","            f_out.write(f\"Generated: {time.strftime('%c')}\\n\")\n","            f_out.write(f\"Source Analysis File: {os.path.basename(final_analysis_file)}\\n\")\n","            f_out.write(\"=\"*70 + \"\\n\\n\")\n","\n","            sorted_qids = sorted(final_analysis.keys(), key=lambda q: int(re.sub(r'\\D', '', q)))\n","\n","            for qid in sorted_qids:\n","                question_text = qid_to_text.get(qid, f\"Text for {qid} not found.\")\n","                f_out.write(f\"Question {qid}: {question_text}\\n\\n\")\n","                f_out.write(\"  The most common points made by respondents were:\\n\\n\")\n","\n","                themes = final_analysis[qid]\n","                for i, theme in enumerate(themes):\n","                    theme_name_clean = re.sub(r'^\\d+[_]', '', theme['core_theme_name']).replace('_', ' ').title()\n","                    count = theme['unique_respondent_count']\n","                    percentage = (count / total_respondents * 100) if total_respondents > 0 else 0\n","\n","                    f_out.write(f\"  {i+1}. Theme: [{theme_name_clean}]\\n\")\n","                    f_out.write(f\"     This point was indicated by approx {percentage:.2f}% of respondents (in {count} unique responses).\\n\")\n","\n","                    if theme['example_quotes']:\n","                        f_out.write(\"     Examples include:\\n\")\n","                        for quote_info in theme['example_quotes']:\n","                            f_out.write(f'       â€œ...{quote_info[\"quote\"]}...â€\\n')\n","                    f_out.write(\"\\n\")\n","\n","                f_out.write(\"--- End of Question ---\\n\\n\")\n","\n","        logger.log(f\"--- Phase 5 Complete: Stakeholder report saved to {output_txt_file} ---\")\n","        return True\n","    except Exception as e:\n","        logger.log(f\"CRITICAL ERROR in Phase 5: {e}\\n{traceback.format_exc()}\")\n","        return False\n","\n","# Placeholder for execution\n","print(\"Phase 5 function defined.\")"]},{"cell_type":"markdown","metadata":{"id":"i7FO3nNL2WPc"},"source":["\n","## Cell 8: Main Pipeline Orchestration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCLOYuLrM88J"},"outputs":[],"source":["# ==============================================================================\n","#                          PIPELINE ORCHESTRATION\n","# ==============================================================================\n","# Set these flags to control which phases are executed.\n","RUN_PHASE_1 = True\n","RUN_PHASE_2 = True\n","RUN_PHASE_3 = True\n","RUN_PHASE_4 = True\n","RUN_PHASE_5 = True\n","\n","def main_pipeline():\n","    master_logger.log(\"======== STARTING MDL THEMATIC ANALYSIS PIPELINE ========\")\n","\n","    # Phase 1\n","    if RUN_PHASE_1:\n","        if not run_phase1_generate_corpus(RAW_RESPONSES_JSONL, QUESTION_DEFINITIONS_FILE, P1_QUESTION_CORPUS_FILE, master_logger):\n","            master_logger.log(\"CRITICAL: Phase 1 failed. Halting pipeline.\")\n","            return\n","\n","    # Phase 2\n","    if RUN_PHASE_2:\n","        if not os.path.exists(P1_QUESTION_CORPUS_FILE):\n","            master_logger.log(f\"CRITICAL: Input for Phase 2 ({P1_QUESTION_CORPUS_FILE}) not found. Halting.\")\n","            return\n","        if not run_phase2_discover_granular_topics(P1_QUESTION_CORPUS_FILE, P2_GRANULAR_TOPICS_FILE, master_logger):\n","            master_logger.log(\"CRITICAL: Phase 2 failed. Halting pipeline.\")\n","            return\n","\n","    # Phase 3\n","    if RUN_PHASE_3:\n","        if not os.path.exists(P2_GRANULAR_TOPICS_FILE):\n","            master_logger.log(f\"CRITICAL: Input for Phase 3 ({P2_GRANULAR_TOPICS_FILE}) not found. Halting.\")\n","            return\n","        if not run_phase3_mdl_clustering(P2_GRANULAR_TOPICS_FILE, P3_MDL_CORE_THEMES_FILE, master_logger):\n","            master_logger.log(\"CRITICAL: Phase 3 failed. Halting pipeline.\")\n","            return\n","\n","    # Phase 4\n","    if RUN_PHASE_4:\n","        if not all(os.path.exists(f) for f in [P1_QUESTION_CORPUS_FILE, P3_MDL_CORE_THEMES_FILE]):\n","            master_logger.log(f\"CRITICAL: Inputs for Phase 4 not found. Halting.\")\n","            return\n","        if not run_phase4_quantify_and_quote(P1_QUESTION_CORPUS_FILE, P3_MDL_CORE_THEMES_FILE, P4_FINAL_ANALYSIS_FILE, master_logger):\n","            master_logger.log(\"CRITICAL: Phase 4 failed. Halting pipeline.\")\n","            return\n","\n","    # Phase 5\n","    if RUN_PHASE_5:\n","        if not os.path.exists(P4_FINAL_ANALYSIS_FILE):\n","            master_logger.log(f\"CRITICAL: Input for Phase 5 ({P4_FINAL_ANALYSIS_FILE}) not found. Halting.\")\n","            return\n","        if not run_phase5_generate_report(P4_FINAL_ANALYSIS_FILE, P5_STAKEHOLDER_REPORT_FILE, QUESTION_DEFINITIONS_FILE, master_logger):\n","            master_logger.log(\"CRITICAL: Phase 5 failed.\")\n","            return\n","\n","    master_logger.log(\"======== PIPELINE FINISHED SUCCESSFULLY ========\")\n","\n","# --- Execute the Pipeline ---\n","if __name__ == \"__main__\":\n","    main_pipeline()"]},{"cell_type":"markdown","metadata":{"id":"uPfIdlGdP7RP"},"source":["# Pre-requisites"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlRaGVHDebOm"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwkuBBweecko"},"outputs":[],"source":["cd /content/drive/MyDrive/Colab Notebooks/Legal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bJKB08Q00N_"},"outputs":[],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94RukJ_YITAA"},"outputs":[],"source":["pip install -U transformers bitsandbytes accelerate"]},{"cell_type":"markdown","metadata":{"id":"zc_pBaF8Y4W0"},"source":["# 30th May"]},{"cell_type":"markdown","metadata":{"id":"-HqrZt7uZbMM"},"source":["## UseCase - Classes Mapping"]},{"cell_type":"markdown","metadata":{"id":"43aZ6lqaZrLB"},"source":["| **Use Case**                                  | **Primary Service(s)**                                                                    | **Key Domain Objects (Created/Used)**                                                                                                     | **Core Interaction Summary**                                                         | **Clarified Notes / Output**                                                                                   |\n","| --------------------------------------------- | ----------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------- |\n","| **UC-01: Process Single Survey Response**     | `UDSGenerationService`, `FormProcessor`, `PDFProcessor`                                   | `SurveyResponse` (E), `FormEntry` (VO), `PDFDocument` (VO)                                                                                | Parses form and PDF input into unified structure                                     | Embeddings via `EmbeddingModel` used inside `PDFProcessor`; output is a complete `SurveyResponse`              |\n","| **UC-02: Aggregate Responses for QID Corpus** | `UDSGenerationService`, `PDFProcessor`                                                    | `QuestionCentricCorpus` (VO), `ResponseContentItem` (E)                                                                                   | Gathers form and PDF-derived text per QID                                            | Semantic match from `PDFProcessor`; builds QID-aligned `ResponseContentItem`s                                  |\n","| **UC-03: Generate Candidate Motifs**          | `MDLThematicAnalysisService`, `MotifCandidateGenerator`, `LLMService`, `MDLTextProcessor` | `ResponseContentItem` (E, used), `MDLMotif` (E, created)                                                                                  | Seeds and labels generated by LLM, scored with `L_H_cost`                            | `MDLTextProcessor` computes `L(H)` for each surface form candidate                                             |\n","| **UC-04: Optimize Codebook**                  | `MDLThematicAnalysisService`, `CodebookOptimizer`, `MDLTextProcessor`                     | `ThematicUnit` (E, selected from candidates), `OptimizedCodebookOutput` (VO, created), `MDLMotif` (E, input pool), `CostFunctionStrategy` | Selects motifs to minimize total cost *L(H) + L(Dâ€‰\\|â€‰H)* using greedy or hybrid loop | Uses all `ResponseContentItem.text_content` (via `QuestionCentricCorpus`) to compute *L(Dâ€‰\\|â€‰H)*               |\n","| **UC-05: Enrich Thematic Units**              | `ThemeEnricher`, `LLMService`                                                             | `ThematicUnit` (E, enriched), `SentimentValue` (VO)                                                                                       | LLM generates explanations, extracts sentiment                                       | Each `ThematicUnit` is enriched with natural language explanation and sentiment                                |\n","| **UC-06: Human Validates Themes**             | `HumanReviewService`, `ReviewUI`                                                          | `ValidatedThematicModel` (E collection), `QIDStreamAnalysisResult` (VO)                                                                   | Analyst edits labels, merges/deletes motifs via UI                                   | Feedback updates `ValidatedThematicModel`; may trigger re-optimization if edits affect coverage significantly  |\n","| **UC-07: Generate Stakeholder Report**        | `ReportGenerationService`, `LLMService`                                                   | `StakeholderReport` (E), `QIDReportSection` (VO), `MetaTheme` (VO), `Quote` (VO)                                                          | Synthesizes themes, assembles quotes and summaries                                   | Pulls quotes from `QuestionCentricCorpus`, themes from `ValidatedThematicModel`, meta-themes from `LLMService` |\n"]},{"cell_type":"markdown","metadata":{"id":"0X0403tNiK_5"},"source":["## Textual Mapping\n","\n","---\n","\n","**Mapping Use Cases to Domain Model Components (Textual Description)**\n","\n","**Recall Key Domain Model Components:**\n","*   **Services:** `ConfigurationService`, `UDSGenerationService`, `MDLThematicAnalysisService` (uses `MotifCandidateGenerator`, `CodebookOptimizer`, `ThemeEnricher`), `BDMProcessorService`, `LLMService`, `HumanReviewService`, `ReportGenerationService`.\n","*   **Entities:** `SurveyResponse`, `QuestionDefinition`, `ThematicUnit` (was `MDLMotif`), `ResponseContentItem`, `StakeholderReport`.\n","*   **Value Objects:** `FormEntry`, `PDFDocument`, `QuestionCentricCorpus`, `OptimizedCodebookOutput`, `SentimentValue`, `Quote`, `QIDReportSection`, `MetaTheme`.\n","\n","---\n","\n","**UC-01: Process a Single Survey Response to Create Unified Response Object (Phase 1B)**\n","\n","*   **Primary Service:** `UDSGenerationService`\n","*   **Key Entities/VOs Involved:**\n","    *   **Input:** Raw response data (dict), `QuestionDefinition`s (read)\n","    *   **Created/Populated:** `SurveyResponse` (Entity), `FormEntry` (VO), `PDFDocument` (VO)\n","*   **High-Level Interaction:**\n","    1.  `UDSGenerationService` receives raw data for one submission.\n","    2.  It calls internal helper methods (or sub-services like `FormProcessor`, `PDFProcessor` from earlier discussions, which are now conceptually part of `UDSGenerationService`'s responsibility for this phase).\n","    3.  `FormProcessor` logic creates `FormEntry` VOs.\n","    4.  `PDFProcessor` logic creates a `PDFDocument` VO (with sentences/simple chunks from PDF text).\n","    5.  `UDSGenerationService` instantiates a `SurveyResponse` entity and populates it with the `FormEntry` list, `PDFDocument`, and metadata.\n","    6.  The created `SurveyResponse` entity is returned/stored.\n","\n","---\n","\n","**UC-02: Aggregate All Responses to Create Question-Centric Corpus (Phase 1C)**\n","\n","*   **Primary Service:** `UDSGenerationService`\n","*   **Key Entities/VOs Involved:**\n","    *   **Input:** List of `SurveyResponse` (Entities - read), `QuestionDefinition`s (Entities - read)\n","    *   **Created/Populated:** `QuestionCentricCorpus` (VO), `ResponseContentItem` (Entities within the corpus)\n","*   **High-Level Interaction:**\n","    1.  `UDSGenerationService` iterates through each `QuestionDefinition`.\n","    2.  For each QID, it iterates through all `SurveyResponse` entities.\n","    3.  It extracts relevant `FormEntry.cleaned_answer_text/cleaned_selection_labels`.\n","    4.  It invokes logic (conceptually from `PDFProcessor`, now part of `UDSGenerationService` or called by it) to perform semantic matching between the `QuestionDefinition.question_text` and the `SurveyResponse.pdf_document.sentences/simple_chunks`.\n","    5.  For each piece of relevant form text or matched PDF snippet, it creates a `ResponseContentItem` entity, linking it to the `SurveyResponse` and `QuestionDefinition`.\n","    6.  These `ResponseContentItem`s are aggregated into the `QuestionCentricCorpus` VO, typically as a dictionary mapping QID to a list of its `ResponseContentItem`s.\n","\n","---\n","\n","**UC-03: Generate Candidate Motifs for a QID's Text Stream (Phase 2A)**\n","\n","*   **Primary Service:** `MDLThematicAnalysisService` (delegates to `MotifCandidateGenerator`)\n","*   **Key Entities/VOs Involved:**\n","    *   **Input:** List of `ResponseContentItem` (Entities for a specific QID/stream - read from `QuestionCentricCorpus`), `QuestionDefinition.question_text` (read)\n","    *   **Services Used:** `LLMService`, `BDMProcessorService` (new name for `MDLTextProcessor` if focusing on BDM)\n","    *   **Created/Populated:** List of `ThematicUnit` (Entities - in their candidate \"motif\" stage, `L_H_cost` calculated)\n","*   **High-Level Interaction:**\n","    1.  `MDLThematicAnalysisService` receives `ResponseContentItem`s for a QID.\n","    2.  It calls `MotifCandidateGenerator.generate_candidate_motifs()`.\n","    3.  `MotifCandidateGenerator` generates seeds from the input texts.\n","    4.  For each seed, it calls `LLMService` to propose raw motif labels and surface forms.\n","    5.  (If multi-agent: Coder -> Aggregator flow to refine raw proposals).\n","    6.  For each refined proposal, it instantiates a `ThematicUnit` (motif candidate).\n","    7.  It calls `BDMProcessorService.calculate_L_H_motif_bdm()` (using the `get_motif_definition_representation_string`) to set the `L_H_cost` on each candidate `ThematicUnit`.\n","    8.  Returns the pool of candidate `ThematicUnit`s.\n","\n","---\n","\n","**UC-04: Optimize Codebook for a QID's Text Stream (Phase 2B)**\n","\n","*   **Primary Service:** `MDLThematicAnalysisService` (delegates to `CodebookOptimizer`)\n","*   **Key Entities/VOs Involved:**\n","    *   **Input:** Pool of candidate `ThematicUnit` (Entities - read), List of `ResponseContentItem.text_content` (for the QID/stream - read)\n","    *   **Services Used:** `BDMProcessorService` (and its configured `CostFunctionStrategy` which is now BDM-based)\n","    *   **Created/Populated:** `OptimizedCodebookOutput` (VO)\n","*   **High-Level Interaction:**\n","    1.  `MDLThematicAnalysisService` calls `CodebookOptimizer.optimize_codebook_greedy()`.\n","    2.  `CodebookOptimizer` iterates:\n","        *   Tries adding candidate `ThematicUnit`s to a temporary codebook.\n","        *   For each temporary codebook, it calls `BDMProcessorService.get_mdl_description_string_one_part()` to get the combined model+data string.\n","        *   It then calls `BDMProcessorService.calculate_bdm_for_text()` on this string to get the total MDL cost.\n","        *   Selects candidates that minimize this total cost.\n","    3.  Once the optimal codebook (list of selected `ThematicUnit` entities) is found, the `CodebookOptimizer` (or `MDLThematicAnalysisService`) uses the `BDMProcessorService` again to calculate final stats for the `OptimizedCodebookOutput` VO:\n","        *   `total_L_H_codebook`: Sum of `L_H_cost` for selected `ThematicUnit`s.\n","        *   `total_L_D_given_H_codebook`: `BDM(BDMProcessorService.get_corpus_encoded_with_codebook_string(...))`\n","        *   `thematic_unit_usage_counts`: Derived during the calculation of the encoded corpus string. These counts are then used to populate `ThematicUnit.usage_in_optimized_codebook_count` for the selected units.\n","    4.  Returns the `OptimizedCodebookOutput` VO.\n","\n","---\n","\n","**UC-05: Enrich Thematic Units in an Optimized Codebook (Phase 2C)**\n","\n","*   **Primary Service:** `MDLThematicAnalysisService` (delegates to `ThemeEnricher`)\n","*   **Key Entities/VOs Involved:**\n","    *   **Input:** `OptimizedCodebookOutput` (VO - specifically the list of selected `ThematicUnit.unit_id`s), and the corresponding `ThematicUnit` entities.\n","    *   **Services Used:** `LLMService`\n","    *   **Updated:** `ThematicUnit` (Entities - `explanation` and `sentiment_summary` attributes populated)\n","*   **High-Level Interaction:**\n","    1.  `MDLThematicAnalysisService` calls `ThemeEnricher.enrich_themes()`.\n","    2.  `ThemeEnricher` iterates through the selected `ThematicUnit`s.\n","    3.  Calls `LLMService.get_theme_explanation()` for each.\n","    4.  Calls `LLMService.get_text_sentiment()` (on samples of text covered by the theme, or on its surface forms/explanation).\n","    5.  Updates the `explanation` and `sentiment_summary` (VO) attributes of the `ThematicUnit` entities.\n","\n","---\n","\n","**UC-06: Human Validates/Refines System-Generated Themes for a QID (Phase 3)**\n","\n","*   **Primary Service:** `HumanReviewService`\n","*   **Key Entities/VOs Involved:**\n","    *   **Input:** The output of `MDLThematicAnalysisService` (which is the `OptimizedCodebookOutput` and the list of enriched `ThematicUnit`s it represents for each QID/stream).\n","    *   **External:** Human Analyst, Review UI/Tool.\n","    *   **Created/Updated:** `ValidatedThematicModel` (a collection/structure holding the final, human-approved `ThematicUnit` entities). `ThematicUnit` attributes like `is_human_validated`, `human_validation_notes`, and potentially `label`, `explanation`, `surface_forms_or_keyphrases` can be modified.\n","*   **High-Level Interaction:**\n","    1.  `HumanReviewService` fetches/formats data for the UI.\n","    2.  Human Analyst interacts, makes changes.\n","    3.  `HumanReviewService` processes feedback, updates the `ThematicUnit` entities (potentially creating new ones or versioning).\n","    4.  If a `ThematicUnit`'s definition (surface forms, label) changes significantly, its `L_H_cost` needs re-calculation (via `BDMProcessorService`), and the parent `OptimizedCodebookOutput` becomes \"stale\" for that QID, potentially triggering a re-optimization run via `MDLThematicAnalysisService`.\n","\n","---\n","\n","**UC-07: Generate Stakeholder Report (Phase 4)**\n","\n","*   **Primary Service:** `ReportGenerationService`\n","*   **Key Entities/VOs Involved:**\n","    *   **Input:** `ValidatedThematicModel` (collection of final `ThematicUnit` entities), `QuestionCentricCorpus` (for `ResponseContentItem`s to get original quote text), `QuestionDefinition`s.\n","    *   **Services Used:** `LLMService` (for meta-theme synthesis).\n","    *   **Created/Populated:** `StakeholderReport` (Entity), `QIDReportSection` (VO), `MetaTheme` (VO), `Quote` (VO).\n","*   **High-Level Interaction:**\n","    1.  `ReportGenerationService` iterates through QIDs in the `ValidatedThematicModel`.\n","    2.  For each validated `ThematicUnit`:\n","        *   Uses its `usage_in_optimized_codebook_count` for prevalence.\n","        *   Finds associated `ResponseContentItem`s (those classified under this `ThematicUnit.unit_id`).\n","        *   Extracts `Quote` VOs from the `text_content` of these items.\n","    3.  Calls `LLMService` (or an internal algorithm) to synthesize `MetaTheme` VOs from all validated `ThematicUnit`s.\n","    4.  Assembles all information into `QIDReportSection` VOs and the overall `StakeholderReport` entity.\n","\n","---\n","\n","This mapping provides a clearer picture of:\n","\n","*   **Who does what:** Responsibilities of each service.\n","*   **Data dependencies:** What data each service needs and produces.\n","*   **Lifecycle of key entities:** How `ThematicUnit` evolves from a candidate to an enriched, validated theme.\n","*   **Centrality of `BDMProcessorService` and `LLMService`:** They are key utilities.\n"]},{"cell_type":"markdown","metadata":{"id":"4KNLkRfZjCXr"},"source":["## Class-centric tables\n","\n","---\n","\n","### ðŸ“˜ `SurveyResponse` (Entity)\n","\n","```\n","Use Cases:\n","  UC-01: CREATE (parsed from PDF)\n","  UC-02: READ (for organizing into QID corpus)\n","```\n","\n","---\n","\n","### ðŸ“˜ `ResponseContentItem` (Entity)\n","\n","```\n","Use Cases:\n","  UC-02: CREATE (split from SurveyResponse)\n","  UC-03: READ (to generate motifs)\n","  UC-04: READ (to compute L(D|H_codebook))\n","  UC-07: READ (source of quotes)\n","```\n","\n","---\n","\n","### ðŸ“˜ `QuestionCentricCorpus` (VO)\n","\n","```\n","Use Cases:\n","  UC-02: CREATE (output of corpus builder)\n","  UC-03: READ (for motif generation)\n","  UC-04: READ (for BDM-based optimization)\n","```\n","\n","---\n","\n","### ðŸ“˜ `MDLMotif` (VO â€” candidate motifs)\n","\n","```\n","Use Cases:\n","  UC-03: CREATE (generated motifs with L_H_cost)\n","  UC-04: READ/SELECT (from candidate set)\n","```\n","\n","---\n","\n","### ðŸ“˜ `L_H_cost` (VO â€” motif-level BDM cost)\n","\n","```\n","Use Cases:\n","  UC-03: CREATE (computed using BDMCalculator)\n","  UC-04: READ (used for codebook selection)\n","```\n","\n","---\n","\n","### ðŸ“˜ `ThematicUnit` (Entity â€” selected motifs)\n","\n","```\n","Use Cases:\n","  UC-04: CREATE (selected motif â†’ thematic unit)\n","  UC-05: UPDATE (enriched with sentiment, metadata)\n","  UC-06: READ/UPDATE (reviewer may accept/reject, annotate)\n","  UC-07: READ (for meta-theme synthesis)\n","```\n","\n","---\n","\n","### ðŸ“˜ `OptimizedCodebookOutput` (VO, versioned snapshot)\n","\n","```\n","Use Cases:\n","  UC-04: CREATE (after motif selection)\n","  UC-06: READ (validated model built from this)\n","```\n","\n","---\n","\n","### ðŸ“˜ `ValidatedThematicModel` (Entity)\n","\n","```\n","Use Cases:\n","  UC-06: CREATE (based on codebook output)\n","  UC-07: READ (for meta-theme synthesis)\n","```\n","\n","---\n","\n","### ðŸ“˜ `MetaTheme` (VO)\n","\n","```\n","Use Cases:\n","  UC-07: CREATE (clustered across QIDs)\n","```\n","\n","---\n","\n","### ðŸ“˜ `Quote` (VO)\n","\n","```\n","Use Cases:\n","  UC-07: CREATE (extracted from `ResponseContentItem`)\n","```\n","\n","---\n","\n","### ðŸ“˜ `StakeholderReport` (Entity)\n","\n","```\n","Use Cases:\n","  UC-07: CREATE (report object with quotes + themes)\n","```\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"La4gC9_a_KGG"},"source":[]},{"cell_type":"markdown","metadata":{"id":"I_vAeOoBY9np"},"source":["# 29th May"]},{"cell_type":"markdown","metadata":{"id":"COEbo_tFUTFP"},"source":["## Use Cases\n","\n","That's an excellent next step! Turning the static domain model into dynamic diagrams (like sequence diagrams or activity diagrams) helps visualize how these components *collaborate* to achieve specific tasks. These diagrams are crucial for understanding the runtime behavior and interactions.\n","\n","Here are key **Use Cases** that would be ideal for illustrating with dynamic diagrams:\n","\n","**I. Core Data Processing & UDS Generation (Conceptual Phase 1):**\n","\n","1.  **UC-01: Process a Single Survey Response to Create Unified Response Object:**\n","    *   **Goal:** Show how raw form data and PDF text for one submission are processed and unified.\n","    *   **Actors/Components:** `UDSGenerationService`, `FormProcessor`, `PDFProcessor`, `EmbeddingModel`, `SurveyResponse` (Entity being created), `FormEntry` (VO), `PDFDocument` (VO).\n","    *   **Flow Highlights:**\n","        *   `UDSGenerationService` receives raw response data.\n","        *   Delegates form processing to `FormProcessor`.\n","        *   Delegates PDF processing to `PDFProcessor` (extraction, minimal structuring like sentence tokenization).\n","        *   `UDSGenerationService` assembles the `SurveyResponse` entity.\n","    *   **Diagram Type:** Sequence Diagram.\n","\n","2.  **UC-02: Aggregate All Responses to Create Question-Centric Corpus:**\n","    *   **Goal:** Show how all unified responses are processed to collate text relevant to each question.\n","    *   **Actors/Components:** `UDSGenerationService`, `SurveyResponse` (Entities being read), `QuestionDefinition` (Entities being read), `PDFProcessor` (for semantic matching of PDF snippets to questions), `QuestionCentricCorpus` (VO being created), `ResponseContentItem` (Entities being created).\n","    *   **Flow Highlights:**\n","        *   `UDSGenerationService` iterates through `SurveyResponse` objects.\n","        *   For each QID, it extracts relevant form entries.\n","        *   For each QID, it uses `PDFProcessor.get_relevant_pdf_snippets_for_question()` (or similar method) to find matching PDF content from each `SurveyResponse.pdf_document.sentences/simple_chunks`.\n","        *   Populates `QuestionCentricCorpus` with `ResponseContentItem`s.\n","    *   **Diagram Type:** Activity Diagram (for overall flow) or a Sequence Diagram focusing on one QID's aggregation.\n","\n","**II. MDL-Driven Thematic Analysis (Conceptual Phase 2 - per QID, per Stream):**\n","\n","3.  **UC-03: Generate Candidate Motifs for a QID's Text Stream:**\n","    *   **Goal:** Illustrate the \"New Phase 3\" logic â€“ specifically the `MotifCandidateGenerator`'s operation.\n","    *   **Actors/Components:** `MDLThematicAnalysisService`, `MotifCandidateGenerator`, `LLMService`, `MDLTextProcessor`, `ResponseContentItem`s (for the QID/stream), `MDLMotif` (Entities being created).\n","        *   *If using Multi-Agent:* `CoderAgent`, `AggregatorAgent`.\n","    *   **Flow Highlights:**\n","        *   `MDLThematicAnalysisService` invokes `MotifCandidateGenerator`.\n","        *   `MotifCandidateGenerator` generates seeds.\n","        *   Calls `LLMService` to get raw motif proposals (labels, surface forms).\n","        *   (If multi-agent: show Coder -> Aggregator flow).\n","        *   Uses `MDLTextProcessor` to tokenize surface forms and calculate initial `L_H_cost` for each `MDLMotif`.\n","    *   **Diagram Type:** Sequence Diagram.\n","\n","4.  **UC-04: Optimize Codebook for a QID's Text Stream:**\n","    *   **Goal:** Illustrate the \"New Phase 3.5\" logic â€“ specifically the `CodebookOptimizer`'s operation.\n","    *   **Actors/Components:** `MDLThematicAnalysisService`, `CodebookOptimizer`, `MDLTextProcessor`, `CostFunctionStrategy`, candidate `MDLMotif` pool, `ResponseContentItem`s (for the QID/stream), `OptimizedCodebookOutput` (VO being created).\n","    *   **Flow Highlights:**\n","        *   `MDLThematicAnalysisService` invokes `CodebookOptimizer`.\n","        *   `CodebookOptimizer`'s iterative greedy loop:\n","            *   For each candidate motif:\n","                *   Temporarily add to current codebook.\n","                *   Call `MDLTextProcessor.calculate_L_D_given_codebook()` (which uses `CostFunctionStrategy`).\n","                *   Call `MDLTextProcessor.calculate_L_H_for_motif()` for the sum of `L(H)` for the temporary codebook.\n","                *   Evaluate total MDL cost.\n","            *   Select best motif to add.\n","        *   Return `OptimizedCodebookOutput`.\n","    *   **Diagram Type:** Activity Diagram (for the iterative loop) and potentially a Sequence Diagram for one iteration.\n","\n","5.  **UC-05: Enrich Thematic Units in an Optimized Codebook:**\n","    *   **Goal:** Show how selected themes are enriched with explanations and sentiment.\n","    *   **Actors/Components:** `MDLThematicAnalysisService`, `ThemeEnricher`, `LLMService`, selected `ThematicUnit`s (from `OptimizedCodebookOutput`), `SentimentValue` (VO).\n","    *   **Flow Highlights:**\n","        *   `MDLThematicAnalysisService` invokes `ThemeEnricher`.\n","        *   `ThemeEnricher` iterates through `ThematicUnit`s.\n","        *   Calls `LLMService` to generate explanations.\n","        *   Calls `LLMService` (or a dedicated sentiment model) to get sentiment for text covered by the theme.\n","        *   Updates `ThematicUnit` objects.\n","    *   **Diagram Type:** Sequence Diagram.\n","\n","**III. Human Review & Validation (Conceptual Phase 3):**\n","\n","6.  **UC-06: Human Validates/Refines System-Generated Themes for a QID:**\n","    *   **Goal:** Show the interaction between the system and a human analyst.\n","    *   **Actors/Components:** `HumanReviewService`, `Human Analyst` (External Actor), `Review UI/Tool` (External System), `QIDStreamAnalysisResult` (VO being reviewed), `ValidatedThematicModel` (Entity collection being updated/created).\n","    *   **Flow Highlights:**\n","        *   `HumanReviewService` presents themes from `QIDStreamAnalysisResult` via UI.\n","        *   Human Analyst performs actions (validate, edit label/explanation/SFs, merge, add, delete).\n","        *   UI sends feedback to `HumanReviewService`.\n","        *   `HumanReviewService` updates/creates `ThematicUnit`s in the `ValidatedThematicModel`.\n","        *   *Crucial loop:* If a motif definition changes significantly, `HumanReviewService` might flag the need to re-run `CodebookOptimizer` for that QID.\n","    *   **Diagram Type:** Use Case Diagram (high level) plus a Sequence Diagram for a specific interaction (e.g., \"Human edits a theme label\").\n","\n","**IV. Reporting & Synthesis (Conceptual Phase 4):**\n","\n","7.  **UC-07: Generate Stakeholder Report:**\n","    *   **Goal:** Show how the final report is assembled.\n","    *   **Actors/Components:** `ReportGenerationService`, `ValidatedThematicModel`, `QuestionCentricCorpus` (for quotes), `LLMService` (for meta-theme synthesis), `StakeholderReport` (Entity being created), `QIDReportSection` (VO), `MetaTheme` (VO), `Quote` (VO).\n","    *   **Flow Highlights:**\n","        *   `ReportGenerationService` retrieves validated themes.\n","        *   Extracts quotes from `QuestionCentricCorpus` based on `ResponseContentItem`s linked to validated `ThematicUnit`s.\n","        *   Optionally calls `LLMService` to synthesize meta-themes from all validated `ThematicUnit`s.\n","        *   Assembles all data into the `StakeholderReport` structure.\n","    *   **Diagram Type:** Sequence Diagram or Activity Diagram.\n","\n","**Why these use cases?**\n","\n","*   They cover the **critical paths** through your proposed architecture.\n","*   They highlight interactions between the **key new components** (MDL-related services).\n","*   They show how **data flows and transforms** from raw input to final report.\n","*   They help clarify the **responsibilities** of each service and entity.\n","*   They address the **main goals** of the problem statement (theme discovery, quantification, quoting, synthesis, human review optimization).\n","\n","By sketching out these dynamic interactions, you'll get a much better feel for the runtime behavior, identify potential bottlenecks or missing interfaces, and have a clearer guide for implementation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NG1eelTlnS0o"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"43Z64jZVZBxx"},"source":["## Revised Problem Statement\n","\n","\"To design and implement a scalable, end-to-end system for analyzing heterogeneous survey submissions, which include **sensitive and private** structured form data and unstructured PDF documents. The system must:\n","\n","1.  **Identify, delineate, and categorize recurring common points, arguments, themes, or concepts** (\"thematic units\") expressed by respondents. This includes **classifying relevant segments of source text as instances of these thematic units**, both within individual survey questions and at a holistic, cross-question level.\n","2.  Determine the prevalence (e.g., frequency, number of unique respondents, extent of discussion) of each identified thematic unit based on evidence from the original source texts.\n","3.  Extract concise, representative excerpts (quotes) from the original source texts that clearly illustrate each thematic unit.\n","4.  Optionally classify additional attributes associated with thematic units or responses, such as sentiment or respondent type, where feasible and valuable for deeper understanding.\n","5.  Enable the synthesis of insights by facilitating comparison of thematic units (and their associated attributes) derived from structured form data versus unstructured PDF content, and by revealing connections or overarching narratives across different survey questions.\n","6.  Produce verifiable, auditable, and actionable reports summarizing the thematic findings, their classifications, prevalence, and supporting textual evidence, suitable for stakeholder communication and decision-making.\n","7.  Optimize the efficiency and effectiveness of the human review process, enabling analysts to focus on high-level validation, interpretation of complex or ambiguous findings, and strategic insight generation, rather than exhaustive manual coding and classification of all raw data.\n","8.  **Support robust system development, testing, and demonstration through the capability to generate high-fidelity synthetic survey data that mimics the characteristics of real submissions while ensuring the privacy and confidentiality of the original sensitive data.**\"\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H35S75GGnQZG"},"source":["# 28th May\n"]},{"cell_type":"markdown","metadata":{"id":"zgGXMEt-YG7u"},"source":["## Problem Statement\n","\n","\"Develop an end-to-end pipeline to analyze survey submissions, which include structured form data and unstructured PDF text, to identify common points/themes, their frequency, and illustrative quotes for stakeholder communication.\"\n","\n","**Revised Problem Statement (V2 - Solution Agnostic):**\n","\n","To design and implement a scalable, end-to-end system for analyzing heterogeneous survey submissions, which include sensitive and private structured form data and unstructured PDF documents. The system must:\n","\n","1.  **Identify, delineate, and categorize recurring common points, arguments, themes, or concepts** (\"thematic units\") expressed by respondents. This includes **classifying relevant segments of source text as instances of these thematic units**, both within individual survey questions and at a holistic, cross-question level.\n","2.  Determine the prevalence (e.g., frequency, number of unique respondents, extent of discussion) of each identified thematic unit based on evidence from the original source texts.\n","3.  Extract concise, representative excerpts (quotes) from the original source texts that clearly illustrate each thematic unit.\n","4.  **Optionally classify additional attributes associated with thematic units or responses**, such as sentiment or respondent type, where feasible and valuable for deeper understanding.\n","5.  Enable the synthesis of insights by facilitating comparison of thematic units (and their associated attributes) derived from structured form data versus unstructured PDF content, and by revealing connections or overarching narratives across different survey questions.\n","6.  Produce verifiable, auditable, and actionable reports summarizing the thematic findings, their classifications, prevalence, and supporting textual evidence, suitable for stakeholder communication and decision-making.\n","7.  Optimize the efficiency and effectiveness of the human review process, enabling analysts to focus on high-level validation, interpretation of complex or ambiguous findings, and strategic insight generation, rather than exhaustive manual coding and classification of all raw data."]},{"cell_type":"markdown","metadata":{"id":"YhsRiVY1X5Kb"},"source":["## Conceptual Model\n","\n","Excellent! Let's translate that conceptual solution into a **Domain Model** by identifying key Entities, Value Objects, and Services. This will provide the blueprint for an Object-Oriented implementation.\n","\n","We'll aim for a model that is rich enough to support the functionality but also cohesive and maintainable.\n","\n","---\n","\n","**I. Core Entities (Representing key domain concepts with identity and lifecycle):**\n","\n","1.  **`SurveyResponse` (Replaces `UnifiedResponse` for clarity, focus on the input submission)**\n","    *   **Attributes:**\n","        *   `response_id: str` (Primary Key)\n","        *   `submission_timestamp: datetime`\n","        *   `source_metadata: Dict[str, Any]` (e.g., original filename, URL if applicable, submitter type if known initially)\n","        *   `form_entries: List[FormEntry]` (see Value Objects)\n","        *   `pdf_document: Optional[PDFDocument]` (see Value Objects)\n","        *   `processing_status: str` (e.g., \"raw\", \"uds_generated\", \"analysis_complete\")\n","        *   `analysis_errors: List[str]`\n","    *   **Purpose:** Represents a single, complete submission from one respondent. This is the atomic unit of input.\n","\n","2.  **`QuestionDefinition`**\n","    *   **Attributes:**\n","        *   `qid: str` (Primary Key, e.g., \"Q1\", \"Q4\")\n","        *   `question_text: str`\n","        *   `question_type: str` (e.g., \"open_text_form\", \"multiple_choice_form\", \"pdf_section_general\")\n","        *   `response_options: Optional[List[str]]` (for multiple choice/selection)\n","        *   `related_form_field_ids: Optional[List[str]]` (mapping to raw form field identifiers)\n","    *   **Purpose:** Defines each question in the survey instrument.\n","\n","3.  **`ThematicUnit` (Represents an MDL Motif / Theme after selection & potential enrichment)**\n","    *   **Attributes:**\n","        *   `unit_id: str` (UUID or composite key: `qid_stream_labelhash`)\n","        *   `qid: str` (Foreign Key to `QuestionDefinition`)\n","        *   `data_stream_source: str` (\"form_text\", \"pdf_text\", \"synthesized\")\n","        *   `label: str` (e.g., `[PRICE_EXCESSIVE]`, or human-readable refined label)\n","        *   `explanation: Optional[str]` (LLM or human-generated)\n","        *   `surface_forms_or_keyphrases: List[str]` (Key defining phrases)\n","        *   `L_H_cost: Optional[float]` (MDL: Cost of defining this unit)\n","        *   `usage_count: Optional[int]` (MDL: How many times used to compress source)\n","        *   `overall_mdl_score: Optional[float]` (Or other confidence/relevance score)\n","        *   `sentiment_summary: Optional[SentimentValue]` (see Value Objects)\n","        *   `is_human_validated: bool`\n","        *   `human_validation_notes: Optional[str]`\n","    *   **Purpose:** Represents a single, distinct theme or motif identified for a specific question and data stream. This is a core output of the MDL process (Phase 2B/2C) and is refined in Phase 3.\n","\n","4.  **`ResponseContentItem` (Represents a piece of text from a response relevant to a QID)**\n","    *   **Attributes:**\n","        *   `item_id: str` (UUID)\n","        *   `response_id: str` (Foreign Key to `SurveyResponse`)\n","        *   `qid: str` (Foreign Key to `QuestionDefinition`)\n","        *   `source_type: str` (\"form_text_field\", \"form_selection_label\", \"pdf_sentence\", \"pdf_simple_chunk\")\n","        *   `text_content: str`\n","        *   `original_location_info: Optional[Dict[str, Any]]` (e.g., PDF page, form field ID)\n","        *   `relevance_score_to_qid: Optional[float]` (if applicable from matching)\n","        *   `assigned_thematic_unit_ids: List[str]` (Foreign Keys to `ThematicUnit` if classified)\n","    *   **Purpose:** Atomic unit of text collated for a QID from a specific response, used as input for MDL and for quote extraction. This forms the content of the `QuestionCentricCorpus`.\n","\n","5.  **`StakeholderReport`**\n","    *   **Attributes:**\n","        *   `report_id: str` (UUID)\n","        *   `generation_timestamp: datetime`\n","        *   `configuration_snapshot: Dict[str, Any]` (key parameters used for this run)\n","        *   `per_qid_analysis: Dict[str, QIDReportSection]` (see Value Objects)\n","        *   `meta_themes: List[MetaTheme]` (see Value Objects)\n","        *   `summary_statistics: Dict[str, Any]`\n","    *   **Purpose:** Represents a generated report.\n","\n","---\n","\n","**II. Value Objects (Immutable data structures, describe attributes of Entities):**\n","\n","1.  **`FormEntry` (Attribute of `SurveyResponse`)**\n","    *   **Attributes:**\n","        *   `field_id: str` (Original ID from the form data)\n","        *   `field_label_or_qid: str` (The QID or a descriptive label)\n","        *   `answer_type: str` (\"text\", \"single_selection\", \"multi_selection\")\n","        *   `raw_answer: Any`\n","        *   `cleaned_answer_text: Optional[str]` (for text fields)\n","        *   `cleaned_selection_labels: Optional[List[str]]` (for selections)\n","\n","2.  **`PDFDocument` (Attribute of `SurveyResponse`)**\n","    *   **Attributes:**\n","        *   `full_text: str`\n","        *   `sentences: List[str]` (or `simple_chunks: List[str]`)\n","        *   `page_count: Optional[int]`\n","        *   `word_count: Optional[int]`\n","        *   `legibility_score: Optional[float]`\n","        *   `processing_notes: List[str]` (e.g., \"OCR issues on page X\")\n","\n","3.  **`SentimentValue` (Attribute of `ThematicUnit`)**\n","    *   **Attributes:**\n","        *   `overall_sentiment_label: str` (\"positive\", \"negative\", \"neutral\", \"mixed\")\n","        *   `sentiment_score: float`\n","        *   `positive_score: float`\n","        *   `negative_score: float`\n","        *   `neutral_score: float`\n","\n","4.  **`Quote` (Part of `QIDReportSection`)**\n","    *   **Attributes:**\n","        *   `text: str`\n","        *   `response_id: str`\n","        *   `source_content_item_id: str` (linking back to the `ResponseContentItem`)\n","        *   `context_snippet: Optional[str]`\n","\n","5.  **`QIDReportSection` (Attribute of `StakeholderReport`)**\n","    *   **Attributes:**\n","        *   `qid: str`\n","        *   `question_text: str`\n","        *   `form_data_themes: List[ThematicUnit]` (or just their IDs/summaries)\n","        *   `pdf_data_themes: List[ThematicUnit]` (or just their IDs/summaries)\n","        *   `synthesized_qid_summary: Optional[str]`\n","        *   `illustrative_quotes_by_theme: Dict[str, List[Quote]]` (theme_unit_id -> quotes)\n","        *   `statistics: Dict[str, Any]` (e.g., num_responses for this QID)\n","\n","6.  **`MetaTheme` (Attribute of `StakeholderReport`)**\n","    *   **Attributes:**\n","        *   `meta_theme_id: str`\n","        *   `meta_theme_label: str`\n","        *   `meta_theme_explanation: str`\n","        *   `contributing_qids: List[str]`\n","        *   `contributing_thematic_unit_ids: List[str]`\n","        *   `supporting_evidence_summary: Optional[str]`\n","\n","---\n","\n","**III. Services (Encapsulate domain logic and operations/processes):**\n","\n","1.  **`UDSGenerationService` (Handles Conceptual Phase 1)**\n","    *   **Methods:**\n","        *   `load_raw_survey_data(filepath: str) -> List[RawResponseDict]`\n","        *   `load_question_definitions(filepath: str) -> Dict[str, QuestionDefinition]`\n","        *   `create_response_centric_uds(raw_responses: List[RawResponseDict], question_defs: Dict[str, QuestionDefinition]) -> List[SurveyResponse]`\n","            *   Internally uses `FormProcessor` and `PDFProcessor` (which could also be services or just helper classes instantiated here).\n","        *   `create_question_centric_corpus(survey_responses: List[SurveyResponse], question_defs: Dict[str, QuestionDefinition]) -> Dict[str, List[ResponseContentItem]]` (*This output is the `QuestionCentricCorpus`*)\n","\n","2.  **`MDLThematicAnalysisService` (Handles Conceptual Phase 2 for one stream & QID)**\n","    *   **Attributes:** `mdl_processor: MDLTextProcessor`, `llm_client`, `logger`.\n","    *   **Methods:**\n","        *   `generate_candidate_motifs(text_items: List[ResponseContentItem], question_text: str) -> List[MDLMotif]` (New Phase 3 logic)\n","        *   `optimize_codebook(candidate_motifs: List[MDLMotif], text_items: List[ResponseContentItem]) -> OptimizedCodebookValueObject` (New Phase 3.5 logic; `OptimizedCodebookValueObject` would contain selected motifs, stats, etc.)\n","        *   `enrich_codebook_themes(codebook: OptimizedCodebookValueObject) -> EnrichedOptimizedCodebookValueObject` (Phase 2C: add explanations, sentiment)\n","        *   `analyze_qid_stream(qid_text_items: List[ResponseContentItem], question_text: str) -> EnrichedOptimizedCodebookValueObject` (Orchestrates the above for one QID stream)\n","\n","3.  **`HumanReviewService` (Handles Conceptual Phase 3 - more of an interface/workflow)**\n","    *   **Methods:**\n","        *   `present_themes_for_review(qid_analysis_form: EnrichedOptimizedCodebook, qid_analysis_pdf: EnrichedOptimizedCodebook) -> ReviewInterfaceData`\n","        *   `ingest_human_feedback(feedback_data: HumanFeedbackData) -> ValidatedThematicModelForQID`\n","    *   *This service might be more about defining data exchange formats for a human review UI/tool rather than being fully automated code.*\n","\n","4.  **`ReportGenerationService` (Handles Conceptual Phase 4)**\n","    *   **Attributes:** `logger`.\n","    *   **Methods:**\n","        *   `extract_quotes_for_validated_themes(validated_model: ValidatedThematicModel, q_centric_corpus: Dict[str, List[ResponseContentItem]]) -> ReportReadyDataWithQuotes`\n","        *   `synthesize_cross_qid_meta_themes(validated_model: ValidatedThematicModel) -> List[MetaTheme]`\n","        *   `generate_final_report(report_data_with_quotes, meta_themes, config_snapshot) -> StakeholderReport` (constructs the entity)\n","        *   `save_report_to_file(report: StakeholderReport, filepath: str)`\n","\n","5.  **`LLMService` (Wraps LLM interactions for different tasks)**\n","    *   **Methods:**\n","        *   `get_motif_candidates(text_context: str, seed_phrases: List[str]) -> List[RawCandidateMotifDict]`\n","        *   `get_theme_explanation(motif_label: str, surface_forms: List[str]) -> str`\n","        *   `get_text_sentiment(text: str) -> SentimentValue`\n","        *   `synthesize_meta_themes_from_list(themes: List[ThematicUnit]) -> List[MetaThemeProposal]`\n","    *   This helps centralize prompt engineering and LLM API calls.\n","\n","6.  **`ConfigurationService` (Not a domain service, but an application service)**\n","    *   Loads and provides access to all system configurations (MDL costs, file paths, model IDs, thresholds).\n","\n","---\n","\n","**Key Relationships (Conceptual):**\n","\n","*   A `SurveyResponse` contains multiple `FormEntry` objects and one optional `PDFDocument`.\n","*   A `QuestionCentricCorpus` (managed by `UDSGenerationService`) groups `ResponseContentItem` objects by `QuestionDefinition` (QID).\n","*   `ResponseContentItem` objects link back to their parent `SurveyResponse` and target `QuestionDefinition`.\n","*   `MDLThematicAnalysisService` processes `ResponseContentItem`s for a QID to produce an `OptimizedCodebook` (a collection of `ThematicUnit` entities).\n","*   `HumanReviewService` takes `OptimizedCodebook`s and produces a `ValidatedThematicModel` (conceptually, a refined collection of `ThematicUnit`s).\n","*   `ReportGenerationService` uses the `ValidatedThematicModel` and original `ResponseContentItem`s (for quotes) to create a `StakeholderReport`, which contains `QIDReportSection`s and `MetaTheme`s.\n","*   `ThematicUnit`s are the core elements described in `QIDReportSection`s and `MetaTheme`s.\n","\n","**Benefits of this Domain Model:**\n","\n","*   **Clear Separation of Data and Logic:** Entities & VOs hold data; Services perform operations.\n","*   **Encapsulation:** Each class has a well-defined responsibility.\n","*   **Modularity:** Easier to change or extend one part without breaking others (e.g., swap out an LLM model via `LLMService`, change MDL cost calculation in `MDLTextProcessor`).\n","*   **Testability:** Individual services and entity logic can be unit-tested.\n","*   **Scalability:** Provides a structure that can handle increasing complexity.\n","\n","This is a comprehensive starting point for the domain model. We would refine attributes and methods as we delve into the implementation details of each service and how they interact with the MDL cost calculations and LLM prompts.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QCvzEk7snW4_"},"source":["Yes, this is an **excellent and very opportune moment** to consider these core architectural changes. Your reasoning is sound, and the proposed changes align perfectly with building a more robust, scalable, and sophisticated textual intelligence pipeline, especially as you integrate MDL principles.\n","\n","Let's break down why this is a good idea and how it addresses your points:\n","\n","**Why Now is a Good Time for Architectural Changes:**\n","\n","1.  **Integrating MDL is a Paradigm Shift:** Moving towards an MDL-driven approach (as discussed in your prototype and future plans for Phases 3/3.5) is more than just adding a new step. It changes how themes are discovered and validated. A cleaner, more foundational data architecture will make this integration smoother and more powerful.\n","2.  **Current Focus on PDF, but Broader Vision:** You've acknowledged the parallel form data pipeline. Architecting for both streams *now* prevents future major refactoring when you decide to fully integrate them.\n","3.  **Need for Unified Data Structures:**\n","    *   **Response-Centric Unified View:** Having all data (form fields, PDF text, metadata) for a single response in one easily accessible structure is invaluable for holistic analysis of that respondent's complete input.\n","    *   **Question-Centric Unified View:** Aggregating all responses (both form text and PDF snippets) relevant to a specific question into a unified structure is essential for comprehensive per-question thematic analysis.\n","4.  **Scalability with Object-Oriented Paradigm (OOP):**\n","    *   As the complexity grows (MDL components, parallel data streams, synthesis logic), OOP will help manage this complexity through encapsulation, inheritance, and polymorphism.\n","    *   It will make the codebase easier to understand, maintain, test, and extend. Different parts of the MDL process (candidate generation, codebook optimization, cost calculation) can be distinct classes. Different data sources (PDF, form) can have common interfaces.\n","5.  **Foundation for Downstream Analysis & Meta-Understanding:** The proposed unified structures and parallel processing naturally lead to the ability to synthesize insights across questions and data sources, which is a key goal.\n","\n","**Addressing Your Proposed Architectural Changes:**\n","\n","**1. Unified Data Structures (as a Foundational \"Phase 0\" or \"Phase 1\"):**\n","\n","*   **Response-Centric Unified Data Structure:**\n","    *   **Input:** Raw survey responses (your `processed_responses.jsonl` which presumably has form data and links/content for PDFs).\n","    *   **Process:** For each response:\n","        *   Extract and clean all structured form data (text fields, selections).\n","        *   Extract and clean all unstructured PDF text (as in your current Phase 1, but perhaps storing it more directly associated with the response object).\n","        *   Include all relevant metadata (response ID, submission date, demographics if available).\n","    *   **Output:** A list/stream of \"Unified Response Objects.\" Each object contains all information for one submission.\n","        *   Example: `{'id': 'resp_001', 'metadata': {...}, 'form_data': {'Q1_text': '...', 'Q2_selection': 'Option A'}, 'pdf_content': {'full_text': '...', 'sentences': [...]}}`\n","*   **Question-Centric Unified Data Structure:**\n","    *   **Input:** Unified Response Objects, `question_centric_aggregation_output.json` (for QID-text map and question types).\n","    *   **Process:** For each QID:\n","        *   Iterate through all Unified Response Objects.\n","        *   Extract relevant text/data for that QID:\n","            *   From specific form fields mapped to that QID.\n","            *   From PDF passages semantically matched to that QID (your current Phase 1 PDF logic).\n","        *   Store this collected information per QID, linking back to the `response_id`.\n","    *   **Output:** A \"Question-Centric Aggregation.\"\n","        *   Example: `{'Q1': {'question_text': '...', 'responses': [{'response_id': 'resp_001', 'form_text_snippet': '...', 'pdf_text_snippets': ['...']}, ...]}}`\n","        *   This output becomes the primary input for the parallel thematic analysis streams.\n","\n","**2. Branching to Two Parallel Streams (Thematic Analysis):**\n","\n","This is a natural consequence of the unified structures.\n","\n","*   **Stream 2.1: Structured Text (Forms, Selections):**\n","    *   **Input:** Question-Centric Aggregation (specifically the `form_text_snippet` and selection data for each QID).\n","    *   **Process:**\n","        *   **Collate:** Group all form text for a QID. For selections, aggregate counts directly.\n","        *   **(New) Summarize (Optional for short form text):** If form text fields are long, an LLM could summarize them.\n","        *   **(New) Extract Themes (MDL-driven):** Apply your new MDL-based thematic analysis (revised Phases 3 & 3.5) to the collated form text for each QID.\n","        *   **Count:** Quantify themes based on occurrences in the original form text snippets.\n","    *   **Output:** Thematic analysis results for structured data, per QID.\n","\n","*   **Stream 2.2: Unstructured Text (PDFs):**\n","    *   **Input:** Question-Centric Aggregation (specifically the `pdf_text_snippets` for each QID).\n","    *   **Process:** This largely aligns with your planned revised Phases 3 & 3.5 for PDFs.\n","        *   **Collate:** (Already done by Question-Centric Aggregation).\n","        *   **(New) Summarize (as per your current Phase 1, now more integrated):** PDF passage summarization can still be a pre-step if desired, feeding into the MDL process.\n","        *   **(New) Extract Themes (MDL-driven):** Apply revised Phases 3 & 3.5.\n","        *   **Count:** Quantify themes based on occurrences in the original PDF snippets.\n","    *   **Output:** Thematic analysis results for PDF data, per QID.\n","\n","**Crucial Point on Counting (as per your 2.1):**\n","> \"count them in the original source not the output of phase 1 as occur currently\"\n","This is an important clarification. The MDL process itself, when calculating `L(D|H_codebook)`, will determine how many times a motif (theme) is \"used\" to compress the source text (form snippets or PDF snippets). So, the counting becomes an intrinsic part of the MDL-driven theme discovery. This is good. Quote extraction would also pull from these original snippets.\n","\n","**3. Synthesize Results (Meta-Understanding - a \"Phase 5\"):**\n","\n","*   **Input:** Thematic analysis results from Stream 2.1 and Stream 2.2 (both per QID).\n","*   **Process:**\n","    *   **Per-QID Synthesis:** For each QID, compare and integrate the themes found in form data versus PDF data.\n","        *   Are there overlapping themes?\n","        *   Do certain data sources yield unique themes?\n","        *   Can an LLM synthesize a combined understanding for the QID based on themes from both sources?\n","    *   **Cross-QID Meta-Theme Synthesis (as discussed for \"Boundary Spanning\"):**\n","        *   Collect all validated themes (from both streams, across all QIDs).\n","        *   Use an LLM (or another MDL-like process on the themes themselves) to identify overarching meta-themes that cut across multiple questions and data sources.\n","        *   This could involve looking for motifs/themes with similar labels or semantically similar explanations/surface forms across QIDs.\n","*   **Output:** A holistic report including per-QID synthesized insights and cross-cutting meta-themes.\n","\n","**Object-Oriented Paradigm (OOP) Implementation:**\n","\n","This is highly recommended.\n","\n","*   **Data Classes:**\n","    *   `UnifiedResponse`\n","    *   `QuestionCentricData`\n","    *   `MDLMotif` (you already started this)\n","    *   `Theme` (could be an evolution of `MDLMotif` with explanations, final counts, quotes)\n","*   **Processor Classes:**\n","    *   `DataSourceExtractor` (abstract base class)\n","        *   `FormDataSourceExtractor` (subclass)\n","        *   `PDFDataSourceExtractor` (subclass)\n","    *   `MDLTextProcessor` (you have this)\n","    *   `MotifCandidateGenerator` (you have this)\n","    *   `CodebookOptimizer` (you have this)\n","    *   `ThematicAnalyzer` (could orchestrate candidate generation and codebook optimization for a given QID's text)\n","        *   Could have subclasses or strategies for form vs. PDF if needed, but the core MDL logic might be reusable.\n","    *   `InsightSynthesizer` (for Phase 5)\n","*   **Pipeline Orchestrator Class:** To manage the overall flow, dependencies, and logging.\n","\n","**Benefits of This Architectural Refactoring:**\n","\n","1.  **Clarity & Maintainability:** Clear separation of concerns.\n","2.  **Extensibility:** Easier to add new data sources, new MDL techniques, or new analysis modules.\n","3.  **Testability:** Individual components (classes) can be unit-tested more easily.\n","4.  **True Parallel Processing:** The architecture naturally supports analyzing form and PDF data in parallel once the initial unified data structures are built.\n","5.  **Foundation for Sophistication:** Provides the right building blocks for advanced features like cross-QID synthesis and iterative MDL refinement.\n","\n","**Your Thoughts?**\n","\n","This is a significant undertaking, but it's a strategic one. It sets up the project for long-term success and allows for the sophisticated MDL-driven automation you're aiming for.\n","\n","**My Recommendation:**\n","\n","1.  **Prioritize the Unified Data Structures:** Get the \"Response-Centric\" and then \"Question-Centric\" data structures right. This is the bedrock. Define clear schemas for these.\n","2.  **Refactor PDF Stream First (with OOP):** Take your existing PDF analysis logic (current Phases 1-4, evolving into new Phases 3, 3.5, 4 with MDL) and refactor it using the OOP approach, operating on the new unified data structures.\n","    *   Integrate your `MDLMotif`, `MDLTextProcessor`, `MotifCandidateGenerator`, `CodebookOptimizer` into this stream.\n","3.  **Develop Form Data Stream:** Implement the parallel stream for structured text, reusing the MDL components where applicable.\n","4.  **Develop Synthesis Phase:** Once both streams are producing thematic outputs, work on the synthesis logic.\n","\n","This staged approach makes the refactoring manageable. The prototype you've built for MDL evaluation is a fantastic proof-of-concept for the core of the new thematic analysis engine that will be used in both streams.\n","\n","This is a well-thought-out vision for evolving the pipeline! It's ambitious but also logical."]},{"cell_type":"markdown","metadata":{"id":"UffhpVT8w0OS"},"source":["You're right, we've had some excellent refinements, and the domain model needs to reflect them clearly. Let's consolidate those changes into a revised domain model description.\n","\n","The core insight for \"ThematicUnit can belong to more than 1 qid\" was to keep `ThematicUnit` QID-specific for primary analysis and introduce a `MetaTheme` concept for cross-QID synthesis. For `L_D_H_cost`, it's contextual to a codebook rather than an intrinsic property of a single `ThematicUnit`.\n","\n","Here's the revised Domain Model:\n","\n","---\n","\n","**Revised Domain Model**\n","\n","**I. Core Entities:**\n","\n","1.  **`SurveyResponse`**\n","    *   **Attributes:**\n","        *   `response_id: str` (Primary Key)\n","        *   `submission_timestamp: datetime`\n","        *   `source_metadata: Dict[str, Any]` (e.g., original filename, URL, submitter type if known initially)\n","        *   `form_entries: List[FormEntry]` (Value Object)\n","        *   `pdf_document: Optional[PDFDocument]` (Value Object)\n","        *   `processing_status: str` (e.g., \"raw\", \"uds_generated\", \"analysis_complete\")\n","        *   `analysis_errors: List[str]`\n","    *   **Purpose:** Represents a single, complete submission from one respondent.\n","\n","2.  **`QuestionDefinition`**\n","    *   **Attributes:**\n","        *   `qid: str` (Primary Key)\n","        *   `question_text: str`\n","        *   `question_types: List[str]` **(REVISED)** (e.g., `[\"open_text_form\", \"pdf_general_response\"]`, `[\"multi_selection_form\"]`)\n","        *   `response_options: Optional[List[str]]` (for multiple choice/selection)\n","        *   `related_form_field_ids: Optional[List[str]]`\n","    *   **Purpose:** Defines each question in the survey instrument, acknowledging it can be addressed via multiple input types.\n","\n","3.  **`ThematicUnit`**\n","    *   **Attributes:**\n","        *   `unit_id: str` (UUID or composite key, e.g., `qid_stream_source_labelhash`)\n","        *   `qid: str` **(REMAINS SINGULAR)** (Foreign Key to `QuestionDefinition` - this unit is defined *in the context of this QID*)\n","        *   `data_stream_source: str` (\"form_text\", \"pdf_text\")\n","        *   `label: str` (e.g., `[PRICE_EXCESSIVE]`, or human-readable refined label)\n","        *   `explanation: Optional[str]` (LLM or human-generated)\n","        *   `surface_forms_or_keyphrases: List[str]` (Key defining phrases identified by MDL/LLM)\n","        *   `L_H_cost: float` **(CLARIFIED)** (MDL: Cost of defining this unit's label and surface forms. Calculated once for the motif.)\n","        *   `usage_in_optimized_codebook_count: Optional[int]` **(NEW/CLARIFIED)** (MDL: How many times this unit was used to compress data *within its specific optimized codebook for its QID/stream*. This is derived from the `L(D|H_codebook)` calculation.)\n","        *   `sentiment_summary: Optional[SentimentValue]` (Value Object)\n","        *   `is_human_validated: bool` (default False)\n","        *   `human_validation_notes: Optional[str]`\n","        *   `llm_generation_source_info: Optional[Dict[str, Any]]` (e.g., seed phrase, LLM model used for its generation)\n","    *   **Purpose:** Represents a single, distinct theme or motif identified and selected by the MDL process *for a specific QID and data stream*. Its \"value\" or \"impact\" on data compression (`L(D|H)`) is realized within an `OptimizedCodebook`.\n","\n","4.  **`ResponseContentItem`**\n","    *   **Attributes:**\n","        *   `item_id: str` (UUID)\n","        *   `response_id: str` (Foreign Key to `SurveyResponse`)\n","        *   `qid: str` (Foreign Key to `QuestionDefinition` to which this content is primarily matched/collated)\n","        *   `source_type: str` (\"form_text_field\", \"form_selection_label\", \"pdf_sentence\", \"pdf_simple_chunk\")\n","        *   `text_content: str`\n","        *   `original_location_info: Optional[Dict[str, Any]]` (e.g., PDF page, form field ID)\n","        *   `relevance_score_to_qid: Optional[float]` (if from a matching process)\n","        *   `assigned_thematic_unit_ids_for_qid: List[str]` **(CLARIFIED)** (List of `ThematicUnit.unit_id` values that \"cover\" or \"classify\" this item *for its associated QID*)\n","    *   **Purpose:** Atomic unit of text collated for a specific QID from a specific response, forming the basis for thematic analysis and quote extraction for that QID.\n","\n","5.  **`StakeholderReport`**\n","    *   **Attributes:**\n","        *   `report_id: str` (UUID)\n","        *   `generation_timestamp: datetime`\n","        *   `configuration_snapshot: Dict[str, Any]`\n","        *   `per_qid_analysis: Dict[str, QIDReportSection]` (Value Object)\n","        *   `meta_themes: List[MetaTheme]` (Value Object - handles cross-QID themes)\n","        *   `summary_statistics: Dict[str, Any]`\n","    *   **Purpose:** Represents a generated report output.\n","\n","---\n","\n","**II. Key Value Objects:**\n","\n","1.  **`FormEntry`** (Attribute of `SurveyResponse`) - *No change*\n","    *   `field_id: str`, `field_label_or_qid: str`, `answer_type: str`, `raw_answer: Any`, `cleaned_answer_text: Optional[str]`, `cleaned_selection_labels: Optional[List[str]]`\n","\n","2.  **`PDFDocument`** (Attribute of `SurveyResponse`) - *No change*\n","    *   `full_text: str`, `sentences: List[str]` (or `simple_chunks`), `page_count: Optional[int]`, `word_count: Optional[int]`, `legibility_score: Optional[float]`, `processing_notes: List[str]`\n","\n","3.  **`SentimentValue`** (Attribute of `ThematicUnit`) - *No change*\n","    *   `overall_sentiment_label: str`, `sentiment_score: float`, `positive_score: float`, `negative_score: float`, `neutral_score: float`\n","\n","4.  **`Quote`** (Part of `QIDReportSection`) - *No change*\n","    *   `text: str`, `response_id: str`, `source_content_item_id: str`, `context_snippet: Optional[str]`\n","\n","5.  **`QIDReportSection`** (Attribute of `StakeholderReport`) - *No change*\n","    *   `qid: str`, `question_text: str`, `form_data_themes: List[ThematicUnit]` (or summaries), `pdf_data_themes: List[ThematicUnit]` (or summaries), `synthesized_qid_summary: Optional[str]`, `illustrative_quotes_by_theme: Dict[str, List[Quote]]`, `statistics: Dict[str, Any]`\n","\n","6.  **`MetaTheme`** (Attribute of `StakeholderReport`) - **(REMAINS KEY for cross-QID)**\n","    *   `meta_theme_id: str`\n","    *   `meta_theme_label: str`\n","    *   `meta_theme_explanation: str`\n","    *   `contributing_qids: List[str]`\n","    *   `contributing_thematic_unit_ids: List[str]` (Links specific QID-level `ThematicUnit` instances that embody this meta-theme)\n","    *   `supporting_evidence_summary: Optional[str]`\n","\n","7.  **`OptimizedCodebookOutput`** **(NEW/CLARIFIED - Value Object output by Phase 2B)**\n","    *   **Attributes:**\n","        *   `qid: str`\n","        *   `data_stream_source: str` (\"form_text\", \"pdf_text\")\n","        *   `selected_thematic_unit_ids: List[str]` (IDs of the `ThematicUnit` entities forming this codebook)\n","        *   `total_L_H_codebook: float`\n","        *   `total_L_D_given_H_codebook: float`\n","        *   `final_total_mdl_cost: float`\n","        *   `original_corpus_L_D_cost: float`\n","        *   `total_compression_achieved: float`\n","        *   `thematic_unit_usage_counts: Dict[str, int]` (Maps `ThematicUnit.unit_id` to its usage count in *this specific codebook's context*)\n","        *   `mdl_optimizer_run_stats: Optional[Dict[str, Any]]` (e.g., iterations, candidates considered)\n","    *   **Purpose:** Represents the result of the MDL codebook optimization for a specific QID and data stream. It provides the quantitative justification for the selected themes. The actual `ThematicUnit` entities (with their full details like surface forms, explanations) would be retrieved using the `selected_thematic_unit_ids`.\n","\n","---\n","\n","**III. Services (Conceptual roles, structure as before):**\n","\n","1.  **`UDSGenerationService`**\n","2.  **`MDLThematicAnalysisService`** (Now produces `OptimizedCodebookOutput` for each QID/stream, which contains IDs linking to `ThematicUnit` entities whose `L_H_cost` is set and whose `usage_in_optimized_codebook_count` gets populated from this output).\n","3.  **`HumanReviewService`** (Takes `OptimizedCodebookOutput` and allows interaction with the underlying `ThematicUnit` entities).\n","4.  **`ReportGenerationService`** (Uses human-validated `ThematicUnit`s and can access `OptimizedCodebookOutput` stats for context).\n","5.  **`LLMService`**\n","6.  **`ConfigurationService`**\n","7.  **(New Optional) `SyntheticDataGenerationService`**\n","\n","---\n","\n","**Summary of Revisions Based on Your Points:**\n","\n","*   **`QuestionDefinition.question_types: List[str]`:** Correctly allows a QID to be associated with multiple input methods.\n","*   **`ThematicUnit.qid: str` (Singular):** Maintained. Cross-QID themes are captured by the `MetaTheme` value object within the `StakeholderReport`, which links multiple QID-specific `ThematicUnit` entities. This keeps the primary thematic analysis focused per QID.\n","*   **MDL Costs (`L_H` vs. `L_D|H`):**\n","    *   `ThematicUnit.L_H_cost` stores the intrinsic definition cost of the motif.\n","    *   `ThematicUnit.usage_in_optimized_codebook_count` is introduced. This count is derived from the `L(D|H_codebook)` calculation performed by the `MDLThematicAnalysisService` when it produces an `OptimizedCodebookOutput`. The service calculates how many times each selected motif was used to compress the data *within that optimal set*. This count is then populated onto the `ThematicUnit` entity.\n","    *   The `OptimizedCodebookOutput` (a Value Object, not an Entity) explicitly stores `total_L_D_given_H_codebook` and the `thematic_unit_usage_counts` providing the full MDL context for that specific optimization run.\n","\n","This revised model should now more accurately reflect our discussions and provide a clearer structure for implementation. The distinction between a `ThematicUnit` (an entity representing a theme) and an `OptimizedCodebookOutput` (a value object summarizing an MDL run's results, including which themes were selected and how they performed as a set) is key."]},{"cell_type":"markdown","metadata":{"id":"n3zt5oujf6CY"},"source":["## Class Diagram"]},{"cell_type":"markdown","metadata":{"id":"8w-bDtkye9VJ"},"source":["\n","==================================================================================\n","Phase 0: Setup & Configuration\n","==================================================================================\n","Inputs:\n","  - Raw Survey Files, Question Definitions File\n","  - MDL Cost Params, LLM Configs, Paths\n","\n","Component:\n","  [Svc] ConfigurationService\n","    |-- Provides configs to --> [Many Services below]\n","\n","==================================================================================\n","Phase 1: Unified Data Ingestion & Preparation\n","==================================================================================\n","Input:\n","  - (from Phase 0 Inputs)\n","  - (QuestionDefinition Entity)\n","\n","Service: [Svc] UDSGenerationService\n","  |-- Uses:\n","  |   |-- [HelperCls] FormProcessor\n","  |   |-- [HelperCls] PDFProcessor\n","  |       |-- Uses: [Model] EmbeddingModel_PDF\n","  |       |-- Uses: [Util] Tokenizer_PDF\n","  |-- Creates/Outputs:\n","      |-- (Entity) SurveyResponse\n","      |     |-- Contains: {VO} FormEntry\n","      |     |-- Contains: {VO} PDFDocument (with sentences/simple_chunks)\n","      |-- {VO} QuestionCentricCorpus\n","            |-- Contains: List of (Entity) ResponseContentItem for each QID\n","\n","==================================================================================\n","Phase 2: MDL+LLM Thematic Analysis (Per QID, Per Data Stream - Form/PDF)\n","==================================================================================\n","Input:\n","  - {VO} QuestionCentricCorpus (specifically, List[(Entity)ResponseContentItem] for a QID/stream)\n","  - (Entity) QuestionDefinition (for question_text)\n","  - Configs from [Svc] ConfigurationService\n","\n","Service: [Svc] MDLThematicAnalysisService\n","  |-- Uses:\n","  |   |-- [Svc] LLMService (connects to [External] LLM API)\n","  |   |-- [UtilCls] MDLTextProcessor\n","  |   |   |-- Uses: [Strategy] CostFunctionStrategy (e.g., BasicTokenReplacementCostStrategy)\n","  |   |\n","  |   |-- [Component] MotifCandidateGenerator\n","  |   |   |-- Uses: [Svc] LLMService\n","  |   |   |-- Uses: [UtilCls] MDLTextProcessor\n","  |   |   |-- (Optional Internal Agents: Coder, Aggregator, Reviewer)\n","  |   |   |-- Creates: Pool of (Entity) MDLMotif (candidates)\n","  |   |\n","  |   |-- [Component] CodebookOptimizer\n","  |   |   |-- Uses: [UtilCls] MDLTextProcessor\n","  |   |   |-- Receives: Pool of (Entity) MDLMotif (candidates)\n","  |   |   |-- Outputs: {VO} OptimizedCodebookOutput\n","  |   |       |-- Contains: List of selected (Entity)ThematicUnit IDs\n","  |   |       |-- Contains: MDL stats (L(H)_codebook, L(D|H)_codebook, usage_counts)\n","  |   |\n","  |   |-- [Component] ThemeEnricher\n","  |       |-- Uses: [Svc] LLMService\n","  |       |-- Receives: Selected (Entity)ThematicUnit(s) from OptimizedCodebook\n","  |       |-- Adds: Explanations, Sentiment ({VO}SentimentValue) to ThematicUnits\n","  |\n","  |-- Outputs (for a QID/stream):\n","      |-- {VO} QIDStreamAnalysisResult\n","            |-- Contains: List of enriched (Entity)ThematicUnit\n","            |-- Contains: {VO} OptimizedCodebookOutput (stats)\n","\n","==================================================================================\n","Phase 3: Human Review & Validation\n","==================================================================================\n","Input:\n","  - {VO} QIDStreamAnalysisResult (from Phase 2, for multiple QIDs/streams)\n","\n","Service: [Svc] HumanReviewService\n","  |-- Interacts with: [External] Human Review UI/Tool\n","  |-- Outputs:\n","      |-- (EntityCollection) ValidatedThematicModel\n","            |-- Contains: Collection of human-reviewed/validated (Entity)ThematicUnit\n","  |-- May trigger re-run of: [Svc] MDLThematicAnalysisService (if MDL params/motif defs change)\n","\n","==================================================================================\n","Phase 4: Reporting & Synthesis\n","==================================================================================\n","Input:\n","  - (EntityCollection) ValidatedThematicModel (from Phase 3)\n","  - {VO} QuestionCentricCorpus (from Phase 1, for original text for quotes)\n","  - Configs from [Svc] ConfigurationService\n","\n","Service: [Svc] ReportGenerationService\n","  |-- Uses: [Svc] LLMService (for meta-theme synthesis, QID summaries)\n","  |-- Creates/Outputs:\n","      |-- (Entity) StakeholderReport\n","            |-- Contains: {VO} QIDReportSection\n","            |   |-- Contains: List of validated (Entity)ThematicUnit (summaries)\n","            |   |-- Contains: Dict of {VO}Quote per ThematicUnit\n","            |-- Contains: List of {VO}MetaTheme\n","\n","==================================================================================\n","Key Supporting Entities & Value Objects (Referenced Above)\n","==================================================================================\n","Entities:\n","  - SurveyResponse\n","  - QuestionDefinition\n","  - ThematicUnit (evolves from MDLMotif candidate)\n","  - ResponseContentItem\n","  - StakeholderReport\n","\n","Value Objects:\n","  - FormEntry\n","  - PDFDocument\n","  - QuestionCentricCorpus\n","  - OptimizedCodebookOutput\n","  - QIDStreamAnalysisResult\n","  - SentimentValue\n","  - Quote\n","  - QIDReportSection\n","  - MetaTheme\n","\n","==================================================================================\n","Key Utility/Support Classes:\n","==================================================================================\n","  - [UtilCls] MDLTextProcessor (tokenization, core MDL cost calculations)\n","  - [Strategy] CostFunctionStrategy (interface for different MDL cost models)\n","\n","==================================================================================\n","External Dependencies (Conceptual):\n","==================================================================================\n","  - [External] LLM API (e.g., Gemini)\n","  - [External] Embedding Model API/Library\n","  - [External] Human Review UI/Tool (interface defined by HumanReviewService)"]},{"cell_type":"markdown","metadata":{"id":"lvtd07r-hCmJ"},"source":["# 25th May\n","Conceptual Description of the Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCs6Sz7fT4tx"},"outputs":[],"source":["# @title ! Cell 0: Initial Setup & Global Imports (DO NOT MODIFY unless adding new global libraries)\n","\n","# --- Basic Python and System ---\n","import json\n","import os\n","import sys\n","import traceback\n","import time\n","import gc\n","import re\n","from collections import defaultdict, Counter\n","from typing import Dict, Any, List, Tuple, Optional\n","import warnings\n","# import argparse # Kept for potential future use, but not strictly needed for this notebook flow\n","# import tempfile # Was for single response testing in Phase 1, optional here\n","\n","# --- ML/NLP Core Imports ---\n","import torch\n","import numpy as np\n","import nltk\n","\n","# --- Hugging Face Transformers & Related ---\n","from sentence_transformers import SentenceTransformer, util as sbert_util\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","\n","# --- Suppress Warnings (Optional) ---\n","warnings.filterwarnings(\"ignore\", message=\".*Using the latest cached version.*\")\n","warnings.filterwarnings(\"ignore\", message=\".*torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.*\")\n","# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers.generation.configuration_utils\")\n","\n","\n","# --- BitsAndBytes (Optional, for Quantization) ---\n","try:\n","    from bitsandbytes.cextension import ColabNotSupportedError\n","except ImportError:\n","    ColabNotSupportedError = type('ColabNotSupportedError', (Exception,), {}) # Define dummy class\n","    print(\"WARN: bitsandbytes library not found. Quantization will be disabled if requested in config.\")\n","    pass\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 0 - Initial imports complete.\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xufAr8o_UU2t"},"outputs":[],"source":["# @title ! Cell 1: Utility - FileLogger Class & NLTK Resource Management (DO NOT MODIFY)\n","\n","# =====================================================\n","#                Custom FileLogger Class\n","# =====================================================\n","class FileLogger:\n","    # --- PASTE THE FULL FileLogger CLASS DEFINITION HERE ---\n","    # (As provided and used throughout our discussion)\n","    # Example:\n","    def __init__(self, log_file_path):\n","        self.log_file_path = log_file_path\n","        log_dir = os.path.dirname(log_file_path)\n","        if log_dir and not os.path.exists(log_dir):\n","            os.makedirs(log_dir, exist_ok=True)\n","        try:\n","            # Open in append mode for pipeline runs, but 'w' if each phase re-initializes\n","            # For orchestration, each phase gets its own logger, so 'w' is fine.\n","            with open(self.log_file_path, 'w', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: Starting new log at {self.log_file_path}\\n\")\n","            # print(f\"INFO: Logger initialized. Log file: {self.log_file_path}\") # Logged by orchestrator now\n","        except IOError as e:\n","            print(f\"CRITICAL: Failed to initialize log file {self.log_file_path}: {e}\")\n","    def _get_timestamp(self): return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","    def log(self, message: str):\n","        try:\n","            with open(self.log_file_path, 'a', encoding='utf-8') as f: f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","        except Exception as e: print(f\"!!! LOGGER FAILED TO WRITE TO {self.log_file_path}: {e} !!!\\n{self._get_timestamp()}: {message}\")\n","    def _get_serializer(self):\n","        def default_serializer(obj):\n","            if isinstance(obj, (np.integer, np.floating)): return obj.item()\n","            if isinstance(obj, np.ndarray): return obj.tolist()\n","            if isinstance(obj, set): return sorted(list(obj))\n","            if 'torch' in sys.modules and isinstance(obj, torch.Tensor): return obj.detach().cpu().numpy().tolist()\n","            try: return json.JSONEncoder().default(obj)\n","            except TypeError: return f\"UNSERIALIZABLE:{type(obj).__name__}\"\n","        return default_serializer\n","    def log_json(self, data: Dict[str, Any], prefix=\"JSON_DATA\"):\n","        serializer = self._get_serializer(); log_entry = {\"timestamp\": self._get_timestamp(), \"type\": prefix, \"data\": data}\n","        try:\n","            json_string = json.dumps(log_entry, default=serializer, indent=2)\n","            with open(self.log_file_path, 'a', encoding='utf-8') as f: f.write(json_string + \"\\n\")\n","        except Exception as e: self.log(f\"ERROR: log_json (prefix={prefix}): {e}\\n{traceback.format_exc()}\")\n","# =====================================================\n","#         NLTK Setup Helper\n","# =====================================================\n","def download_nltk_resource_with_logger(logger_instance, resource_id, download_name):\n","    # --- PASTE THE download_nltk_resource_with_logger FUNCTION DEFINITION HERE ---\n","    # (As used in the Orchestration cell)\n","    try: nltk.data.find(resource_id)\n","    except LookupError:\n","        logger_instance.log(f\"INFO: NLTK resource '{download_name}' not found by {resource_id}. Downloading...\")\n","        try: nltk.download(download_name, quiet=True); logger_instance.log(f\"INFO: Downloaded NLTK resource: {download_name}\")\n","        except Exception as dl_e: logger_instance.log(f\"ERROR: Failed NLTK download {download_name}: {dl_e}\")\n","    except Exception as e: logger_instance.log(f\"ERROR: NLTK check error {download_name}: {e}\")\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 1 - Utilities (FileLogger, NLTK Downloader) defined.\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkrolKM8UbJm"},"outputs":[],"source":["# @title ! Cell 2: Global Project Configuration (REVIEW AND UPDATE ALL PATHS/SETTINGS)\n","\n","import os\n","import time\n","\n","# --- Base Paths ---\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","PHASE1_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase1_PDF_Extraction_Outputs/')\n","PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n","PHASE3_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase3_LLM_Generated_Themes/')\n","PHASE4_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase4_Final_Thematic_Analysis/')\n","LOG_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Logs/')\n","for d_path in [PHASE1_OUTPUT_DIR, PHASE2_OUTPUT_DIR, PHASE3_OUTPUT_DIR, PHASE4_OUTPUT_DIR, LOG_DIR]:\n","    os.makedirs(d_path, exist_ok=True)\n","\n","# --- Common Input Files ---\n","ORIGINAL_RESPONSES_JSONL = os.path.join(BASE_PROJECT_DIR, 'processed_responses.jsonl')\n","QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","\n","# --- Phase 1 Config ---\n","P1_LOG_FILE = os.path.join(LOG_DIR, \"phase1_pdf_extraction.log\")\n","P1_OUTPUT_FILE = os.path.join(PHASE1_OUTPUT_DIR, 'phase1_pdf_analysis_by_response.json') # Ensure this matches your existing complete file\n","P1_STATE_FILE = os.path.join(PHASE1_OUTPUT_DIR, 'phase1_processing_state.json')\n","P1_EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n","P1_LLM_ID_FOR_SUMMARIES = 'google/gemma-2b-it'\n","P1_USE_QUANTIZATION = True # Changed to True for gemma-2b-it\n","P1_MIN_PDF_TEXT_LENGTH = 100; P1_MIN_SIMILARITY_SCORE = 0.5; P1_CONTEXT_WINDOW_SIZE_SENTENCES = 1\n","P1_TOP_K_PASSAGES = 3; P1_NUM_SUMMARY_SENTENCES_PER_RESPONSE_QID = 3; P1_MIN_CHARS_FOR_SUMMARY = 150\n","P1_LLM_MAX_NEW_TOKENS_FOR_PER_RESPONSE_SUMMARY = 250\n","P1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE = { # As defined before }\n","    \"Q1\": True, \"Q2\": True, \"Q3\": True, \"Q4\": True, \"Q5\": True, \"Q6\": True, \"Q7\": True,\n","    \"Q8\": True, \"Q9\": True, \"Q10\": True,\"Q11\": True, \"Q12\": True, \"Q13\": True, \"Q14\": True,\n","    \"Q15\": True, \"Q16\": False,\"Q17\": True, \"Q18\": True, \"Q19\": True,\"Q20\": False, \"Q21\": True,\n","    \"Q22\": True, \"Q23\": True, \"Q24\": True, \"Q25\": True, \"Q26\": True, \"Q27\": True, \"Q28\": True,\n","    \"Q29\": True, \"Q30\": True, \"Q31\": True,\"Q32\": False,\"Q33\": False,\"Q34\": False,\"Q35\": False,\n","    \"Q36\": False,\n","}\n","P1_MAX_RUNTIME_MINUTES = 60 * 1 # For reruns using state; increase for full initial run\n","P1_SAVE_STATE_INTERVAL_SECONDS = 60 * 10\n","\n","# --- Phase 2 Config ---\n","P2_LOG_FILE = os.path.join(LOG_DIR, f'phase2_text_collation.log') # Simpler log name for repeated runs\n","P2_OUTPUT_COLLATED_JSON = os.path.join(PHASE2_OUTPUT_DIR, f'phase2_collated_pdf_texts.json') # Simpler name\n","\n","# --- Phase 3 Config ---\n","P3_LOG_FILE = os.path.join(LOG_DIR, f'phase3_llm_themes.log')\n","P3_INPUT_COLLATED_JSON = P2_OUTPUT_COLLATED_JSON\n","P3_OUTPUT_LLM_THEMES_JSON = os.path.join(PHASE3_OUTPUT_DIR, f'phase3_llm_generated_themes.json')\n","P3_LLM_ID_FOR_THEMES = \"Qwen/Qwen2-4B-Instruct\"\n","P3_USE_QUANTIZATION = True\n","P3_NUM_THEMES_MAP_STR = \"up to 2\"; P3_NUM_THEMES_MAP_INT = 2\n","P3_NUM_THEMES_REDUCE_STR = \"top 3\"; P3_NUM_THEMES_REDUCE_INT = 3\n","P3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 3000\n","P3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 800\n","P3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 1000\n","P3_QIDS_TO_PROCESS_THEMATICALLY = None # Process all, relies on P3 output file for resumption\n","\n","# --- Phase 4 Config ---\n","P4_LOG_FILE = os.path.join(LOG_DIR, f'phase4_theme_quantification.log')\n","P4_INPUT_LLM_THEMES_JSON = P3_OUTPUT_LLM_THEMES_JSON\n","P4_INPUT_COLLATED_TEXTS_JSON = P2_OUTPUT_COLLATED_JSON\n","P4_OUTPUT_FINAL_ANALYSIS_JSON = os.path.join(PHASE4_OUTPUT_DIR, f'phase4_final_thematic_report.json')\n","P4_HUMAN_DEFINED_KEYPHRASES_FILE = os.path.join(BASE_PROJECT_DIR, 'human_defined_keyphrases_for_themes.json')\n","P4_NUM_EXAMPLE_QUOTES_PER_THEME = 2 # Default from Cell 6\n","\n","# --- General ---\n","ENABLE_DETAILED_DEBUG_LOGGING = False\n","\n","print(\"=\"*50)\n","print(f\"INFO: Cell 2 - Global Configuration loaded. Base project directory: {BASE_PROJECT_DIR}\")\n","# Print key output file paths to confirm\n","print(f\"  P1 Output File: {P1_OUTPUT_FILE}\")\n","print(f\"  P2 Output File: {P2_OUTPUT_COLLATED_JSON}\")\n","print(f\"  P3 Output File: {P3_OUTPUT_LLM_THEMES_JSON}\")\n","print(f\"  P4 Output File: {P4_OUTPUT_FINAL_ANALYSIS_JSON}\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"418aAfR4UeKl"},"outputs":[],"source":["# @title ! Cell 3: Phase 1 - PDF Extraction & Per-Response Summarization (Original Stage 4)\n","\n","# --- PASTE THE FULL CODE FOR Phase 1 HELPERS AND run_phase1_pdf_extraction() HERE ---\n","# This includes:\n","#   - save_phase1_state(state_file, processed_response_ids_set, current_results_dict, logger_instance)\n","#   - load_phase1_state(state_file, logger_instance)\n","#   - get_phase1_local_embeddings(texts: List[str], model: SentenceTransformer, device: str, logger_instance, batch_size: int = 64)\n","#   - retrieve_phase1_relevant_passages(question_embedding: np.ndarray, ..., logger_instance)\n","#   - summarize_phase1_context_block(text_chunk: str, ..., logger_instance)\n","#   - run_phase1_pdf_extraction(logger_p1_instance)\n","# Ensure all these functions use the `logger_instance` passed to them for logging\n","# and use P1_... prefixed global config variables (e.g., P1_EMBEDDING_MODEL).\n","# The run_phase1_pdf_extraction should read from ORIGINAL_RESPONSES_JSONL and QUESTION_DEFINITIONS_FILE,\n","# use P1_STATE_FILE for resumption, and write its final full output to P1_OUTPUT_FILE.\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 3 - Phase 1 logic defined (PDF Extraction & Per-Response Summarization).\")\n","print(\"      (Ensure function bodies are pasted from development notebook).\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgzYccEVUf_s"},"outputs":[],"source":["# @title ! Cell 4: Phase 2 - Collation of PDF Content (Original Stage 4a)\n","\n","# --- PASTE THE FULL CODE FOR Phase 2 HELPERS (if any, like clean_text_p2) AND run_phase2_collate_pdf_texts() HERE ---\n","# This includes:\n","#   - clean_text_p2(text: str) -> str\n","#   - run_phase2_collate_pdf_texts(logger_p2_instance)\n","# Ensure it uses `logger_instance` and P2_... global config variables.\n","# It reads from P1_OUTPUT_FILE (via P2_INPUT_FILE_OR_DIR config) and writes to P2_OUTPUT_COLLATED_JSON.\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 4 - Phase 2 logic defined (Collation of PDF Content).\")\n","print(\"      (Ensure function bodies are pasted from development notebook).\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dB0TIMyTUitp"},"outputs":[],"source":["# @title ! Cell 5: Phase 3 - LLM Thematic Point Generation (Map-Reduce)\n","\n","# --- PASTE THE FULL CODE FOR Phase 3 HELPERS AND run_phase3_llm_thematic_generation() HERE ---\n","# This includes:\n","#   - batch_text_items_p3(text_items_list: List[str], ..., logger_instance) - (this is the one that batches List[str] of items)\n","#   - get_llm_json_themes_p3(text_input_for_llm: str, ..., logger_instance) - (the Qwen2-NoThink version requesting JSON)\n","#   - parse_llm_json_themes_p3(llm_json_output_text: str, max_themes_to_extract: int, logger_instance)\n","#   - run_phase3_llm_thematic_generation(logger_p3_instance)\n","# Ensure all use `logger_instance` and P3_... global config variables.\n","# It reads P2_OUTPUT_COLLATED_JSON (via P3_INPUT_COLLATED_JSON) and QUESTION_DEFINITIONS_FILE.\n","# It writes to P3_OUTPUT_LLM_THEMES_JSON and uses its own output file for resumption.\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 5 - Phase 3 logic defined (LLM Thematic Point Generation).\")\n","print(\"      (Ensure function bodies are pasted from development notebook).\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2CGgDuIUlgZ"},"outputs":[],"source":["# @title ! Cell 6: Phase 4 - Theme Validation, Quantification & Quotation Extraction\n","\n","# --- PASTE THE FULL CODE FOR Phase 4 HELPERS AND run_phase4_theme_quantification_and_quoting() HERE ---\n","# This includes:\n","#   - load_human_defined_keyphrases_p4(keyphrase_file_path: str, logger_instance)\n","#   - auto_derive_keyphrases_from_llm_theme(theme_label: str, ..., logger_instance)\n","#   - count_llm_themes_and_extract_quotes_p4_auto(llm_themes_by_qid: Dict[str, Any], ..., logger_instance)\n","#   - run_phase4_theme_quantification_and_quoting(logger_p4_instance, p3_llm_themes_output_file: str, ...)\n","# Ensure all use `logger_instance` and P4_... global config variables.\n","# It reads P3_OUTPUT_LLM_THEMES_JSON (via P4_INPUT_LLM_THEMES_JSON),\n","# P2_OUTPUT_COLLATED_JSON (via P4_INPUT_COLLATED_TEXTS_JSON),\n","# and P4_HUMAN_DEFINED_KEYPHRASES_FILE.\n","# It writes to P4_OUTPUT_FINAL_ANALYSIS_JSON.\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 6 - Phase 4 logic defined (Theme Quantification & Quoting).\")\n","print(\"      (Ensure function bodies are pasted from development notebook).\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3dMgdQoUtdh"},"outputs":[],"source":["# @title ! Cell 7: Main Pipeline Orchestration (SET RUN_PHASE_X FLAGS BEFORE EXECUTION)\n","\n","# Initialize a master logger for orchestration steps\n","master_logger_file = os.path.join(LOG_DIR, f\"pipeline_orchestration_master_{time.strftime('%Y%m%d_%H%M%S')}.log\")\n","master_logger = FileLogger(master_logger_file) # Assumes FileLogger is defined in Cell 1\n","master_logger.log(\"INFO: Orchestration Cell Started.\")\n","\n","# --- Flags to control which phases to run ---\n","# !!! REVIEW AND SET THESE CAREFULLY BEFORE EACH RUN !!!\n","RUN_PHASE_1 = False # Set True if P1_OUTPUT_FILE needs to be (re)generated from scratch or resumed\n","RUN_PHASE_2 = True  # Set True if P2_OUTPUT_COLLATED_JSON needs to be (re)generated\n","RUN_PHASE_3 = True  # Set True if P3_OUTPUT_LLM_THEMES_JSON needs to be (re)generated/updated\n","RUN_PHASE_4 = True  # Set True if P4_OUTPUT_FINAL_ANALYSIS_JSON needs to be (re)generated\n","\n","master_logger.log(f\"RUN FLAGS: P1={RUN_PHASE_1}, P2={RUN_PHASE_2}, P3={RUN_PHASE_3}, P4={RUN_PHASE_4}\")\n","\n","# --- NLTK Resource Check (using master_logger) ---\n","master_logger.log(\"INFO: Orchestrator - Checking NLTK resources...\")\n","try:\n","    # download_nltk_resource_with_logger should be defined in Cell 1\n","    download_nltk_resource_with_logger(master_logger, 'tokenizers/punkt', 'punkt')\n","    download_nltk_resource_with_logger(master_logger, 'corpora/stopwords', 'stopwords')\n","    # Add 'sentiment/vader_lexicon' if any part of your pipeline uses it (not in current PDF focus)\n","    # nltk.download('averaged_perceptron_tagger') # For POS tagging if auto_derive_keyphrases uses it\n","    master_logger.log(\"INFO: Orchestrator - NLTK resource check complete.\")\n","except NameError:\n","    master_logger.log(\"WARN: Orchestrator - download_nltk_resource_with_logger not defined. NLTK resources might be missing.\")\n","except Exception as e_nltk_orch:\n","    master_logger.log(f\"ERROR: Orchestrator - NLTK download failed: {e_nltk_orch}\")\n","\n","\n","# --- Execute Pipeline Phases ---\n","phase_success_flags = {\n","    \"phase1\": not RUN_PHASE_1, # Assume success if not run\n","    \"phase2\": not RUN_PHASE_2,\n","    \"phase3\": not RUN_PHASE_3,\n","    \"phase4\": not RUN_PHASE_4,\n","}\n","\n","# This guard is good practice for script-like execution\n","if __name__ == \"__main__\": # In Colab, this is usually true for the cell being run\n","    master_logger.log(\"INFO: Starting Pipeline Execution based on RUN_PHASE flags...\")\n","\n","    active_phases_to_run = []\n","    if RUN_PHASE_1: active_phases_to_run.append(\"Phase 1\")\n","    if RUN_PHASE_2: active_phases_to_run.append(\"Phase 2\")\n","    if RUN_PHASE_3: active_phases_to_run.append(\"Phase 3\")\n","    if RUN_PHASE_4: active_phases_to_run.append(\"Phase 4\")\n","    master_logger.log(f\"INFO: Active phases for this run: {', '.join(active_phases_to_run) if active_phases_to_run else 'None (all RUN_PHASE_X flags are False)'}\")\n","\n","\n","    # --- Phase 1 ---\n","    if RUN_PHASE_1:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 1: PDF Extraction & Per-Response Summarization <<<\")\n","        logger_p1_orch = FileLogger(P1_LOG_FILE)\n","        success_p1 = run_phase1_pdf_extraction(logger_p1_orch) # Pass logger\n","        phase_success_flags[\"phase1\"] = success_p1\n","        if not success_p1: master_logger.log(\"CRITICAL: Phase 1 failed.\")\n","        else: master_logger.log(\">>> PHASE 1 COMPLETED SUCCESSFULLY <<<\")\n","    else: # Phase 1 Not Run\n","        master_logger.log(\">>> SKIPPING PHASE 1 (RUN_PHASE_1 is False) <<<\")\n","        if not os.path.exists(P1_OUTPUT_FILE): # Check if its output exists\n","            master_logger.log(f\"ERROR: Phase 1 skipped, but required output {P1_OUTPUT_FILE} not found! Dependent phases may fail.\")\n","            phase_success_flags[\"phase1\"] = False # Explicitly mark as failed for dependency\n","        else:\n","            master_logger.log(f\"INFO: Assuming Phase 1 output {P1_OUTPUT_FILE} is valid.\")\n","            phase_success_flags[\"phase1\"] = True # If skipped and output exists, assume it's good\n","\n","    # --- Phase 2 ---\n","    if phase_success_flags[\"phase1\"] and RUN_PHASE_2:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 2: Collation of PDF Content <<<\")\n","        logger_p2_orch = FileLogger(P2_LOG_FILE)\n","        success_p2 = run_phase2_collate_pdf_texts(logger_p2_orch) # Pass logger\n","        phase_success_flags[\"phase2\"] = success_p2\n","        if not success_p2: master_logger.log(\"CRITICAL: Phase 2 failed.\")\n","        else: master_logger.log(\">>> PHASE 2 COMPLETED SUCCESSFULLY <<<\")\n","    elif not phase_success_flags[\"phase1\"] and RUN_PHASE_2:\n","        master_logger.log(\"WARN: Phase 1 did not complete successfully or output missing. Skipping Phase 2.\")\n","        phase_success_flags[\"phase2\"] = False\n","    elif not RUN_PHASE_2:\n","        master_logger.log(\">>> SKIPPING PHASE 2 <<<\")\n","        if not os.path.exists(P2_OUTPUT_COLLATED_JSON) and phase_success_flags[\"phase1\"]:\n","            master_logger.log(f\"ERROR: Phase 2 skipped, but required output {P2_OUTPUT_COLLATED_JSON} not found! Dependent phases may fail.\")\n","            phase_success_flags[\"phase2\"] = False\n","        elif os.path.exists(P2_OUTPUT_COLLATED_JSON):\n","            phase_success_flags[\"phase2\"] = True\n","            master_logger.log(f\"INFO: Assuming Phase 2 output {P2_OUTPUT_COLLATED_JSON} is valid.\")\n","\n","\n","    # --- Phase 3 ---\n","    if phase_success_flags[\"phase2\"] and RUN_PHASE_3:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 3: LLM Thematic Point Generation <<<\")\n","        logger_p3_orch = FileLogger(P3_LOG_FILE)\n","        success_p3 = run_phase3_llm_thematic_generation(logger_p3_orch) # Pass logger\n","        phase_success_flags[\"phase3\"] = success_p3\n","        if not success_p3: master_logger.log(\"CRITICAL: Phase 3 failed.\")\n","        else: master_logger.log(\">>> PHASE 3 COMPLETED SUCCESSFULLY <<<\")\n","    elif not phase_success_flags[\"phase2\"] and RUN_PHASE_3:\n","        master_logger.log(\"WARN: Phase 2 did not complete successfully or output missing. Skipping Phase 3.\")\n","        phase_success_flags[\"phase3\"] = False\n","    elif not RUN_PHASE_3:\n","        master_logger.log(\">>> SKIPPING PHASE 3 <<<\")\n","        if not os.path.exists(P3_OUTPUT_LLM_THEMES_JSON) and phase_success_flags[\"phase2\"]:\n","            master_logger.log(f\"ERROR: Phase 3 skipped, but required output {P3_OUTPUT_LLM_THEMES_JSON} not found! Dependent phases may fail.\")\n","            phase_success_flags[\"phase3\"] = False\n","        elif os.path.exists(P3_OUTPUT_LLM_THEMES_JSON):\n","            phase_success_flags[\"phase3\"] = True\n","            master_logger.log(f\"INFO: Assuming Phase 3 output {P3_OUTPUT_LLM_THEMES_JSON} is valid.\")\n","\n","    # --- Phase 4 ---\n","    if phase_success_flags[\"phase3\"] and RUN_PHASE_4:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 4: Theme Validation, Quantification & Quotation Extraction <<<\")\n","        logger_p4_orch = FileLogger(P4_LOG_FILE)\n","\n","        if not os.path.exists(P4_HUMAN_DEFINED_KEYPHRASES_FILE): # Create empty keyphrase file if missing\n","            master_logger.log(f\"WARN: Phase 4 - Human keyphrases file {P4_HUMAN_DEFINED_KEYPHRASES_FILE} not found. Creating empty file.\")\n","            try:\n","                with open(P4_HUMAN_DEFINED_KEYPHRASES_FILE, 'w') as f_empty_kp: json.dump({}, f_empty_kp)\n","            except Exception as e_kp_create: master_logger.log(f\"ERROR: Could not create empty keyphrase file: {e_kp_create}\")\n","\n","        success_p4 = run_phase4_theme_quantification_and_quoting( # Pass all required args\n","            logger_p4_orch,\n","            P4_INPUT_LLM_THEMES_JSON,\n","            P4_INPUT_COLLATED_TEXTS_JSON,\n","            P4_HUMAN_DEFINED_KEYPHRASES_FILE,\n","            P4_OUTPUT_FINAL_ANALYSIS_JSON,\n","            P4_NUM_EXAMPLE_QUOTES_PER_THEME\n","        )\n","        phase_success_flags[\"phase4\"] = success_p4\n","        if not success_p4: master_logger.log(\"ERROR: Phase 4 encountered issues.\")\n","        else: master_logger.log(\">>> PHASE 4 COMPLETED SUCCESSFULLY <<<\")\n","    elif not phase_success_flags[\"phase3\"] and RUN_PHASE_4:\n","        master_logger.log(\"WARN: Phase 3 did not complete successfully or output missing. Skipping Phase 4.\")\n","        phase_success_flags[\"phase4\"] = False\n","    elif not RUN_PHASE_4:\n","        master_logger.log(\">>> SKIPPING PHASE 4 <<<\")\n","        # If P4 is skipped, its output might not exist, but P4 is the end of this analysis pipeline.\n","        if os.path.exists(P4_OUTPUT_FINAL_ANALYSIS_JSON):\n","            phase_success_flags[\"phase4\"] = True # Assume existing output is fine if not run\n","            master_logger.log(f\"INFO: Assuming Phase 4 output {P4_OUTPUT_FINAL_ANALYSIS_JSON} is valid.\")\n","        # No critical error if P4 output is missing and P4 is skipped, as it's the final product here.\n","\n","\n","    master_logger.log(\"\\n\" + \"=\"*50)\n","    master_logger.log(\"Overall Pipeline Orchestration Concluded.\")\n","    for phase_key in [\"phase1\", \"phase2\", \"phase3\", \"phase4\"]:\n","        is_run_flag_true = False\n","        if phase_key == \"phase1\": is_run_flag_true = RUN_PHASE_1\n","        elif phase_key == \"phase2\": is_run_flag_true = RUN_PHASE_2\n","        elif phase_key == \"phase3\": is_run_flag_true = RUN_PHASE_3\n","        elif phase_key == \"phase4\": is_run_flag_true = RUN_PHASE_4\n","\n","        status_msg = \"NOT APPLICABLE (Flag False, Not Checked)\"\n","        if is_run_flag_true: # If it was set to run\n","            status_msg = 'SUCCESS' if phase_success_flags[phase_key] else 'FAILED'\n","        elif phase_success_flags[phase_key]: # If it was set to False (skip) AND its output was assumed valid\n","            status_msg = 'SKIPPED (Output Assumed Valid)'\n","        else: # If it was set to False (skip) AND its output was MISSING (and potentially needed)\n","             status_msg = 'SKIPPED (Output MISSING - May Impact Downstream)'\n","             # Refine for P4 since it's the last one:\n","             if phase_key == \"phase4\" and not is_run_flag_true and not os.path.exists(P4_OUTPUT_FINAL_ANALYSIS_JSON):\n","                 status_msg = 'SKIPPED (Final Output Not Generated)'\n","             elif phase_key == \"phase4\" and not is_run_flag_true and os.path.exists(P4_OUTPUT_FINAL_ANALYSIS_JSON):\n","                  status_msg = 'SKIPPED (Existing Final Output Assumed Valid)'\n","\n","\n","        master_logger.log(f\"  {phase_key.upper()} Status: {status_msg}\")\n","\n","    master_logger.log(\"Please check individual phase log files for details.\")\n","    master_logger.log(\"=\"*50)\n","\n","print(\"INFO: Cell 7 - Main Orchestration Cell defined. Set RUN_PHASE_X flags and execute this cell to run the pipeline.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGfqaG3cUwPk"},"outputs":[],"source":["# @title ! Cell 8: Stakeholder Report Generation\n","\n","# logger_sr will be specific to this reporting cell if needed for complex reporting\n","# For now, using print and direct file write.\n","# If FileLogger is needed here, initialize:\n","# STAKEHOLDER_REPORT_LOG = os.path.join(LOG_DIR, f\"stakeholder_report_gen_{time.strftime('%Y%m%d_%H%M%S')}.log\")\n","# logger_sr = FileLogger(STAKEHOLDER_REPORT_LOG)\n","\n","def generate_stakeholder_report_final(phase4_data_file: str, output_report_file: str, num_top_points: int, num_quotes: int):\n","    report_lines = []\n","    report_lines.append(\"=\" * 80)\n","    report_lines.append(\"STAKEHOLDER COMMUNICATION: SUMMARY OF THEMATIC ANALYSIS FROM PDF SUBMISSIONS\")\n","    report_lines.append(\"=\" * 80)\n","    report_lines.append(f\"Report generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n","    report_lines.append(f\"Source data: {os.path.basename(phase4_data_file)}\\n\")\n","\n","    try:\n","        with open(phase4_data_file, 'r', encoding='utf-8') as f:\n","            phase4_data = json.load(f) # This is expected to be QID -> P4 analysis dict\n","    except FileNotFoundError:\n","        msg = f\"ERROR: Phase 4 report file not found at {phase4_data_file}. Cannot generate stakeholder report.\"\n","        print(msg); report_lines.append(msg)\n","    except json.JSONDecodeError:\n","        msg = f\"ERROR: Could not decode JSON from {phase4_data_file}.\"\n","        print(msg); report_lines.append(msg)\n","    except Exception as e:\n","        msg = f\"ERROR: Loading {phase4_data_file} failed: {e}\"\n","        print(msg); report_lines.append(msg)\n","    else: # If loading phase4_data was successful\n","        sorted_qids = []\n","        try: sorted_qids = sorted(phase4_data.keys(), key=lambda q: int(q[1:]) if q.startswith('Q') and q[1:].isdigit() else float('inf'))\n","        except: sorted_qids = sorted(phase4_data.keys())\n","\n","        for qid in sorted_qids:\n","            qid_data = phase4_data.get(qid, {})\n","            question_text = qid_data.get(\"question_text\", f\"Question text for {qid} N/A.\")\n","            report_lines.append(\"-\" * 70); report_lines.append(f\"\\nQUESTION {qid}: {question_text}\\n\")\n","\n","            quantified_points = qid_data.get(\"quantified_points\", [])\n","            valid_quantified_points = [pt for pt in quantified_points if isinstance(pt.get(\"unique_response_count\"), int) and pt.get(\"point_label\")]\n","            sorted_points = sorted(valid_quantified_points, key=lambda x: x.get(\"unique_response_count\", 0), reverse=True)\n","\n","            if sorted_points:\n","                report_lines.append(\"  Most Common Points Raised (Counts based on keyphrase matching from PDF content):\")\n","                for i, point_data in enumerate(sorted_points[:num_top_points]):\n","                    point_label = point_data.get(\"point_label\", \"N/A\") # Should use the human-defined or auto-keyphrased label\n","                    count = point_data.get(\"unique_response_count\", 0)\n","                    llm_expl = point_data.get(\"llm_explanation\", point_data.get(\"llm_explanation_if_direct_match\", \"\")) # Check both potential keys\n","                    source_of_kps = point_data.get(\"source_of_keyphrases\", \"N/A\")\n","\n","                    report_lines.append(f\"\\n    {i+1}. {point_label} (Mentioned by {count} unique responses; Keyphrases: {source_of_kps})\")\n","                    if llm_expl and llm_expl != \"This point label was human-defined/refined.\":\n","                        report_lines.append(f\"       LLM-Generated Explanation Snippet: {llm_expl[:200]}...\") # Show snippet\n","\n","                    example_quotes = point_data.get(\"example_quotes\", [])\n","                    if example_quotes:\n","                        report_lines.append(\"       Example Quotes from PDF Content:\")\n","                        for quote_idx, quote_info in enumerate(example_quotes[:num_quotes]):\n","                            quote_text = quote_info.get(\"quote_snippet\", quote_info.get(\"quote\", \"Quote N/A\")).strip()\n","                            resp_id = quote_info.get(\"response_id\", \"N/A\")\n","                            src_type = quote_info.get(\"source_document_type\", \"N/A\")\n","                            report_lines.append(f\"         - \\\"{quote_text}\\\" (RespID: {resp_id}, SrcType: {src_type})\")\n","                    else:\n","                        report_lines.append(\"       (No example quotes met extraction criteria for this point)\")\n","                if len(sorted_points) > num_top_points:\n","                    report_lines.append(f\"\\n    (...and {len(sorted_points) - num_top_points} other quantified points with lower counts.)\")\n","            else:\n","                report_lines.append(\"  No points were quantified for this question (e.g., keyphrases might be pending definition or no matches found).\")\n","            report_lines.append(\"\")\n","\n","    report_lines.append(\"=\" * 80); report_lines.append(\"END OF STAKEHOLDER SUMMARY REPORT\"); report_lines.append(\"=\" * 80)\n","    final_report_str = \"\\n\".join(report_lines)\n","    print(\"\\n\" + final_report_str)\n","    try:\n","        with open(output_report_file, 'w', encoding='utf-8') as f_out: f_out.write(final_report_str)\n","        print(f\"\\nINFO: Stakeholder report saved to: {output_report_file}\")\n","    except Exception as e: print(f\"ERROR: Could not save stakeholder report: {e}\")\n","\n","# --- To run this Stakeholder Report Generation Cell ---\n","# 1. Ensure Phase 4 has completed and P4_OUTPUT_FINAL_ANALYSIS_JSON (from Cell 2) points to its output.\n","# 2. Update STAKEHOLDER_REPORT_OUTPUT_FILENAME if desired.\n","# 3. Execute this cell.\n","\n","if __name__ == \"__main__\": # Guard for when this cell is run directly\n","    # Use config from Cell 2, ensure Cell 2 was run\n","    try:\n","        phase4_output_to_report = P4_OUTPUT_FINAL_ANALYSIS_JSON\n","        num_quotes_for_report = P4_NUM_EXAMPLE_QUOTES_PER_THEME\n","\n","        # Define output filename for this specific report generation run\n","        stakeholder_report_filename = os.path.join(PHASE4_OUTPUT_DIR, f\"stakeholder_summary_report_{time.strftime('%Y%m%d_%H%M%S')}.txt\")\n","\n","        if not os.path.exists(phase4_output_to_report):\n","            print(f\"ERROR: Cannot generate stakeholder report. Input file missing: {phase4_output_to_report}\")\n","            print(\"       Please ensure Phase 4 has run successfully and P4_OUTPUT_FINAL_ANALYSIS_JSON in Cell 2 is correct.\")\n","        else:\n","            generate_stakeholder_report_final(\n","                phase4_output_to_report,\n","                stakeholder_report_filename,\n","                NUM_TOP_POINTS_TO_DISPLAY = 3, # Can be made a global config too\n","                num_quotes = num_quotes_for_report\n","            )\n","    except NameError as ne:\n","        print(f\"ERROR: A configuration variable (e.g., P4_OUTPUT_FINAL_ANALYSIS_JSON) is not defined. Please run Cell 2 (Global Configuration). Details: {ne}\")\n","    except Exception as e_report_main:\n","        print(f\"An unexpected error occurred in stakeholder report generation: {e_report_main}\")\n","        traceback.print_exc()\n","\n","print(\"INFO: Cell 8 - Stakeholder Report Generation logic defined.\")"]},{"cell_type":"markdown","metadata":{"id":"FAw6QdSlT2Ph"},"source":["# 24th May"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5B5q-pUVB2H"},"outputs":[],"source":["# @title Cell 0: Initial Setup & Global Imports (DO NOT MODIFY unless adding new global libraries)\n","\n","# --- Basic Python and System ---\n","import json\n","import os\n","import sys\n","import traceback\n","import time\n","import gc\n","import re\n","from collections import defaultdict, Counter\n","from typing import Dict, Any, List, Tuple, Optional\n","import warnings\n","# import argparse # Kept for potential future use, but not strictly needed for this notebook flow\n","# import tempfile # Was for single response testing in Phase 1, optional here\n","\n","# --- ML/NLP Core Imports ---\n","import torch\n","import numpy as np\n","import nltk\n","\n","# --- Hugging Face Transformers & Related ---\n","from sentence_transformers import SentenceTransformer, util as sbert_util\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","\n","# --- Suppress Warnings (Optional) ---\n","warnings.filterwarnings(\"ignore\", message=\".*Using the latest cached version.*\")\n","warnings.filterwarnings(\"ignore\", message=\".*torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.*\")\n","# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers.generation.configuration_utils\")\n","\n","\n","# --- BitsAndBytes (Optional, for Quantization) ---\n","try:\n","    from bitsandbytes.cextension import ColabNotSupportedError\n","except ImportError:\n","    ColabNotSupportedError = type('ColabNotSupportedError', (Exception,), {}) # Define dummy class\n","    print(\"WARN: bitsandbytes library not found. Quantization will be disabled if requested in config.\")\n","    pass\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 0 - Initial imports complete.\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZvAEQrNVai3"},"outputs":[],"source":["# @title Cell 1: Utility - FileLogger Class & NLTK Resource Management (DO NOT MODIFY)\n","\n","# =====================================================\n","#                Custom FileLogger Class\n","# =====================================================\n","class FileLogger:\n","    def __init__(self, log_file_path): # Renamed log_file to log_file_path for clarity\n","        self.log_file_path = log_file_path\n","        log_dir = os.path.dirname(log_file_path)\n","        if log_dir and not os.path.exists(log_dir): # Ensure directory exists\n","            os.makedirs(log_dir, exist_ok=True)\n","        try:\n","            # Open in write mode to start a new log for each FileLogger instance\n","            with open(self.log_file_path, 'w', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: Starting new log at {self.log_file_path}\\n\")\n","            # This print is useful for confirming which log file is active if multiple loggers are used.\n","            # print(f\"INFO: Logger initialized. Log file: {self.log_file_path}\")\n","        except IOError as e:\n","            print(f\"CRITICAL: Failed to initialize log file {self.log_file_path}: {e}\")\n","\n","    def _get_timestamp(self):\n","        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","\n","    def log(self, message: str):\n","        try:\n","            with open(self.log_file_path, 'a', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","        except Exception as e:\n","            # Fallback print if logging to file fails\n","            print(f\"!!! LOGGER FAILED TO WRITE TO {self.log_file_path}: {e} !!!\")\n","            print(f\"{self._get_timestamp()}: {message}\") # Log to console as fallback\n","\n","    def _get_serializer(self):\n","        # Define a nested function to handle custom serialization types\n","        def default_serializer(obj):\n","            if isinstance(obj, (np.integer, np.floating)): # Handle numpy int and float types\n","                return obj.item()\n","            if isinstance(obj, np.ndarray): # Handle numpy arrays\n","                return obj.tolist()\n","            if isinstance(obj, set): # Handle sets\n","                return sorted(list(obj))\n","            if 'torch' in sys.modules and isinstance(obj, torch.Tensor): # Handle torch tensors\n","                return obj.detach().cpu().numpy().tolist()\n","            try: # Try the default JSON encoder\n","                return json.JSONEncoder().default(obj)\n","            except TypeError: # If default fails, return string representation\n","                return f\"UNSERIALIZABLE:{type(obj).__name__}\"\n","        return default_serializer\n","\n","    def log_json(self, data: Dict[str, Any], prefix=\"JSON_DATA\"):\n","        serializer = self._get_serializer()\n","        log_entry = {\"timestamp\": self._get_timestamp(), \"type\": prefix, \"data\": data}\n","        try:\n","            json_string = json.dumps(log_entry, default=serializer, indent=2)\n","            with open(self.log_file_path, 'a', encoding='utf-8') as f:\n","                f.write(json_string + \"\\n\")\n","        except Exception as e:\n","            self.log(f\"ERROR: Unexpected error during log_json (prefix={prefix}): {e}\")\n","            # Optionally log the full traceback for log_json errors if they are hard to debug\n","            # self.log(f\"TRACEBACK for log_json error:\\n{traceback.format_exc()}\")\n","\n","# =====================================================\n","#         NLTK Setup Helper\n","# =====================================================\n","def download_nltk_resource_with_logger(logger_instance, resource_id, download_name):\n","    \"\"\"Checks for an NLTK resource and downloads it if missing, using provided logger.\"\"\"\n","    try:\n","        nltk.data.find(resource_id)\n","        # logger_instance.log(f\"INFO: NLTK resource '{download_name}' already present for {resource_id}.\") # Optional: can be verbose\n","    except LookupError:\n","        logger_instance.log(f\"INFO: NLTK resource '{download_name}' (for {resource_id}) not found. Downloading...\")\n","        try:\n","            nltk.download(download_name, quiet=True) # quiet=True suppresses nltk's own console output\n","            logger_instance.log(f\"INFO: Successfully downloaded NLTK resource: {download_name}\")\n","        except Exception as dl_e:\n","            logger_instance.log(f\"ERROR: Failed NLTK download for {download_name}: {dl_e}\")\n","    except Exception as e: # Other potential errors with nltk.data.find\n","        logger_instance.log(f\"ERROR: NLTK check error for {download_name} ({resource_id}): {e}\")\n","\n","# --- Initial NLTK Downloads (using print as master_logger might not be set yet for Cell 1 execution) ---\n","# This section ensures essential NLTK packages are available when other cells import/use NLTK.\n","# The Orchestration Cell (Cell 7) will use download_nltk_resource_with_logger with its own logger.\n","print(\"INFO: Cell 1 - Performing initial NLTK resource check/download (using print)...\")\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    print(\"  NLTK 'punkt' not found. Downloading...\")\n","    nltk.download('punkt', quiet=True)\n","try:\n","    nltk.data.find('tokenizers/punkt_tab')\n","except LookupError:\n","    print(\"  NLTK 'punkt_tab' not found. Downloading...\")\n","    nltk.download('punkt_tab', quiet=True)\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    print(\"  NLTK 'stopwords' not found. Downloading...\")\n","    nltk.download('stopwords', quiet=True)\n","# Add other essential NLTK downloads here if needed globally, e.g. 'averaged_perceptron_tagger' for POS tagging\n","# try:\n","#     nltk.data.find('taggers/averaged_perceptron_tagger')\n","# except LookupError:\n","#     print(\"  NLTK 'averaged_perceptron_tagger' not found. Downloading...\")\n","#     nltk.download('averaged_perceptron_tagger', quiet=True)\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 1 - Utilities (FileLogger, NLTK Downloader function) defined, initial NLTK resources checked.\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YKSxcN6BV6kq"},"outputs":[],"source":["# @title Cell 2: Global Project Configuration (REVIEW AND UPDATE ALL PATHS/SETTINGS)\n","\n","import os\n","import time # For timestamped output filenames\n","\n","# --- Base Paths ---\n","# !!! CRITICAL: UPDATE THIS TO YOUR MAIN PROJECT FOLDER !!!\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","\n","# --- Derived Output Directories (will be created if they don't exist by FileLogger or explicitly) ---\n","PHASE1_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase1_PDF_Extraction_Outputs/')\n","PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n","PHASE3_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase3_LLM_Generated_Themes/')\n","PHASE4_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase4_Final_Thematic_Analysis/')\n","LOG_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Logs/') # Centralized log directory\n","\n","# Ensure base output directories exist (FileLogger also does this for its specific log file)\n","for d_path in [PHASE1_OUTPUT_DIR, PHASE2_OUTPUT_DIR, PHASE3_OUTPUT_DIR, PHASE4_OUTPUT_DIR, LOG_DIR]:\n","    os.makedirs(d_path, exist_ok=True)\n","\n","# --- Common Input Files ---\n","# !!! CRITICAL: ENSURE THESE PATHS ARE CORRECT !!!\n","ORIGINAL_RESPONSES_JSONL = os.path.join(BASE_PROJECT_DIR, 'processed_responses.jsonl')\n","# This file (output of your original form-data Stage 2) is crucial for QID->Question Text mapping\n","QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","\n","# --- Phase 1: PDF Extraction & Per-Response Summarization (Original Stage 4) ---\n","P1_LOG_FILE = os.path.join(LOG_DIR, \"phase1_pdf_extraction.log\")\n","# !!! CRITICAL: This should point to your ALREADY EXISTING, COMPLETE output from the long Stage 4 run !!!\n","# If you are running Phase 1 from scratch, this will be its output target.\n","P1_OUTPUT_FILE = os.path.join(PHASE1_OUTPUT_DIR, 'phase1_pdf_analysis_by_response.json')\n","P1_STATE_FILE = os.path.join(PHASE1_OUTPUT_DIR, 'phase1_processing_state.json') # State file for Phase 1\n","\n","P1_EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n","P1_LLM_ID_FOR_SUMMARIES = 'google/gemma-3-1b-it' # 'google/gemma-2b-it' # Using smaller model for many per-response summaries\n","P1_USE_QUANTIZATION = True                     # Recommended for gemma-2b to save memory\n","P1_MIN_PDF_TEXT_LENGTH = 100\n","P1_MIN_SIMILARITY_SCORE = 0.5\n","P1_CONTEXT_WINDOW_SIZE_SENTENCES = 1 # Sentences before/after matched sentence for passage\n","P1_TOP_K_PASSAGES = 3                # Max relevant passages to retrieve/store per QID in a response\n","P1_NUM_SUMMARY_SENTENCES_PER_RESPONSE_QID = 3 # Target for AI summaries in Phase 1\n","P1_MIN_CHARS_FOR_SUMMARY = 150       # Min total chars in combined passages to attempt P1 summary\n","P1_LLM_MAX_NEW_TOKENS_FOR_PER_RESPONSE_SUMMARY = 250 # For Phase 1 AI summaries\n","\n","P1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE = {\n","    \"Q1\": True, \"Q2\": True, \"Q3\": True, \"Q4\": True, \"Q5\": True, \"Q6\": True, \"Q7\": True,\n","    \"Q8\": True, \"Q9\": True, \"Q10\": True,\"Q11\": True, \"Q12\": True, \"Q13\": True, \"Q14\": True,\n","    \"Q15\": True, \"Q16\": False,\"Q17\": True, \"Q18\": True, \"Q19\": True,\"Q20\": False, \"Q21\": True,\n","    \"Q22\": True, \"Q23\": True, \"Q24\": True, \"Q25\": True, \"Q26\": True, \"Q27\": True, \"Q28\": True,\n","    \"Q29\": True, \"Q30\": True, \"Q31\": True,\"Q32\": False,\"Q33\": False,\"Q34\": False,\"Q35\": False,\n","    \"Q36\": False,\n","}\n","P1_MAX_RUNTIME_MINUTES = 60 * 1 # Set low if using existing P1_OUTPUT_FILE and just need to ensure it's \"touched\" by a quick run\n","                                # Set high (e.g., 60 * 8) for a full initial run of Phase 1.\n","P1_SAVE_STATE_INTERVAL_SECONDS = 60 * 10 # How often to save state during a long Phase 1 run\n","\n","# --- Phase 2: Collation of PDF Content (Original Stage 4a) ---\n","P2_LOG_FILE = os.path.join(LOG_DIR, f'phase2_text_collation.log') # Consistent log name for reruns\n","# P2_INPUT_FILE_OR_DIR is P1_OUTPUT_FILE, defined above.\n","P2_OUTPUT_COLLATED_JSON = os.path.join(PHASE2_OUTPUT_DIR, f'phase2_collated_pdf_texts.json') # Consistent name for reruns\n","\n","# --- Phase 3: LLM Thematic Point Generation (Map-Reduce) ---\n","P3_LOG_FILE = os.path.join(LOG_DIR, f'phase3_llm_themes.log') # Consistent log name\n","P3_INPUT_COLLATED_JSON = P2_OUTPUT_COLLATED_JSON # Input is output from Phase 2\n","P3_OUTPUT_LLM_THEMES_JSON = os.path.join(PHASE3_OUTPUT_DIR, f'phase3_llm_generated_themes.json') # Consistent name\n","P3_LLM_ID_FOR_THEMES = 'google/gemma-3-1b-it' # \"Qwen/Qwen2-4B-Instruct\" # Based on successful tests\n","P3_USE_QUANTIZATION = True                     # Based on successful tests\n","P3_NUM_THEMES_MAP_STR = \"up to 2\"              # String for LLM prompt for map step\n","P3_NUM_THEMES_MAP_INT = 2                      # Integer for parsing map step output\n","P3_NUM_THEMES_REDUCE_STR = \"top 3\"             # String for LLM prompt for reduce step\n","P3_NUM_THEMES_REDUCE_INT = 3                   # Integer for parsing reduce step output\n","P3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 3000 # Max *content* tokens for a batch of items for map step\n","P3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 800     # Max new tokens LLM generates for map step (JSON output)\n","P3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 1000 # Max new tokens LLM generates for reduce step (JSON output)\n","P3_QIDS_TO_PROCESS_THEMATICALLY = None # Set to None for all QIDs; Phase 3 script will skip already processed ones.\n","                                       # Or e.g., [\"Q1\", \"Q4\"] to force reprocessing/testing only these.\n","\n","# --- Phase 4: Theme Validation, Quantification & Quotation Extraction ---\n","P4_LOG_FILE = os.path.join(LOG_DIR, f'phase4_theme_quantification.log') # Consistent log name\n","P4_INPUT_LLM_THEMES_JSON = P3_OUTPUT_LLM_THEMES_JSON     # Input is output from Phase 3\n","P4_INPUT_COLLATED_TEXTS_JSON = P2_OUTPUT_COLLATED_JSON # Input is output from Phase 2 (for source texts)\n","P4_OUTPUT_FINAL_ANALYSIS_JSON = os.path.join(PHASE4_OUTPUT_DIR, f'phase4_final_thematic_report.json') # Consistent name\n","P4_HUMAN_DEFINED_KEYPHRASES_FILE = os.path.join(BASE_PROJECT_DIR, 'human_defined_keyphrases_for_themes.json') # Critical manual input\n","P4_NUM_EXAMPLE_QUOTES_PER_THEME = 2 # Default number of quotes to extract per theme\n","\n","# --- General ---\n","# This flag can be checked by helper functions within phases for more verbose logging if needed.\n","ENABLE_DETAILED_DEBUG_LOGGING = False # Set True for very verbose logs, False for standard runs\n","\n","print(\"=\"*50)\n","print(f\"INFO: Cell 2 - Global Configuration loaded. Base project directory: {BASE_PROJECT_DIR}\")\n","print(f\"  P1 Output File Target: {P1_OUTPUT_FILE}\")\n","print(f\"  P2 Output File Target: {P2_OUTPUT_COLLATED_JSON}\")\n","print(f\"  P3 Output File Target: {P3_OUTPUT_LLM_THEMES_JSON}\")\n","print(f\"  P4 Output File Target: {P4_OUTPUT_FINAL_ANALYSIS_JSON}\")\n","print(f\"  Keyphrase File for P4: {P4_HUMAN_DEFINED_KEYPHRASES_FILE}\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0OG-xa9AWcsm"},"outputs":[],"source":["# @title Cell 3: Phase 1 - PDF Extraction & Per-Response Summarization (Original Stage 4)\n","\n","# Logger for this phase will be initialized in the run_phase1_pdf_extraction function\n","# when called by the orchestrator, or can be initialized here for standalone testing.\n","# Example for standalone testing (Orchestrator in Cell 7 handles this for pipeline runs):\n","# if 'logger_p1' not in locals():\n","#     logger_p1 = FileLogger(P1_LOG_FILE) # P1_LOG_FILE from Cell 2\n","\n","# --- Helper Functions for Phase 1 ---\n","\n","def save_phase1_state(state_file: str, processed_response_ids_set: set, current_results_dict: Dict[str, Any], logger_instance: FileLogger):\n","    logger_instance.log(f\"INFO P1_State: Saving Phase 1 processing state to {state_file}...\")\n","    try:\n","        state_data = {\n","            \"processed_responses_ids\": sorted(list(processed_response_ids_set)),\n","            \"results_so_far\": current_results_dict\n","        }\n","        temp_state_file = state_file + \".tmp\"\n","        with open(temp_state_file, 'w', encoding='utf-8') as f:\n","            json.dump(state_data, f, indent=2, default=logger_instance._get_serializer())\n","        os.replace(temp_state_file, state_file)\n","        logger_instance.log(f\"INFO P1_State: Phase 1 State saved successfully ({len(processed_response_ids_set)} responses processed).\")\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P1_State: Failed to save Phase 1 state to {state_file}: {e}\")\n","        logger_instance.log(f\"TRACEBACK P1_State:\\n{traceback.format_exc()}\")\n","\n","def load_phase1_state(state_file: str, logger_instance: FileLogger) -> Tuple[set, Dict[str, Any]]:\n","    if os.path.exists(state_file):\n","        logger_instance.log(f\"INFO P1_State: Found Phase 1 state file {state_file}. Attempting to load state...\")\n","        try:\n","            with open(state_file, 'r', encoding='utf-8') as f:\n","                state_data = json.load(f)\n","            processed_ids = set(state_data.get(\"processed_responses_ids\", []))\n","            results = state_data.get(\"results_so_far\", {})\n","            if not isinstance(results, dict):\n","                logger_instance.log(\"ERROR P1_State: Loaded 'results_so_far' is not a dictionary. Starting fresh.\")\n","                return set(), {}\n","            logger_instance.log(f\"INFO P1_State: Successfully loaded Phase 1 state. Resuming after {len(processed_ids)} processed responses.\")\n","            return processed_ids, results\n","        except Exception as e:\n","            logger_instance.log(f\"ERROR P1_State: Failed to load/parse Phase 1 state from {state_file}: {e}. Corrupted file? Starting fresh.\")\n","            return set(), {}\n","    else:\n","        logger_instance.log(\"INFO P1_State: No Phase 1 state file found at {state_file}. Starting fresh.\")\n","        return set(), {}\n","\n","def get_phase1_local_embeddings(texts: List[str], model: SentenceTransformer, device: str, logger_instance: FileLogger, batch_size: int = 32) -> Optional[np.ndarray]:\n","    if not texts: return None\n","    try:\n","        # Use global ENABLE_DETAILED_DEBUG_LOGGING from Cell 2\n","        if ENABLE_DETAILED_DEBUG_LOGGING:\n","            logger_instance.log(f\"DEBUG P1_Embed: Generating local embeddings for {len(texts)} texts on {device} (batch: {batch_size})...\")\n","\n","        model.to(device) # Ensure model is on the correct device\n","        embeddings_tensor = model.encode(texts, convert_to_tensor=True,\n","                                         show_progress_bar=ENABLE_DETAILED_DEBUG_LOGGING, # Show progress if debugging\n","                                         batch_size=batch_size, device=device)\n","        return embeddings_tensor.cpu().numpy()\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P1_Embed: Failed to generate local embeddings: {e}\")\n","        logger_instance.log(f\"TRACEBACK P1_Embed:\\n{traceback.format_exc()}\")\n","        return None\n","\n","def retrieve_phase1_relevant_passages(question_embedding: np.ndarray,\n","                                     pdf_sentences: List[str],\n","                                     pdf_sentence_embeddings: np.ndarray,\n","                                     similarity_threshold: float, # P1_MIN_SIMILARITY_SCORE\n","                                     top_k: int, # P1_TOP_K_PASSAGES\n","                                     context_window_sentences: int, # P1_CONTEXT_WINDOW_SIZE_SENTENCES\n","                                     logger_instance: FileLogger) -> Tuple[List[str], Optional[float]]:\n","    relevant_passages_texts = []\n","    highest_score_found = None\n","    if question_embedding is None or pdf_sentence_embeddings is None or not pdf_sentences:\n","        return relevant_passages_texts, highest_score_found\n","\n","    if question_embedding.ndim == 1:\n","        question_embedding = question_embedding.reshape(1, -1)\n","    if pdf_sentence_embeddings.ndim == 1: # Should not happen if multiple sentences\n","        pdf_sentence_embeddings = pdf_sentence_embeddings.reshape(1, -1)\n","    if pdf_sentence_embeddings.shape[0] != len(pdf_sentences):\n","        logger_instance.log(f\"ERROR P1_Retrieve: Sentences ({len(pdf_sentences)}) and embeddings ({pdf_sentence_embeddings.shape[0]}) count mismatch!\")\n","        return [], None\n","    if question_embedding.shape[1] != pdf_sentence_embeddings.shape[1]:\n","        logger_instance.log(f\"ERROR P1_Retrieve: Embedding dimension mismatch Q({question_embedding.shape[1]}) vs S({pdf_sentence_embeddings.shape[1]})!\")\n","        return [], None\n","\n","    try:\n","        # sbert_util is sentence_transformers.util\n","        # Ensure tensors are on the same device for cos_sim\n","        # Assuming pdf_sentence_embeddings were generated on a certain device, try to match it.\n","        # Or, move both to CPU for this calculation if device issues persist.\n","        # For simplicity, if pdf_sentence_embeddings is numpy, convert to tensor on default device or CPU.\n","        current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        q_tensor = torch.from_numpy(question_embedding).to(dtype=torch.float32, device=current_device)\n","        s_tensor = torch.from_numpy(pdf_sentence_embeddings).to(dtype=torch.float32, device=current_device)\n","\n","        cosine_scores_tensor = sbert_util.cos_sim(q_tensor, s_tensor)[0]\n","        cosine_scores = cosine_scores_tensor.cpu().numpy()\n","        sorted_indices = np.argsort(cosine_scores)[::-1]\n","        added_passage_hashes = set()\n","\n","        for idx in sorted_indices:\n","            score = cosine_scores[idx]\n","            if score < similarity_threshold:\n","                break\n","            if highest_score_found is None:\n","                highest_score_found = float(score)\n","\n","            start_idx = max(0, idx - context_window_sentences)\n","            end_idx = min(len(pdf_sentences), idx + context_window_sentences + 1)\n","            if start_idx >= end_idx : continue\n","\n","            context_passage_list = pdf_sentences[start_idx:end_idx]\n","            passage_text = \" \".join(context_passage_list).strip()\n","            passage_hash = hash(passage_text)\n","\n","            if passage_text and passage_hash not in added_passage_hashes:\n","                relevant_passages_texts.append(passage_text)\n","                added_passage_hashes.add(passage_hash)\n","                if len(relevant_passages_texts) >= top_k:\n","                    break\n","        return relevant_passages_texts, highest_score_found\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P1_Retrieve: Failed during passage retrieval: {e}\")\n","        logger_instance.log(f\"TRACEBACK P1_Retrieve:\\n{traceback.format_exc()}\")\n","        return [], None\n","\n","def summarize_phase1_context_block(text_chunk: str, question_text_for_prompt: str, qid_for_log: str,\n","                                 llm_pipe: pipeline, num_target_sentences: int, # P1_NUM_SUMMARY_SENTENCES_PER_RESPONSE_QID\n","                                 max_new_tokens: int, # P1_LLM_MAX_NEW_TOKENS_FOR_PER_RESPONSE_SUMMARY\n","                                 logger_instance: FileLogger) -> Optional[str]:\n","    if not text_chunk or not llm_pipe: return None\n","    # Prompt for summarizing specific passages from a single PDF for a single question\n","    messages = [{\n","        \"role\": \"user\",\n","        \"content\": f\"Carefully review the following text excerpt extracted from a PDF submission, which is relevant to the question: \\\"{question_text_for_prompt}\\\". \"\n","                   f\"Concisely summarize the main points from THIS EXCERPT in approximately {num_target_sentences} distinct sentences. \"\n","                   f\"Focus on capturing the essence of this specific excerpt as it relates to the question.\\n\\nExcerpt:\\n\\\"\\\"\\\"\\n{text_chunk}\\n\\\"\\\"\\\"\\n\\nSummary of This Excerpt:\"\n","    }]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        generation_args = {\n","            \"max_new_tokens\": max_new_tokens, \"do_sample\": False,\n","            \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id,\n","            \"top_p\": None, \"top_k\": None # Silence warnings if do_sample=False\n","        }\n","        if ENABLE_DETAILED_DEBUG_LOGGING:\n","            logger_instance.log(f\"DEBUG P1_Summarize: LLM summary call for QID {qid_for_log} (Excerpt chars: {len(text_chunk)})...\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger_instance.log(f\"ERROR P1_Summarize: LLM summary pipeline returned unexpected/empty output for QID {qid_for_log}: {outputs}\")\n","            return None # Indicates an error in pipeline execution\n","\n","        generated_text_full = outputs[0]['generated_text']\n","        summary_text = \"\"\n","\n","        # Try to strip prompt (more robustly for different models)\n","        if generated_text_full.startswith(prompt_formatted):\n","            summary_text = generated_text_full[len(prompt_formatted):].strip()\n","        else:\n","            # Common model response markers (try a few, Gemma specific first)\n","            markers_to_try = [\"<start_of_turn>model\\n\", \"<|assistant|>\\n\", \"<|im_start|>assistant\\n\"]\n","            found_marker = False\n","            for marker in markers_to_try:\n","                summary_start_index = generated_text_full.rfind(marker)\n","                if summary_start_index != -1:\n","                    summary_text = generated_text_full[summary_start_index + len(marker):].strip()\n","                    if ENABLE_DETAILED_DEBUG_LOGGING: logger_instance.log(f\"DEBUG P1_Summarize: Extracted summary using marker '{marker}' for QID {qid_for_log}.\")\n","                    found_marker = True\n","                    break\n","            if not found_marker:\n","                logger_instance.log(f\"WARN P1_Summarize: Could not find standard response marker for QID {qid_for_log}. Using full output as summary (may contain prompt). Raw start: {generated_text_full[:100]}\")\n","                summary_text = generated_text_full # As a last resort, might be just the response\n","\n","        # Final check for empty or error-like summaries\n","        if not summary_text.strip() or summary_text.lower().startswith(\"error:\") or \"i am unable to\" in summary_text.lower() or \"cannot fulfill this request\" in summary_text.lower():\n","            if ENABLE_DETAILED_DEBUG_LOGGING or not summary_text.strip(): # Log if debug or actually empty\n","                 logger_instance.log(f\"WARN P1_Summarize: Final extracted summary for QID {qid_for_log} is empty or indicates inability. Raw LLM output: {generated_text_full[:300]}...\")\n","            return \"\" # Return empty string to signify a meaningful summary was not produced\n","        return summary_text.strip()\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P1_Summarize: LLM summarization exception for QID {qid_for_log}: {e}\")\n","        logger_instance.log(f\"TRACEBACK P1_Summarize:\\n{traceback.format_exc()}\")\n","        return None # Indicates an error during the call\n","\n","# --- Main Execution Function for Phase 1 ---\n","def run_phase1_pdf_extraction(logger_p1_instance: FileLogger):\n","    logger_p1 = logger_p1_instance # Use the passed logger\n","    logger_p1.log(\"=\" * 50); logger_p1.log(\" INFO P1: Starting Phase 1: PDF Content Extraction & Per-Response Summarization\"); logger_p1.log(\"=\" * 50)\n","    # Log key configurations from global (Cell 2)\n","    logger_p1.log(f\"INFO P1: Input Raw Responses: {ORIGINAL_RESPONSES_JSONL}\")\n","    logger_p1.log(f\"INFO P1: Input QID Definitions: {QUESTION_DEFINITIONS_FILE}\")\n","    logger_p1.log(f\"INFO P1: Target Output File: {P1_OUTPUT_FILE}\")\n","    logger_p1.log(f\"INFO P1: State File: {P1_STATE_FILE}\")\n","    logger_p1.log(f\"INFO P1: Embedding Model: {P1_EMBEDDING_MODEL}\")\n","    logger_p1.log(f\"INFO P1: LLM for Summaries: {P1_LLM_ID_FOR_SUMMARIES} (Quant: {P1_USE_QUANTIZATION})\")\n","    logger_p1.log(f\"INFO P1: Max Runtime (mins): {P1_MAX_RUNTIME_MINUTES if P1_MAX_RUNTIME_MINUTES > 0 else 'Unlimited'}\")\n","\n","    phase1_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    logger_p1.log(f\"INFO P1: Using device: {phase1_device}\")\n","\n","    embedding_model_p1 = None; llm_tokenizer_p1 = None; llm_model_p1 = None; llm_summarization_pipe_p1 = None\n","    try:\n","        logger_p1.log(f\"INFO P1: Loading embedding model: {P1_EMBEDDING_MODEL}...\")\n","        embedding_model_p1 = SentenceTransformer(P1_EMBEDDING_MODEL, device=phase1_device)\n","\n","        logger_p1.log(f\"INFO P1: Loading LLM tokenizer for: {P1_LLM_ID_FOR_SUMMARIES}...\")\n","        llm_tokenizer_p1 = AutoTokenizer.from_pretrained(P1_LLM_ID_FOR_SUMMARIES)\n","        if getattr(llm_tokenizer_p1, 'pad_token', None) is None and llm_tokenizer_p1.eos_token is not None:\n","            llm_tokenizer_p1.pad_token = llm_tokenizer_p1.eos_token\n","\n","        bnb_config_p1 = None; current_run_p1_quant = P1_USE_QUANTIZATION\n","        if P1_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config_p1 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n","                logger_p1.log(f\"INFO P1: BNB config for {P1_LLM_ID_FOR_SUMMARIES} created, dtype: {compute_dtype}.\")\n","            except ImportError: current_run_p1_quant=False; logger_p1.log(\"WARN P1: bitsandbytes not found, disabling quantization for P1 LLM.\")\n","            except Exception as qe: logger_p1.log(f\"WARN P1: BNB config failed for P1 LLM: {qe}. Disabling quant.\"); current_run_p1_quant=False\n","        elif P1_USE_QUANTIZATION: current_run_p1_quant=False; logger_p1.log(\"WARN P1: Quantization requested for P1 LLM, but No CUDA.\")\n","\n","        logger_p1.log(f\"INFO P1: Loading LLM model: {P1_LLM_ID_FOR_SUMMARIES} (Quant active: {current_run_p1_quant})...\")\n","        model_torch_dtype = None\n","        if not current_run_p1_quant: # Only set torch_dtype if not quantizing\n","            if torch.cuda.is_available() and torch.cuda.is_bf16_supported(): model_torch_dtype = torch.bfloat16\n","            elif torch.cuda.is_available(): model_torch_dtype = torch.float16 # fallback for older GPUs\n","            # else: None (CPU will use float32 by default)\n","\n","        llm_model_p1 = AutoModelForCausalLM.from_pretrained(\n","            P1_LLM_ID_FOR_SUMMARIES,\n","            quantization_config=bnb_config_p1 if current_run_p1_quant else None,\n","            torch_dtype=model_torch_dtype,\n","            device_map=\"auto\", trust_remote_code=True) # Gemma models often require trust_remote_code\n","        llm_summarization_pipe_p1 = pipeline(\"text-generation\", model=llm_model_p1, tokenizer=llm_tokenizer_p1)\n","        logger_p1.log(\"INFO P1: All models for Phase 1 loaded successfully.\")\n","    except Exception as le:\n","        logger_p1.log(f\"CRITICAL P1: Model loading failed: {le}\\n{traceback.format_exc()}\"); return False\n","\n","    qid_to_text_map_p1 = {}; qid_to_embedding_map_p1 = {}\n","    try:\n","        with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f: q_data_defs = json.load(f)\n","        for qid, data in q_data_defs.get(\"question_data\", {}).items():\n","            q_text = data.get(\"question_text\")\n","            if q_text: qid_to_text_map_p1[qid] = q_text\n","        if not qid_to_text_map_p1: raise ValueError(\"No QID texts in QUESTION_DEFINITIONS_FILE.\")\n","        logger_p1.log(f\"INFO P1: Loaded {len(qid_to_text_map_p1)} QID texts.\")\n","\n","        target_qids_p1 = list(qid_to_text_map_p1.keys())\n","        target_qtexts_p1 = list(qid_to_text_map_p1.values())\n","        question_embeddings_all_p1 = get_phase1_local_embeddings(target_qtexts_p1, embedding_model_p1, phase1_device, logger_p1, batch_size=128) # Larger batch for Q embeddings\n","        if question_embeddings_all_p1 is None: raise ValueError(\"Question embedding generation failed.\")\n","        qid_to_embedding_map_p1 = {qid: emb for qid, emb in zip(target_qids_p1, question_embeddings_all_p1)}\n","    except Exception as qle:\n","        logger_p1.log(f\"ERROR P1: Loading QID definitions/embeddings failed: {qle}\\n{traceback.format_exc()}\"); return False\n","\n","    processed_response_ids_p1, results_p1 = load_phase1_state(P1_STATE_FILE, logger_p1)\n","    initial_processed_count = len(processed_response_ids_p1)\n","\n","    processed_in_this_run_p1 = 0; pdfs_analyzed_this_run_p1 = 0; llm_sums_att_p1 = 0; llm_sums_succ_p1 = 0\n","    start_processing_time_p1 = time.time(); last_save_time_p1 = start_processing_time_p1\n","    MAX_RUNTIME_SECONDS_P1 = P1_MAX_RUNTIME_MINUTES * 60 if P1_MAX_RUNTIME_MINUTES > 0 else float('inf')\n","\n","    total_lines_in_input = 0\n","    try:\n","        with open(ORIGINAL_RESPONSES_JSONL, 'r', encoding='utf-8') as infile_count: total_lines_in_input = sum(1 for _ in infile_count)\n","    except: total_lines_in_input = -1; logger_p1.log(f\"WARN P1: Could not count lines in {ORIGINAL_RESPONSES_JSONL}\")\n","    logger_p1.log(f\"INFO P1: Total lines in input: {total_lines_in_input if total_lines_in_input >0 else 'Unknown'}. Resuming from {initial_processed_count} already processed responses.\")\n","\n","    try:\n","        with open(ORIGINAL_RESPONSES_JSONL, 'r', encoding='utf-8') as infile:\n","            for line_num, line in enumerate(infile, 1):\n","                if total_lines_in_input > 0 and line_num % (max(1, total_lines_in_input // 100)) == 0: # Log every ~1%\n","                    logger_p1.log(f\"PROGRESS P1: Iterated line {line_num}/{total_lines_in_input} ({(line_num/total_lines_in_input)*100:.1f}%).\")\n","\n","                current_elapsed_time = time.time() - start_processing_time_p1\n","                if current_elapsed_time >= MAX_RUNTIME_SECONDS_P1 :\n","                    logger_p1.log(f\"WARN P1: Runtime limit ({P1_MAX_RUNTIME_MINUTES}m) reached. Stopping.\"); break\n","                if time.time() - last_save_time_p1 >= P1_SAVE_STATE_INTERVAL_SECONDS:\n","                    save_phase1_state(P1_STATE_FILE, processed_response_ids_p1, results_p1, logger_p1)\n","                    last_save_time_p1 = time.time()\n","\n","                response_id_p1 = f\"UnknownLine{line_num}\"\n","                try:\n","                    response_obj_p1 = json.loads(line); response_id_p1 = str(response_obj_p1.get(\"id\", response_id_p1))\n","                    if response_id_p1 in processed_response_ids_p1: continue\n","\n","                    t_response_start = time.time()\n","                    log_this_response = (processed_in_this_run_p1 < 5 or processed_in_this_run_p1 % 50 == 0 or ENABLE_DETAILED_DEBUG_LOGGING)\n","                    if log_this_response: logger_p1.log(f\"INFO P1: BEGIN Response ID {response_id_p1} (Line: {line_num}, New in run: {processed_in_this_run_p1 + 1})\")\n","\n","                    results_p1[response_id_p1] = {} # Initialize\n","                    pdf_text_p1 = response_obj_p1.get(\"pdf_text\")\n","\n","                    if not pdf_text_p1 or len(pdf_text_p1) < P1_MIN_PDF_TEXT_LENGTH:\n","                        results_p1[response_id_p1][\"status\"] = \"pdf_missing_or_short\"\n","                    else:\n","                        _cleaned_pdf_text_p1 = re.sub(r'\\s+', ' ', pdf_text_p1).strip()\n","                        _pdf_sentences_p1 = nltk.sent_tokenize(_cleaned_pdf_text_p1)\n","                        _pdf_sentences_p1 = [s.strip() for s in _pdf_sentences_p1 if s.strip()]\n","                        _pdf_sentence_embeddings_p1 = None\n","                        if _pdf_sentences_p1:\n","                            _pdf_sentence_embeddings_p1 = get_phase1_local_embeddings(_pdf_sentences_p1, embedding_model_p1, phase1_device, logger_p1, batch_size=64)\n","\n","                        if not _pdf_sentences_p1 or _pdf_sentence_embeddings_p1 is None:\n","                            results_p1[response_id_p1][\"status\"] = \"pdf_no_sentences_or_embedding_error\"\n","                        else:\n","                            pdfs_analyzed_this_run_p1 +=1\n","                            for q_idx, qid_p1 in enumerate(target_qids_p1):\n","                                if ENABLE_DETAILED_DEBUG_LOGGING and q_idx % 10 == 0 : logger_p1.log(f\"    DEBUG P1: Resp {response_id_p1}, QID {qid_p1} ({q_idx+1}/{len(target_qids_p1)})...\")\n","                                q_text_p1 = qid_to_text_map_p1[qid_p1]; q_emb_p1 = qid_to_embedding_map_p1.get(qid_p1)\n","                                current_q_result_p1 = {\"question_text\": q_text_p1}\n","                                if q_emb_p1 is None: current_q_result_p1[\"status\"] = \"error_missing_q_embedding\"\n","                                else:\n","                                    passages_p1, top_score_p1 = retrieve_phase1_relevant_passages(\n","                                        q_emb_p1, _pdf_sentences_p1, _pdf_sentence_embeddings_p1,\n","                                        P1_MIN_SIMILARITY_SCORE, P1_TOP_K_PASSAGES, P1_CONTEXT_WINDOW_SIZE_SENTENCES, logger_p1)\n","                                    if passages_p1:\n","                                        current_q_result_p1.update({\"status\": \"passages_extracted\", \"extracted_passages\": passages_p1,\n","                                                                    \"top_passage_score\": float(top_score_p1) if top_score_p1 is not None else None})\n","                                        if llm_summarization_pipe_p1 and P1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE.get(qid_p1, False):\n","                                            combined_passage_text_p1 = \"\\n\\n\".join(passages_p1)\n","                                            if len(combined_passage_text_p1) >= P1_MIN_CHARS_FOR_SUMMARY:\n","                                                llm_sums_att_p1 += 1\n","                                                summary_p1 = summarize_phase1_context_block(combined_passage_text_p1, q_text_p1, qid_p1,\n","                                                                                            llm_summarization_pipe_p1, P1_NUM_SUMMARY_SENTENCES_PER_RESPONSE_QID,\n","                                                                                            P1_LLM_MAX_NEW_TOKENS_FOR_PER_RESPONSE_SUMMARY, logger_p1)\n","                                                if summary_p1 is not None: # Check for None from error\n","                                                    current_q_result_p1[\"summary\"] = summary_p1\n","                                                    if summary_p1: current_q_result_p1[\"status\"] = \"success_summarized\"; llm_sums_succ_p1 +=1\n","                                                    else: current_q_result_p1[\"status\"] = \"summary_attempted_empty\" # LLM returned empty string\n","                                                else: current_q_result_p1[\"status\"] = \"error_llm_summarization\" # summarize_phase1_context_block returned None\n","                                            else: current_q_result_p1[\"status\"] = \"passages_too_short_for_summary\"\n","                                    else: current_q_result_p1[\"status\"] = \"no_relevant_passages_found\"\n","                                results_p1[response_id_p1][qid_p1] = current_q_result_p1\n","\n","                    processed_response_ids_p1.add(response_id_p1)\n","                    processed_in_this_run_p1 += 1\n","                    if log_this_response:\n","                        logger_p1.log(f\"  DONE Response ID {response_id_p1}. Time: {time.time() - t_response_start:.2f}s. (New this run: {processed_in_this_run_p1})\")\n","                    if processed_in_this_run_p1 > 0 and processed_in_this_run_p1 % 200 == 0 : # More frequent GPU cache clearing\n","                        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","                        logger_p1.log(f\"  INFO P1: Cleared cache after {processed_in_this_run_p1} new responses.\")\n","\n","                except json.JSONDecodeError:\n","                    logger_p1.log(f\"WARN P1: JSON decode error line {line_num}. Raw line: '{line[:200]}...' Skipping.\");\n","                    results_p1[response_id_p1]={\"status\":\"json_error_line_skip\"}; processed_response_ids_p1.add(response_id_p1) # Still mark as processed\n","                except Exception as e_resp:\n","                    logger_p1.log(f\"ERROR P1: Processing response {response_id_p1} (line {line_num}) failed: {e_resp}\\n{traceback.format_exc()}\");\n","                    results_p1[response_id_p1]={\"status\":\"error_processing_response\", \"error_msg\": str(e_resp)}; processed_response_ids_p1.add(response_id_p1)\n","\n","    except FileNotFoundError:\n","        logger_p1.log(f\"CRITICAL P1: Input file {ORIGINAL_RESPONSES_JSONL} not found.\"); return False\n","    except Exception as e_main_loop:\n","        logger_p1.log(f\"CRITICAL P1: Main loop error: {e_main_loop}\\n{traceback.format_exc()}\"); return False\n","    finally:\n","        final_processed_count = len(processed_response_ids_p1)\n","        logger_p1.log(f\"INFO P1: Loop finished. New responses this specific run: {processed_in_this_run_p1}. Total unique responses in state: {final_processed_count}.\")\n","        logger_p1.log(f\"INFO P1: PDFs analyzed this run: {pdfs_analyzed_this_run_p1}. Summaries Att/Succ this run: {llm_sums_att_p1}/{llm_sums_succ_p1}.\")\n","        save_phase1_state(P1_STATE_FILE, processed_response_ids_p1, results_p1, logger_p1)\n","\n","        logger_p1.log(f\"INFO P1: Saving all {len(results_p1)} accumulated results to {P1_OUTPUT_FILE}...\")\n","        try:\n","            # The results_p1 dict is {\"response_id\": {qid_data}}\n","            # The output file expects {\"pdf_analysis_by_response\": results_p1}\n","            with open(P1_OUTPUT_FILE, 'w', encoding='utf-8') as f_final:\n","                json.dump({\"pdf_analysis_by_response\": results_p1}, f_final, indent=2, default=logger_p1._get_serializer())\n","            logger_p1.log(f\"INFO P1: Final Phase 1 results saved to {P1_OUTPUT_FILE}.\")\n","        except Exception as e_final_save:\n","            logger_p1.log(f\"ERROR P1: Final save to {P1_OUTPUT_FILE} failed: {e_final_save}\\n{traceback.format_exc()}\")\n","\n","        logger_p1.log(\"INFO P1: Cleaning up Phase 1 models...\")\n","        del embedding_model_p1, llm_tokenizer_p1, llm_model_p1, llm_summarization_pipe_p1\n","        if 'question_embeddings_all_p1' in locals(): del question_embeddings_all_p1\n","        del qid_to_embedding_map_p1\n","        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","        logger_p1.log(\"INFO P1: Phase 1 cleanup complete.\")\n","\n","    logger_p1.log(\"=\" * 50); logger_p1.log(\" INFO: Phase 1 Processing Finished \"); logger_p1.log(\"=\" * 50)\n","    return True\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 3 - Phase 1 logic defined (PDF Extraction & Per-Response Summarization).\")\n","print(\"      (This cell contains the main workload for initial PDF processing).\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QrG-QhALXiQZ"},"outputs":[],"source":["# @title Cell 4: Phase 2 - Collation of PDF Content (Original Stage 4a)\n","\n","# Logger for this phase (logger_p2) will be initialized in the Orchestration Cell (Cell 7)\n","# before calling run_phase2_collate_pdf_texts.\n","# Example for standalone testing (Orchestrator handles this for pipeline runs):\n","# if 'logger_p2' not in locals():\n","#     logger_p2 = FileLogger(P2_LOG_FILE) # P2_LOG_FILE from Cell 2\n","\n","def clean_text_p2(text: str) -> str: # Added type hint for clarity\n","    \"\"\"Basic text cleaning: consolidate whitespace and strip.\"\"\"\n","    if not text or not isinstance(text, str):\n","        return \"\"\n","    text = re.sub(r'\\s*\\n\\s*', '\\n', text).strip() # Consolidate multiple newlines then strip\n","    text = re.sub(r'[ \\t]+', ' ', text).strip()    # Consolidate multiple spaces/tabs then strip\n","    return text\n","\n","def run_phase2_collate_pdf_texts(logger_p2_instance: FileLogger): # Pass logger instance\n","    logger_p2 = logger_p2_instance # Use the passed logger\n","    logger_p2.log(\"=\" * 50); logger_p2.log(\" INFO P2: Starting Phase 2: PDF Text Collation for Thematic Analysis\"); logger_p2.log(\"=\" * 50)\n","\n","    # Configuration variables from Cell 2\n","    input_phase1_output_file = P1_OUTPUT_FILE\n","    output_collated_json_file = P2_OUTPUT_COLLATED_JSON\n","\n","    logger_p2.log(f\"INFO P2: Input Phase 1 data from: {input_phase1_output_file}\")\n","    logger_p2.log(f\"INFO P2: Output collated texts to: {output_collated_json_file}\")\n","\n","    if not os.path.exists(input_phase1_output_file):\n","        logger_p2.log(f\"ERROR P2: Input file from Phase 1 '{input_phase1_output_file}' does not exist. Phase 1 must run successfully first.\")\n","        return False\n","\n","    # Initialize data structure to hold texts per QID\n","    # Structure: {'QID1': [{'source_file': ..., 'response_id': ..., 'question_id': ..., 'type': 'pdf_summary'/'pdf_passages', 'text': ...}, ...], ...}\n","    qid_collected_content_p2 = defaultdict(list)\n","\n","    total_content_items_collated_p2 = 0\n","    summaries_collated_count_p2 = 0\n","    passages_collated_count_p2 = 0\n","    all_processed_response_ids_p2 = set() # To count unique responses that had content\n","\n","    try:\n","        with open(input_phase1_output_file, 'r', encoding='utf-8') as f:\n","            phase1_data_full = json.load(f)\n","\n","        # The main data from Phase 1 is under the \"pdf_analysis_by_response\" key\n","        pdf_analysis_by_response_from_p1 = phase1_data_full.get(\"pdf_analysis_by_response\")\n","\n","        if not pdf_analysis_by_response_from_p1 or not isinstance(pdf_analysis_by_response_from_p1, dict):\n","            logger_p2.log(f\"ERROR P2: 'pdf_analysis_by_response' key not found, empty, or not a dictionary in '{input_phase1_output_file}'. Cannot proceed.\")\n","            return False\n","\n","        logger_p2.log(f\"INFO P2: Processing {len(pdf_analysis_by_response_from_p1)} responses from Phase 1 output.\")\n","\n","        processed_count_this_run = 0\n","        for response_id, response_q_data_map in pdf_analysis_by_response_from_p1.items():\n","            all_processed_response_ids_p2.add(response_id) # Track unique responses\n","\n","            if not isinstance(response_q_data_map, dict):\n","                # This handles top-level status entries like {\"response_id_X\": {\"status\": \"pdf_missing_or_short\"}}\n","                if ENABLE_DETAILED_DEBUG_LOGGING:\n","                    logger_p2.log(f\"DEBUG P2: Skipping non-dict entry for response_id '{response_id}' (likely a status). Value: {response_q_data_map}\")\n","                continue\n","\n","            for qid, q_content in response_q_data_map.items():\n","                if not isinstance(q_content, dict):\n","                    # This handles cases where a QID might just have a status string instead of a content dict\n","                    if ENABLE_DETAILED_DEBUG_LOGGING:\n","                         logger_p2.log(f\"DEBUG P2: Skipping non-dict content for QID '{qid}' in response '{response_id}'. Value: {q_content}\")\n","                    continue\n","\n","                # Prepare item to store for Phase 2 output\n","                content_item_p2 = {\n","                    \"source_file\": os.path.basename(input_phase1_output_file),\n","                    \"response_id\": response_id,\n","                    \"question_id\": qid, # Store QID in the item for easier downstream use\n","                    \"type\": None,       # Will be 'pdf_summary' or 'pdf_passages'\n","                    \"text\": None\n","                }\n","                text_to_add_p2 = None\n","\n","                # 1. Prioritize non-blank summary from Phase 1 (Original Stage 4)\n","                phase1_summary = q_content.get(\"summary\") # Summary generated by Phase 1 LLM\n","\n","                if phase1_summary and isinstance(phase1_summary, str) and phase1_summary.strip() and not phase1_summary.lower().startswith(\"error:\"):\n","                    text_to_add_p2 = clean_text_p2(phase1_summary)\n","                    content_item_p2[\"type\"] = \"pdf_summary\" # Clearly mark it as a summary from PDF processing\n","                    summaries_collated_count_p2 +=1\n","                # 2. Fallback to extracted passages if summary is not valid/present\n","                #    The \"status\" field from Phase 1 can also be checked here for more context, e.g.\n","                #    q_content.get(\"status\") == \"passages_extracted\"\n","                elif q_content.get(\"extracted_passages\"):\n","                    phase1_passages = q_content.get(\"extracted_passages\")\n","                    if phase1_passages and isinstance(phase1_passages, list):\n","                        # Join all passages for this response-QID into a single text block\n","                        valid_passages_p2 = [p_text for p_text in phase1_passages if isinstance(p_text, str) and p_text.strip()]\n","                        if valid_passages_p2:\n","                            text_to_add_p2 = clean_text_p2(\"\\n\\n--- Passage Separator ---\\n\".join(valid_passages_p2))\n","                            content_item_p2[\"type\"] = \"pdf_passages\" # Clearly mark as passages from PDF\n","                            passages_collated_count_p2 +=1\n","\n","                if text_to_add_p2: # Only add if text was actually found (either summary or passages)\n","                    content_item_p2[\"text\"] = text_to_add_p2\n","                    qid_collected_content_p2[qid].append(content_item_p2)\n","                    total_content_items_collated_p2 += 1\n","\n","            processed_count_this_run +=1\n","            if processed_count_this_run % 200 == 0:\n","                logger_p2.log(f\"  INFO P2: Collated content from {processed_count_this_run} responses...\")\n","\n","    except FileNotFoundError: # Should be caught by the initial check, but good to have\n","        logger_p2.log(f\"ERROR P2: Input file '{input_phase1_output_file}' vanished during processing.\")\n","        return False\n","    except json.JSONDecodeError as e:\n","        logger_p2.log(f\"ERROR P2: Invalid JSON in Phase 1 output file '{input_phase1_output_file}': {e}\")\n","        logger_p2.log(f\"TRACEBACK P2:\\n{traceback.format_exc()}\")\n","        return False\n","    except Exception as e:\n","        logger_p2.log(f\"ERROR P2: Unexpected error during Phase 2 collation: {e}\")\n","        logger_p2.log(f\"TRACEBACK P2:\\n{traceback.format_exc()}\")\n","        return False\n","\n","    # --- Prepare final JSON structure for Phase 2 output ---\n","    final_output_data_p2 = {\n","        \"metadata\": {\n","            \"phase2_creation_timestamp\": time.strftime('%Y-%m-%d %H:%M:%S %Z', time.gmtime()),\n","            \"source_phase1_output_file\": os.path.basename(input_phase1_output_file),\n","            \"total_unique_qids_collated\": len(qid_collected_content_p2),\n","            \"total_text_items_collated\": total_content_items_collated_p2, # Each item is a summary or a block of passages for one resp-QID\n","            \"pdf_summaries_collated_count\": summaries_collated_count_p2,\n","            \"pdf_passages_collated_count\": passages_collated_count_p2,\n","            \"total_unique_responses_with_content_covered\": len(all_processed_response_ids_p2)\n","        },\n","        \"aggregated_pdf_content_by_qid\": dict(qid_collected_content_p2) # Convert defaultdict to dict\n","    }\n","    logger_p2.log(f\"\\n--- Phase 2 Collation Summary ---\")\n","    for key, value in final_output_data_p2[\"metadata\"].items():\n","        logger_p2.log(f\"  {key}: {value}\")\n","\n","    try:\n","        with open(output_collated_json_file, 'w', encoding='utf-8') as outfile:\n","            json.dump(final_output_data_p2, outfile, indent=2, default=logger_p2._get_serializer())\n","        logger_p2.log(f\"INFO P2: Successfully saved Phase 2 collated PDF content to '{output_collated_json_file}'.\")\n","    except Exception as e:\n","        logger_p2.log(f\"ERROR P2: Could not write Phase 2 output JSON to '{output_collated_json_file}': {e}\")\n","        logger_p2.log(f\"TRACEBACK P2:\\n{traceback.format_exc()}\")\n","        return False\n","\n","    logger_p2.log(\"=\" * 50); logger_p2.log(\" INFO P2: Phase 2 (PDF Text Collation) Finished Successfully\"); logger_p2.log(\"=\" * 50)\n","    return True\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 4 - Phase 2 logic defined (Collation of PDF Content).\")\n","print(\"      (Ensure function bodies are pasted from development notebook).\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xaHOew8YV2w"},"outputs":[],"source":["# @title Cell 5: Phase 3 - LLM Thematic Point Generation (Map-Reduce)\n","\n","# Logger for this phase (logger_p3) will be initialized in the Orchestration Cell (Cell 7)\n","# before calling run_phase3_llm_thematic_generation.\n","# Example for standalone testing:\n","# if 'logger_p3' not in locals():\n","#     logger_p3 = FileLogger(P3_LOG_FILE) # P3_LOG_FILE from Cell 2\n","\n","# --- Helper: Batch Individual Text Items (for Map step on list of texts from Phase 2) ---\n","def batch_text_items_p3(text_items_list: List[str],\n","                        tokenizer_for_batching, # Expects an initialized Hugging Face tokenizer\n","                        max_tokens_per_batch: int, # P3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK\n","                        logger_instance: FileLogger):\n","    batches = [] # List of lists of strings (each inner list is a batch of item texts)\n","    current_batch_item_texts = []\n","    current_batch_concatenated_text = \"\"\n","\n","    if ENABLE_DETAILED_DEBUG_LOGGING:\n","        logger_instance.log(f\"DEBUG P3_BATCHER: Batching {len(text_items_list)} items into max {max_tokens_per_batch} token batches.\")\n","\n","    for item_idx, item_text_original in enumerate(text_items_list):\n","        if not item_text_original or not item_text_original.strip():\n","            if ENABLE_DETAILED_DEBUG_LOGGING: logger_instance.log(f\"  DEBUG P3_BATCHER: Item {item_idx} is empty, skipping.\")\n","            continue\n","\n","        text_to_try_adding = item_text_original\n","        # Use a consistent separator that the LLM prompt for map step also expects\n","        if current_batch_item_texts:\n","            text_to_try_adding = \"\\n\\n--- Next Excerpt/Summary in Batch ---\\n\" + item_text_original\n","\n","        potential_full_batch_text = current_batch_concatenated_text + text_to_try_adding\n","\n","        try:\n","            num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))\n","        except Exception as e_tok_batch: # Catch potential errors during tokenization if text is very unusual\n","            logger_instance.log(f\"ERROR P3_BATCHER: Tokenization error in batcher for item {item_idx}. Text snippet: '{item_text_original[:100]}...'. Error: {e_tok_batch}\")\n","            continue # Skip this problematic item\n","\n","        if num_tokens <= max_tokens_per_batch:\n","            current_batch_item_texts.append(item_text_original)\n","            current_batch_concatenated_text = potential_full_batch_text\n","        else:\n","            if current_batch_item_texts: # Finalize current batch if it has items\n","                batches.append(current_batch_item_texts)\n","                if ENABLE_DETAILED_DEBUG_LOGGING: logger_instance.log(f\"  DEBUG P3_BATCHER: Added batch {len(batches)} with {len(current_batch_item_texts)} items, approx {len(tokenizer_for_batching.encode(current_batch_concatenated_text,add_special_tokens=False))} tokens.\")\n","\n","            # Start new batch with the current item_text_original\n","            try:\n","                single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))\n","            except Exception as e_tok_single:\n","                logger_instance.log(f\"ERROR P3_BATCHER: Tokenization error for single item {item_idx}. Text snippet: '{item_text_original[:100]}...'. Error: {e_tok_single}\")\n","                current_batch_item_texts = [] # Reset\n","                current_batch_concatenated_text = \"\"\n","                continue # Skip this problematic item\n","\n","            if single_item_tokens > max_tokens_per_batch:\n","                logger_instance.log(f\"  WARN P3_BATCHER: Item {item_idx} (tokens: {single_item_tokens}) > limit ({max_tokens_per_batch}). Will be its own oversized batch (potential truncation by LLM).\")\n","                batches.append([item_text_original])\n","                if ENABLE_DETAILED_DEBUG_LOGGING: logger_instance.log(f\"  DEBUG P3_BATCHER: Added oversized item as batch {len(batches)}.\")\n","                current_batch_item_texts = []\n","                current_batch_concatenated_text = \"\"\n","            else:\n","                current_batch_item_texts = [item_text_original]\n","                current_batch_concatenated_text = item_text_original # Just this item for token count start\n","\n","    if current_batch_item_texts: # Add any remaining batch\n","        batches.append(current_batch_item_texts)\n","        if ENABLE_DETAILED_DEBUG_LOGGING: logger_instance.log(f\"  DEBUG P3_BATCHER: Added final batch {len(batches)} with {len(current_batch_item_texts)} items, approx {len(tokenizer_for_batching.encode(current_batch_concatenated_text,add_special_tokens=False))} tokens.\")\n","\n","    logger_instance.log(f\"INFO P3_BATCHER: Created {len(batches)} batches of items from {len(text_items_list)} source items.\")\n","    return batches\n","\n","# --- Helper: LLM Call for Thematic Points (JSON Output, Qwen2-NoThink adapted) ---\n","def get_llm_json_themes_p3(text_input_for_llm: str, question_text_for_prompt: str,\n","                           num_themes_instruction_str: str, # e.g., \"up to 2\" or \"top 3\" from P3_..._STR config\n","                           llm_pipe: pipeline, max_new_tokens: int,\n","                           log_prefix_qid_step: str, map_reduce_step_type: str, # \"MAP_ON_BATCHED_ITEMS\", \"REDUCE_INTERMEDIATE_THEMES\", \"DIRECT_PROCESSING\"\n","                           logger_instance: FileLogger):\n","    json_format_example = (\"{\\n  \\\"themes\\\": [\\n    {\\\"theme_label\\\": \\\"Concise Theme Label 1\\\", \\\"explanation\\\": \\\"Brief 1-2 sentence explanation of theme 1.\\\"},\\n\"\n","                           \"    {\\\"theme_label\\\": \\\"Concise Theme Label 2\\\", \\\"explanation\\\": \\\"Brief 1-2 sentence explanation of theme 2.\\\"}\\n  ]\\n}\")\n","    system_prompt = \"You are an expert policy analyst tasked with identifying key themes from public consultation submissions.\"\n","    user_content = \"\"\n","\n","    if map_reduce_step_type == \"MAP_ON_BATCHED_ITEMS\":\n","        user_content = (\n","            f\"Carefully review the following BATCH of excerpts/summaries from PDF submissions. These texts are all responses to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on the information within THIS BATCH, identify the most prominent and distinct points, arguments, or themes, {num_themes_instruction_str}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object. The JSON object must contain a key 'themes' which is a list of theme objects. Each theme object must have two string keys: 'theme_label' (a concise title for the theme) and 'explanation' (a brief 1-2 sentence summary of the theme as derived from this batch).\\n\"\n","            f\"Example JSON format (if requesting {num_themes_instruction_str}):\\n{json_format_example}\\n\\n\"\n","            f\"Batch of Excerpts/Summaries:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output:\"\n","        )\n","    elif map_reduce_step_type == \"REDUCE_INTERMEDIATE_THEMES\":\n","        user_content = (\n","            f\"The following are sets of preliminary themes/points, each with a label and explanation. These were individually extracted from different batches of responses to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Your task is to synthesize these preliminary themes into the overall most significant and distinct common themes, {num_themes_instruction_str}, that represent the entire collection of original responses.\\n\"\n","            f\"Consolidate similar preliminary themes. Ensure the final synthesized themes are distinct and well-supported by the range of preliminary themes provided.\\n\"\n","            f\"Respond *only* with a single, valid JSON object with a 'themes' list, each theme having 'theme_label' and 'explanation'.\\n\"\n","            f\"Example JSON format (if requesting {num_themes_instruction_str}):\\n{json_format_example}\\n\\n\"\n","            f\"Preliminary Themes/Points Provided:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output (Synthesized {num_themes_instruction_str} Themes):\"\n","        )\n","    elif map_reduce_step_type == \"DIRECT_PROCESSING\": # If only one batch, treat as direct request\n","        user_content = (\n","            f\"Carefully review all the following text (which consists of combined PDF summaries OR a single batch of excerpts/passages) from PDF submissions responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on the provided text, identify the most common and distinct points, arguments, or themes, {num_themes_instruction_str}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object with a 'themes' list, each theme having 'theme_label' and 'explanation'.\\n\"\n","            f\"Example JSON format (if requesting {num_themes_instruction_str}):\\n{json_format_example}\\n\\n\"\n","            f\"Provided Text:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output:\"\n","        )\n","    else:\n","        logger_instance.log(f\"ERROR P3_LLM: Unknown map_reduce_step_type: {map_reduce_step_type} for {log_prefix_qid_step}\");\n","        return \"LLM_ERROR: Unknown map_reduce_step_type\"\n","\n","    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_content}]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n","\n","        terminators = [llm_pipe.tokenizer.eos_token_id]\n","        if hasattr(llm_pipe.tokenizer, 'convert_tokens_to_ids'): # Qwen specific\n","            im_end_token_id = llm_pipe.tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n","            if im_end_token_id != llm_pipe.tokenizer.unk_token_id : terminators.append(im_end_token_id)\n","        valid_terminators = [t for t in terminators if t is not None]\n","\n","        generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": True,\n","                           \"temperature\": 0.7, \"top_p\": 0.8, \"top_k\": 20, # Qwen non-thinking mode recommendations\n","                           \"eos_token_id\": valid_terminators,\n","                           \"pad_token_id\": llm_pipe.tokenizer.eos_token_id} # Qwen often uses eos as pad\n","\n","        model_config = llm_pipe.model.config\n","        model_max_len = getattr(model_config, 'max_position_embeddings', None) or \\\n","                        getattr(model_config, 'max_sequence_length', None) or \\\n","                        getattr(model_config, 'sliding_window', None) or \\\n","                        getattr(model_config, 'window_size', None) or \\\n","                        getattr(llm_pipe.tokenizer, 'model_max_length', None) or 32768 # Default for many Qwen2 models\n","\n","        if ENABLE_DETAILED_DEBUG_LOGGING:\n","            logger_instance.log(f\"DEBUG P3_LLM: {map_reduce_step_type} for {log_prefix_qid_step}. Effective model_max_len for check: {model_max_len}.\")\n","\n","        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True) # Full prompt including special tokens for generation\n","        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95: # Check against model's context\n","             logger_instance.log(f\"WARN P3_LLM: {log_prefix_qid_step} - Step {map_reduce_step_type} - Prompt tokens ({len(prompt_tokens)}) + max_new_tokens ({max_new_tokens}) might exceed model's context window ({model_max_len}). Input text length (chars): {len(text_input_for_llm)}\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger_instance.log(f\"ERROR P3_LLM: {map_reduce_step_type} LLM returned unexpected/empty output for {log_prefix_qid_step}.\")\n","            return \"LLM_ERROR: No output generated.\"\n","        generated_text_full = outputs[0]['generated_text']\n","\n","        # Output stripping for Qwen when add_generation_prompt=True and enable_thinking=False\n","        assistant_response_start_tag = \"<|im_start|>assistant\\n\"\n","        actual_response_text = generated_text_full\n","        last_assistant_tag_idx = generated_text_full.rfind(assistant_response_start_tag)\n","\n","        if last_assistant_tag_idx != -1:\n","            actual_response_text = generated_text_full[last_assistant_tag_idx + len(assistant_response_start_tag):].strip()\n","        elif prompt_formatted in generated_text_full and len(generated_text_full) > len(prompt_formatted): # Fallback if tag is missing\n","            actual_response_text = generated_text_full[len(prompt_formatted):].strip()\n","        # else: actual_response_text remains generated_text_full, which might be just the JSON if LLM is very good.\n","\n","        if actual_response_text.endswith(\"<|im_end|>\"):\n","            actual_response_text = actual_response_text[:-len(\"<|im_end|>\")].strip()\n","\n","        # Extract JSON block more robustly\n","        llm_json_part = actual_response_text\n","        # Try to find ```json ... ``` block first\n","        json_markdown_match = re.search(r\"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\", actual_response_text, re.DOTALL)\n","        if json_markdown_match:\n","            llm_json_part = json_markdown_match.group(1).strip()\n","        else:\n","            # If no markdown, try to find the first '{' and last '}' that form a valid JSON object\n","            # This is a common pattern if the LLM just outputs the JSON directly.\n","            first_brace_idx = actual_response_text.find('{')\n","            last_brace_idx = actual_response_text.rfind('}')\n","            if first_brace_idx != -1 and last_brace_idx != -1 and last_brace_idx > first_brace_idx:\n","                potential_json = actual_response_text[first_brace_idx : last_brace_idx+1]\n","                try:\n","                    json.loads(potential_json) # Test if it's valid JSON\n","                    llm_json_part = potential_json\n","                except json.JSONDecodeError:\n","                    logger_instance.log(f\"WARN P3_LLM: Found braces but not valid JSON for {log_prefix_qid_step}. Full output used.\")\n","                    # llm_json_part remains actual_response_text\n","            elif first_brace_idx != -1 : # if it starts with { but no clear end, take it all and hope for the best\n","                 llm_json_part = actual_response_text[first_brace_idx:].strip()\n","                 logger_instance.log(f\"WARN P3_LLM: Found starting brace but no clear end for JSON in {log_prefix_qid_step}. Using partial.\")\n","            else:\n","                logger_instance.log(f\"WARN P3_LLM: No clear JSON block (```json or {{...}}) found for {log_prefix_qid_step}. Raw output might be non-JSON or incomplete.\")\n","\n","        if ENABLE_DETAILED_DEBUG_LOGGING:\n","            logger_instance.log(f\"    DEBUG P3_LLM RAW RESPONSE for {log_prefix_qid_step} ({map_reduce_step_type}):\\n{generated_text_full}\")\n","        logger_instance.log(f\"    INFO P3_LLM JSON-candidate PART for {log_prefix_qid_step} ({map_reduce_step_type}):\\n{llm_json_part}\")\n","        return llm_json_part\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P3_LLM: {map_reduce_step_type} LLM exception for {log_prefix_qid_step}: {e}\")\n","        logger_instance.log(f\"TRACEBACK P3_LLM:\\n{traceback.format_exc()}\")\n","        return f\"LLM_ERROR: Exception during generation - {str(e)}\"\n","\n","# --- Helper: Parse LLM JSON Thematic Output ---\n","def parse_llm_json_themes_p3(llm_json_output_text: str, max_themes_to_extract: int, logger_instance: FileLogger) -> List[Dict[str, str]]:\n","    if ENABLE_DETAILED_DEBUG_LOGGING:\n","        logger_instance.log(f\"DEBUG P3_JSON_PARSER: Attempting to parse up to {max_themes_to_extract} themes from: '{llm_json_output_text[:150]}...'\")\n","\n","    parsed_themes = []\n","    error_prefix = \"LLM Error/Parsing:\" # Common prefix for error theme_labels\n","\n","    if not llm_json_output_text or llm_json_output_text.startswith(\"LLM_ERROR:\"):\n","        logger_instance.log(f\"DEBUG P3_JSON_PARSER: LLM returned an error string directly: {llm_json_output_text}\")\n","        return [{\"theme_label\": f\"{error_prefix} LLM Generation Error\", \"explanation\": llm_json_output_text}]\n","    try:\n","        data = json.loads(llm_json_output_text) # Expects cleaned JSON string\n","\n","        if isinstance(data, dict) and \"themes\" in data and isinstance(data[\"themes\"], list):\n","            for theme_obj in data[\"themes\"]:\n","                if len(parsed_themes) >= max_themes_to_extract:\n","                    break\n","                if isinstance(theme_obj, dict) and \\\n","                   \"theme_label\" in theme_obj and isinstance(theme_obj[\"theme_label\"], str) and \\\n","                   \"explanation\" in theme_obj and isinstance(theme_obj[\"explanation\"], str):\n","                    parsed_themes.append({\n","                        \"theme_label\": theme_obj[\"theme_label\"].strip(),\n","                        \"explanation\": theme_obj[\"explanation\"].strip()\n","                    })\n","                else:\n","                    logger_instance.log(f\"WARN P3_JSON_PARSER: Theme object malformed or missing keys: {str(theme_obj)[:100]}\")\n","\n","            if parsed_themes:\n","                logger_instance.log(f\"INFO P3_JSON_PARSER: Successfully parsed {len(parsed_themes)} themes.\")\n","                return parsed_themes\n","            else:\n","                 logger_instance.log(\"WARN P3_JSON_PARSER: Valid JSON received, but 'themes' list was empty or items were malformed, yielding no valid themes.\")\n","                 return [{\"theme_label\": f\"{error_prefix} Valid JSON, but no valid themes extracted\", \"explanation\": llm_json_output_text[:200]+\"...\"}]\n","        else: # Valid JSON but not the expected structure\n","            logger_instance.log(f\"WARN P3_JSON_PARSER: LLM output was valid JSON but not in the expected structure {{'themes': [...]}}. Structure received: {list(data.keys()) if isinstance(data, dict) else type(data)}\")\n","            return [{\"theme_label\": f\"{error_prefix} Valid JSON, unexpected structure\", \"explanation\": llm_json_output_text[:200]+\"...\"}]\n","    except json.JSONDecodeError as e:\n","        logger_instance.log(f\"ERROR P3_JSON_PARSER: LLM output was not valid JSON. Error: {e}. Raw output snippet:\\n{llm_json_output_text[:500]}\")\n","        return [{\"theme_label\": f\"{error_prefix} Invalid JSON Output\", \"explanation\": f\"JSONDecodeError: {e}. Raw snippet: {llm_json_output_text[:200]}...\"}]\n","    except Exception as e_gen: # Catch any other unexpected error during parsing\n","        logger_instance.log(f\"ERROR P3_JSON_PARSER: Unexpected error during parsing: {e_gen}\\n{traceback.format_exc()}\")\n","        return [{\"theme_label\": f\"{error_prefix} Unexpected Parsing Error\", \"explanation\": f\"Error: {e_gen}. Raw snippet: {llm_json_output_text[:200]}...\"}]\n","\n","# --- Main Function for Phase 3 ---\n","def run_phase3_llm_thematic_generation(logger_p3_instance: FileLogger):\n","    logger_p3 = logger_p3_instance\n","    logger_p3.log(\"=\" * 50); logger_p3.log(\" INFO P3: Starting Phase 3: LLM Thematic Point Generation (Qwen2, Map-Reduce, JSON)\"); logger_p3.log(\"=\" * 50)\n","\n","    # --- Load LLM ---\n","    llm_thematic_pipe_p3 = None\n","    tokenizer_p3_for_utils = None # For batcher/chunker\n","    try:\n","        logger_p3.log(f\"INFO P3: Loading LLM: {P3_LLM_ID_FOR_THEMES} (Quant: {P3_USE_QUANTIZATION})\")\n","        tokenizer_p3_for_utils = AutoTokenizer.from_pretrained(P3_LLM_ID_FOR_THEMES, trust_remote_code=True)\n","        if getattr(tokenizer_p3_for_utils, 'pad_token', None) is None and tokenizer_p3_for_utils.eos_token is not None:\n","            tokenizer_p3_for_utils.pad_token = tokenizer_p3_for_utils.eos_token\n","\n","        bnb_config_p3 = None; current_p3_quant_active = P3_USE_QUANTIZATION\n","        if P3_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16 # Qwen2 often uses bfloat16\n","                bnb_config_p3 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n","                logger_p3.log(f\"INFO P3: BNB config created for {P3_LLM_ID_FOR_THEMES}, compute_dtype: {compute_dtype}\")\n","            except ImportError: current_p3_quant_active=False; logger_p3.log(\"WARN P3: bitsandbytes not imported. Disabling quantization.\")\n","            except ColabNotSupportedError: current_p3_quant_active=False; logger_p3.log(\"WARN P3: ColabNotSupportedError for bitsandbytes. Disabling quantization.\")\n","            except Exception as qe: logger_p3.log(f\"WARN P3: BNB config failed: {qe}. Disabling quant.\"); current_p3_quant_active=False\n","        elif P3_USE_QUANTIZATION: current_p3_quant_active=False; logger_p3.log(\"WARN P3: Quantization requested, but No CUDA. Disabling quantization.\")\n","\n","        model_p3 = AutoModelForCausalLM.from_pretrained( P3_LLM_ID_FOR_THEMES,\n","            quantization_config=bnb_config_p3 if current_p3_quant_active else None,\n","            torch_dtype=torch.bfloat16, # Qwen2 models often use bfloat16 as default if supported\n","            device_map=\"auto\", trust_remote_code=True)\n","        llm_thematic_pipe_p3 = pipeline(\"text-generation\", model=model_p3, tokenizer=tokenizer_p3_for_utils)\n","        logger_p3.log(f\"INFO P3: LLM Pipeline for {P3_LLM_ID_FOR_THEMES} Loaded. Quantization active: {current_p3_quant_active}\")\n","    except Exception as e:\n","        logger_p3.log(f\"CRITICAL P3: LLM loading failed: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    # --- Load Collated Texts (Phase 2 output) & Question Definitions ---\n","    collated_texts_by_qid_p3 = {}; qid_to_actual_question_text_p3 = {}\n","    try:\n","        with open(P3_INPUT_COLLATED_JSON, 'r', encoding='utf-8') as f: collated_data = json.load(f)\n","        collated_texts_by_qid_p3 = collated_data.get(\"aggregated_pdf_content_by_qid\", {})\n","        if not collated_texts_by_qid_p3 : raise ValueError(f\"Phase 2 output {P3_INPUT_COLLATED_JSON} 'aggregated_pdf_content_by_qid' is empty or missing.\")\n","\n","        with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f: q_defs = json.load(f)\n","        for qid_key,data_val in q_defs.get(\"question_data\", {}).items():\n","            q_text = data_val.get(\"question_text\")\n","            if q_text: qid_to_actual_question_text_p3[qid_key] = q_text\n","        if not qid_to_actual_question_text_p3: raise ValueError(\"No question texts loaded from QUESTION_DEFINITIONS_FILE.\")\n","        logger_p3.log(f\"INFO P3: Loaded collated texts for {len(collated_texts_by_qid_p3)} QIDs and {len(qid_to_actual_question_text_p3)} Q-texts.\")\n","    except Exception as e:\n","        logger_p3.log(f\"FATAL P3: Data loading failed: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    overall_thematic_results_p3 = {};\n","    if os.path.exists(P3_OUTPUT_LLM_THEMES_JSON):\n","        try:\n","            with open(P3_OUTPUT_LLM_THEMES_JSON,'r', encoding='utf-8') as f_exist:\n","                overall_thematic_results_p3 = json.load(f_exist)\n","            logger_p3.log(f\"INFO P3: Loaded {len(overall_thematic_results_p3)} existing results from {P3_OUTPUT_LLM_THEMES_JSON}\")\n","        except Exception as e_load: # Catch more general errors during load\n","            logger_p3.log(f\"WARN P3: Could not load or parse existing output {P3_OUTPUT_LLM_THEMES_JSON}: {e_load}. Starting fresh for this run if QIDs are re-processed.\")\n","            overall_thematic_results_p3 = {}\n","\n","\n","    qids_to_run_this_session = P3_QIDS_TO_PROCESS_THEMATICALLY if P3_QIDS_TO_PROCESS_THEMATICALLY else sorted(collated_texts_by_qid_p3.keys())\n","    if not qids_to_run_this_session:\n","        logger_p3.log(\"INFO P3: No QIDs specified or found to process in this session.\"); return True\n","\n","\n","    for qid_idx, qid in enumerate(qids_to_run_this_session):\n","        logger_p3.log(f\"\\n--- P3 Processing QID {qid_idx + 1}/{len(qids_to_run_this_session)}: {qid} ---\")\n","\n","        # Skip if successfully processed previously AND P3_QIDS_TO_PROCESS_THEMATICALLY is None (meaning process all / resume)\n","        if P3_QIDS_TO_PROCESS_THEMATICALLY is None and \\\n","           qid in overall_thematic_results_p3 and \\\n","           isinstance(overall_thematic_results_p3[qid], dict) and \\\n","           overall_thematic_results_p3[qid].get(\"status\", \"\").startswith(\"success\"):\n","            logger_p3.log(f\"INFO P3: QID {qid} already successfully processed and P3_QIDS_TO_PROCESS_THEMATICALLY is None. Skipping.\");\n","            continue\n","\n","        question_actual_text = qid_to_actual_question_text_p3.get(qid, f\"Question {qid} (Text not found)\")\n","        # Get list of text strings for the current QID\n","        source_text_items_for_qid = [item.get(\"text\") for item in collated_texts_by_qid_p3.get(qid, []) if item.get(\"text\")]\n","\n","        if not source_text_items_for_qid:\n","            logger_p3.log(f\"WARN P3: No source texts found for QID {qid} in collated data. Skipping LLM analysis.\");\n","            overall_thematic_results_p3[qid] = {\"question_text\": question_actual_text, \"status\":\"no_source_texts_in_p2_output\", \"parsed_top_themes\":[]};\n","            # Save intermediate result even if skipped\n","            try:\n","                with open(P3_OUTPUT_LLM_THEMES_JSON, 'w', encoding='utf-8') as f_out_skip:\n","                    json.dump(overall_thematic_results_p3, f_out_skip, indent=2, default=logger_p3._get_serializer())\n","            except Exception as es_skip: logger_p3.log(f\"ERROR P3: Save failed after skipping QID {qid}: {es_skip}\")\n","            continue\n","\n","        logger_p3.log(f\"INFO P3: QID {qid} has {len(source_text_items_for_qid)} text items for thematic analysis.\")\n","\n","        # Batch the individual text items (summaries or passages from Phase 2)\n","        map_batches_p3 = batch_text_items_p3(\n","            source_text_items_for_qid,\n","            llm_thematic_pipe_p3.tokenizer,\n","            P3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK,\n","            logger_p3\n","        )\n","\n","        final_llm_output_str = \"N/A_DEFAULT\"; raw_map_outputs = []; status_p3=\"initialized_for_qid\"\n","\n","        if not map_batches_p3:\n","            logger_p3.log(f\"WARN P3: No map batches created for QID {qid} (all items might have been empty after filtering).\");\n","            status_p3=\"no_map_batches_created\"\n","            final_llm_output_str = \"Error: No map batches could be created.\"\n","        elif len(map_batches_p3) == 1:\n","            logger_p3.log(f\"INFO P3: QID {qid} - Single batch formed with {len(map_batches_p3[0])} items. Direct LLM call.\")\n","            direct_input_text = \"\\n\\n--- Next Excerpt/Summary in Batch ---\\n\".join(map_batches_p3[0])\n","            final_llm_output_str = get_llm_json_themes_p3(\n","                direct_input_text, question_actual_text, P3_NUM_THEMES_REDUCE_STR, # Use REDUCE num string for direct\n","                llm_thematic_pipe_p3, P3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                f\"{qid}_direct_batch\", \"DIRECT_PROCESSING\", logger_p3\n","            )\n","            raw_map_outputs.append(final_llm_output_str) # Store this single output as if it was a map output\n","        else: # Map-Reduce for multiple batches\n","            logger_p3.log(f\"INFO P3: QID {qid} - Applying Map-Reduce with {len(map_batches_p3)} map steps.\")\n","            intermediate_llm_json_outputs_p3 = [] # Store raw JSON strings from map steps\n","            for i, batch_of_item_strings in enumerate(map_batches_p3):\n","                batch_concat_text = \"\\n\\n--- Next Excerpt/Summary in Batch ---\\n\".join(batch_of_item_strings)\n","                map_out_json_str = get_llm_json_themes_p3(\n","                    batch_concat_text, question_actual_text, P3_NUM_THEMES_MAP_STR, # Use MAP num string\n","                    llm_thematic_pipe_p3, P3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                    f\"{qid}_map_batch{i+1}\", \"MAP_ON_BATCHED_ITEMS\", logger_p3\n","                )\n","                raw_map_outputs.append(map_out_json_str)\n","                if not map_out_json_str.startswith(\"LLM_ERROR:\"):\n","                    intermediate_llm_json_outputs_p3.append(map_out_json_str) # Add raw JSON string\n","                time.sleep(0.2) # Small pause\n","\n","            if intermediate_llm_json_outputs_p3:\n","                # For reduce step, parse intermediate JSONs and format their content as text\n","                parsed_intermediate_themes_for_reduce_input = []\n","                for idx_map, raw_json_map_out_str in enumerate(intermediate_llm_json_outputs_p3):\n","                    # Use MAP int for parsing map step outputs\n","                    parsed_map_batch_themes = parse_llm_json_themes_p3(raw_json_map_out_str, P3_NUM_THEMES_MAP_INT, logger_p3)\n","                    theme_block_for_reduce = [f\"Preliminary Themes from Batch {idx_map+1}:\"]\n","                    for pt_map_idx, pt_map in enumerate(parsed_map_batch_themes):\n","                        if not pt_map.get(\"theme_label\",\"\").lower().startswith((\"llm error\", \"parsing/llm error\", \"unparsed\", \"valid json\")):\n","                            theme_block_for_reduce.append(\n","                                f\"  Point {pt_map_idx+1}:\\n    Theme Label: {pt_map.get('theme_label')}\\n    Explanation: {pt_map.get('explanation')}\"\n","                            )\n","                        else:\n","                            logger_p3.log(f\"  WARN P3: Map batch {idx_map+1} for QID {qid} had a parsing/LLM error for an intermediate theme: {pt_map.get('explanation')}\")\n","                            theme_block_for_reduce.append(f\"  Point {pt_map_idx+1}: Error or unparsed - {pt_map.get('explanation')}\")\n","                    if len(theme_block_for_reduce) > 1: # Only add if there were actual themes, not just the header\n","                        parsed_intermediate_themes_for_reduce_input.append(\"\\n\".join(theme_block_for_reduce))\n","\n","                if parsed_intermediate_themes_for_reduce_input:\n","                    combined_intermediate_text_for_reduce = \"\\n\\n------------------------------\\n\\n\".join(parsed_intermediate_themes_for_reduce_input)\n","                    logger_p3.log(f\"INFO P3: QID {qid} - REDUCE Step input (Chars: {len(combined_intermediate_text_for_reduce)}). Snippet:\\n{combined_intermediate_text_for_reduce[:1000]}...\")\n","                    final_llm_output_str = get_llm_json_themes_p3(\n","                        combined_intermediate_text_for_reduce, question_actual_text, P3_NUM_THEMES_REDUCE_STR, # Use REDUCE num string\n","                        llm_thematic_pipe_p3, P3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                        f\"{qid}_reduce\", \"REDUCE_INTERMEDIATE_THEMES\", logger_p3)\n","                else:\n","                    status_p3 = \"error_map_parsing_no_valid_themes_for_reduce\"\n","                    final_llm_output_str = \"Error: Could not parse any valid intermediate themes from map steps to feed into reduce.\"\n","                    logger_p3.log(f\"ERROR P3: QID {qid} - {status_p3}\")\n","            else:\n","                status_p3 = \"error_map_step_all_failed_or_empty_outputs\"\n","                final_llm_output_str = \"Error: All map steps failed or produced no valid content.\"\n","                logger_p3.log(f\"ERROR P3: QID {qid} - {status_p3}\")\n","\n","        # Final parsing of the output from either direct call or reduce step\n","        parsed_themes_list_p3 = parse_llm_json_themes_p3(final_llm_output_str, P3_NUM_THEMES_REDUCE_INT, logger_p3) # Use REDUCE int for final\n","\n","        if status_p3 == \"initialized_for_qid\": # If no error status was set before\n","            if final_llm_output_str.startswith(\"LLM_ERROR:\"):\n","                status_p3 = \"error_final_llm_generation\"\n","            elif not parsed_themes_list_p3 or \\\n","                 (len(parsed_themes_list_p3)==1 and parsed_themes_list_p3[0].get(\"theme_label\",\"\").lower().startswith((\"llm error\", \"parsing/llm error\", \"unparsed\", \"valid json\"))):\n","                status_p3 = \"error_final_llm_output_parsing\"\n","            else:\n","                status_p3 = \"success_thematic_analysis\"\n","\n","        overall_thematic_results_p3[qid] = {\n","            \"question_text\": question_actual_text,\n","            \"status\": status_p3,\n","            \"total_source_text_items_for_qid\": len(source_text_items_for_qid),\n","            \"map_reduce_applied\": len(map_batches_p3) > 1 if map_batches_p3 else False,\n","            \"num_map_batches\": len(map_batches_p3) if map_batches_p3 else 0,\n","            \"parsed_top_themes\": parsed_themes_list_p3,\n","            \"llm_raw_output_final_synthesis\": final_llm_output_str,\n","            \"llm_raw_output_map_batches\": raw_map_outputs # List of raw JSON strings from map steps\n","        }\n","        try: # Save after each QID\n","            with open(P3_OUTPUT_LLM_THEMES_JSON, 'w', encoding='utf-8') as f_out:\n","                json.dump(overall_thematic_results_p3, f_out, indent=2, default=logger_p3._get_serializer())\n","            logger_p3.log(f\"INFO P3: Saved/Updated results to '{P3_OUTPUT_LLM_THEMES_JSON}' after QID {qid}.\")\n","        except Exception as es:\n","            logger_p3.log(f\"ERROR P3: Save failed for QID {qid} results: {es}\\n{traceback.format_exc()}\")\n","\n","        gc.collect()\n","        if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","    logger_p3.log(\"=\" * 50); logger_p3.log(\" INFO P3: Phase 3 (LLM Thematic Point Generation) Finished \"); logger_p3.log(\"=\" * 50)\n","    # Clean up main LLM components for Phase 3\n","    del llm_thematic_pipe_p3, model_p3, tokenizer_p3_for_utils\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","    logger_p3.log(\"INFO P3: Phase 3 LLM resources cleaned up.\")\n","    return True\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 5 - Phase 3 logic defined (LLM Thematic Point Generation with Qwen2 & JSON).\")\n","print(\"      (Ensure function bodies are complete and use P3_... configs from Cell 2).\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUMMuIRaZFDU"},"outputs":[],"source":["# @title Cell 6: Phase 4 - Theme Validation, Quantification & Quotation Extraction\n","\n","# Logger for this phase (logger_p4) will be initialized in the Orchestration Cell (Cell 7)\n","# before calling run_phase4_theme_quantification_and_quoting.\n","# Example for standalone testing:\n","# if 'logger_p4' not in locals():\n","#     logger_p4 = FileLogger(P4_LOG_FILE) # P4_LOG_FILE from Cell 2\n","#     ENABLE_DETAILED_DEBUG_LOGGING = True # For testing this cell in isolation\n","\n","# --- Helper Function for Phase 4: Load Human-Defined Keyphrases ---\n","def load_human_defined_keyphrases_p4(keyphrase_file_path: str, logger_instance: FileLogger) -> Dict[str, Dict[str, List[str]]]:\n","    \"\"\"\n","    Loads a JSON file mapping QID -> Human_Theme_Label -> List of Keyphrases.\n","    \"\"\"\n","    if not os.path.exists(keyphrase_file_path):\n","        logger_instance.log(f\"WARN P4_Keyphrases: Keyphrase file not found at: {keyphrase_file_path}. Will be treated as empty. Orchestrator may create an empty one if it doesn't exist.\")\n","        return {}\n","    try:\n","        with open(keyphrase_file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","        if not isinstance(data, dict):\n","            logger_instance.log(f\"ERROR P4_Keyphrases: Keyphrase file {keyphrase_file_path} does not contain a valid JSON dictionary at the top level. Returning empty.\")\n","            return {}\n","        if data: # Log only if not empty, to confirm successful load\n","            logger_instance.log(f\"INFO P4_Keyphrases: Successfully loaded {len(data)} QIDs with keyphrases from {keyphrase_file_path}.\")\n","            if ENABLE_DETAILED_DEBUG_LOGGING and len(data) > 0:\n","                 first_qid = list(data.keys())[0]\n","                 logger_instance.log(f\"  DEBUG P4_Keyphrases: Example for QID '{first_qid}': {list(data[first_qid].keys()) if isinstance(data[first_qid], dict) else 'No themes'}\")\n","        else:\n","            logger_instance.log(f\"INFO P4_Keyphrases: Keyphrase file {keyphrase_file_path} is empty. All themes will rely on auto-derivation if applicable.\")\n","        return data\n","    except json.JSONDecodeError as e:\n","        logger_instance.log(f\"ERROR P4_Keyphrases: Failed to decode JSON from keyphrase file {keyphrase_file_path}: {e}\")\n","        return {}\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P4_Keyphrases: Failed to load keyphrase file {keyphrase_file_path}: {e}\\n{traceback.format_exc()}\")\n","        return {}\n","\n","# --- Helper Function for Phase 4: Auto-Derive Keyphrases from LLM Theme ---\n","def auto_derive_keyphrases_from_llm_theme(theme_label: str,\n","                                        theme_explanation: str,\n","                                        num_keyphrases=3, # Request fewer, more targeted auto-keyphrases\n","                                        logger_instance=None) -> List[str]:\n","    if logger_instance and ENABLE_DETAILED_DEBUG_LOGGING:\n","        logger_instance.log(f\"DEBUG P4_AutoKeyphrase: Deriving up to {num_keyphrases} for Label='{theme_label}', Expl='{theme_explanation[:50]}...'\")\n","\n","    # Combine label and explanation, prioritize label words\n","    text_for_kps = f\"{theme_label.lower()} {theme_label.lower()} {theme_explanation.lower()}\".strip() # Weight label more\n","\n","    text_for_kps = re.sub(r'[^\\w\\s-]', '', text_for_kps) # Allow hyphens in words\n","    text_for_kps = re.sub(r'\\d+', '', text_for_kps)\n","    words = [word for word in text_for_kps.split() if word.strip()] # Split and remove empty strings\n","\n","    derived_keyphrases = []\n","    if not words:\n","        if logger_instance: logger_instance.log(f\"WARN P4_AutoKeyphrase: No words to process for '{theme_label}'.\")\n","        return []\n","\n","    try:\n","        from nltk.corpus import stopwords\n","        stop_words = set(stopwords.words('english'))\n","        # More specific stopwords related to LLM output style and generic terms\n","        custom_generic_words = {\n","            \"theme\", \"label\", \"explanation\", \"point\", \"submission\", \"response\", \"excerpt\",\n","            \"several\", \"multiple\", \"various\", \"highlight\", \"discuss\", \"mention\", \"suggest\",\n","            \"argue\", \"concern\", \"need\", \"focus\", \"aspect\", \"also\", \"however\", \"therefore\",\n","            \"this\", \"that\", \"they\", \"are\", \"is\", \"was\", \"were\", \"and\", \"or\", \"the\", \"for\", \"with\", \"from\", \"based\",\n","            \"example\", \"approximately\", \"around\", \"often\", \"particularly\", \"especially\",\n","            \"distinct\", \"common\", \"prominent\", \"key\", \"main\", \"overall\", \"significant\", \"important\",\n","            \"advocate\", \"suggests\", \"emphasize\", \"indicate\", \"propose\", \"require\", \"allow\", \"view\", \"viewed\",\n","            \"across\", \"within\", \"among\", \"rather\", \"including\", \"provide\", \"topic\", \"issue\", \"argument\",\n","            \"text\", \"content\", \"data\", \"information\", \"approach\", \"framework\", \"context\", \"concept\",\n","            \"summary\", \"summaries\", \"passages\", \"pdf\", \"llm\"\n","        }\n","        stop_words.update(custom_generic_words)\n","\n","        # Consider 2-word phrases (bigrams) if possible, or just single important words\n","        # For simplicity, focusing on single important words for auto-derivation.\n","        # N-gram approach could be:\n","        # from nltk import ngrams\n","        # bigrams = [\" \".join(grams) for grams in ngrams(words, 2) if not any(s_word in grams for s_word in stop_words)]\n","        # word_counts = Counter(bigrams)\n","        # For now, stick to unigrams.\n","\n","        filtered_words = [word for word in words if word and word not in stop_words and len(word) > 3] # Min word length 4\n","    except ImportError:\n","        if logger_instance: logger_instance.log(\"WARN P4_AutoKeyphrase: NLTK stopwords not available. Keyphrase quality may be reduced.\")\n","        filtered_words = [word for word in words if len(word) > 3]\n","\n","    if not filtered_words: # Fallback if filtering removes everything\n","        if logger_instance: logger_instance.log(f\"WARN P4_AutoKeyphrase: No significant words after stopword removal for '{theme_label}'. Using top words from label.\")\n","        # Try to get from label only if explanation yielded nothing useful\n","        label_only_words = [w for w in theme_label.lower().split() if w and w not in (stop_words if 'stop_words' in locals() else set()) and len(w)>3]\n","        if label_only_words:\n","            word_counts = Counter(label_only_words)\n","            derived_keyphrases = [word for word, count in word_counts.most_common(num_keyphrases)]\n","        else: # Absolute fallback\n","             word_counts = Counter(words) # From original combined text\n","             derived_keyphrases = [word for word, count in word_counts.most_common(num_keyphrases * 2) if len(word) > 3][:num_keyphrases]\n","    else:\n","        word_counts = Counter(filtered_words)\n","        derived_keyphrases = [word for word, count in word_counts.most_common(num_keyphrases)]\n","\n","    # Further refine: if keyphrases are too similar (e.g., \"law\" and \"laws\"), keep one. Simple check here.\n","    final_kps = []\n","    seen_stems = set()\n","    try:\n","        from nltk.stem.porter import PorterStemmer\n","        stemmer = PorterStemmer()\n","        for kp in derived_keyphrases:\n","            stem = stemmer.stem(kp)\n","            if stem not in seen_stems:\n","                final_kps.append(kp)\n","                seen_stems.add(stem)\n","            if len(final_kps) >= num_keyphrases: break\n","        derived_keyphrases = final_kps\n","    except ImportError:\n","        if logger_instance and ENABLE_DETAILED_DEBUG_LOGGING: logger_instance.log(\"DEBUG P4_AutoKeyphrase: NLTK stemmer not available for de-duplication.\")\n","        # No stemming, use as is\n","\n","    if logger_instance: logger_instance.log(f\"INFO P4_AutoKeyphrase: Derived for '{theme_label}': {derived_keyphrases}\")\n","    return derived_keyphrases\n","\n","\n","# --- Main Quantification and Quoting Function for Phase 4 ---\n","def count_themes_and_extract_quotes_p4( # Renamed from _auto to be the main one\n","    llm_themes_by_qid: Dict[str, Any], # Output from Phase 3 (QID -> P3 analysis dict)\n","    collated_source_texts_data: Dict[str, Any], # Output from Phase 2 (P2_OUTPUT_COLLATED_JSON)\n","    human_keyphrases_by_qid: Dict[str, Dict[str, List[str]]], # Loaded from P4_HUMAN_DEFINED_KEYPHRASES_FILE\n","    num_quotes_per_theme: int,\n","    logger_instance: FileLogger\n",") -> Dict[str, Any]:\n","    final_report_data_p4 = {}\n","    source_texts_for_all_qids = collated_source_texts_data.get(\"aggregated_pdf_content_by_qid\", {})\n","\n","    for qid, p3_qid_analysis_data in llm_themes_by_qid.items():\n","        if ENABLE_DETAILED_DEBUG_LOGGING: logger_instance.log(f\"DEBUG P4: Processing QID: {qid} for quantification and quoting.\")\n","\n","        question_text = p3_qid_analysis_data.get(\"question_text\", f\"Question text N/A for {qid}\")\n","        llm_suggested_themes_from_p3 = p3_qid_analysis_data.get(\"parsed_top_themes\", []) # List of {\"theme_label\": ..., \"explanation\": ...}\n","\n","        final_report_data_p4[qid] = {\n","            \"question_text\": question_text,\n","            \"llm_generated_themes_from_p3\": llm_suggested_themes_from_p3,\n","            \"status_from_p3\": p3_qid_analysis_data.get(\"status\", \"N/A\"),\n","            \"quantified_points\": []\n","        }\n","\n","        source_text_items_for_this_qid = source_texts_for_all_qids.get(qid, [])\n","        if not source_text_items_for_this_qid:\n","            logger_instance.log(f\"  WARN P4: No source texts (from Phase 2) found for QID {qid}. Cannot quantify or quote.\");\n","            final_report_data_p4[qid][\"quantified_points\"].append({\"point_label\": \"No P2 source texts for QID\", \"unique_response_count\":0, \"example_quotes\":[]})\n","            continue\n","\n","        # These are the themes/points for which we *have human-defined keyphrases*.\n","        qid_human_keyphrase_map = human_keyphrases_by_qid.get(qid, {})\n","\n","        themes_to_quantify_and_their_keyphrases = {} # Store \"Theme Label To Use\" -> [keyphrases]\n","\n","        # First, process themes that have human-defined keyphrases\n","        if qid_human_keyphrase_map:\n","            logger_instance.log(f\"INFO P4: QID {qid} - Using human-defined keyphrases for {len(qid_human_keyphrase_map)} themes.\")\n","            for human_theme_label, phrases in qid_human_keyphrase_map.items():\n","                if phrases: # Only if keyphrases are provided\n","                    themes_to_quantify_and_their_keyphrases[human_theme_label] = {\n","                        \"keyphrases\": phrases,\n","                        \"source\": \"Human-Defined\",\n","                        \"llm_explanation\": next((t.get(\"explanation\") for t in llm_suggested_themes_from_p3 if isinstance(t,dict) and t.get(\"theme_label\") == human_theme_label), \"N/A - Label may be refined or not from P3 direct output.\")\n","                    }\n","\n","        # Second, for LLM themes not covered by human keyphrases, auto-derive them\n","        if isinstance(llm_suggested_themes_from_p3, list):\n","            for llm_theme_obj in llm_suggested_themes_from_p3:\n","                if not isinstance(llm_theme_obj, dict) or llm_theme_obj.get(\"theme_label\",\"\").lower().startswith((\"llm error\", \"parsing/llm error\", \"unparsed\", \"valid json\")):\n","                    continue # Skip error/malformed LLM themes\n","\n","                llm_label = llm_theme_obj.get(\"theme_label\")\n","                # Check if this llm_label (or a human-refined version of it) is already in themes_to_quantify\n","                already_covered = any(llm_label == Tlabel or llm_label in Tlabel for Tlabel in themes_to_quantify_and_their_keyphrases.keys())\n","\n","                if not already_covered:\n","                    llm_explanation = llm_theme_obj.get(\"explanation\", \"\")\n","                    auto_kps = auto_derive_keyphrases_from_llm_theme(llm_label, llm_explanation, logger_instance=logger_instance)\n","                    if auto_kps:\n","                        themes_to_quantify_and_their_keyphrases[llm_label + \" (Auto-Keyphrased)\"] = { # Mark as auto\n","                             \"keyphrases\": auto_kps,\n","                             \"source\": \"Auto-Derived from LLM Theme\",\n","                             \"llm_explanation\": llm_explanation\n","                        }\n","                    else: # Auto-derivation failed, still list it with 0 count\n","                         themes_to_quantify_and_their_keyphrases[llm_label + \" (Auto-Keyphrased - Derivation Failed)\"] = {\n","                             \"keyphrases\": [], \"source\": \"Auto-Derived (Failed)\", \"llm_explanation\": llm_explanation\n","                         }\n","\n","        if not themes_to_quantify_and_their_keyphrases:\n","            logger_instance.log(f\"  WARN P4: QID {qid} - No themes to quantify (neither human-defined nor auto-derivable from P3 output).\")\n","            final_report_data_p4[qid][\"quantified_points\"].append({\"point_label\": \"No Themes to Quantify for QID\", \"unique_response_count\":0, \"example_quotes\":[]})\n","            continue\n","\n","        # Now, iterate through themes_to_quantify_and_their_keyphrases for counting and quoting\n","        for point_label_to_report, data_for_point in themes_to_quantify_and_their_keyphrases.items():\n","            keyphrases_for_this_point = data_for_point[\"keyphrases\"]\n","            source_of_kps_info = data_for_point[\"source\"]\n","            llm_expl_for_this_point = data_for_point[\"llm_explanation\"]\n","\n","            if not keyphrases_for_this_point: # If auto-derivation failed and left empty list\n","                final_report_data_p4[qid][\"quantified_points\"].append({\n","                    \"point_label\": point_label_to_report, \"llm_explanation\": llm_expl_for_this_point,\n","                    \"keyphrases_used_for_counting\": [\"N/A - Keyphrase Generation Failed\"], \"source_of_keyphrases\": source_of_kps_info,\n","                    \"unique_response_count\": 0, \"example_quotes\": [] })\n","                continue\n","\n","            logger_instance.log(f\"  P4 Counting for QID {qid} - Point: '{point_label_to_report}' using {source_of_kps_info} KPs: {keyphrases_for_this_point}\")\n","\n","            point_matching_response_ids = set()\n","            source_items_for_potential_quotes = []\n","            # ... (The rest of the counting and quote extraction logic is the same as in the previous complete Cell 6)\n","            # ... (Using point_label_to_report, keyphrases_for_this_point, etc.)\n","            for source_item in source_text_items_for_this_qid:\n","                item_text_content = source_item.get(\"text\", \"\"); item_text_lower = item_text_content.lower()\n","                response_id = source_item.get(\"response_id\")\n","                if not item_text_content or not response_id: continue\n","                for phrase in keyphrases_for_this_point:\n","                    if re.search(r'\\b' + re.escape(phrase.lower()) + r'\\b', item_text_lower):\n","                        point_matching_response_ids.add(response_id)\n","                        source_items_for_potential_quotes.append({ \"response_id\": response_id, \"full_text_item\": item_text_content,\n","                            \"source_document_type\": source_item.get(\"type\"), \"matched_phrase\": phrase }); break\n","            unique_response_count_for_point = len(point_matching_response_ids); selected_quotes_p4 = []\n","            if unique_response_count_for_point > 0 and source_items_for_potential_quotes:\n","                quotes_extracted_so_far = 0; shuffled_potential_quotes = sorted(source_items_for_potential_quotes, key=lambda x: x.get(\"response_id\",\"\"))\n","                seen_responses_for_this_point_quotes = set()\n","                for quote_candidate_info in shuffled_potential_quotes:\n","                    if quotes_extracted_so_far >= num_quotes_per_theme: break\n","                    current_response_id = quote_candidate_info[\"response_id\"]\n","                    if current_response_id in seen_responses_for_this_point_quotes and quotes_extracted_so_far >= len(point_matching_response_ids): pass # Allow multiple if all unique resp covered\n","                    elif current_response_id in seen_responses_for_this_point_quotes: continue\n","                    full_text = quote_candidate_info[\"full_text_item\"]; phrase_to_find_in_sentence = quote_candidate_info[\"matched_phrase\"]\n","                    try: sentences = nltk.sent_tokenize(full_text)\n","                    except: sentences = [full_text]\n","                    for sent_idx, sentence in enumerate(sentences):\n","                        if re.search(r'\\b' + re.escape(phrase_to_find_in_sentence.lower()) + r'\\b', sentence.lower()):\n","                            context_start = max(0, sent_idx - 1); context_end = min(len(sentences), sent_idx + 2)\n","                            context_snippet = \" \".join(sentences[context_start:context_end]).strip()\n","                            selected_quotes_p4.append({ \"response_id\": current_response_id, \"source_document_type\": quote_candidate_info[\"source_document_type\"],\n","                                                        \"matched_phrase\": phrase_to_find_in_sentence, \"quote\": sentence.strip(), \"broader_context\": context_snippet })\n","                            quotes_extracted_so_far += 1\n","                            if current_response_id: seen_responses_for_this_point_quotes.add(current_response_id)\n","                            break\n","                    if quotes_extracted_so_far >= num_quotes_per_theme: break\n","            final_report_data_p4[qid][\"quantified_points\"].append({\n","                \"point_label\": point_label_to_report, \"llm_explanation\": llm_expl_for_this_point,\n","                \"keyphrases_used_for_counting\": keyphrases_for_this_point, \"source_of_keyphrases\": source_of_kps_info,\n","                \"unique_response_count\": unique_response_count_for_point, \"example_quotes\": selected_quotes_p4 })\n","            logger_instance.log(f\"    P4 QID {qid} - Point '{point_label_to_report}': Count {unique_response_count_for_point}. Quotes: {len(selected_quotes_p4)}.\")\n","\n","    return final_report_data_p4\n","\n","# --- Main Execution Function for Phase 4 ---\n","def run_phase4_theme_quantification_and_quoting(\n","    logger_p4_instance: FileLogger,\n","    p3_llm_themes_output_file: str,     # Path to P3_OUTPUT_LLM_THEMES_JSON\n","    p2_collated_texts_output_file: str,  # Path to P2_OUTPUT_COLLATED_JSON\n","    human_keyphrases_file: str,         # Path to P4_HUMAN_DEFINED_KEYPHRASES_FILE\n","    final_analysis_output_file: str,    # Path to P4_OUTPUT_FINAL_ANALYSIS_JSON\n","    num_quotes_config: int              # Value from P4_NUM_EXAMPLE_QUOTES_PER_THEME\n","    ):\n","    logger_p4 = logger_p4_instance\n","    logger_p4.log(\"=\" * 50); logger_p4.log(\" INFO P4: Starting Phase 4: Theme Quantification & Quotation Extraction\"); logger_p4.log(\"=\" * 50)\n","\n","    # 1. Load LLM-generated themes from Phase 3\n","    llm_themes_data_from_p3 = {} # This will be QID -> P3 analysis dict\n","    try:\n","        logger_p4.log(f\"INFO P4: Loading LLM themes from: {p3_llm_themes_output_file}\")\n","        with open(p3_llm_themes_output_file, 'r', encoding='utf-8') as f:\n","            llm_themes_data_from_p3 = json.load(f)\n","        if not llm_themes_data_from_p3 or not isinstance(llm_themes_data_from_p3, dict):\n","             logger_p4.log(f\"ERROR P4: Phase 3 output file {p3_llm_themes_output_file} is empty or not a dict. Cannot proceed.\"); return False\n","        logger_p4.log(f\"INFO P4: Loaded LLM-generated themes for {len(llm_themes_data_from_p3)} QIDs from {p3_llm_themes_output_file}\")\n","    except FileNotFoundError:\n","        logger_p4.log(f\"ERROR P4: Phase 3 output file {p3_llm_themes_output_file} not found. Please ensure Phase 3 has run successfully.\"); return False\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to load LLM themes from {p3_llm_themes_output_file}: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    # 2. Load collated source texts from Phase 2\n","    collated_texts_data_from_p2 = {} # This will hold the full structure from P2, including metadata and aggregated_pdf_content_by_qid\n","    try:\n","        logger_p4.log(f\"INFO P4: Loading collated texts from: {p2_collated_texts_output_file}\")\n","        with open(p2_collated_texts_output_file, 'r', encoding='utf-8') as f:\n","            collated_texts_data_from_p2 = json.load(f)\n","        if not collated_texts_data_from_p2 or not isinstance(collated_texts_data_from_p2.get(\"aggregated_pdf_content_by_qid\"), dict):\n","            logger_p4.log(f\"ERROR P4: Phase 2 output file {p2_collated_texts_output_file} is empty or missing 'aggregated_pdf_content_by_qid'.\"); return False\n","        logger_p4.log(f\"INFO P4: Loaded collated texts from {p2_collated_texts_output_file}\")\n","    except FileNotFoundError:\n","        logger_p4.log(f\"ERROR P4: Phase 2 collated texts file {p2_collated_texts_output_file} not found. Please ensure Phase 2 has run successfully.\"); return False\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to load collated texts from {p2_collated_texts_output_file}: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    # 3. Load human-defined keyphrases\n","    human_keyphrases_override = load_human_defined_keyphrases_p4(human_keyphrases_file, logger_p4)\n","    # load_human_defined_keyphrases_p4 handles logging if file is empty/not found.\n","\n","    # 4. Perform counting and quote extraction\n","    final_report_content = count_themes_and_extract_quotes_p4( # Changed to call the renamed main internal function\n","        llm_themes_data_from_p3,\n","        collated_texts_data_from_p2,\n","        human_keyphrases_override,\n","        num_quotes_config,\n","        logger_p4\n","    )\n","\n","    # 5. Save the final analysis\n","    try:\n","        logger_p4.log(f\"INFO P4: Saving final thematic report to {final_analysis_output_file}\")\n","        with open(final_analysis_output_file, 'w', encoding='utf-8') as f_out_p4: # Use unique file handle\n","            json.dump(final_report_content, f_out_p4, indent=2, default=logger_p4._get_serializer())\n","        logger_p4.log(f\"INFO P4: Successfully saved final thematic report to {final_analysis_output_file}\")\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to save final analysis to {final_analysis_output_file}: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    logger_p4.log(\"=\" * 50); logger_p4.log(\" INFO: Phase 4 Finished \"); logger_p4.log(\"=\" * 50)\n","    return True\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 6 - Phase 4 logic defined (Theme Quantification & Quoting with human keyphrase override).\")\n","print(\"      (Ensure function bodies are complete and use P4_... configs from Cell 2).\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1O4ww2MhZsw9"},"outputs":[],"source":["# @title Cell 7: Main Pipeline Orchestration (SET RUN_PHASE_X FLAGS BEFORE EXECUTION)\n","\n","# Initialize a master logger for orchestration steps\n","# LOG_DIR is defined in Cell 2\n","master_logger_file_path = os.path.join(LOG_DIR, f\"pipeline_orchestration_master_{time.strftime('%Y%m%d_%H%M%S')}.log\")\n","master_logger = FileLogger(master_logger_file_path) # Assumes FileLogger class is defined in Cell 1\n","master_logger.log(f\"INFO: Orchestration Cell Started. Master log: {master_logger_file_path}\")\n","\n","# --- Flags to control which phases to run ---\n","# !!! REVIEW AND SET THESE CAREFULLY BEFORE EACH RUN !!!\n","RUN_PHASE_1 = True # PDF Extraction & Per-Response Summarization. Set False if P1_OUTPUT_FILE is complete and verified.\n","RUN_PHASE_2 = True  # Collation of PDF Content. Set True if P1_OUTPUT_FILE changed or P2 output needed.\n","RUN_PHASE_3 = True  # LLM Thematic Point Generation. Set True if P2 output changed or P3 output needed.\n","RUN_PHASE_4 = True  # Theme Validation, Quantification & Quotation Extraction. Set True if P3 output changed or P4 keyphrases updated.\n","\n","master_logger.log(f\"RUN FLAGS SET: Phase_1={RUN_PHASE_1}, Phase_2={RUN_PHASE_2}, Phase_3={RUN_PHASE_3}, Phase_4={RUN_PHASE_4}\")\n","\n","# --- NLTK Resource Check (using master_logger) ---\n","# This ensures NLTK resources are available before any phase might need them.\n","master_logger.log(\"INFO: Orchestrator - Checking NLTK resources using download_nltk_resource_with_logger...\")\n","try:\n","    # download_nltk_resource_with_logger should be defined in Cell 1\n","    download_nltk_resource_with_logger(master_logger, 'tokenizers/punkt', 'punkt')\n","    download_nltk_resource_with_logger(master_logger, 'corpora/stopwords', 'stopwords')\n","    # Add other NLTK resources if required by any phase, e.g.:\n","    # download_nltk_resource_with_logger(master_logger, 'taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger')\n","    # download_nltk_resource_with_logger(master_logger, 'sentiment/vader_lexicon', 'vader_lexicon') # If using VADER elsewhere\n","    master_logger.log(\"INFO: Orchestrator - NLTK resource check/download attempt complete.\")\n","except NameError:\n","    master_logger.log(\"WARN: Orchestrator - download_nltk_resource_with_logger function not defined (expected in Cell 1). NLTK resources might be missing if not pre-downloaded.\")\n","except Exception as e_nltk_orch:\n","    master_logger.log(f\"ERROR: Orchestrator - NLTK download process encountered an error: {e_nltk_orch}\")\n","\n","# --- Execute Pipeline Phases ---\n","# Stores the actual success/failure of phases that were *attempted* to run.\n","# If a phase is skipped, its flag here remains False unless explicitly set to True based on existing output.\n","attempted_phase_success_flags = {\n","    \"phase1\": False, \"phase2\": False, \"phase3\": False, \"phase4\": False\n","}\n","# Stores whether a phase is considered \"complete\" for dependency checking,\n","# either by running successfully or by having its output assumed valid when skipped.\n","dependency_phase_complete_flags = {\n","    \"phase1\": not RUN_PHASE_1,\n","    \"phase2\": not RUN_PHASE_2,\n","    \"phase3\": not RUN_PHASE_3,\n","    \"phase4\": not RUN_PHASE_4, # Phase 4 doesn't have dependents in this pipeline\n","}\n","\n","\n","if __name__ == \"__main__\": # Standard guard, typically true for Colab cell execution\n","    master_logger.log(\"INFO: Starting Pipeline Execution based on RUN_PHASE flags...\")\n","\n","    active_phases_to_run_list = []\n","    if RUN_PHASE_1: active_phases_to_run_list.append(\"Phase 1\")\n","    if RUN_PHASE_2: active_phases_to_run_list.append(\"Phase 2\")\n","    if RUN_PHASE_3: active_phases_to_run_list.append(\"Phase 3\")\n","    if RUN_PHASE_4: active_phases_to_run_list.append(\"Phase 4\")\n","    master_logger.log(f\"INFO: Active phases for this run: {', '.join(active_phases_to_run_list) if active_phases_to_run_list else 'None (all RUN_PHASE_X flags are False, will only check for existing files)'}\")\n","\n","    # --- Phase 1 ---\n","    if RUN_PHASE_1:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 1: PDF Extraction & Per-Response Summarization <<<\")\n","        logger_p1_orch = FileLogger(P1_LOG_FILE) # Initialize logger for this phase\n","        success_p1 = run_phase1_pdf_extraction(logger_p1_orch) # Pass logger instance\n","        attempted_phase_success_flags[\"phase1\"] = success_p1\n","        dependency_phase_complete_flags[\"phase1\"] = success_p1\n","        if not success_p1: master_logger.log(\"CRITICAL: Phase 1 failed. Dependent phases might be affected.\")\n","        else: master_logger.log(\">>> PHASE 1 COMPLETED SUCCESSFULLY <<<\")\n","    else:\n","        master_logger.log(\">>> SKIPPING PHASE 1 (RUN_PHASE_1 is False) <<<\")\n","        if not os.path.exists(P1_OUTPUT_FILE): # P1_OUTPUT_FILE from Cell 2\n","            master_logger.log(f\"ERROR: Phase 1 skipped, but required output {P1_OUTPUT_FILE} not found! Dependent phases will likely fail.\")\n","            dependency_phase_complete_flags[\"phase1\"] = False # Mark as not complete for dependency\n","        else:\n","            master_logger.log(f\"INFO: Assuming Phase 1 output {P1_OUTPUT_FILE} is valid and complete.\")\n","            dependency_phase_complete_flags[\"phase1\"] = True\n","\n","    # --- Phase 2 ---\n","    if dependency_phase_complete_flags[\"phase1\"] and RUN_PHASE_2:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 2: Collation of PDF Content <<<\")\n","        logger_p2_orch = FileLogger(P2_LOG_FILE)\n","        success_p2 = run_phase2_collate_pdf_texts(logger_p2_orch) # Pass logger\n","        attempted_phase_success_flags[\"phase2\"] = success_p2\n","        dependency_phase_complete_flags[\"phase2\"] = success_p2\n","        if not success_p2: master_logger.log(\"CRITICAL: Phase 2 failed. Dependent phases might be affected.\")\n","        else: master_logger.log(\">>> PHASE 2 COMPLETED SUCCESSFULLY <<<\")\n","    elif not dependency_phase_complete_flags[\"phase1\"] and RUN_PHASE_2:\n","        master_logger.log(\"WARN: Phase 1 did not complete successfully or its output is missing. Skipping Phase 2.\")\n","        dependency_phase_complete_flags[\"phase2\"] = False # Mark as not complete\n","    elif not RUN_PHASE_2:\n","        master_logger.log(\">>> SKIPPING PHASE 2 (RUN_PHASE_2 is False) <<<\")\n","        if not os.path.exists(P2_OUTPUT_COLLATED_JSON): # P2_OUTPUT_COLLATED_JSON from Cell 2\n","            master_logger.log(f\"ERROR: Phase 2 skipped, but required output {P2_OUTPUT_COLLATED_JSON} not found! Dependent phases may fail.\")\n","            dependency_phase_complete_flags[\"phase2\"] = False\n","        else:\n","            master_logger.log(f\"INFO: Assuming Phase 2 output {P2_OUTPUT_COLLATED_JSON} is valid and complete.\")\n","            dependency_phase_complete_flags[\"phase2\"] = True\n","\n","    # --- Phase 3 ---\n","    if dependency_phase_complete_flags[\"phase2\"] and RUN_PHASE_3:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 3: LLM Thematic Point Generation <<<\")\n","        logger_p3_orch = FileLogger(P3_LOG_FILE)\n","        success_p3 = run_phase3_llm_thematic_generation(logger_p3_orch) # Pass logger\n","        attempted_phase_success_flags[\"phase3\"] = success_p3\n","        dependency_phase_complete_flags[\"phase3\"] = success_p3\n","        if not success_p3: master_logger.log(\"CRITICAL: Phase 3 failed. Dependent phases might be affected.\")\n","        else: master_logger.log(\">>> PHASE 3 COMPLETED SUCCESSFULLY <<<\")\n","    elif not dependency_phase_complete_flags[\"phase2\"] and RUN_PHASE_3:\n","        master_logger.log(\"WARN: Phase 2 did not complete successfully or its output is missing. Skipping Phase 3.\")\n","        dependency_phase_complete_flags[\"phase3\"] = False\n","    elif not RUN_PHASE_3:\n","        master_logger.log(\">>> SKIPPING PHASE 3 (RUN_PHASE_3 is False) <<<\")\n","        if not os.path.exists(P3_OUTPUT_LLM_THEMES_JSON): # P3_OUTPUT_LLM_THEMES_JSON from Cell 2\n","            master_logger.log(f\"ERROR: Phase 3 skipped, but required output {P3_OUTPUT_LLM_THEMES_JSON} not found! Dependent phases may fail.\")\n","            dependency_phase_complete_flags[\"phase3\"] = False\n","        else:\n","            master_logger.log(f\"INFO: Assuming Phase 3 output {P3_OUTPUT_LLM_THEMES_JSON} is valid and complete.\")\n","            dependency_phase_complete_flags[\"phase3\"] = True\n","\n","    # --- Phase 4 ---\n","    if dependency_phase_complete_flags[\"phase3\"] and RUN_PHASE_4:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 4: Theme Validation, Quantification & Quotation Extraction <<<\")\n","        logger_p4_orch = FileLogger(P4_LOG_FILE)\n","\n","        # Ensure P4_HUMAN_DEFINED_KEYPHRASES_FILE exists (or is created empty) by Phase 4 logic itself\n","        # The load_human_defined_keyphrases_p4 function in Cell 6 handles missing file by returning {}.\n","        # Orchestrator can ensure an empty file is physically present if that's preferred for external tools.\n","        if not os.path.exists(P4_HUMAN_DEFINED_KEYPHRASES_FILE): # P4_HUMAN_DEFINED_KEYPHRASES_FILE from Cell 2\n","            master_logger.log(f\"WARN: Phase 4 - Human defined keyphrases file not found at {P4_HUMAN_DEFINED_KEYPHRASES_FILE}. Attempting to create empty file.\")\n","            try:\n","                with open(P4_HUMAN_DEFINED_KEYPHRASES_FILE, 'w', encoding='utf-8') as f_empty_kp:\n","                    json.dump({}, f_empty_kp) # Create an empty JSON object file\n","                master_logger.log(f\"INFO: Created empty keyphrase file at {P4_HUMAN_DEFINED_KEYPHRASES_FILE}. Quantification will rely on auto-derivation until populated.\")\n","            except Exception as e_kp_create:\n","                 master_logger.log(f\"ERROR: Could not create empty keyphrase file: {e_kp_create}. Phase 4 might behave unexpectedly if it requires this file.\")\n","\n","        success_p4 = run_phase4_theme_quantification_and_quoting(\n","            logger_p4_orch,\n","            P4_INPUT_LLM_THEMES_JSON,\n","            P4_INPUT_COLLATED_TEXTS_JSON,\n","            P4_HUMAN_DEFINED_KEYPHRASES_FILE,\n","            P4_OUTPUT_FINAL_ANALYSIS_JSON,\n","            P4_NUM_EXAMPLE_QUOTES_PER_THEME\n","        )\n","        attempted_phase_success_flags[\"phase4\"] = success_p4\n","        dependency_phase_complete_flags[\"phase4\"] = success_p4 # Phase 4 is last in this chain for now\n","        if not success_p4: master_logger.log(\"ERROR: Phase 4 encountered issues.\")\n","        else: master_logger.log(\">>> PHASE 4 COMPLETED SUCCESSFULLY <<<\")\n","    elif not dependency_phase_complete_flags[\"phase3\"] and RUN_PHASE_4:\n","        master_logger.log(\"WARN: Phase 3 did not complete successfully or its output is missing. Skipping Phase 4.\")\n","        dependency_phase_complete_flags[\"phase4\"] = False\n","    elif not RUN_PHASE_4:\n","        master_logger.log(\">>> SKIPPING PHASE 4 (RUN_PHASE_4 is False) <<<\")\n","        # If P4 is skipped, we still check if its output exists as it's the final product\n","        if os.path.exists(P4_OUTPUT_FINAL_ANALYSIS_JSON):\n","            dependency_phase_complete_flags[\"phase4\"] = True\n","            master_logger.log(f\"INFO: Assuming Phase 4 output {P4_OUTPUT_FINAL_ANALYSIS_JSON} is valid (as it was skipped).\")\n","        else:\n","            dependency_phase_complete_flags[\"phase4\"] = False\n","            master_logger.log(f\"INFO: Phase 4 skipped and output file {P4_OUTPUT_FINAL_ANALYSIS_JSON} not found.\")\n","\n","\n","    master_logger.log(\"\\n\" + \"=\"*50)\n","    master_logger.log(\"Overall Pipeline Orchestration Concluded.\")\n","    for phase_key, run_flag in [(\"phase1\", RUN_PHASE_1), (\"phase2\", RUN_PHASE_2),\n","                                (\"phase3\", RUN_PHASE_3), (\"phase4\", RUN_PHASE_4)]:\n","        final_status_msg = \"\"\n","        if run_flag: # If the phase was set to run\n","            final_status_msg = 'SUCCESS' if attempted_phase_success_flags[phase_key] else 'FAILED'\n","        else: # If the phase was set to be skipped\n","            if dependency_phase_complete_flags[phase_key]:\n","                 final_status_msg = 'SKIPPED (Existing Output Assumed Valid)'\n","            else:\n","                 # Determine if this skipped phase's missing output would block an *active* downstream phase\n","                 downstream_needs_this_output = False\n","                 if phase_key == \"phase1\" and (RUN_PHASE_2 or RUN_PHASE_3 or RUN_PHASE_4): downstream_needs_this_output = True\n","                 if phase_key == \"phase2\" and (RUN_PHASE_3 or RUN_PHASE_4): downstream_needs_this_output = True\n","                 if phase_key == \"phase3\" and RUN_PHASE_4: downstream_needs_this_output = True\n","\n","                 if downstream_needs_this_output:\n","                     final_status_msg = 'SKIPPED (Output MISSING - Will Impact Active Downstream Phase!)'\n","                 else:\n","                     final_status_msg = 'SKIPPED (Output Not Found - No Active Downstream Impact)'\n","                     if phase_key == \"phase4\": final_status_msg = 'SKIPPED (Final Output Not Generated this run)'\n","\n","\n","        master_logger.log(f\"  {phase_key.upper()} Status: {final_status_msg}\")\n","\n","    master_logger.log(\"Please check individual phase log files for details:\")\n","    if RUN_PHASE_1 or os.path.exists(P1_LOG_FILE): master_logger.log(f\"  Phase 1 Log: {P1_LOG_FILE}\")\n","    if RUN_PHASE_2 or os.path.exists(P2_LOG_FILE): master_logger.log(f\"  Phase 2 Log: {P2_LOG_FILE}\")\n","    if RUN_PHASE_3 or os.path.exists(P3_LOG_FILE): master_logger.log(f\"  Phase 3 Log: {P3_LOG_FILE}\")\n","    if RUN_PHASE_4 or os.path.exists(P4_LOG_FILE): master_logger.log(f\"  Phase 4 Log: {P4_LOG_FILE}\")\n","    master_logger.log(\"=\"*50)\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 7 - Main Orchestration Cell defined.\")\n","print(\"      SET THE RUN_PHASE_X FLAGS AT THE TOP OF THIS CELL before execution.\")\n","print(\"      Ensure Cells 0-6 have been executed to define configurations and functions.\")\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G0b1kFzyaRWX"},"outputs":[],"source":["# @title Cell 8: Stakeholder Report Generation\n","\n","import json\n","import os\n","import time # For timestamp in default output filename\n","\n","# This cell does not define a new FileLogger for itself by default,\n","# but uses print statements. If extensive logging is needed for report generation\n","# itself, a logger could be initialized using LOG_DIR from Cell 2.\n","\n","def generate_stakeholder_report_final(\n","    phase4_final_analysis_file: str, # Path to P4_OUTPUT_FINAL_ANALYSIS_JSON\n","    output_stakeholder_report_file: str,\n","    num_top_points_to_display: int = 3, # How many top points per QID\n","    num_quotes_to_display_per_point: int = 2 # How many quotes per point\n","    ):\n","    \"\"\"\n","    Reads the Phase 4 final analysis JSON and generates a human-readable\n","    summary report suitable for stakeholder communication.\n","    \"\"\"\n","    report_lines = []\n","    report_lines.append(\"=\" * 80)\n","    report_lines.append(\"STAKEHOLDER COMMUNICATION: SUMMARY OF THEMATIC ANALYSIS FROM PDF SUBMISSIONS\")\n","    report_lines.append(\"=\" * 80)\n","    report_lines.append(f\"Report generated on: {time.strftime('%Y-%m-%d %H:%M:%S %Z', time.gmtime())}\") # Using UTC\n","    report_lines.append(f\"Source data file: {os.path.basename(phase4_final_analysis_file)}\\n\")\n","\n","    try:\n","        with open(phase4_final_analysis_file, 'r', encoding='utf-8') as f:\n","            # phase4_data is expected to be a dictionary: QID -> QID_analysis_data\n","            phase4_data = json.load(f)\n","    except FileNotFoundError:\n","        msg = f\"ERROR: Phase 4 analysis file not found at '{phase4_final_analysis_file}'. Cannot generate stakeholder report.\"\n","        print(msg); report_lines.append(msg)\n","        # Write error to output file as well\n","        with open(output_stakeholder_report_file, 'w', encoding='utf-8') as f_out_err: f_out_err.write(\"\\n\".join(report_lines))\n","        return\n","    except json.JSONDecodeError as e:\n","        msg = f\"ERROR: Could not decode JSON from Phase 4 analysis file '{phase4_final_analysis_file}'. Error: {e}\"\n","        print(msg); report_lines.append(msg)\n","        with open(output_stakeholder_report_file, 'w', encoding='utf-8') as f_out_err: f_out_err.write(\"\\n\".join(report_lines))\n","        return\n","    except Exception as e_load:\n","        msg = f\"ERROR: Unexpected error loading Phase 4 analysis file '{phase4_final_analysis_file}'. Error: {e_load}\"\n","        print(msg); report_lines.append(msg)\n","        with open(output_stakeholder_report_file, 'w', encoding='utf-8') as f_out_err: f_out_err.write(\"\\n\".join(report_lines))\n","        return\n","\n","    if not phase4_data or not isinstance(phase4_data, dict):\n","        msg = f\"INFO: Phase 4 analysis data loaded from '{phase4_final_analysis_file}' is empty or not in the expected QID-keyed dictionary format.\"\n","        print(msg); report_lines.append(msg)\n","        # Still write what we have to the report file\n","        with open(output_stakeholder_report_file, 'w', encoding='utf-8') as f_out_empty: f_out_empty.write(\"\\n\".join(report_lines))\n","        return\n","\n","    # Sort QIDs for consistent report order\n","    sorted_qids = []\n","    try:\n","        sorted_qids = sorted(phase4_data.keys(), key=lambda q_key: int(q_key[1:]) if q_key.startswith('Q') and q_key[1:].isdigit() else float('inf'))\n","    except:\n","        sorted_qids = sorted(phase4_data.keys()) # Fallback to alphabetical if parsing QID number fails\n","\n","    for qid in sorted_qids:\n","        qid_analysis_content = phase4_data.get(qid, {})\n","        question_text = qid_analysis_content.get(\"question_text\", f\"Question text for {qid} not available.\")\n","\n","        report_lines.append(\"-\" * 70)\n","        report_lines.append(f\"\\nQUESTION {qid}: {question_text}\\n\")\n","\n","        quantified_points_list = qid_analysis_content.get(\"quantified_points\", [])\n","\n","        # Filter for points that actually have a count (or are LLM suggestions if no keyphrases were used)\n","        # and sort them by unique_response_count\n","        valid_points_for_display = [\n","            pt for pt in quantified_points_list\n","            if isinstance(pt, dict) and pt.get(\"point_label\") # Ensure it's a dict with a label\n","        ]\n","        # Sort primarily by unique_response_count, then by how the point_label is structured (Human-defined might be prioritized)\n","        sorted_display_points = sorted(\n","            valid_points_for_display,\n","            key=lambda x: (x.get(\"unique_response_count\", 0), 0 if \"Human-Defined\" in x.get(\"source_of_keyphrases\",\"\") else 1),\n","            reverse=True\n","        )\n","\n","        if sorted_display_points:\n","            report_lines.append(\"  Most Common Points/Themes Raised (derived from PDF content):\")\n","            points_shown_count = 0\n","            for i, point_data in enumerate(sorted_display_points):\n","                if points_shown_count >= num_top_points_to_display and point_data.get(\"unique_response_count\", 0) == 0 and \"Keyphrases Needed\" in point_data.get(\"point_label\",\"\"):\n","                    # If we've shown enough top points and this is just an LLM suggestion with 0 count, maybe skip\n","                    pass # Or add a threshold: if count is 0 and we already showed N, skip\n","\n","                point_label_display = point_data.get(\"point_label\", \"N/A\")\n","                count = point_data.get(\"unique_response_count\", 0)\n","                keyphrase_source_info = point_data.get(\"source_of_keyphrases\", \"\")\n","                llm_explanation = point_data.get(\"llm_explanation\",\n","                                  point_data.get(\"llm_explanation_if_direct_match\",\n","                                                 \"LLM explanation not directly available for this version of the point.\"))\n","\n","                report_lines.append(f\"\\n    {points_shown_count+1}. {point_label_display}\")\n","                report_lines.append(f\"       (Count: {count} unique responses. Keyphrases: {keyphrase_source_info})\")\n","\n","                if llm_explanation and not llm_explanation.lower().startswith((\"n/a\", \"this point label was human-defined\")):\n","                    report_lines.append(f\"       LLM-Generated Explanation Snippet: {llm_explanation[:250]}{'...' if len(llm_explanation)>250 else ''}\")\n","\n","                example_quotes = point_data.get(\"example_quotes\", [])\n","                if example_quotes:\n","                    report_lines.append(\"       Example Quotes from PDF Content:\")\n","                    for quote_idx, quote_info in enumerate(example_quotes[:num_quotes_to_display_per_point]):\n","                        quote_text = quote_info.get(\"quote_snippet\", quote_info.get(\"quote\", \"Quote N/A\")).strip()\n","                        resp_id = quote_info.get(\"response_id\", \"N/A\")\n","                        src_type = quote_info.get(\"source_document_type\", \"N/A\") # pdf_summary or pdf_passages\n","                        matched_kp = quote_info.get(\"matched_phrase\", \"N/A\")\n","                        report_lines.append(f\"         - \\\"{quote_text}\\\"\")\n","                        report_lines.append(f\"           (Source: Response ID '{resp_id}', Type: '{src_type}', Matched Keyphrase: '{matched_kp}')\")\n","                elif count > 0 : # If count > 0 but no quotes (e.g. quote extraction failed or not enough items)\n","                    report_lines.append(\"       (No example quotes extracted for this point despite matching responses.)\")\n","                # If count is 0, usually implies no keyphrases or no matches, so no quotes expected.\n","\n","                points_shown_count += 1\n","                if points_shown_count >= num_top_points_to_display and point_data.get(\"unique_response_count\",0) > 0 : # if we have shown N points with actual counts\n","                    if len(sorted_display_points) > points_shown_count:\n","                         report_lines.append(f\"\\n    (...and {len(sorted_display_points) - points_shown_count} other points identified.)\")\n","                    break # Stop after N top points with counts, or all points if fewer than N have counts > 0\n","\n","            if points_shown_count == 0 and sorted_display_points: # If all points had 0 count (e.g. all are \"Keyphrases Needed\")\n","                 report_lines.append(\"  (LLM suggested themes are listed above; keyphrases need to be defined for quantification and quote extraction.)\")\n","\n","\n","        else: # No \"quantified_points\" entries at all for this QID\n","            report_lines.append(\"  No thematic points were processed or available for this question in the Phase 4 output.\")\n","            # Check if there were at least LLM themes from Phase 3\n","            if qid_analysis_content.get(\"llm_generated_themes_from_p3\"):\n","                report_lines.append(\"  LLM-suggested themes from Phase 3 (awaiting quantification):\")\n","                for llm_theme in qid_analysis_content[\"llm_generated_themes_from_p3\"][:num_top_points_to_display]: # Show a few\n","                    if isinstance(llm_theme, dict):\n","                        report_lines.append(f\"    - Label: {llm_theme.get('theme_label')}\")\n","                        report_lines.append(f\"      Explanation: {llm_theme.get('explanation')}\")\n","        report_lines.append(\"\") # Blank line for spacing\n","\n","    report_lines.append(\"=\" * 80)\n","    report_lines.append(\"END OF STAKEHOLDER SUMMARY REPORT\")\n","    report_lines.append(\"=\" * 80)\n","\n","    final_report_str = \"\\n\".join(report_lines)\n","\n","    # Print to console for immediate review\n","    print(\"\\n\" + \"=\"*20 + \" GENERATED STAKEHOLDER REPORT \" + \"=\"*20)\n","    print(final_report_str)\n","    print(\"=\"*20 + \" END OF GENERATED STAKEHOLDER REPORT \" + \"=\"*20 + \"\\n\")\n","\n","\n","    try:\n","        with open(output_stakeholder_report_file, 'w', encoding='utf-8') as f_out:\n","            f_out.write(final_report_str)\n","        print(f\"INFO: Stakeholder report saved to: {output_stakeholder_report_file}\")\n","    except Exception as e:\n","        print(f\"ERROR: Could not save stakeholder report to file '{output_stakeholder_report_file}': {e}\")\n","        traceback.print_exc()\n","\n","# --- To run this Stakeholder Report Generation Cell ---\n","# 1. Ensure Phase 4 has completed and P4_OUTPUT_FINAL_ANALYSIS_JSON (from Cell 2)\n","#    points to its most recent, complete output.\n","# 2. The STAKEHOLDER_REPORT_OUTPUT_FILENAME variable below will use the path from Cell 2.\n","# 3. Execute this cell.\n","\n","if __name__ == \"__main__\": # Guard for when this cell is run directly\n","    print(\"INFO: Cell 8 - Attempting to generate stakeholder report...\")\n","    try:\n","        # These global variables are expected to be defined in Cell 2\n","        # and P4_OUTPUT_FINAL_ANALYSIS_JSON should point to the *actual file* generated by Phase 4.\n","        phase4_input_file_for_report = P4_OUTPUT_FINAL_ANALYSIS_JSON\n","        num_quotes_to_show = P4_NUM_EXAMPLE_QUOTES_PER_THEME\n","\n","        # Define output filename for this specific report generation run, placing it in PHASE4_OUTPUT_DIR\n","        stakeholder_txt_report_filename = os.path.join(\n","            PHASE4_OUTPUT_DIR, # Use directory defined in Cell 2\n","            f\"stakeholder_summary_report_{time.strftime('%Y%m%d_%H%M%S')}.txt\"\n","        )\n","\n","        if not os.path.exists(phase4_input_file_for_report):\n","            print(f\"ERROR: Cannot generate stakeholder report. Input file (Phase 4 output) missing at: {phase4_input_file_for_report}\")\n","            print(\"       Please ensure Phase 4 has run successfully and P4_OUTPUT_FINAL_ANALYSIS_JSON in Cell 2 is correctly defined and points to an existing file.\")\n","        else:\n","            generate_stakeholder_report_final(\n","                phase4_input_file_for_report,\n","                stakeholder_txt_report_filename,\n","                num_top_points_to_display = 3, # Can be made a global config in Cell 2 if desired\n","                num_quotes_to_display_per_point = num_quotes_to_show\n","            )\n","    except NameError as ne:\n","        print(f\"ERROR: A configuration variable (e.g., P4_OUTPUT_FINAL_ANALYSIS_JSON or P4_NUM_EXAMPLE_QUOTES_PER_THEME from Cell 2) is not defined.\")\n","        print(f\"       Please ensure Cell 2 (Global Configuration) has been executed. Details: {ne}\")\n","        traceback.print_exc()\n","    except Exception as e_report_main:\n","        print(f\"An unexpected error occurred in the main block of stakeholder report generation: {e_report_main}\")\n","        traceback.print_exc()\n","\n","print(\"=\"*50)\n","print(\"INFO: Cell 8 - Stakeholder Report Generation logic defined.\")\n","print(\"      (Execute this cell after Phase 4 has successfully produced its output).\")\n","print(\"=\"*50)"]},{"cell_type":"markdown","metadata":{"id":"W2Sj0BIr3JYQ"},"source":["# 24th May"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-_sJ6zUUKwA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ia3aoktp3Gl2"},"outputs":[],"source":["# @title Cell 6: Phase 4 - Theme Validation, Quantification & Quotation Extraction (REVISED)\n","\n","# logger_p4 = FileLogger(P4_LOG_FILE) # Will be initialized by Orchestration Cell (Cell 7)\n","\n","# --- Helper Function for Phase 4: Load Keyphrases ---\n","# !!! THIS FUNCTION NEEDS TO BE DEFINED HERE !!!\n","def load_human_defined_keyphrases_p4(keyphrase_file_path: str, logger_instance) -> Dict[str, Dict[str, List[str]]]:\n","    \"\"\"\n","    Loads a JSON file mapping QID -> Theme Label -> List of Keyphrases.\n","    \"\"\"\n","    if not os.path.exists(keyphrase_file_path):\n","        logger_instance.log(f\"WARN P4: Keyphrase file not found at: {keyphrase_file_path}. Will be treated as empty. An empty file will be created by the orchestrator if it's the first run of Phase 4.\")\n","        # If the orchestrator is supposed to create it, returning {} is fine.\n","        # If it *must* exist here, this could be an error.\n","        return {}\n","    try:\n","        with open(keyphrase_file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","        if not isinstance(data, dict):\n","            logger_instance.log(f\"ERROR P4: Keyphrase file {keyphrase_file_path} does not contain a valid JSON dictionary. Returning empty.\")\n","            return {}\n","        if data: # Log only if not empty\n","            logger_instance.log(f\"INFO P4: Successfully loaded {len(data)} QIDs with keyphrases from {keyphrase_file_path}.\")\n","        else:\n","            logger_instance.log(f\"INFO P4: Keyphrase file {keyphrase_file_path} is empty. Keyphrases will be auto-derived if applicable.\")\n","        return data\n","    except json.JSONDecodeError as e:\n","        logger_instance.log(f\"ERROR P4: Failed to decode JSON from keyphrase file {keyphrase_file_path}: {e}\")\n","        return {}\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P4: Failed to load keyphrase file {keyphrase_file_path}: {e}\\n{traceback.format_exc()}\")\n","        return {}\n","\n","# --- Helper Function for Phase 4: Auto-Derive Keyphrases ---\n","def auto_derive_keyphrases_from_llm_theme(theme_label: str, theme_explanation: str, num_keyphrases=5, logger_instance=None) -> List[str]:\n","    # ... (Full definition as provided previously, using logger_instance) ...\n","    # ... (Ensure nltk.corpus.stopwords can be imported or handle ImportError) ...\n","    if logger_instance: logger_instance.log(f\"DEBUG P4_AutoKeyphrase: Deriving for Label='{theme_label}', Expl='{theme_explanation[:50]}...'\")\n","    text_to_process = (theme_label.lower() + \" \" + theme_explanation.lower()).strip()\n","    text_to_process = re.sub(r'[^\\w\\s]', '', text_to_process); text_to_process = re.sub(r'\\d+', '', text_to_process)\n","    words = text_to_process.split()\n","    try:\n","        from nltk.corpus import stopwords # Ensure NLTK is downloaded in Cell 1 / Orchestrator\n","        stop_words = set(stopwords.words('english'))\n","        custom_generic_words = {\"theme\", \"label\", \"explanation\", \"point\", \"submission\", \"response\", \"excerpt\", \"several\", \"multiple\", \"various\", \"highlight\", \"discuss\", \"mention\", \"suggest\", \"argue\", \"concern\", \"need\", \"focus\", \"aspect\", \"also\", \"however\", \"therefore\",\"this\", \"that\", \"they\", \"are\", \"is\", \"and\", \"or\", \"the\", \"for\", \"with\", \"from\", \"based\", \"effectively\", \"overall\", \"across\", \"within\", \"regarding\", \"particularly\", \"especially\", \"specific\", \"general\", \"different\", \"various\", \"number\", \"part\"}\n","        stop_words.update(custom_generic_words)\n","        filtered_words = [word for word in words if word and word not in stop_words and len(word) > 2]\n","    except ImportError:\n","        if logger_instance: logger_instance.log(\"WARN P4_AutoKeyphrase: NLTK stopwords not available for keyphrase derivation. Results may be less optimal.\")\n","        filtered_words = [word for word in words if len(word) > 2] # Basic filter if stopwords fail\n","\n","    if not filtered_words:\n","        if logger_instance: logger_instance.log(f\"WARN P4_AutoKeyphrase: No significant words after stopword removal for '{theme_label}'. Using raw words from label if possible.\")\n","        # Fallback to words from label only if explanation yields nothing\n","        label_words_only = theme_label.lower().split()\n","        filtered_label_words = [word for word in label_words_only if word and word not in (stop_words if 'stop_words' in locals() else set()) and len(word)>2]\n","        if filtered_label_words:\n","            counts = Counter(filtered_label_words)\n","            derived_keyphrases = [word for word, count in counts.most_common(num_keyphrases)]\n","        else: # Absolute fallback if label also yields nothing\n","            counts = Counter(words)\n","            derived_keyphrases = [word for word, count in counts.most_common(num_keyphrases * 2) if len(word) > 2][:num_keyphrases]\n","    else:\n","        word_counts = Counter(filtered_words)\n","        derived_keyphrases = [word for word, count in word_counts.most_common(num_keyphrases)]\n","\n","    if not derived_keyphrases and theme_label: # If still no keyphrases, try to take 1-2 significant words from label\n","        label_parts = [lp for lp in theme_label.lower().split() if len(lp) > 3 and lp not in (stop_words if 'stop_words' in locals() else set())]\n","        if label_parts: derived_keyphrases = label_parts[:2]\n","\n","\n","    if logger_instance: logger_instance.log(f\"DEBUG P4_AutoKeyphrase: Derived for '{theme_label}': {derived_keyphrases}\")\n","    return derived_keyphrases\n","\n","\n","# --- Main Quantification and Quoting Function for Phase 4 ---\n","def count_llm_themes_and_extract_quotes_p4_auto(\n","    # ... (arguments as before) ...\n","    llm_themes_by_qid: Dict[str, Any],\n","    collated_source_texts_data: Dict[str, Any],\n","    human_keyphrases_override_by_qid: Dict[str, Dict[str, List[str]]],\n","    num_quotes_per_theme: int,\n","    logger_instance\n",") -> Dict[str, Any]:\n","    # ... (Full definition of this function as provided in the previous \"auto-keyphrase\" version) ...\n","    # ... (It should call auto_derive_keyphrases_from_llm_theme) ...\n","    final_report_data_p4 = {}\n","    source_texts_for_all_qids = collated_source_texts_data.get(\"aggregated_pdf_content_by_qid\", {})\n","    for qid, p3_qid_analysis_data in llm_themes_by_qid.items():\n","        logger_instance.log(f\"INFO P4_Auto: Processing QID: {qid} for automated quantification.\")\n","        question_text = p3_qid_analysis_data.get(\"question_text\", f\"Q text N/A for {qid}\")\n","        llm_identified_themes_p3 = p3_qid_analysis_data.get(\"parsed_top_themes\", [])\n","        final_report_data_p4[qid] = {\n","            \"question_text\": question_text, \"llm_generated_themes_from_p3\": llm_identified_themes_p3,\n","            \"status_from_p3\": p3_qid_analysis_data.get(\"status\", \"N/A\"), \"quantified_points\": [] }\n","        source_text_items_for_this_qid = source_texts_for_all_qids.get(qid, [])\n","        if not source_text_items_for_this_qid: logger_instance.log(f\"  WARN P4_Auto: No source texts for QID {qid}.\"); continue\n","        qid_human_keyphrases_override = human_keyphrases_override_by_qid.get(qid, {})\n","        for llm_theme_obj in llm_identified_themes_p3:\n","            if not isinstance(llm_theme_obj, dict) or llm_theme_obj.get(\"theme_label\",\"\").lower().startswith((\"llm error\", \"parsing/llm error\", \"unparsed\", \"valid json\")):\n","                logger_instance.log(f\"  WARN P4_Auto: Skipping malformed LLM theme for QID {qid}: {llm_theme_obj}\"); continue\n","            llm_theme_label = llm_theme_obj.get(\"theme_label\", \"Unknown LLM Theme\"); llm_theme_explanation = llm_theme_obj.get(\"explanation\", \"\")\n","            keyphrases_to_use = []; source_of_keyphrases = \"Auto-derived from LLM theme\"\n","            if llm_theme_label in qid_human_keyphrases_override: # Check if human override for this exact LLM label\n","                keyphrases_to_use = qid_human_keyphrases_override[llm_theme_label]; source_of_keyphrases = \"Human-defined override for LLM Label\"\n","                logger_instance.log(f\"  INFO P4_Auto: QID {qid} - Using human-defined keyphrases for LLM theme: '{llm_theme_label}'\")\n","            elif qid_human_keyphrases_override and not any(llm_theme_label in k for k in qid_human_keyphrases_override.keys()):\n","                # If there are human keyphrases for the QID, but NOT for this specific LLM theme label,\n","                # it implies the human might have renamed/merged themes.\n","                # In a purely automated first pass, we might still auto-derive for this LLM theme,\n","                # or decide to only quantify themes present in the human keyphrase file.\n","                # For now, let's auto-derive if no exact label match in human file.\n","                keyphrases_to_use = auto_derive_keyphrases_from_llm_theme(llm_theme_label, llm_theme_explanation, logger_instance=logger_instance)\n","            else: # No human keyphrases for this QID at all, or no match for this specific LLM theme label\n","                keyphrases_to_use = auto_derive_keyphrases_from_llm_theme(llm_theme_label, llm_theme_explanation, logger_instance=logger_instance)\n","\n","            if not keyphrases_to_use:\n","                logger_instance.log(f\"  WARN P4_Auto: QID {qid} - Could not derive/find keyphrases for LLM theme: '{llm_theme_label}'. Count will be 0.\")\n","                final_report_data_p4[qid][\"quantified_points\"].append({ \"point_label\": llm_theme_label + \" (Auto-Keyphrased)\", \"llm_explanation\": llm_theme_explanation,\n","                    \"keyphrases_used_for_counting\": [\"N/A - Auto-derivation failed\"], \"source_of_keyphrases\": source_of_keyphrases,\n","                    \"unique_response_count\": 0, \"example_quotes\": [] }); continue\n","\n","            logger_instance.log(f\"  P4_Auto Counting for QID {qid} - LLM Theme: '{llm_theme_label}' using phrases: {keyphrases_to_use} (Source: {source_of_keyphrases})\")\n","            point_matching_response_ids = set(); point_potential_quotes_with_context = []\n","            for source_item in source_text_items_for_this_qid:\n","                item_text_content = source_item.get(\"text\", \"\"); item_text_lower = item_text_content.lower()\n","                response_id = source_item.get(\"response_id\")\n","                if not item_text_content or not response_id: continue\n","                for phrase in keyphrases_to_use:\n","                    if re.search(r'\\b' + re.escape(phrase.lower()) + r'\\b', item_text_lower):\n","                        point_matching_response_ids.add(response_id)\n","                        point_potential_quotes_with_context.append({ \"response_id\": response_id, \"full_text_item\": item_text_content,\n","                            \"source_document_type\": source_item.get(\"type\"), \"matched_phrase\": phrase }); break\n","            unique_response_count_for_point = len(point_matching_response_ids); selected_quotes_p4 = []\n","            if unique_response_count_for_point > 0 and point_potential_quotes_with_context:\n","                quotes_extracted_so_far = 0; shuffled_potential_quotes = sorted(point_potential_quotes_with_context, key=lambda x: x.get(\"response_id\",\"\"))\n","                seen_responses_for_this_point_quotes = set()\n","                for quote_candidate_info in shuffled_potential_quotes:\n","                    if quotes_extracted_so_far >= num_quotes_per_theme: break\n","                    current_response_id = quote_candidate_info[\"response_id\"]\n","                    if current_response_id in seen_responses_for_this_point_quotes and quotes_extracted_so_far >= len(point_matching_response_ids): pass\n","                    elif current_response_id in seen_responses_for_this_point_quotes: continue\n","                    full_text = quote_candidate_info[\"full_text_item\"]; phrase_to_find_in_sentence = quote_candidate_info[\"matched_phrase\"]\n","                    try: sentences = nltk.sent_tokenize(full_text)\n","                    except: sentences = [full_text]\n","                    for sent_idx, sentence in enumerate(sentences):\n","                        if re.search(r'\\b' + re.escape(phrase_to_find_in_sentence.lower()) + r'\\b', sentence.lower()):\n","                            context_start = max(0, sent_idx - 1); context_end = min(len(sentences), sent_idx + 2)\n","                            context_snippet = \" \".join(sentences[context_start:context_end]).strip()\n","                            selected_quotes_p4.append({ \"response_id\": current_response_id, \"source_document_type\": quote_candidate_info[\"source_document_type\"],\n","                                                        \"matched_phrase\": phrase_to_find_in_sentence, \"quote\": sentence.strip(), \"broader_context\": context_snippet })\n","                            quotes_extracted_so_far += 1\n","                            if current_response_id: seen_responses_for_this_point_quotes.add(current_response_id)\n","                            break\n","                    if quotes_extracted_so_far >= num_quotes_per_theme: break\n","            final_report_data_p4[qid][\"quantified_points\"].append({\n","                \"point_label\": llm_theme_label + (\" (Human Override)\" if source_of_keyphrases.startswith(\"Human\") else \" (Auto-Keyphrased)\"),\n","                \"llm_explanation\": llm_theme_explanation,\n","                \"keyphrases_used_for_counting\": keyphrases_to_use, \"source_of_keyphrases\": source_of_keyphrases,\n","                \"unique_response_count\": unique_response_count_for_point, \"example_quotes\": selected_quotes_p4 })\n","            logger_instance.log(f\"    P4_Auto QID {qid} - LLM Theme '{llm_theme_label}': Count {unique_response_count_for_point}. Quotes: {len(selected_quotes_p4)}.\")\n","    return final_report_data_p4\n","\n","\n","# --- Main Execution Function for Phase 4 ---\n","# THIS FUNCTION IS CALLED BY CELL 7 (ORCHESTRATION)\n","def run_phase4_theme_quantification_and_quoting(\n","    logger_p4_instance,\n","    p3_llm_themes_output_file: str,\n","    p2_collated_texts_output_file: str,\n","    human_keyphrases_file: str, # This is the path to P4_HUMAN_DEFINED_KEYPHRASES_FILE\n","    final_analysis_output_file: str,\n","    num_quotes_config: int\n","    ):\n","    logger_p4 = logger_p4_instance # Use passed logger\n","    logger_p4.log(\"=\" * 50); logger_p4.log(\" INFO: Starting Phase 4: Theme Auto-Quantification & Quoting\"); logger_p4.log(\"=\" * 50)\n","\n","    # 1. Load LLM-generated themes from Phase 3\n","    llm_themes_data_from_p3 = {}\n","    try:\n","        logger_p4.log(f\"INFO P4: Loading LLM themes from: {p3_llm_themes_output_file}\")\n","        with open(p3_llm_themes_output_file, 'r', encoding='utf-8') as f:\n","            llm_themes_data_from_p3 = json.load(f)\n","        if not llm_themes_data_from_p3 or not isinstance(llm_themes_data_from_p3, dict):\n","             logger_p4.log(f\"ERROR P4: Phase 3 output file {p3_llm_themes_output_file} empty/not dict.\"); return False\n","        logger_p4.log(f\"INFO P4: Loaded {len(llm_themes_data_from_p3)} QIDs with LLM themes from {p3_llm_themes_output_file}\")\n","    except FileNotFoundError:\n","        logger_p4.log(f\"ERROR P4: Phase 3 output file {p3_llm_themes_output_file} not found. Run Phase 3.\"); return False\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to load LLM themes: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    # 2. Load collated source texts from Phase 2\n","    collated_texts_data_from_p2 = {}\n","    try:\n","        logger_p4.log(f\"INFO P4: Loading collated texts from: {p2_collated_texts_output_file}\")\n","        with open(p2_collated_texts_output_file, 'r', encoding='utf-8') as f:\n","            collated_texts_data_from_p2 = json.load(f)\n","        if not collated_texts_data_from_p2 or not isinstance(collated_texts_data_from_p2.get(\"aggregated_pdf_content_by_qid\"), dict):\n","            logger_p4.log(f\"ERROR P4: Phase 2 output file {p2_collated_texts_output_file} empty/malformed.\"); return False\n","        logger_p4.log(f\"INFO P4: Loaded collated texts.\")\n","    except FileNotFoundError:\n","        logger_p4.log(f\"ERROR P4: Phase 2 collated texts file {p2_collated_texts_output_file} not found. Run Phase 2.\"); return False\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to load collated texts: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    # 3. Load human-defined keyphrases\n","    # The human_keyphrases_file path is P4_HUMAN_DEFINED_KEYPHRASES_FILE from global config\n","    human_keyphrases_override = load_human_defined_keyphrases_p4(human_keyphrases_file, logger_p4)\n","    # No error if empty, function handles it.\n","\n","    # 4. Perform counting and quote extraction using the \"auto\" version which prioritizes human overrides\n","    final_report_content = count_llm_themes_and_extract_quotes_p4_auto(\n","        llm_themes_data_from_p3,\n","        collated_texts_data_from_p2,\n","        human_keyphrases_override,\n","        num_quotes_config,\n","        logger_p4\n","    )\n","\n","    # 5. Save the final analysis\n","    try:\n","        logger_p4.log(f\"INFO P4: Saving final thematic report to {final_analysis_output_file}\")\n","        with open(final_analysis_output_file, 'w', encoding='utf-8') as f:\n","            json.dump(final_report_content, f, indent=2, default=logger_p4._get_serializer()) # Use serializer\n","        logger_p4.log(f\"INFO P4: Successfully saved final thematic report.\")\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to save final analysis: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    logger_p4.log(\"=\" * 50); logger_p4.log(\" INFO: Phase 4 Finished \"); logger_p4.log(\"=\" * 50)\n","    return True\n","\n","# This print statement confirms this cell (Cell 6) has defined its functions.\n","# The actual execution is triggered by Cell 7 (Orchestration).\n","print(\"INFO: Cell 6 - Phase 4 logic defined (with load_human_defined_keyphrases_p4 and auto-keyphrase logic).\")"]},{"cell_type":"markdown","metadata":{"id":"6-BSdgrLXnB1"},"source":["# 23rd May"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOSPvF3tp4HP"},"outputs":[],"source":["# @title Cell 6: Phase 4 - Theme Auto-Quantification & Quotation (from LLM Themes)\n","\n","# logger_p4 = FileLogger(P4_LOG_FILE) # Initialized by orchestration\n","# At the top of Cell 6, or in Cell 7 before calling Phase 4\n","try:\n","    print(f\"DEBUG: P4_INPUT_LLM_THEMES_JSON is defined as: {P4_INPUT_LLM_THEMES_JSON}\")\n","    print(f\"DEBUG: P4_INPUT_COLLATED_TEXTS_JSON is defined as: {P4_INPUT_COLLATED_TEXTS_JSON}\")\n","    print(f\"DEBUG: P4_HUMAN_DEFINED_KEYPHRASES_FILE is defined as: {P4_HUMAN_DEFINED_KEYPHRASES_FILE}\")\n","except NameError as e:\n","    print(f\"DEBUG: NameError - A config variable for Phase 4 is not defined: {e}\")\n","\n","# NEW Helper: Auto-derive keyphrases from LLM theme label/explanation\n","def auto_derive_keyphrases_from_llm_theme(theme_label: str, theme_explanation: str, num_keyphrases=5, logger_instance=None) -> List[str]:\n","    \"\"\"\n","    Derives simple keyphrases from an LLM theme label and explanation.\n","    This is a VERY basic approach. More sophisticated NLP could be used.\n","    \"\"\"\n","    if logger_instance: logger_instance.log(f\"DEBUG P4_AutoKeyphrase: Deriving for Label='{theme_label}', Expl='{theme_explanation[:50]}...'\")\n","\n","    text_to_process = (theme_label.lower() + \" \" + theme_explanation.lower()).strip()\n","\n","    # Remove punctuation, numbers (basic cleaning)\n","    text_to_process = re.sub(r'[^\\w\\s]', '', text_to_process)\n","    text_to_process = re.sub(r'\\d+', '', text_to_process)\n","\n","    words = text_to_process.split()\n","\n","    # Remove common English stopwords (ensure nltk.corpus.stopwords is downloaded)\n","    try:\n","        from nltk.corpus import stopwords\n","        stop_words = set(stopwords.words('english'))\n","        # Add some very generic words that might appear in LLM explanations but aren't good keyphrases\n","        custom_generic_words = {\"theme\", \"label\", \"explanation\", \"point\", \"submission\", \"response\", \"excerpt\",\n","                                \"several\", \"multiple\", \"various\", \"highlight\", \"discuss\", \"mention\", \"suggest\",\n","                                \"argue\", \"concern\", \"need\", \"focus\", \"aspect\", \"also\", \"however\", \"therefore\",\n","                                \"this\", \"that\", \"they\", \"are\", \"is\", \"and\", \"or\", \"the\", \"for\", \"with\", \"from\", \"based\"}\n","        stop_words.update(custom_generic_words)\n","\n","        filtered_words = [word for word in words if word and word not in stop_words and len(word) > 2]\n","    except ImportError:\n","        if logger_instance: logger_instance.log(\"WARN P4_AutoKeyphrase: NLTK stopwords not available for keyphrase derivation.\")\n","        filtered_words = [word for word in words if len(word) > 2] # Basic filter if stopwords fail\n","\n","    if not filtered_words:\n","        # Fallback: if all words are stopwords, just take the most frequent from original label/explanation (less good)\n","        if logger_instance: logger_instance.log(f\"WARN P4_AutoKeyphrase: No significant words after stopword removal for '{theme_label}'. Using raw words.\")\n","        counts = Counter(words)\n","        # Take up to num_keyphrases most common words (at least 3 chars long) from original text\n","        derived_keyphrases = [word for word, count in counts.most_common(num_keyphrases * 2) if len(word) > 2][:num_keyphrases]\n","\n","    else:\n","        # Get most frequent remaining words as keyphrases\n","        word_counts = Counter(filtered_words)\n","        derived_keyphrases = [word for word, count in word_counts.most_common(num_keyphrases)]\n","\n","    if logger_instance: logger_instance.log(f\"DEBUG P4_AutoKeyphrase: Derived for '{theme_label}': {derived_keyphrases}\")\n","    return derived_keyphrases\n","\n","\n","def count_llm_themes_and_extract_quotes_p4_auto( # Renamed function\n","    llm_themes_by_qid: Dict[str, Any],\n","    collated_source_texts_data: Dict[str, Any],\n","    # human_keyphrases_by_qid is no longer the primary driver for this automated pass, but can be an override\n","    human_keyphrases_override_by_qid: Dict[str, Dict[str, List[str]]],\n","    num_quotes_per_theme: int,\n","    logger_instance\n",") -> Dict[str, Any]:\n","    final_report_data_p4 = {}\n","    source_texts_for_all_qids = collated_source_texts_data.get(\"aggregated_pdf_content_by_qid\", {})\n","\n","    for qid, p3_qid_analysis_data in llm_themes_by_qid.items():\n","        logger_instance.log(f\"INFO P4_Auto: Processing QID: {qid} for automated quantification.\")\n","\n","        question_text = p3_qid_analysis_data.get(\"question_text\", f\"Q text N/A for {qid}\")\n","        llm_identified_themes_p3 = p3_qid_analysis_data.get(\"parsed_top_themes\", []) # List of {\"theme_label\": ..., \"explanation\": ...}\n","\n","        final_report_data_p4[qid] = {\n","            \"question_text\": question_text,\n","            \"llm_generated_themes_from_p3\": llm_identified_themes_p3, # Keep for reference\n","            \"status_from_p3\": p3_qid_analysis_data.get(\"status\", \"N/A\"),\n","            \"quantified_points\": []\n","        }\n","\n","        source_text_items_for_this_qid = source_texts_for_all_qids.get(qid, [])\n","        if not source_text_items_for_this_qid:\n","            logger_instance.log(f\"  WARN P4_Auto: No source texts for QID {qid}.\"); continue\n","\n","        # Check if human-defined keyphrases exist for this QID as an override\n","        qid_human_keyphrases_override = human_keyphrases_override_by_qid.get(qid, {})\n","\n","        for llm_theme_obj in llm_identified_themes_p3:\n","            if not isinstance(llm_theme_obj, dict) or \\\n","               llm_theme_obj.get(\"theme_label\",\"\").lower().startswith((\"llm error\", \"parsing/llm error\", \"unparsed\", \"valid json\")):\n","                logger_instance.log(f\"  WARN P4_Auto: Skipping malformed or error LLM theme object for QID {qid}: {llm_theme_obj}\")\n","                continue\n","\n","            llm_theme_label = llm_theme_obj.get(\"theme_label\", \"Unknown LLM Theme\")\n","            llm_theme_explanation = llm_theme_obj.get(\"explanation\", \"\")\n","\n","            keyphrases_to_use = []\n","            source_of_keyphrases = \"Auto-derived from LLM theme\"\n","\n","            # Check if there's a human override for this exact LLM theme label\n","            if llm_theme_label in qid_human_keyphrases_override:\n","                keyphrases_to_use = qid_human_keyphrases_override[llm_theme_label]\n","                source_of_keyphrases = \"Human-defined override\"\n","                logger_instance.log(f\"  INFO P4_Auto: QID {qid} - Using human-defined keyphrases for LLM theme: '{llm_theme_label}'\")\n","            else:\n","                keyphrases_to_use = auto_derive_keyphrases_from_llm_theme(llm_theme_label, llm_theme_explanation, logger_instance=logger_instance)\n","\n","            if not keyphrases_to_use:\n","                logger_instance.log(f\"  WARN P4_Auto: QID {qid} - Could not derive/find keyphrases for LLM theme: '{llm_theme_label}'. Count will be 0.\")\n","                final_report_data_p4[qid][\"quantified_points\"].append({\n","                    \"point_label\": llm_theme_label + \" (Auto-Keyphrases)\",\n","                    \"llm_explanation\": llm_theme_explanation,\n","                    \"keyphrases_used_for_counting\": [\"N/A - Auto-derivation failed\"],\n","                    \"source_of_keyphrases\": source_of_keyphrases,\n","                    \"unique_response_count\": 0, \"example_quotes\": [] })\n","                continue\n","\n","            logger_instance.log(f\"  P4_Auto Counting for QID {qid} - LLM Theme: '{llm_theme_label}' using phrases: {keyphrases_to_use}\")\n","\n","            point_matching_response_ids = set()\n","            point_potential_quotes_with_context = []\n","            # ... (The rest of the counting and quote extraction logic is the same as in the previous count_themes_and_extract_quotes_p4)\n","            # ... (It will use `keyphrases_to_use`)\n","            for source_item in source_text_items_for_this_qid:\n","                item_text_content = source_item.get(\"text\", \"\"); item_text_lower = item_text_content.lower()\n","                response_id = source_item.get(\"response_id\")\n","                if not item_text_content or not response_id: continue\n","                for phrase in keyphrases_to_use:\n","                    if re.search(r'\\b' + re.escape(phrase.lower()) + r'\\b', item_text_lower):\n","                        point_matching_response_ids.add(response_id)\n","                        point_potential_quotes_with_context.append({\n","                            \"response_id\": response_id, \"full_text_item\": item_text_content,\n","                            \"source_document_type\": source_item.get(\"type\"), \"matched_phrase\": phrase })\n","                        break\n","            unique_response_count_for_point = len(point_matching_response_ids)\n","            selected_quotes_p4 = []\n","            if unique_response_count_for_point > 0 and point_potential_quotes_with_context:\n","                # ... (Quote selection logic from previous version - can be copied directly) ...\n","                quotes_extracted_so_far = 0\n","                shuffled_potential_quotes = sorted(point_potential_quotes_with_context, key=lambda x: x.get(\"response_id\",\"\"))\n","                seen_responses_for_this_point_quotes = set()\n","                for quote_candidate_info in shuffled_potential_quotes:\n","                    if quotes_extracted_so_far >= num_quotes_per_theme: break\n","                    current_response_id = quote_candidate_info[\"response_id\"]\n","                    if current_response_id in seen_responses_for_this_point_quotes and quotes_extracted_so_far >= len(point_matching_response_ids): pass\n","                    elif current_response_id in seen_responses_for_this_point_quotes: continue\n","                    full_text = quote_candidate_info[\"full_text_item\"]; phrase_to_find_in_sentence = quote_candidate_info[\"matched_phrase\"]\n","                    try: sentences = nltk.sent_tokenize(full_text)\n","                    except: sentences = [full_text]\n","                    for sent_idx, sentence in enumerate(sentences):\n","                        if re.search(r'\\b' + re.escape(phrase_to_find_in_sentence.lower()) + r'\\b', sentence.lower()):\n","                            context_start = max(0, sent_idx - 1); context_end = min(len(sentences), sent_idx + 2)\n","                            context_snippet = \" \".join(sentences[context_start:context_end]).strip()\n","                            selected_quotes_p4.append({ \"response_id\": current_response_id, \"source_document_type\": quote_candidate_info[\"source_document_type\"],\n","                                                        \"matched_phrase\": phrase_to_find_in_sentence, \"quote\": sentence.strip(), \"broader_context\": context_snippet })\n","                            quotes_extracted_so_far += 1\n","                            if current_response_id: seen_responses_for_this_point_quotes.add(current_response_id)\n","                            break\n","                    if quotes_extracted_so_far >= num_quotes_per_theme: break\n","\n","            final_report_data_p4[qid][\"quantified_points\"].append({\n","                \"point_label\": llm_theme_label + \" (Auto-Keyphrased)\",\n","                \"llm_explanation\": llm_theme_explanation,\n","                \"keyphrases_used_for_counting\": keyphrases_to_use,\n","                \"source_of_keyphrases\": source_of_keyphrases,\n","                \"unique_response_count\": unique_response_count_for_point,\n","                \"example_quotes\": selected_quotes_p4\n","            })\n","            logger_instance.log(f\"    P4_Auto QID {qid} - LLM Theme '{llm_theme_label}': Count {unique_response_count_for_point}. Quotes: {len(selected_quotes_p4)}.\")\n","\n","    return final_report_data_p4\n","\n","def run_phase4_theme_quantification_and_quoting(\n","    logger_p4_instance,\n","    p3_llm_themes_output_file: str,    # << NEW ARG\n","    p2_collated_texts_output_file: str, # << NEW ARG\n","    human_keyphrases_file: str,        # << NEW ARG\n","    final_analysis_output_file: str,   # << NEW ARG\n","    num_quotes_config: int             # << NEW ARG for P4_NUM_EXAMPLE_QUOTES_PER_THEME\n","    ):\n","    logger_p4 = logger_p4_instance\n","    logger_p4.log(\"=\" * 50); logger_p4.log(\" INFO: Starting Phase 4: Theme Auto-Quantification & Quoting\"); logger_p4.log(\"=\" * 50)\n","\n","    # 1. Load LLM-generated themes from Phase 3\n","    llm_themes_data_from_p3 = {}\n","    try:\n","        logger_p4.log(f\"INFO P4: Loading LLM themes from: {p3_llm_themes_output_file}\") # Use arg\n","        with open(p3_llm_themes_output_file, 'r', encoding='utf-8') as f:\n","            llm_themes_data_from_p3 = json.load(f)\n","        if not llm_themes_data_from_p3 or not isinstance(llm_themes_data_from_p3, dict):\n","             logger_p4.log(f\"ERROR P4: Phase 3 output file {p3_llm_themes_output_file} is empty or not a dict.\"); return False\n","        logger_p4.log(f\"INFO P4: Loaded {len(llm_themes_data_from_p3)} QIDs with LLM themes.\")\n","    except FileNotFoundError:\n","        logger_p4.log(f\"ERROR P4: Phase 3 output file {p3_llm_themes_output_file} not found. Run Phase 3 first.\"); return False\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to load LLM themes: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    # 2. Load collated source texts from Phase 2\n","    collated_texts_data_from_p2 = {}\n","    try:\n","        logger_p4.log(f\"INFO P4: Loading collated texts from: {p2_collated_texts_output_file}\") # Use arg\n","        with open(p2_collated_texts_output_file, 'r', encoding='utf-8') as f:\n","            collated_texts_data_from_p2 = json.load(f)\n","        if not collated_texts_data_from_p2 or not isinstance(collated_texts_data_from_p2.get(\"aggregated_pdf_content_by_qid\"), dict):\n","            logger_p4.log(f\"ERROR P4: Phase 2 output file {p2_collated_texts_output_file} is empty or malformed.\"); return False\n","        logger_p4.log(f\"INFO P4: Loaded collated texts.\")\n","    except FileNotFoundError:\n","        logger_p4.log(f\"ERROR P4: Phase 2 collated texts file {p2_collated_texts_output_file} not found. Run Phase 2 first.\"); return False\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to load collated texts: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    # 3. Load human-defined keyphrases\n","    human_keyphrases = load_human_defined_keyphrases_p4(human_keyphrases_file, logger_p4) # Use arg\n","    if not human_keyphrases:\n","        logger_p4.log(f\"WARN P4: {human_keyphrases_file} was empty or not found. Quantification will be based on LLM themes only (counts will be 0 unless keyphrases are added).\")\n","\n","    # 4. Perform counting and quote extraction\n","    final_report_content = count_llm_themes_and_extract_quotes_p4_auto(\n","        llm_themes_data_from_p3,\n","        collated_texts_data_from_p2,\n","        human_keyphrases,\n","        num_quotes_config, # Use arg\n","        logger_p4\n","    )\n","\n","    # 5. Save the final analysis\n","    try:\n","        logger_p4.log(f\"INFO P4: Saving final thematic report to {final_analysis_output_file}\") # Use arg\n","        with open(final_analysis_output_file, 'w', encoding='utf-8') as f:\n","            json.dump(final_report_content, f, indent=2, default=logger_p4._get_serializer())\n","        logger_p4.log(f\"INFO P4: Successfully saved final thematic report.\")\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to save final analysis: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    logger_p4.log(\"=\" * 50); logger_p4.log(\" INFO: Phase 4 Finished \"); logger_p4.log(\"=\" * 50)\n","    return True\n","\n","print(\"INFO: Cell 6 - Phase 4 logic defined (functions accept file paths as args).\")\n","\n","print(\"INFO: Cell 6 - Phase 4 logic (Automated Keyphrase Pass) defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbWogtHWXpSL"},"outputs":[],"source":["# @title AGD Staffing Ratios: Trends from 2014-15 to 2024-25\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as mtick\n","\n","# Data\n","years = ['2014-15', '2015-16', '2016-17', '2017-18', '2018-19', '2019-20',\n","         '2020-21', '2021-22', '2022-23', '2023-24 (Rev)', '2024-25 (Est)']\n","\n","ratio1 = [0.990, 1.230, 1.110, 0.964, 0.746, 0.802, 0.983, 1.065, 1.135, 1.076, 0.963] # AGD / Broader APS\n","ratio2 = [11.803, 15.815, 14.786, 31.038, 23.515, 35.502, 28.293, 15.634, 15.894, 15.631, 13.939] # AGD / Other AG Portfolio\n","ratio3 = [37.663, 48.788, 43.631, 38.449, 29.642, 31.936, 48.250, 49.125, 50.812, 45.411, 41.611] # AGD / Central Agencies\n","\n","# Plotting\n","plt.figure(figsize=(14, 8))\n","\n","plt.plot(years, ratio1, marker='o', linestyle='-', label='Ratio #1: AGD / (Broader APS excl. AGD & mil/res)')\n","plt.plot(years, ratio2, marker='s', linestyle='--', label='Ratio #2: AGD / Other AG Portfolio Agencies')\n","plt.plot(years, ratio3, marker='^', linestyle='-.', label='Ratio #3: AGD / Sum of Other Central Agencies')\n","\n","# Adding titles and labels\n","plt.title('AGD Staffing Ratios: Trends from 2014-15 to 2024-25', fontsize=16)\n","plt.xlabel('Financial Year', fontsize=12)\n","plt.ylabel('Ratio (%)', fontsize=12)\n","plt.xticks(rotation=45, ha=\"right\")\n","plt.yticks(fontsize=10)\n","plt.grid(True, linestyle='--', alpha=0.7)\n","plt.legend(fontsize=10)\n","plt.tight_layout()\n","\n","# Format Y-axis as percentage\n","ax = plt.gca()\n","ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n","\n","# Show plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sUUNNN-UTiZq"},"source":["# 22nd May"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JwgZMtZ0HnF6"},"outputs":[],"source":["# @title Cell 8: Stakeholder Report Generation\n","import json\n","import os\n","import time\n","\n","# --- Configuration for Stakeholder Report Generation ---\n","# !!! UPDATE THESE PATHS !!!\n","PHASE4_FINAL_REPORT_JSON_PATH = '/content/drive/MyDrive/Colab Notebooks/Legal/Phase4_Final_Thematic_Analysis/phase4_final_thematic_report_20250523_070820.json' # Input from your Phase 4\n","STAKEHOLDER_REPORT_OUTPUT_TXT_PATH = f'/content/drive/MyDrive/Colab Notebooks/Legal/stakeholder_thematic_summary_report_{time.strftime(\"%Y%m%d_%H%M%S\")}.txt'\n","\n","NUM_TOP_POINTS_TO_DISPLAY = 3 # How many top points to show per question\n","SHOW_LLM_SUGGESTED_THEMES_WITHOUT_COUNTS = True # If True, also lists LLM themes that haven't been quantified yet\n","\n","def generate_stakeholder_report(phase4_data_file: str, output_report_file: str):\n","    \"\"\"\n","    Reads the Phase 4 final analysis JSON and generates a human-readable\n","    summary report for stakeholder communication.\n","    \"\"\"\n","    report_lines = []\n","    report_lines.append(\"=\" * 80)\n","    report_lines.append(\"STAKEHOLDER COMMUNICATION: SUMMARY OF THEMATIC ANALYSIS FROM SUBMISSIONS\")\n","    report_lines.append(\"=\" * 80)\n","    report_lines.append(f\"Report generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n","    report_lines.append(f\"Source data: {os.path.basename(phase4_data_file)}\\n\")\n","\n","    try:\n","        with open(phase4_data_file, 'r', encoding='utf-8') as f:\n","            phase4_data = json.load(f)\n","    except FileNotFoundError:\n","        report_lines.append(f\"ERROR: Phase 4 report file not found at {phase4_data_file}\")\n","        print(\"\\n\".join(report_lines))\n","        with open(output_report_file, 'w', encoding='utf-8') as f_out:\n","            f_out.write(\"\\n\".join(report_lines))\n","        return\n","    except json.JSONDecodeError:\n","        report_lines.append(f\"ERROR: Could not decode JSON from {phase4_data_file}\")\n","        print(\"\\n\".join(report_lines))\n","        with open(output_report_file, 'w', encoding='utf-8') as f_out:\n","            f_out.write(\"\\n\".join(report_lines))\n","        return\n","\n","    # Sort QIDs for consistent report order (optional, e.g., sort by Q number if QIDs are like \"Q1\", \"Q2\")\n","    sorted_qids = []\n","    try:\n","        # Attempt to sort numerically if QIDs are like \"Q1\", \"Q10\", \"Q2\"\n","        sorted_qids = sorted(phase4_data.keys(), key=lambda q: int(q[1:]) if q.startswith('Q') and q[1:].isdigit() else float('inf'))\n","    except:\n","        sorted_qids = sorted(phase4_data.keys()) # Fallback to alphabetical sort\n","\n","    for qid in sorted_qids:\n","        qid_data = phase4_data.get(qid, {})\n","        question_text = qid_data.get(\"question_text\", f\"Question text for {qid} not available.\")\n","\n","        report_lines.append(\"-\" * 70)\n","        report_lines.append(f\"\\nQUESTION {qid}: {question_text}\\n\")\n","\n","        quantified_points = qid_data.get(\"quantified_points\", [])\n","\n","        # Sort quantified points by unique_response_count in descending order\n","        # Ensure 'unique_response_count' exists and is an int for sorting\n","        valid_quantified_points = [\n","            pt for pt in quantified_points\n","            if isinstance(pt.get(\"unique_response_count\"), int) and pt.get(\"point_label\")\n","        ]\n","        sorted_points = sorted(valid_quantified_points, key=lambda x: x.get(\"unique_response_count\", 0), reverse=True)\n","\n","        if sorted_points:\n","            report_lines.append(\"  Most Common Points Raised (based on keyphrase matching in PDF submissions):\")\n","            for i, point_data in enumerate(sorted_points[:NUM_TOP_POINTS_TO_DISPLAY]):\n","                point_label = point_data.get(\"point_label_human_defined\", point_data.get(\"point_label\", \"N/A\"))\n","                count = point_data.get(\"unique_response_count\", 0)\n","                llm_explanation = point_data.get(\"llm_explanation_if_direct_match\", \"\") # Or from llm_generated_themes if linked\n","\n","                report_lines.append(f\"\\n    {i+1}. {point_label} (Mentioned by {count} unique responses)\")\n","                if llm_explanation and llm_explanation != \"This point label was human-defined/refined.\": # Show LLM explanation if available and relevant\n","                    report_lines.append(f\"       LLM Explanation: {llm_explanation}\")\n","\n","                example_quotes = point_data.get(\"example_quotes\", [])\n","                if example_quotes:\n","                    report_lines.append(\"       Example Quotes:\")\n","                    for quote_idx, quote_info in enumerate(example_quotes[:P4_NUM_EXAMPLE_QUOTES_PER_THEME]): # P4_NUM_EXAMPLE_QUOTES_PER_THEME from global config\n","                        quote_text = quote_info.get(\"quote_snippet\", quote_info.get(\"quote\", \"Quote not available\")).strip()\n","                        source_type = quote_info.get(\"source_document_type\", \"N/A\")\n","                        resp_id = quote_info.get(\"response_id\", \"N/A\")\n","                        report_lines.append(f\"         - \\\"{quote_text}\\\" (From Response ID: {resp_id}, Source Type: {source_type})\")\n","                else:\n","                    report_lines.append(\"       (No example quotes extracted for this point based on current keyphrases)\")\n","\n","            if len(sorted_points) > NUM_TOP_POINTS_TO_DISPLAY:\n","                report_lines.append(f\"\\n    (...and {len(sorted_points) - NUM_TOP_POINTS_TO_DISPLAY} other quantified points with lower counts.)\")\n","\n","        else:\n","            report_lines.append(\"  No points were quantified for this question based on the current keyphrase definitions.\")\n","\n","        if SHOW_LLM_SUGGESTED_THEMES_WITHOUT_COUNTS:\n","            llm_themes = qid_data.get(\"llm_generated_themes_from_p3\", [])\n","            unquantified_llm_themes = []\n","            quantified_labels = {pt.get(\"point_label_human_defined\", pt.get(\"point_label\")) for pt in sorted_points}\n","\n","            for llm_theme in llm_themes:\n","                llm_label = llm_theme.get(\"theme_label\")\n","                if llm_label and not llm_label.lower().startswith((\"llm error\", \"unparsed\", \"valid json\")):\n","                    # Check if this LLM theme (or a very similar one) was already covered by quantified_points\n","                    # This simple check assumes human labels in keyphrase file might match LLM labels.\n","                    # A more sophisticated check would involve semantic similarity or manual mapping.\n","                    if llm_label not in quantified_labels:\n","                         # Check if a human-defined point *derived* from this LLM theme already exists\n","                        is_covered = False\n","                        for qp_label in quantified_labels:\n","                            if llm_label in qp_label or qp_label in llm_label: # Simple substring check for relation\n","                                is_covered = True\n","                                break\n","                        if not is_covered:\n","                             unquantified_llm_themes.append(llm_theme)\n","\n","            if unquantified_llm_themes:\n","                report_lines.append(\"\\n  Further LLM-Suggested Themes (awaiting keyphrase definition for quantification):\")\n","                for theme in unquantified_llm_themes:\n","                    report_lines.append(f\"    - Label: {theme.get('theme_label')}\")\n","                    report_lines.append(f\"      Explanation: {theme.get('explanation')}\")\n","        report_lines.append(\"\") # Blank line for spacing\n","\n","    report_lines.append(\"=\" * 80)\n","    report_lines.append(\"END OF REPORT\")\n","    report_lines.append(\"=\" * 80)\n","\n","    final_report_str = \"\\n\".join(report_lines)\n","    print(final_report_str) # Print to console\n","\n","    try:\n","        with open(output_report_file, 'w', encoding='utf-8') as f_out:\n","            f_out.write(final_report_str)\n","        print(f\"\\nINFO: Stakeholder report saved to: {output_report_file}\")\n","    except Exception as e:\n","        print(f\"ERROR: Could not save stakeholder report to file: {e}\")\n","\n","# --- Example Usage (assuming this script is in its own cell or .py file) ---\n","if __name__ == \"__main__\":\n","    # Ensure the PHASE4_FINAL_REPORT_JSON_PATH and STAKEHOLDER_REPORT_OUTPUT_TXT_PATH are correctly set above.\n","    # You might also want to load P4_NUM_EXAMPLE_QUOTES_PER_THEME from your global config if it's defined there.\n","    # For this standalone script, let's use a default if it's not found in globals.\n","    try:\n","        num_quotes_config = P4_NUM_EXAMPLE_QUOTES_PER_THEME\n","    except NameError:\n","        print(\"WARN: P4_NUM_EXAMPLE_QUOTES_PER_THEME not found in global scope, defaulting to 2 for report.\")\n","        P4_NUM_EXAMPLE_QUOTES_PER_THEME = 2 # Default\n","\n","    if not os.path.exists(PHASE4_FINAL_REPORT_JSON_PATH):\n","        print(f\"ERROR: Input Phase 4 JSON report not found at '{PHASE4_FINAL_REPORT_JSON_PATH}'. Please check the path.\")\n","    else:\n","        generate_stakeholder_report(PHASE4_FINAL_REPORT_JSON_PATH, STAKEHOLDER_REPORT_OUTPUT_TXT_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nseQ5GkmTlWD"},"outputs":[],"source":["# @title Cell 2: Global Project Configuration\n","\n","import os\n","import time\n","\n","# --- Base Paths ---\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","PHASE1_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase1_PDF_Extraction_Outputs/')\n","PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n","PHASE3_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase3_LLM_Generated_Themes/')\n","PHASE4_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase4_Final_Thematic_Analysis/')\n","LOG_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Logs/')\n","for d_path in [PHASE1_OUTPUT_DIR, PHASE2_OUTPUT_DIR, PHASE3_OUTPUT_DIR, PHASE4_OUTPUT_DIR, LOG_DIR]:\n","    os.makedirs(d_path, exist_ok=True)\n","\n","# --- Common Input Files ---\n","ORIGINAL_RESPONSES_JSONL = os.path.join(BASE_PROJECT_DIR, 'processed_responses.jsonl')\n","QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","\n","# --- Phase 1: PDF Extraction & Per-Response Summarization ---\n","P1_LOG_FILE = os.path.join(LOG_DIR, \"phase1_pdf_extraction.log\")\n","P1_OUTPUT_FILE = os.path.join(PHASE1_OUTPUT_DIR, 'phase1_pdf_analysis_by_response.json') # Changed from _FULL\n","P1_STATE_FILE = os.path.join(PHASE1_OUTPUT_DIR, 'phase1_processing_state.json')\n","P1_EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n","P1_LLM_ID_FOR_SUMMARIES = 'google/gemma-3-4b-it' # Smaller, faster for many small summaries\n","P1_USE_QUANTIZATION = True # Recommended for 2b to save memory\n","P1_MIN_PDF_TEXT_LENGTH = 100\n","P1_MIN_SIMILARITY_SCORE = 0.5\n","P1_CONTEXT_WINDOW_SIZE_SENTENCES = 1\n","P1_TOP_K_PASSAGES = 3\n","P1_NUM_SUMMARY_SENTENCES_PER_RESPONSE_QID = 3 # For P1's summarizer prompt\n","P1_MIN_CHARS_FOR_SUMMARY = 150\n","P1_LLM_MAX_NEW_TOKENS_FOR_PER_RESPONSE_SUMMARY = 250 # For P1 summaries\n","P1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE = { # As defined before }\n","    \"Q1\": True, \"Q2\": True, \"Q3\": True, \"Q4\": True, \"Q5\": True, \"Q6\": True, \"Q7\": True,\n","    \"Q8\": True, \"Q9\": True, \"Q10\": True,\"Q11\": True, \"Q12\": True, \"Q13\": True, \"Q14\": True,\n","    \"Q15\": True, \"Q16\": False,\"Q17\": True, \"Q18\": True, \"Q19\": True,\"Q20\": False, \"Q21\": True,\n","    \"Q22\": True, \"Q23\": True, \"Q24\": True, \"Q25\": True, \"Q26\": True, \"Q27\": True, \"Q28\": True,\n","    \"Q29\": True, \"Q30\": True, \"Q31\": True,\"Q32\": False,\"Q33\": False,\"Q34\": False,\"Q35\": False,\n","    \"Q36\": False,\n","}\n","P1_MAX_RUNTIME_MINUTES = 60 * 8 # For a full run if needed\n","P1_SAVE_STATE_INTERVAL_SECONDS = 60 * 20\n","\n","# --- Phase 2: Collation of PDF Content ---\n","P2_LOG_FILE = os.path.join(LOG_DIR, f'phase2_text_collation_{time.strftime(\"%Y%m%d\")}.log')\n","P2_INPUT_FILE_OR_DIR = P1_OUTPUT_FILE\n","P2_OUTPUT_COLLATED_JSON = os.path.join(PHASE2_OUTPUT_DIR, f'phase2_collated_pdf_texts_{time.strftime(\"%Y%m%d_%H%M%S\")}.json')\n","\n","# --- Phase 3: LLM Thematic Point Generation ---\n","P3_LOG_FILE = os.path.join(LOG_DIR, f'phase3_llm_themes_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","P3_INPUT_COLLATED_JSON = P2_OUTPUT_COLLATED_JSON\n","P3_OUTPUT_LLM_THEMES_JSON = os.path.join(PHASE3_OUTPUT_DIR, f'phase3_llm_generated_themes_{time.strftime(\"%Y%m%d_%H%M%S\")}.json')\n","P3_LLM_ID_FOR_THEMES = \"Qwen/Qwen3-4B\" # <<<--- REVISED based on tests\n","P3_USE_QUANTIZATION = True                     # <<<--- REVISED based on tests\n","P3_NUM_THEMES_MAP_STR = \"up to 2\"              # <<<--- REVISED: String for map prompt\n","P3_NUM_THEMES_MAP_INT = 2                      # <<<--- REVISED: Integer for parsing map output\n","P3_NUM_THEMES_REDUCE_STR = \"top 3\"             # <<<--- REVISED: String for reduce prompt\n","P3_NUM_THEMES_REDUCE_INT = 3                   # <<<--- REVISED: Integer for parsing reduce output\n","P3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 3000 # Max content tokens for a batch of items\n","P3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 800     # <<<--- REVISED based on tests\n","P3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 1000 # <<<--- REVISED based on tests\n","P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\", \"Q16\"] # e.g. [\"Q1\", \"Q4\", \"Q16\"] for testing, None for all\n","\n","# --- Phase 4: Theme Validation, Quantification & Quotation Extraction ---\n","P4_LOG_FILE = os.path.join(LOG_DIR, f'phase4_theme_quantification_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","P4_INPUT_LLM_THEMES_JSON = P3_OUTPUT_LLM_THEMES_JSON\n","P4_INPUT_COLLATED_TEXTS_JSON = P2_OUTPUT_COLLATED_JSON\n","P4_OUTPUT_FINAL_ANALYSIS_JSON = os.path.join(PHASE4_OUTPUT_DIR, f'phase4_final_thematic_report_{time.strftime(\"%Y%m%d_%H%M%S\")}.json')\n","P4_HUMAN_DEFINED_KEYPHRASES_FILE = os.path.join(BASE_PROJECT_DIR, 'human_defined_keyphrases_for_themes.json')\n","P4_NUM_EXAMPLE_QUOTES_PER_THEME = 2\n","\n","# --- General ---\n","ENABLE_DETAILED_DEBUG_LOGGING = True # Keep False for full runs, True for deep debugging\n","\n","print(f\"INFO: Cell 2 - Global Configuration loaded. Base project directory: {BASE_PROJECT_DIR}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8LFYOznXA8f"},"outputs":[],"source":["# @title Cell 5: Phase 3 - LLM Thematic Point Generation (Map-Reduce)\n","\n","# logger_p3 = FileLogger(P3_LOG_FILE) # Will be initialized in Orchestration cell\n","\n","# --- Helper: Batch Individual Items (for Map step on list of texts) ---\n","def batch_text_items_p3(text_items_list: List[str], tokenizer_for_batching, max_tokens_per_batch: int, logger_instance):\n","    # ... (Same as batch_text_items_q1_test from successful Qwen test, using logger_instance)\n","    batches = []\n","    current_batch_item_texts = []\n","    current_batch_concatenated_text = \"\"\n","    logger_instance.log(f\"DEBUG P3_BATCHER: Batching {len(text_items_list)} items into max {max_tokens_per_batch} token batches.\")\n","    for item_idx, item_text_original in enumerate(text_items_list):\n","        if not item_text_original or not item_text_original.strip(): continue\n","        text_to_try_adding = item_text_original\n","        if current_batch_item_texts: text_to_try_adding = \"\\n\\n--- Next Excerpt ---\\n\" + item_text_original\n","        potential_full_batch_text = current_batch_concatenated_text + text_to_try_adding\n","        num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))\n","        if num_tokens <= max_tokens_per_batch:\n","            current_batch_item_texts.append(item_text_original)\n","            current_batch_concatenated_text = potential_full_batch_text\n","        else:\n","            if current_batch_item_texts: batches.append(current_batch_item_texts)\n","            single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))\n","            if single_item_tokens > max_tokens_per_batch:\n","                logger_instance.log(f\"  WARN P3_BATCHER: Item {item_idx} ({single_item_tokens} tokens) > limit ({max_tokens_per_batch}). Own oversized batch.\")\n","                batches.append([item_text_original])\n","                current_batch_item_texts = []\n","                current_batch_concatenated_text = \"\"\n","            else:\n","                current_batch_item_texts = [item_text_original]\n","                current_batch_concatenated_text = item_text_original\n","    if current_batch_item_texts: batches.append(current_batch_item_texts)\n","    logger_instance.log(f\"INFO P3_BATCHER: Created {len(batches)} batches of items.\")\n","    return batches\n","\n","# --- Helper: LLM Call for Thematic Points (JSON Output, Qwen2-NoThink adapted) ---\n","def get_llm_json_themes_p3(text_input_for_llm: str, question_text_for_prompt: str,\n","                           num_themes_instruction_str: str, # e.g., \"up to 2\" or \"top 3\"\n","                           llm_pipe: pipeline, max_new_tokens: int,\n","                           log_prefix_qid_step: str, map_reduce_step_type: str, logger_instance):\n","    # ... (Same as get_llm_json_themes_q1_qwen2_nothink_test, using logger_instance)\n","    # ... (Ensure prompt reflects \"MAP_ON_BATCHED_ITEMS\" (general items) or \"REDUCE_INTERMEDIATE_THEMES\")\n","    json_format_example = (\"{\\n  \\\"themes\\\": [\\n    {\\\"theme_label\\\": \\\"Label 1\\\", \\\"explanation\\\": \\\"Expl 1.\\\"},\\n\"\n","                           \"    {\\\"theme_label\\\": \\\"Label 2\\\", \\\"explanation\\\": \\\"Expl 2.\\\"}\\n  ]\\n}\")\n","    system_prompt = \"You are an expert policy analyst.\"\n","    user_content = \"\"\n","    if map_reduce_step_type == \"MAP_ON_BATCHED_ITEMS\":\n","        user_content = (\n","            f\"Carefully review the BATCH of excerpts/summaries from PDF submissions, responding to: '{question_text_for_prompt}'.\\n\"\n","            f\"Identify prominent and distinct points from THIS BATCH, {num_themes_instruction_str}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object with a 'themes' list, each theme having 'theme_label' and 'explanation'.\\n\"\n","            f\"Example JSON:\\n{json_format_example}\\n\\nBatch:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output:\")\n","    elif map_reduce_step_type == \"REDUCE_INTERMEDIATE_THEMES\":\n","        user_content = (\n","            f\"The following are preliminary themes/points from different responses to: '{question_text_for_prompt}'.\\n\"\n","            f\"Synthesize these into overall significant and distinct common themes, {num_themes_instruction_str}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object... (etc.)\\nExample JSON:\\n{json_format_example}\\n\\n\"\n","            f\"Preliminary Themes:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output (Synthesized {num_themes_instruction_str} Themes):\")\n","    else: # DIRECT_PROCESSING (if only one batch from Phase 2 items)\n","        user_content = (\n","            f\"Carefully review the following text (summaries/excerpts) from PDF submissions responding to: '{question_text_for_prompt}'.\\n\"\n","            f\"Identify common and distinct points, {num_themes_instruction_str}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object... (etc.)\\nExample JSON:\\n{json_format_example}\\n\\n\"\n","            f\"Provided Text:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output:\")\n","\n","    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_content}]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n","        terminators = [llm_pipe.tokenizer.eos_token_id, llm_pipe.tokenizer.convert_tokens_to_ids(\"<|im_end|>\")]\n","        valid_terminators = [t for t in terminators if t is not None]\n","        generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": True, \"temperature\": 0.7, \"top_p\": 0.8, \"top_k\": 20,\n","                           \"eos_token_id\": valid_terminators, \"pad_token_id\": llm_pipe.tokenizer.eos_token_id}\n","\n","        model_config = llm_pipe.model.config\n","        model_max_len = getattr(model_config, 'max_position_embeddings', None) or \\\n","                        getattr(model_config, 'max_sequence_length', None) or \\\n","                        getattr(model_config, 'sliding_window', None) or \\\n","                        getattr(model_config, 'window_size', None) or \\\n","                        getattr(llm_pipe.tokenizer, 'model_max_length', None) or 32768\n","        logger_instance.log(f\"DEBUG P3_LLM: {map_reduce_step_type} for {log_prefix_qid_step}. Model_max_len: {model_max_len}.\")\n","        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True)\n","        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95:\n","             logger_instance.log(f\"WARN P3_LLM: {log_prefix_qid_step} - Prompt+Output may exceed context {model_max_len}.\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","        generated_text_full = outputs[0]['generated_text']\n","        assistant_response_start_tag = \"<|im_start|>assistant\\n\"\n","        actual_response_text = generated_text_full\n","        last_assistant_tag_idx = generated_text_full.rfind(assistant_response_start_tag)\n","        if last_assistant_tag_idx != -1: actual_response_text = generated_text_full[last_assistant_tag_idx + len(assistant_response_start_tag):].strip()\n","        elif prompt_formatted in generated_text_full: actual_response_text = generated_text_full[len(prompt_formatted):].strip()\n","        if actual_response_text.endswith(\"<|im_end|>\"): actual_response_text = actual_response_text[:-len(\"<|im_end|>\")].strip()\n","        llm_json_part = actual_response_text\n","        json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", actual_response_text, re.DOTALL)\n","        if json_match: llm_json_part = json_match.group(1).strip()\n","        else:\n","            first_brace = actual_response_text.find('{'); last_brace = actual_response_text.rfind('}')\n","            if first_brace != -1 and last_brace != -1 and last_brace > first_brace: llm_json_part = actual_response_text[first_brace : last_brace+1].strip()\n","            else: logger_instance.log(f\"WARN P3_LLM: No clear JSON block for {log_prefix_qid_step}.\")\n","        logger_instance.log(f\"    RAW LLM JSON-candidate for {log_prefix_qid_step} ({map_reduce_step_type}):\\n{llm_json_part}\")\n","        return llm_json_part\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P3_LLM: {map_reduce_step_type} exception for {log_prefix_qid_step}: {e}\\n{traceback.format_exc()}\")\n","        return f\"LLM_ERROR: Exception - {str(e)}\"\n","\n","# --- Helper: Parse LLM JSON Thematic Output ---\n","def parse_llm_json_themes_p3(llm_json_output_text: str, max_themes_to_extract: int, logger_instance) -> List[Dict[str, str]]:\n","    # ... (Same as parse_llm_json_themes_q1_test, using logger_instance) ...\n","    logger_instance.log(f\"DEBUG P3_JSON_PARSER: Parsing up to {max_themes_to_extract} from: '{llm_json_output_text[:100]}...'\")\n","    parsed_themes = []\n","    error_entry = lambda expl: [{\"theme_label\": \"Parsing/LLM Error\", \"explanation\": str(expl)[:500] + \"...\"}]\n","    if not llm_json_output_text or llm_json_output_text.startswith(\"LLM_ERROR:\"): return error_entry(llm_json_output_text)\n","    try:\n","        data = json.loads(llm_json_output_text)\n","        if isinstance(data, dict) and \"themes\" in data and isinstance(data[\"themes\"], list):\n","            for theme_obj in data[\"themes\"]:\n","                if len(parsed_themes) >= max_themes_to_extract: break\n","                if isinstance(theme_obj, dict) and \"theme_label\" in theme_obj and \"explanation\" in theme_obj:\n","                    parsed_themes.append({\"theme_label\": str(theme_obj[\"theme_label\"]), \"explanation\": str(theme_obj[\"explanation\"])})\n","            if parsed_themes: logger_instance.log(f\"DEBUG P3_JSON_PARSER: Parsed {len(parsed_themes)} themes.\"); return parsed_themes\n","            else: logger_instance.log(\"WARN P3_JSON_PARSER: Valid JSON, 'themes' list empty/malformed.\"); return error_entry(f\"Valid JSON, themes empty. Raw: {llm_json_output_text[:200]}\")\n","        else: logger_instance.log(f\"WARN P3_JSON_PARSER: LLM JSON not expected structure. Got: {str(data)[:200]}\"); return error_entry(f\"Valid JSON, unexpected struct. Raw: {llm_json_output_text[:200]}\")\n","    except json.JSONDecodeError as e:\n","        logger_instance.log(f\"ERROR P3_JSON_PARSER: Not valid JSON. Error: {e}. Raw:\\n{llm_json_output_text}\")\n","        return error_entry(f\"JSONDecodeError: {e}. Raw: {llm_json_output_text[:200]}\")\n","    except Exception as e_gen:\n","        logger_instance.log(f\"ERROR P3_JSON_PARSER: Unexpected error: {e_gen}\\n{traceback.format_exc()}\")\n","        return error_entry(f\"Unexpected Parsing Error: {e_gen}. Raw: {llm_json_output_text[:200]}\")\n","\n","# --- Main Function for Phase 3 ---\n","def run_phase3_llm_thematic_generation(logger_p3_instance): # Pass logger\n","    logger_p3 = logger_p3_instance\n","    logger_p3.log(\"=\" * 50); logger_p3.log(\" INFO: Starting Phase 3: LLM Thematic Point Generation (Qwen2, Map-Reduce, JSON)\"); logger_p3.log(\"=\" * 50)\n","\n","    # --- Load LLM ---\n","    llm_thematic_pipe_p3 = None; tokenizer_p3_for_batcher = None\n","    try:\n","        logger_p3.log(f\"INFO P3: Loading LLM: {P3_LLM_ID_FOR_THEMES} (Quant: {P3_USE_QUANTIZATION})\")\n","        tokenizer_p3_for_batcher = AutoTokenizer.from_pretrained(P3_LLM_ID_FOR_THEMES, trust_remote_code=True)\n","        if getattr(tokenizer_p3_for_batcher, 'pad_token', None) is None : tokenizer_p3_for_batcher.pad_token = tokenizer_p3_for_batcher.eos_token\n","\n","        bnb_config_p3 = None; current_p3_quant = P3_USE_QUANTIZATION\n","        if P3_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16\n","                bnb_config_p3 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n","            except Exception as qe: logger_p3.log(f\"WARN P3: BNB config failed: {qe}. Disabling quant.\"); current_p3_quant=False\n","        elif P3_USE_QUANTIZATION: current_p3_quant=False; logger_p3.log(\"WARN P3: Quant requested, No CUDA.\")\n","\n","        model_p3 = AutoModelForCausalLM.from_pretrained( P3_LLM_ID_FOR_THEMES,\n","            quantization_config=bnb_config_p3 if current_p3_quant else None,\n","            torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True)\n","        llm_thematic_pipe_p3 = pipeline(\"text-generation\", model=model_p3, tokenizer=tokenizer_p3_for_batcher)\n","        logger_p3.log(\"INFO P3: LLM Pipeline Loaded.\")\n","    except Exception as e: logger_p3.log(f\"CRITICAL P3: LLM loading failed: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    # --- Load Collated Texts (Phase 2 output) & Question Definitions ---\n","    try:\n","        with open(P3_INPUT_COLLATED_JSON, 'r', encoding='utf-8') as f: collated_data = json.load(f)\n","        collated_texts_by_qid_p3 = collated_data.get(\"aggregated_pdf_content_by_qid\", {})\n","        if not collated_texts_by_qid_p3 : raise ValueError(\"P2 output empty or malformed.\")\n","        with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f: q_defs = json.load(f)\n","        qid_to_actual_question_text_p3 = {k:v.get(\"question_text\",\"\") for k,v in q_defs.get(\"question_data\", {}).items()}\n","        logger_p3.log(f\"INFO P3: Loaded collated texts for {len(collated_texts_by_qid_p3)} QIDs and {len(qid_to_actual_question_text_p3)} Q-texts.\")\n","    except Exception as e: logger_p3.log(f\"FATAL P3: Data loading failed: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    overall_thematic_results_p3 = {};\n","    if os.path.exists(P3_OUTPUT_LLM_THEMES_JSON): # Load existing results to resume\n","        try:\n","            with open(P3_OUTPUT_LLM_THEMES_JSON,'r', encoding='utf-8') as f_exist: overall_thematic_results_p3=json.load(f_exist)\n","            logger_p3.log(f\"INFO P3: Loaded {len(overall_thematic_results_p3)} existing results from {P3_OUTPUT_LLM_THEMES_JSON}\")\n","        except: logger_p3.log(f\"WARN P3: Could not load existing output {P3_OUTPUT_LLM_THEMES_JSON}, starting fresh for this run.\")\n","\n","    qids_to_run = P3_QIDS_TO_PROCESS_THEMATICALLY if P3_QIDS_TO_PROCESS_THEMATICALLY else sorted(collated_texts_by_qid_p3.keys())\n","\n","    for qid_idx, qid in enumerate(qids_to_run):\n","        logger_p3.log(f\"\\n--- P3 Processing QID {qid_idx + 1}/{len(qids_to_run)}: {qid} ---\")\n","        if qid in overall_thematic_results_p3 and overall_thematic_results_p3[qid].get(\"status\", \"\").startswith(\"success\"):\n","            logger_p3.log(f\"INFO P3: QID {qid} already successfully processed. Skipping.\"); continue\n","\n","        question_actual_text = qid_to_actual_question_text_p3.get(qid, f\"Question {qid}\")\n","        source_text_items_for_qid = [item for item in collated_texts_by_qid_p3.get(qid, []) if item.get(\"text\")] # list of dicts\n","\n","        if not source_text_items_for_qid:\n","            logger_p3.log(f\"WARN P3: No source texts for QID {qid}. Skipping.\");\n","            overall_thematic_results_p3[qid] = {\"question_text\": question_actual_text, \"status\":\"no_source_texts\", \"parsed_top_themes\":[]};\n","            continue\n","\n","        # Default to Scenario B type: batching individual items from Stage4a\n","        # (These items can be 'pdf_summary' or 'pdf_passages' type from Phase 2)\n","        list_of_texts_for_qid = [item['text'] for item in source_text_items_for_qid]\n","        map_batches_p3 = batch_text_items_p3(\n","            list_of_texts_for_qid,\n","            llm_thematic_pipe_p3.tokenizer, # Use the loaded tokenizer\n","            P3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK,\n","            logger_p3\n","        )\n","\n","        final_llm_output_str = \"N/A\"; raw_map_outputs = []; status_p3=\"init\"\n","\n","        if not map_batches_p3:\n","            logger_p3.log(f\"WARN P3: No map batches created for QID {qid}.\"); status_p3=\"no_map_batches\"\n","        elif len(map_batches_p3) == 1:\n","            logger_p3.log(f\"INFO P3: QID {qid} - Single batch ({len(map_batches_p3[0])} items). Direct LLM call.\")\n","            direct_input_text = \"\\n\\n--- Next Excerpt ---\\n\".join(map_batches_p3[0])\n","            final_llm_output_str = get_llm_json_themes_p3(\n","                direct_input_text, question_actual_text, P3_NUM_THEMES_REDUCE_STR, # Use REDUCE num for direct\n","                llm_thematic_pipe_p3, P3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS, # Use REDUCE tokens for final\n","                f\"{qid}_direct_batch\", \"DIRECT_PROCESSING_PASSAGES\", logger_p3 # Generic type\n","            )\n","            raw_map_outputs.append(final_llm_output_str)\n","        else: # Map-Reduce\n","            logger_p3.log(f\"INFO P3: QID {qid} - Map-Reduce with {len(map_batches_p3)} map steps.\")\n","            intermediate_themes_text_p3 = []\n","            for i, batch_of_item_strings in enumerate(map_batches_p3):\n","                batch_concat_text = \"\\n\\n--- Next Excerpt ---\\n\".join(batch_of_item_strings)\n","                map_out = get_llm_json_themes_p3(\n","                    batch_concat_text, question_actual_text, P3_NUM_THEMES_MAP_STR, # Use MAP num for prompt\n","                    llm_thematic_pipe_p3, P3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                    f\"{qid}_map_batch{i+1}\", \"MAP_ON_BATCHED_ITEMS\", logger_p3\n","                )\n","                raw_map_outputs.append(map_out)\n","                if not map_out.startswith(\"LLM_ERROR:\"): intermediate_themes_text_p3.append(map_out)\n","                time.sleep(0.2) # Brief pause\n","\n","            if intermediate_themes_text_p3:\n","                parsed_intermediate_for_reduce = []\n","                for idx, raw_json_map_out in enumerate(intermediate_themes_text_p3):\n","                    parsed_map_themes = parse_llm_json_themes_p3(raw_json_map_out, P3_NUM_THEMES_MAP_INT, logger_p3) # Use MAP int for parsing\n","                    for pt_map in parsed_map_themes:\n","                        if not pt_map.get(\"theme_label\",\"\").lower().startswith((\"llm error\", \"parsing/llm error\", \"unparsed\", \"valid json\")):\n","                            parsed_intermediate_for_reduce.append(f\"Theme Label: {pt_map.get('theme_label')}\\nExplanation: {pt_map.get('explanation')}\")\n","\n","                if parsed_intermediate_for_reduce:\n","                    combined_intermediate_text = \"\\n\\n---\\n\\n\".join(parsed_intermediate_for_reduce)\n","                    final_llm_output_str = get_llm_json_themes_p3(\n","                        combined_intermediate_text, question_actual_text, P3_NUM_THEMES_REDUCE_STR, # Use REDUCE num for prompt\n","                        llm_thematic_pipe_p3, P3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                        f\"{qid}_reduce\", \"REDUCE_INTERMEDIATE_THEMES\", logger_p3)\n","                else: status_p3 = \"error_map_parsing_no_valid_themes\"; final_llm_output_str = \"Error: Could not parse valid intermediate themes.\"\n","            else: status_p3 = \"error_map_step_all_failed\"; final_llm_output_str = \"Error: All map steps failed.\"\n","\n","        parsed_themes_list_p3 = parse_llm_json_themes_p3(final_llm_output_str, P3_NUM_THEMES_REDUCE_INT, logger_p3) # Use REDUCE int for final parsing\n","        if status_p3 == \"init\":\n","            status_p3 = \"success_thematic_analysis\" if not final_llm_output_str.startswith(\"LLM_ERROR:\") and not (len(parsed_themes_list_p3)==1 and parsed_themes_list_p3[0][\"theme_label\"].lower().startswith((\"llm error\", \"unparsed\", \"valid json\"))) else \"error_final_llm_or_parse\"\n","\n","        overall_thematic_results_p3[qid] = {\n","            \"question_text\": question_actual_text, \"status\": status_p3,\n","            \"total_source_items_for_qid\": len(source_text_items_for_qid),\n","            \"map_reduce_applied\": len(map_batches_p3) > 1 if map_batches_p3 else False,\n","            \"num_map_batches\": len(map_batches_p3) if map_batches_p3 else 0,\n","            \"parsed_top_themes\": parsed_themes_list_p3,\n","            \"llm_raw_output_final_synthesis\": final_llm_output_str,\n","            \"llm_raw_output_map_batches\": raw_map_outputs\n","        }\n","        try:\n","            with open(P3_OUTPUT_LLM_THEMES_JSON, 'w', encoding='utf-8') as f_out:\n","                json.dump(overall_thematic_results_p3, f_out, indent=2, default=logger_p3._get_serializer())\n","            logger_p3.log(f\"INFO P3: Saved results to '{P3_OUTPUT_LLM_THEMES_JSON}' after QID {qid}.\")\n","        except Exception as es: logger_p3.log(f\"ERROR P3: Save failed for QID {qid}: {es}\")\n","        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","    logger_p3.log(\"=\" * 50); logger_p3.log(\" INFO: Phase 3 Finished \"); logger_p3.log(\"=\" * 50)\n","    return True\n","\n","print(\"INFO: Cell 5 - Phase 3 logic defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwkKTfjb7U2v"},"outputs":[],"source":["# @title Cell 6: Phase 4 - Theme Validation, Quantification & Quotation Extraction (REVISED)\n","\n","# logger_p4 will be initialized in the Orchestration Cell (Cell 7)\n","# For standalone testing of this cell, you would initialize it here:\n","# if 'logger_p4' not in locals():\n","#     P4_LOG_FILE_TEST = os.path.join(LOG_DIR, f'phase4_test_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","#     logger_p4 = FileLogger(P4_LOG_FILE_TEST) # Needs LOG_DIR from Cell 2\n","#     ENABLE_DETAILED_DEBUG_LOGGING = True # For testing\n","\n","def load_human_defined_keyphrases_p4(keyphrase_file_path: str, logger_instance) -> Dict[str, Dict[str, List[str]]]:\n","    if not os.path.exists(keyphrase_file_path):\n","        logger_instance.log(f\"WARN P4: Keyphrase file not found: {keyphrase_file_path}. Will be treated as empty. Quantification will be limited.\")\n","        return {}\n","    try:\n","        with open(keyphrase_file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","        if not isinstance(data, dict):\n","            logger_instance.log(f\"ERROR P4: Keyphrase file {keyphrase_file_path} is not a valid JSON dictionary. Returning empty.\")\n","            return {}\n","        logger_instance.log(f\"INFO P4: Successfully loaded {len(data)} QIDs with keyphrases from {keyphrase_file_path}.\")\n","        return data\n","    except json.JSONDecodeError as e:\n","        logger_instance.log(f\"ERROR P4: Failed to decode JSON from keyphrase file {keyphrase_file_path}: {e}\")\n","        return {}\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P4: Failed to load keyphrase file {keyphrase_file_path}: {e}\")\n","        return {}\n","\n","def count_themes_and_extract_quotes_p4(\n","    llm_themes_by_qid: Dict[str, Any], # Output from Phase 3 (QID -> P3 analysis dict)\n","    collated_source_texts_data: Dict[str, Any], # Output from Phase 2 (P2_OUTPUT_COLLATED_JSON)\n","    human_keyphrases_by_qid: Dict[str, Dict[str, List[str]]], # Loaded from P4_HUMAN_DEFINED_KEYPHRASES_FILE\n","    num_quotes_per_theme: int,\n","    logger_instance # Instance of FileLogger\n",") -> Dict[str, Any]:\n","    final_report_data_p4 = {}\n","    # aggregated_pdf_content_by_qid is the key in Phase 2's output\n","    source_texts_for_all_qids = collated_source_texts_data.get(\"aggregated_pdf_content_by_qid\", {})\n","\n","    for qid, p3_qid_analysis_data in llm_themes_by_qid.items():\n","        # Ensure ENABLE_DETAILED_DEBUG_LOGGING is accessible, e.g., passed or global\n","        # For simplicity, assuming it's a global variable from Cell 2\n","        # If not, pass it as an argument: def count_themes_and_extract_quotes_p4(..., enable_debug_logging_flag)\n","\n","        logger_instance.log(f\"INFO P4: Processing QID: {qid} for quantification and quoting.\")\n","\n","        question_text = p3_qid_analysis_data.get(\"question_text\", f\"Question text N/A for {qid}\")\n","        llm_identified_themes_p3 = p3_qid_analysis_data.get(\"parsed_top_themes\", [])\n","\n","        final_report_data_p4[qid] = {\n","            \"question_text\": question_text,\n","            \"llm_generated_themes_from_p3\": llm_identified_themes_p3,\n","            \"status_from_p3\": p3_qid_analysis_data.get(\"status\", \"N/A\"),\n","            \"quantified_points\": []\n","        }\n","\n","        qid_human_keyphrases = human_keyphrases_by_qid.get(qid, {})\n","        source_text_items_for_this_qid = source_texts_for_all_qids.get(qid, []) # List of dicts from Phase 2\n","\n","        if not source_text_items_for_this_qid:\n","            logger_instance.log(f\"  WARN P4: No source texts (from Phase 2 output) found for QID {qid}. Cannot quantify or quote.\")\n","            final_report_data_p4[qid][\"quantified_points\"].append({\n","                \"point_label\": \"No Source Texts Available for QID\",\n","                \"keyphrases_used\": [], \"unique_response_count\": 0, \"example_quotes\": []\n","            })\n","            continue # Next QID\n","\n","        if not qid_human_keyphrases:\n","            logger_instance.log(f\"  WARN P4: No human-defined keyphrases for QID {qid}. Listing LLM themes from Phase 3 with 0 count.\")\n","            for llm_theme in llm_identified_themes_p3:\n","                if isinstance(llm_theme, dict) and not llm_theme.get(\"theme_label\",\"\").lower().startswith((\"llm error\", \"parsing/llm error\", \"unparsed\", \"valid json\")):\n","                    final_report_data_p4[qid][\"quantified_points\"].append({\n","                        \"point_label\": llm_theme.get(\"theme_label\") + \" (LLM Suggested - Keyphrases Needed for Count)\",\n","                        \"llm_explanation\": llm_theme.get(\"explanation\"),\n","                        \"keyphrases_used_for_counting\": [\"N/A - Define keyphrases\"],\n","                        \"unique_response_count\": 0,\n","                        \"example_quotes\": []\n","                    })\n","            continue # Next QID\n","\n","        # If we have human-defined keyphrases for this QID\n","        for human_point_label, phrases_for_point in qid_human_keyphrases.items():\n","            if not phrases_for_point: # Skip if a point label has no keyphrases\n","                logger_instance.log(f\"  WARN P4: QID {qid} - Point '{human_point_label}' has no keyphrases defined. Skipping.\")\n","                continue\n","\n","            logger_instance.log(f\"  P4 Counting for QID {qid} - Point: '{human_point_label}' using phrases: {phrases_for_point}\")\n","\n","            point_matching_response_ids = set()\n","            # Stores dicts: {'response_id':..., 'full_text_item': (str), 'source_document_type': ..., 'matched_phrase':...}\n","            source_items_for_potential_quotes = []\n","\n","            # --- First Pass: Identify all matching responses and the specific text items for this point ---\n","            for source_item in source_text_items_for_this_qid:\n","                item_text_content = source_item.get(\"text\", \"\")\n","                if not item_text_content: continue\n","\n","                item_text_lower = item_text_content.lower()\n","                response_id = source_item.get(\"response_id\")\n","\n","                for phrase in phrases_for_point:\n","                    phrase_lower = phrase.lower()\n","                    # Using regex for whole word matching\n","                    if re.search(r'\\b' + re.escape(phrase_lower) + r'\\b', item_text_lower):\n","                        if response_id: point_matching_response_ids.add(response_id)\n","                        source_items_for_potential_quotes.append({\n","                            \"response_id\": response_id,\n","                            \"full_text_item\": item_text_content,\n","                            \"source_document_type\": source_item.get(\"type\"),\n","                            \"matched_phrase\": phrase # Store original case phrase for context\n","                        })\n","                        if ENABLE_DETAILED_DEBUG_LOGGING:\n","                            logger_instance.log(f\"    DEBUG P4 MATCH: QID {qid}, Point '{human_point_label}', RespID '{response_id}' matched phrase '{phrase}'. Text snippet: '{item_text_lower[:100]}...'\")\n","                        break # This source_item matches this human_point_label, move to next source_item\n","\n","            unique_response_count_for_point = len(point_matching_response_ids)\n","            selected_quotes_for_point = []\n","\n","            if unique_response_count_for_point > 0 and source_items_for_potential_quotes:\n","                if ENABLE_DETAILED_DEBUG_LOGGING:\n","                    logger_instance.log(f\"    DEBUG P4 QUOTE PREP: QID {qid}, Point '{human_point_label}': {unique_response_count_for_point} unique responses. Found {len(source_items_for_potential_quotes)} text items containing keyphrases to search for quotes.\")\n","\n","                # --- Second Pass: Extract actual quote snippets from the matched text items ---\n","                quotes_extracted_so_far = 0\n","                # To get diverse quotes, we can shuffle or iterate based on response_id\n","                # For simplicity, let's try to get one quote per matched response first, then fill if needed.\n","\n","                # Get one quote per response ID first\n","                temp_quotes_by_response = defaultdict(list)\n","                for quote_candidate_info in source_items_for_potential_quotes:\n","                    full_text = quote_candidate_info[\"full_text_item\"]\n","                    phrase_to_find_in_sentence = quote_candidate_info[\"matched_phrase\"] # Use the phrase that caused the item to be selected\n","\n","                    try:\n","                        sentences = nltk.sent_tokenize(full_text)\n","                    except Exception as e_sent_tokenize:\n","                        logger_instance.log(f\"    WARN P4 QUOTE: NLTK sent_tokenize failed for a text item for QID {qid}, RespID {quote_candidate_info['response_id']}. Error: {e_sent_tokenize}. Using full text as single sentence.\")\n","                        sentences = [full_text]\n","\n","                    for sent_idx, sentence in enumerate(sentences):\n","                        if re.search(r'\\b' + re.escape(phrase_to_find_in_sentence.lower()) + r'\\b', sentence.lower()):\n","                            context_start = max(0, sent_idx - 1)\n","                            context_end = min(len(sentences), sent_idx + 2)\n","                            context_snippet = \" \".join(sentences[context_start:context_end]).strip()\n","\n","                            temp_quotes_by_response[quote_candidate_info[\"response_id\"]].append({\n","                                \"response_id\": quote_candidate_info[\"response_id\"],\n","                                \"source_document_type\": quote_candidate_info[\"source_document_type\"],\n","                                \"matched_phrase\": phrase_to_find_in_sentence,\n","                                \"quote\": sentence.strip(),\n","                                \"broader_context\": context_snippet\n","                            })\n","                            break # Take first matching sentence for this phrase from this source_item\n","\n","                # Select quotes, prioritizing different responses\n","                seen_responses_for_this_point_quotes = set()\n","                for resp_id in point_matching_response_ids: # Iterate through IDs that matched the theme\n","                    if len(selected_quotes_for_point) >= num_quotes_per_theme: break\n","                    if resp_id in temp_quotes_by_response and temp_quotes_by_response[resp_id]:\n","                        selected_quotes_for_point.append(temp_quotes_by_response[resp_id][0]) # Take the first quote found for this response\n","                        seen_responses_for_this_point_quotes.add(resp_id)\n","\n","                # If still need more quotes, fill from any available (even if duplicate response_id)\n","                if len(selected_quotes_for_point) < num_quotes_per_theme:\n","                    for resp_id, quotes_list in temp_quotes_by_response.items():\n","                        if len(selected_quotes_for_point) >= num_quotes_per_theme: break\n","                        if resp_id not in seen_responses_for_this_point_quotes: # Should not happen if logic above is correct\n","                            for quote_obj in quotes_list:\n","                                if len(selected_quotes_for_point) < num_quotes_per_theme:\n","                                    selected_quotes_for_point.append(quote_obj)\n","                                else: break\n","                        elif len(quotes_list) > 1 and resp_id in seen_responses_for_this_point_quotes: # If we already have a quote from this response, pick another if available\n","                             for quote_obj_idx, quote_obj in enumerate(quotes_list):\n","                                 if quote_obj_idx == 0 and resp_id in seen_responses_for_this_point_quotes: continue # skip first one if already added\n","                                 if len(selected_quotes_for_point) < num_quotes_per_theme:\n","                                     selected_quotes_for_point.append(quote_obj)\n","                                 else: break\n","\n","            # Find corresponding LLM explanation for this human_point_label\n","            # This assumes human_point_label in keyphrases might match an LLM-generated theme_label\n","            llm_explanation_for_point = \"N/A - Human point label may not directly map to a single Phase 3 LLM theme.\"\n","            if isinstance(llm_identified_themes_p3, list):\n","                for llm_theme_obj in llm_identified_themes_p3:\n","                    if isinstance(llm_theme_obj, dict) and llm_theme_obj.get(\"theme_label\") == human_point_label:\n","                        llm_explanation_for_point = llm_theme_obj.get(\"explanation\", \"N/A\")\n","                        break\n","\n","            final_report_data_p4[qid][\"quantified_points\"].append({\n","                \"point_label\": human_point_label,\n","                \"llm_explanation_if_direct_match\": llm_explanation_for_point,\n","                \"keyphrases_used_for_counting\": phrases_for_point,\n","                \"unique_response_count\": unique_response_count_for_point,\n","                \"example_quotes\": selected_quotes_for_point\n","            })\n","            logger_instance.log(f\"    P4 QID {qid} - Point '{human_point_label}': Final count {unique_response_count_for_point}. Extracted {len(selected_quotes_for_point)} quotes.\")\n","            if ENABLE_DETAILED_DEBUG_LOGGING and not selected_quotes_for_point and unique_response_count_for_point > 0:\n","                 logger_instance.log(f\"      DEBUG P4 NO QUOTES: Point '{human_point_label}' had count {unique_response_count_for_point} but no quotes selected. Check potential_quotes and selection logic.\")\n","                 if point_potential_quotes_with_context:\n","                     logger_instance.log(f\"        First potential quote source info: {point_potential_quotes_with_context[0]}\")\n","\n","    return final_report_data_p4\n","\n","\n","def run_phase4_theme_quantification_and_quoting(logger_p4_instance):\n","    # global logger_p4 # Not needed if passed\n","    logger_p4 = logger_p4_instance\n","    logger_p4.log(\"=\" * 50); logger_p4.log(\" INFO: Starting Phase 4: Theme Quantification & Quotation Extraction\"); logger_p4.log(\"=\" * 50)\n","\n","    # 1. Load LLM-generated themes from Phase 3\n","    llm_themes_data_from_p3 = {}\n","    try:\n","        logger_p4.log(f\"INFO P4: Loading LLM themes from: {P4_INPUT_LLM_THEMES_JSON}\")\n","        with open(P4_INPUT_LLM_THEMES_JSON, 'r', encoding='utf-8') as f:\n","            llm_themes_data_from_p3 = json.load(f)\n","        if not llm_themes_data_from_p3 or not isinstance(llm_themes_data_from_p3, dict):\n","             logger_p4.log(f\"ERROR P4: Phase 3 output file {P4_INPUT_LLM_THEMES_JSON} is empty or not a dict. Cannot proceed with quantification.\"); return False\n","        logger_p4.log(f\"INFO P4: Loaded {len(llm_themes_data_from_p3)} QIDs with LLM themes from {P4_INPUT_LLM_THEMES_JSON}\")\n","    except FileNotFoundError:\n","        logger_p4.log(f\"ERROR P4: Phase 3 output file {P4_INPUT_LLM_THEMES_JSON} not found. Run Phase 3 first.\"); return False\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to load LLM themes: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    # 2. Load collated source texts from Phase 2\n","    collated_texts_data_from_p2 = {}\n","    try:\n","        logger_p4.log(f\"INFO P4: Loading collated texts from: {P4_INPUT_COLLATED_TEXTS_JSON}\")\n","        with open(P4_INPUT_COLLATED_TEXTS_JSON, 'r', encoding='utf-8') as f:\n","            collated_texts_data_from_p2 = json.load(f)\n","        if not collated_texts_data_from_p2 or not isinstance(collated_texts_data_from_p2.get(\"aggregated_pdf_content_by_qid\"), dict):\n","            logger_p4.log(f\"ERROR P4: Phase 2 output file {P4_INPUT_COLLATED_TEXTS_JSON} is empty or malformed.\"); return False\n","        logger_p4.log(f\"INFO P4: Loaded collated texts from {P4_INPUT_COLLATED_TEXTS_JSON}\")\n","    except FileNotFoundError:\n","        logger_p4.log(f\"ERROR P4: Phase 2 collated texts file {P4_INPUT_COLLATED_TEXTS_JSON} not found. Run Phase 2 first.\"); return False\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to load collated texts: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    # 3. Load human-defined keyphrases\n","    human_keyphrases = load_human_defined_keyphrases_p4(P4_HUMAN_DEFINED_KEYPHRASES_FILE, logger_p4)\n","    if not human_keyphrases:\n","        logger_p4.log(f\"WARN P4: {P4_HUMAN_DEFINED_KEYPHRASES_FILE} was empty or not found. Quantification will be based on LLM themes only (counts will be 0 unless keyphrases are added).\")\n","\n","    # 4. Perform counting and quote extraction\n","    # Pass the actual dicts to the processing function\n","    final_report_content = count_themes_and_extract_quotes_p4(\n","        llm_themes_data_from_p3,             # This is the dict of QID -> P3 analysis results\n","        collated_texts_data_from_p2,         # This is the dict from P2 (contains \"aggregated_pdf_content_by_qid\")\n","        human_keyphrases,\n","        P4_NUM_EXAMPLE_QUOTES_PER_THEME, # From global config\n","        logger_p4\n","    )\n","\n","    # 5. Save the final analysis\n","    try:\n","        logger_p4.log(f\"INFO P4: Saving final thematic report to {P4_OUTPUT_FINAL_ANALYSIS_JSON}\")\n","        with open(P4_OUTPUT_FINAL_ANALYSIS_JSON, 'w', encoding='utf-8') as f:\n","            json.dump(final_report_content, f, indent=2, default=logger_p4._get_serializer())\n","        logger_p4.log(f\"INFO P4: Successfully saved final thematic report to {P4_OUTPUT_FINAL_ANALYSIS_JSON}\")\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to save final analysis: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    logger_p4.log(\"=\" * 50); logger_p4.log(\" INFO: Phase 4 Finished \"); logger_p4.log(\"=\" * 50)\n","    return True\n","\n","print(\"INFO: Cell 6 - Phase 4 logic defined.\")"]},{"cell_type":"markdown","metadata":{"id":"-wl4pO_GJnqg"},"source":["# 21st May"]},{"cell_type":"markdown","metadata":{"id":"0ijzD24xJ9DK"},"source":["Overall Pipeline Structure:\n","- Cell 0: Initial Setup & Global Imports (As you have it - Python, ML/NLP, Transformers)\n","- Cell 1: FileLogger Class & NLTK Downloads (As you have it)\n","- Cell 2: Global Configuration (Define paths, model IDs, key parameters for ALL phases here, or have per-phase config cells)\n","- --- PDF CONTENT ANALYSIS PIPELINE ---\n","- Cell 3: Phase 1 - PDF Text & Passage Extraction, Per-Response Summarization (Original Stage 4)\n"," - Configuration specific to Phase 1 (if not in global config).\n"," - Helper functions for Phase 1.\n"," - Main execution function run_phase1_pdf_extraction().\n","- Cell 4: Phase 2 - Collation of PDF Content (Original Stage 4a)\n"," - Configuration specific to Phase 2.\n"," - Helper functions for Phase 2 (like clean_text).\n"," - Main execution function run_phase2_collate_pdf_texts().\n","- Cell 5: Phase 3 - LLM Thematic Point Generation (Map-Reduce)\n"," - Configuration specific to Phase 3 (LLM, Map-Reduce params, QID selection).\n"," - Helper functions for Phase 3 (chunkers, batchers, LLM call, JSON parser).\n"," - Main execution function run_phase3_llm_thematic_generation().\n","- Cell 6: Phase 4 - Theme Validation, Quantification & Quotation Extraction (NEW)\n"," - Configuration specific to Phase 4.\n"," - Helper functions for Phase 4 (keyphrase matching, quote extraction).\n"," - Main execution function run_phase4_theme_quantification_and_quoting().\n","- Cell 7: Main Orchestration Cell (To run selected phases sequentially)"]},{"cell_type":"markdown","metadata":{"id":"U6II4ip_GHJd"},"source":["## Direct Comparison (if desired on same QID):\n","- While Q4 didn't have passages for Scenario B, you could now run Scenario A (on summaries) for Q16, since Q16 also had PHASE1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE = False. This means for Q16, the collated_texts_by_qid_p3.get(\"Q16\", []) in the Phase 3 input would only contain passage items.\n","- To run a \"Scenario A-like\" process on Q16, you would first need to generate Stage 4 summaries for the Q16 passages. This would mean:\n"," 1. Modify PHASE1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE in Cell 2 to set \"Q16\": True.\n"," 1. Re-run Phase 1 (Cell 3 via Cell 9 orchestration). This will generate summaries for Q16.\n"," 1. Re-run Phase 2 (Cell 4 via Cell 9) to create a new phase2_...json file that now includes pdf_summary type items for Q16.\n"," 1. Then, you could run your Scenario A logic (Map-Reduce on those newly generated summaries for Q16) using the test cell.\n","- This is a more involved process but would give you a direct comparison of themes from \"summaries of Q16 passages\" vs. themes from \"Q16 passages themselves.\" For now, comparing Q4-Summaries with Q16-Passages gives you a good sense of the two approaches."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgRxG6L3G0HL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"npD91yuC3Uje"},"outputs":[],"source":["# @title Q16 TARGETED TEST CELL (SCENARIO B with Qwen2-4B, NoThinking): Thematic Analysis on Passages\n","\n","# --- Test Cell Imports ---\n","import json\n","import os\n","import sys\n","import traceback\n","import time\n","import gc\n","import re\n","from collections import defaultdict\n","from typing import Dict, Any, List\n","\n","import torch\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","# try:\n","#     from bitsandbytes.cextension import ColabNotSupportedError\n","# except ImportError:\n","#     ColabNotSupportedError = None\n","\n","# --- Configuration for this Q16 Scenario B Test Cell ---\n","Q16B_TEST_BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # <<<--- ADJUST IF NEEDED\n","Q16B_TEST_LOG_FILE = os.path.join(Q16B_TEST_BASE_PROJECT_DIR, f'q16_scenario_b_qwen2_4b_test_v2_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","\n","# LLM Config for Qwen2-4B-Instruct\n","Q16B_TEST_LLM_MODEL_ID = \"Qwen/Qwen3-4B\"\n","Q16B_TEST_USE_QUANTIZATION = True\n","\n","# Thematic Analysis Parameters\n","Q16B_TEST_NUM_THEMES_FROM_MAP_CHUNK_STR = \"up to 2\"\n","Q16B_TEST_NUM_THEMES_FROM_MAP_CHUNK_INT = 2\n","Q16B_TEST_FINAL_NUM_THEMES = 3\n","\n","Q16B_TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 3000\n","Q16B_TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 800\n","Q16B_TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 1000\n","\n","Q16B_TEST_INPUT_COLLATED_PDF_TEXTS_JSON = os.path.join(Q16B_TEST_BASE_PROJECT_DIR, 'phase2_stage4a_collated_pdf_texts_20250518_233834.json') # <<<--- UPDATE\n","\n","Q16B_TEST_QUESTION_TEXT_SOURCE_FILE = os.path.join(Q16B_TEST_BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","\n","QID_FOR_SCENARIO_B_TEST = \"Q16\"\n","\n","# --- FileLogger ---\n","try:\n","    logger_q16b_test = FileLogger(Q16B_TEST_LOG_FILE)\n","    logger_q16b_test.log(f\"INFO: Q16 Scenario B Test (Qwen2-4B, NoThink) - Logger Initialized for QID: {QID_FOR_SCENARIO_B_TEST}.\")\n","except NameError:\n","    class PrintLogger:\n","        def log(self, msg): print(f\"{time.strftime('%Y%m%d_%H%M%S')}: {msg}\")\n","    logger_q16b_test = PrintLogger()\n","    logger_q16b_test.log(f\"WARN: Q16 Scenario B Test (Qwen2-4B, NoThink) - Using fallback PrintLogger for QID: {QID_FOR_SCENARIO_B_TEST}.\")\n","\n","# --- Helper: Batch Individual Text Items ---\n","def batch_text_items_for_map_step(text_items_list: List[str], tokenizer_for_batching, max_tokens_per_batch: int, logger_instance): # Added logger_instance\n","    batches = []\n","    current_batch_item_texts = []\n","    current_batch_concatenated_text = \"\"\n","    logger_instance.log(f\"DEBUG BATCHER: Batching {len(text_items_list)} items into max {max_tokens_per_batch} token batches.\")\n","    for item_idx, item_text_original in enumerate(text_items_list):\n","        if not item_text_original or not item_text_original.strip(): continue\n","        text_to_try_adding = item_text_original\n","        if current_batch_item_texts: text_to_try_adding = \"\\n\\n--- Next Excerpt ---\\n\" + item_text_original\n","        potential_full_batch_text = current_batch_concatenated_text + text_to_try_adding\n","        num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))\n","        if num_tokens <= max_tokens_per_batch:\n","            current_batch_item_texts.append(item_text_original)\n","            current_batch_concatenated_text = potential_full_batch_text\n","        else:\n","            if current_batch_item_texts: batches.append(current_batch_item_texts)\n","            single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))\n","            if single_item_tokens > max_tokens_per_batch:\n","                logger_instance.log(f\"  WARN BATCHER: Item {item_idx} ({single_item_tokens} tokens) > limit ({max_tokens_per_batch}). Own oversized batch.\")\n","                batches.append([item_text_original])\n","                current_batch_item_texts = []\n","                current_batch_concatenated_text = \"\"\n","            else:\n","                current_batch_item_texts = [item_text_original]\n","                current_batch_concatenated_text = item_text_original\n","    if current_batch_item_texts: batches.append(current_batch_item_texts)\n","    logger_instance.log(f\"INFO BATCHER: Created {len(batches)} batches of items.\")\n","    return batches\n","\n","# --- Helper: LLM Call ---\n","def get_llm_json_themes(text_input_for_llm: str, question_text_for_prompt: str,\n","                        num_themes_to_request_in_prompt: str,\n","                        llm_pipe: pipeline, max_new_tokens: int,\n","                        log_prefix_qid_step: str, map_reduce_step_type: str, logger_instance):\n","    # ... (Function content as provided in the previous \"Qwen2-4B No Thinking\" test cell, using logger_instance) ...\n","    # ... (Ensuring prompts are correct for SCENARIO_B_MAP and REDUCE_INTERMEDIATE_THEMES)\n","    json_format_example = (\"{\\n  \\\"themes\\\": [\\n    {\\\"theme_label\\\": \\\"Label 1\\\", \\\"explanation\\\": \\\"Expl 1.\\\"},\\n\"\n","                           \"    {\\\"theme_label\\\": \\\"Label 2\\\", \\\"explanation\\\": \\\"Expl 2.\\\"}\\n  ]\\n}\")\n","    system_prompt = \"You are an expert policy analyst.\"\n","    user_content = \"\"\n","    input_type_description = \"excerpts/summaries\"\n","    if map_reduce_step_type == \"SCENARIO_A_MAP\": input_type_description = \"CHUNK of concatenated PREVIOUSLY-GENERATED SUMMARIES\"\n","    elif map_reduce_step_type == \"SCENARIO_B_MAP\": input_type_description = \"BATCH of original excerpts/passages\"\n","    elif map_reduce_step_type == \"DIRECT_PROCESSING_SUMMARIES\": input_type_description = \"combined PDF summaries\"\n","    elif map_reduce_step_type == \"DIRECT_PROCESSING_PASSAGES\": input_type_description = \"single batch of excerpts/passages\"\n","\n","    if map_reduce_step_type in [\"SCENARIO_A_MAP\", \"SCENARIO_B_MAP\", \"DIRECT_PROCESSING_SUMMARIES\", \"DIRECT_PROCESSING_PASSAGES\"]:\n","        prompt_verb = \"identify\"\n","        if map_reduce_step_type.startswith(\"DIRECT_PROCESSING\"): prompt_verb = \"identify the most common and distinct\"\n","        user_content = (\n","            f\"Carefully review the following {input_type_description} from PDF submissions, all responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on this provided text, {prompt_verb} points, arguments, or themes, {num_themes_to_request_in_prompt}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object formatted as follows, containing a list called 'themes'. Each object in the 'themes' list must have two string keys: 'theme_label' and 'explanation'. If no clear themes are present or fewer than requested are distinct, return only those that are clear within the JSON structure.\\n\"\n","            f\"Example JSON format (if requesting {num_themes_to_request_in_prompt}):\\n{json_format_example}\\n\\n\"\n","            f\"Provided Text:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output:\" )\n","    elif map_reduce_step_type == \"REDUCE_INTERMEDIATE_THEMES\":\n","        user_content = (\n","            f\"The following are sets of preliminary themes/points ... question: '{question_text_for_prompt}'.\\n\"\n","            f\"Your task is to synthesize these preliminary themes into the overall most significant and distinct common themes, {num_themes_to_request_in_prompt}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object... (etc.)\\n\"\n","            f\"Example JSON format:\\n{json_format_example}\\n\\n\"\n","            f\"Preliminary Themes/Points Provided:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output (Synthesized {num_themes_to_request_in_prompt} Themes):\" )\n","    else: logger_instance.log(f\"ERROR LLM_CALL: Unknown map_reduce_step_type: {map_reduce_step_type}\"); return \"LLM_ERROR: Unknown step type\"\n","\n","    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_content}]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n","        terminators = [llm_pipe.tokenizer.eos_token_id, llm_pipe.tokenizer.convert_tokens_to_ids(\"<|im_end|>\")]\n","        valid_terminators = [t for t in terminators if t is not None]\n","        generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": True, \"temperature\": 0.7, \"top_p\": 0.8, \"top_k\": 20,\n","                           \"eos_token_id\": valid_terminators, \"pad_token_id\": llm_pipe.tokenizer.eos_token_id}\n","\n","        model_config = llm_pipe.model.config\n","        model_max_len = getattr(model_config, 'max_position_embeddings', None) or \\\n","                        getattr(model_config, 'max_sequence_length', None) or \\\n","                        getattr(model_config, 'sliding_window', None) or \\\n","                        getattr(model_config, 'window_size', None) or \\\n","                        getattr(llm_pipe.tokenizer, 'model_max_length', None) or 32768\n","        logger_instance.log(f\"DEBUG LLM_CALL: {map_reduce_step_type} for {log_prefix_qid_step}. Effective model_max_len: {model_max_len}.\")\n","        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True)\n","        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95:\n","             logger_instance.log(f\"WARN LLM_CALL: {log_prefix_qid_step} - Step {map_reduce_step_type} - Prompt+Output may exceed context {model_max_len}.\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","        generated_text_full = outputs[0]['generated_text']\n","\n","        assistant_response_start_tag = \"<|im_start|>assistant\\n\"\n","        actual_response_text = generated_text_full\n","        last_assistant_tag_idx = generated_text_full.rfind(assistant_response_start_tag)\n","        if last_assistant_tag_idx != -1: actual_response_text = generated_text_full[last_assistant_tag_idx + len(assistant_response_start_tag):].strip()\n","        elif prompt_formatted in generated_text_full: actual_response_text = generated_text_full[len(prompt_formatted):].strip()\n","        if actual_response_text.endswith(\"<|im_end|>\"): actual_response_text = actual_response_text[:-len(\"<|im_end|>\")].strip()\n","\n","        llm_json_part = actual_response_text\n","        json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", actual_response_text, re.DOTALL)\n","        if json_match: llm_json_part = json_match.group(1).strip()\n","        else:\n","            first_brace = actual_response_text.find('{'); last_brace = actual_response_text.rfind('}')\n","            if first_brace != -1 and last_brace != -1 and last_brace > first_brace:\n","                llm_json_part = actual_response_text[first_brace : last_brace+1].strip()\n","            else: logger_instance.log(f\"WARN LLM_CALL: No clear JSON block for {log_prefix_qid_step}.\")\n","\n","        logger_instance.log(f\"    RAW LLM JSON-candidate PART for {log_prefix_qid_step} ({map_reduce_step_type}):\\n{llm_json_part}\")\n","        return llm_json_part\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR LLM_CALL: {map_reduce_step_type} LLM exception for {log_prefix_qid_step}: {e}\\n{traceback.format_exc()}\")\n","        return f\"LLM_ERROR: Exception - {str(e)}\"\n","\n","# --- Helper: Parse LLM JSON Thematic Output ---\n","def parse_llm_json_themes(llm_json_output_text: str, max_themes_to_extract: int, logger_instance) -> List[Dict[str, str]]:\n","    # ... (Same robust JSON parser, using logger_instance) ...\n","    logger_instance.log(f\"DEBUG JSON_PARSER: Parsing up to {max_themes_to_extract} from: '{llm_json_output_text[:100]}...'\")\n","    parsed_themes = []\n","    error_entry = lambda expl: [{\"theme_label\": \"Parsing/LLM Error\", \"explanation\": str(expl)[:500] + \"...\"}]\n","    if not llm_json_output_text or llm_json_output_text.startswith(\"LLM_ERROR:\"): return error_entry(llm_json_output_text)\n","    try:\n","        data = json.loads(llm_json_output_text)\n","        if isinstance(data, dict) and \"themes\" in data and isinstance(data[\"themes\"], list):\n","            for theme_obj in data[\"themes\"]:\n","                if len(parsed_themes) >= max_themes_to_extract: break\n","                if isinstance(theme_obj, dict) and \"theme_label\" in theme_obj and \"explanation\" in theme_obj:\n","                    parsed_themes.append({\"theme_label\": str(theme_obj[\"theme_label\"]), \"explanation\": str(theme_obj[\"explanation\"])})\n","            if parsed_themes: logger_instance.log(f\"DEBUG JSON_PARSER: Parsed {len(parsed_themes)} themes.\"); return parsed_themes\n","            else: logger_instance.log(\"WARN JSON_PARSER: Valid JSON, 'themes' list empty/malformed.\"); return error_entry(f\"Valid JSON, themes empty. Raw: {llm_json_output_text[:200]}\")\n","        else: logger_instance.log(f\"WARN JSON_PARSER: LLM JSON not expected structure. Got: {str(data)[:200]}\"); return error_entry(f\"Valid JSON, unexpected struct. Raw: {llm_json_output_text[:200]}\")\n","    except json.JSONDecodeError as e:\n","        logger_instance.log(f\"ERROR JSON_PARSER: Not valid JSON. Error: {e}. Raw:\\n{llm_json_output_text}\")\n","        return error_entry(f\"JSONDecodeError: {e}. Raw: {llm_json_output_text[:200]}\")\n","    except Exception as e_gen:\n","        logger_instance.log(f\"ERROR JSON_PARSER: Unexpected error: {e_gen}\\n{traceback.format_exc()}\")\n","        return error_entry(f\"Unexpected Parsing Error: {e_gen}. Raw: {llm_json_output_text[:200]}\")\n","\n","\n","# --- Main Test Execution Function for SCENARIO B ---\n","def run_scenario_b_q16_test():\n","    logger_q16b_test.log(f\"INFO: Scenario B Test (Qwen2-4B, Passages) - Starting for QID: {QID_FOR_SCENARIO_B_TEST}\")\n","\n","    # --- 0. Load Actual Question Text for Q16 ---\n","    question_actual_text_q16 = f\"Question text for {QID_FOR_SCENARIO_B_TEST} not loaded\"\n","    try:\n","        logger_q16b_test.log(f\"INFO Q16B_TEST: Loading question texts from: {Q16B_TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","        if not os.path.exists(Q16B_TEST_QUESTION_TEXT_SOURCE_FILE): raise FileNotFoundError(f\"File not found: {Q16B_TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","        with open(Q16B_TEST_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext: qtext_data = json.load(f_qtext)\n","        if \"question_data\" in qtext_data:\n","            q_data = qtext_data.get(\"question_data\", {}).get(QID_FOR_SCENARIO_B_TEST)\n","            if q_data and \"question_text\" in q_data: question_actual_text_q16 = q_data[\"question_text\"]\n","        logger_q16b_test.log(f\"INFO Q16B_TEST: Using Q text for {QID_FOR_SCENARIO_B_TEST}: '{question_actual_text_q16[:100]}...'\")\n","    except Exception as e: logger_q16b_test.log(f\"ERROR Q16B_TEST: Failed to load Q text: {e}\\n{traceback.format_exc()}\")\n","\n","    # --- 1. Load LLM ---\n","    llm_pipe_q16b_test = None; tokenizer_q16b_test = None\n","    try:\n","        logger_q16b_test.log(f\"INFO Q16B_TEST: Loading LLM: {Q16B_TEST_LLM_MODEL_ID} (Quant: {Q16B_TEST_USE_QUANTIZATION})\")\n","        tokenizer_q16b_test = AutoTokenizer.from_pretrained(Q16B_TEST_LLM_MODEL_ID, trust_remote_code=True)\n","        if getattr(tokenizer_q16b_test, 'pad_token', None) is None : tokenizer_q16b_test.pad_token = tokenizer_q16b_test.eos_token\n","        bnb_config_q16b = None; effective_quant_q16b = Q16B_TEST_USE_QUANTIZATION\n","        if Q16B_TEST_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16\n","                bnb_config_q16b = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n","            except ImportError: effective_quant_q16b = False; logger_q16b_test.log(\"WARN Q16B_TEST: bitsandbytes NI.\")\n","            except Exception as e_bnb: effective_quant_q16b = False; logger_q16b_test.log(f\"WARN Q16B_TEST: BNB failed: {e_bnb}.\")\n","        elif Q16B_TEST_USE_QUANTIZATION: effective_quant_q16b = False; logger_q16b_test.log(\"WARN Q16B_TEST: Quant req, No CUDA.\")\n","        model_q16b_test = AutoModelForCausalLM.from_pretrained(\n","            Q16B_TEST_LLM_MODEL_ID, quantization_config=bnb_config_q16b if effective_quant_q16b else None,\n","            torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True )\n","        llm_pipe_q16b_test = pipeline(\"text-generation\", model=model_q16b_test, tokenizer=tokenizer_q16b_test)\n","        logger_q16b_test.log(\"INFO Q16B_TEST: LLM Pipeline (Qwen2-4B) Loaded.\")\n","    except Exception as e: logger_q16b_test.log(f\"CRITICAL Q16B_TEST: Failed to load LLM: {e}\\n{traceback.format_exc()}\"); return\n","\n","    # --- 2. Load and Filter PASSAGES for Q16 ---\n","    q16_passages_for_analysis = []\n","    try:\n","        with open(Q16B_TEST_INPUT_COLLATED_PDF_TEXTS_JSON, 'r') as f: s4a_data = json.load(f)\n","        q16_all_s4a_items = s4a_data.get(\"aggregated_pdf_content_by_qid\",{}).get(QID_FOR_SCENARIO_B_TEST,[])\n","        q16_passages_for_analysis = [item['text'] for item in q16_all_s4a_items if item.get('type')=='pdf_passages' and item.get('text')]\n","        if not q16_passages_for_analysis:\n","            logger_q16b_test.log(f\"ERROR Q16B_TEST: No 'pdf_passages' items found for {QID_FOR_SCENARIO_B_TEST}. Cannot run Scenario B test.\"); return\n","        logger_q16b_test.log(f\"INFO Q16B_TEST: Loaded {len(q16_passages_for_analysis)} PASSAGES for {QID_FOR_SCENARIO_B_TEST}.\")\n","    except Exception as e: logger_q16b_test.log(f\"CRITICAL Q16B_TEST: Failed data load for {QID_FOR_SCENARIO_B_TEST}: {e}\\n{traceback.format_exc()}\"); return\n","\n","    # --- Map-Reduce Thematic Analysis for Q16 Passages ---\n","    final_parsed_themes_q16b = []\n","    map_llm_outputs_raw_q16b = []\n","    reduce_llm_output_raw_q16b = \"N/A\"\n","\n","    map_batches_q16b = batch_text_items_for_map_step(\n","        q16_passages_for_analysis,\n","        llm_pipe_q16b_test.tokenizer,\n","        Q16B_TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK,\n","        logger_q16b_test # Pass logger\n","    )\n","    logger_q16b_test.log(f\"INFO Q16B_TEST: Grouped items for {QID_FOR_SCENARIO_B_TEST} into {len(map_batches_q16b)} batches for Map step.\")\n","\n","    if not map_batches_q16b: logger_q16b_test.log(f\"WARN Q16B_TEST: No map batches for {QID_FOR_SCENARIO_B_TEST}.\")\n","    elif len(map_batches_q16b) == 1:\n","        direct_input_q16b = \"\\n\\n--- Next Excerpt ---\\n\".join(map_batches_q16b[0])\n","        reduce_llm_output_raw_q16b = get_llm_json_themes(\n","            direct_input_q16b, question_actual_text_q16, f\"top {Q16B_TEST_FINAL_NUM_THEMES}\",\n","            llm_pipe_q16b_test, Q16B_TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","            f\"{QID_FOR_SCENARIO_B_TEST}_direct_passages\", \"DIRECT_PROCESSING_PASSAGES\", logger_q16b_test\n","        )\n","        final_parsed_themes_q16b = parse_llm_json_themes(reduce_llm_output_raw_q16b, Q16B_TEST_FINAL_NUM_THEMES, logger_q16b_test)\n","        map_llm_outputs_raw_q16b.append(reduce_llm_output_raw_q16b)\n","    else:\n","        intermediate_themes_q16b_list = []\n","        for i, batch_of_passage_strings in enumerate(map_batches_q16b):\n","            current_batch_concatenated_text = \"\\n\\n--- Next Excerpt ---\\n\".join(batch_of_passage_strings)\n","            logger_q16b_test.log(f\"  SCENARIO B - MAP BATCH {i+1}/{len(map_batches_q16b)} for {QID_FOR_SCENARIO_B_TEST} (Passage items: {len(batch_of_passage_strings)}, Chars: {len(current_batch_concatenated_text)})\")\n","            map_out = get_llm_json_themes(\n","                current_batch_concatenated_text, question_actual_text_q16,\n","                Q16B_TEST_NUM_THEMES_FROM_MAP_CHUNK_STR,\n","                llm_pipe_q16b_test, Q16B_TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                f\"{QID_FOR_SCENARIO_B_TEST}_map_batch{i+1}\", \"SCENARIO_B_MAP\", logger_q16b_test\n","            )\n","            map_llm_outputs_raw_q16b.append(map_out)\n","            if not map_out.startswith(\"LLM_ERROR:\"): intermediate_themes_q16b_list.append(map_out)\n","            time.sleep(0.5)\n","\n","        if intermediate_themes_q16b_list:\n","            parsed_intermediate_for_reduce_q16b = []\n","            for idx, raw_json_map_out in enumerate(intermediate_themes_q16b_list):\n","                parsed_map_themes = parse_llm_json_themes(raw_json_map_out, Q16B_TEST_NUM_THEMES_FROM_MAP_CHUNK_INT, logger_q16b_test)\n","                logger_q16b_test.log(f\"  Q16B_TEST PARSED MAP OUTPUT for batch {idx+1}: {parsed_map_themes}\")\n","                for pt_map in parsed_map_themes:\n","                    if not pt_map.get(\"theme_label\",\"\").lower().startswith((\"llm error\", \"parsing/llm error\", \"unparsed\", \"valid json\")):\n","                        parsed_intermediate_for_reduce_q16b.append(f\"Preliminary Theme ... Label: {pt_map.get('theme_label')}\\nExplanation: {pt_map.get('explanation')}\")\n","\n","            if parsed_intermediate_for_reduce_q16b:\n","                combined_intermediate_text_q16b = \"\\n\\n---\\n\\n\".join(parsed_intermediate_for_reduce_q16b)\n","                reduce_llm_output_raw_q16b = get_llm_json_themes(\n","                    combined_intermediate_text_q16b, question_actual_text_q16, f\"top {Q16B_TEST_FINAL_NUM_THEMES}\",\n","                    llm_pipe_q16b_test, Q16B_TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                    f\"{QID_FOR_SCENARIO_B_TEST}_reduce\", \"REDUCE_INTERMEDIATE_THEMES\", logger_q16b_test\n","                )\n","                final_parsed_themes_q16b = parse_llm_json_themes(reduce_llm_output_raw_q16b, Q16B_TEST_FINAL_NUM_THEMES, logger_q16b_test)\n","            else: reduce_llm_output_raw_q16b = f\"ERROR: No successfully parsed intermediate themes for {QID_FOR_SCENARIO_B_TEST} (Passages).\"\n","        else: reduce_llm_output_raw_q16b = f\"ERROR: No valid intermediate themes from MAP step for {QID_FOR_SCENARIO_B_TEST} (Passages).\"\n","\n","    logger_q16b_test.log(f\"\\n--- Q16 SCENARIO B (Qwen2-4B, Passages) - FINAL PARSED THEMES for {QID_FOR_SCENARIO_B_TEST} ---\")\n","    if final_parsed_themes_q16b:\n","        for i, theme in enumerate(final_parsed_themes_q16b):\n","            logger_q16b_test.log(f\"  Theme {i+1}: {theme.get('theme_label')}\\n     Explanation: {theme.get('explanation')}\")\n","    else: logger_q16b_test.log(f\"  No final themes parsed for {QID_FOR_SCENARIO_B_TEST}.\")\n","    logger_q16b_test.log(f\"  Raw MAP (first, max 500): {(map_llm_outputs_raw_q16b[0][:500] + '...') if map_llm_outputs_raw_q16b and map_llm_outputs_raw_q16b[0] else 'N/A'}\")\n","    logger_q16b_test.log(f\"  Raw REDUCE (max 500): {(reduce_llm_output_raw_q16b[:500] + '...') if reduce_llm_output_raw_q16b else 'N/A'}\")\n","\n","    logger_q16b_test.log(\"\\nINFO Q16B_TEST: Cleaning up LLM resources...\")\n","    if 'llm_pipe_q16b_test' in locals(): del llm_pipe_q16b_test\n","    if 'model_q16b_test' in locals(): del model_q16b_test\n","    if 'tokenizer_q16b_test' in locals(): del tokenizer_q16b_test\n","    gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","    logger_q16b_test.log(f\"INFO: Q16 Scenario B Test (Qwen2-4B, Passages) Finished for QID: {QID_FOR_SCENARIO_B_TEST}\")\n","\n","# --- Execute the Scenario B Test ---\n","# print(\"REMINDER: Update Q16B_TEST_... config vars. Ensure Cells 0 & 1 ran.\")\n","# print(\"REMINDER: To run Scenario B test for Q16, uncomment the line below.\")\n","# run_scenario_b_q16_test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZW22x-M3qyQJ"},"outputs":[],"source":["# @title Q1 TARGETED TEST CELL (Qwen3-4B, NoThink, Increased Tokens): Thematic Analysis\n","\n","# --- Test Cell Imports ---\n","import json\n","import os\n","import sys\n","import traceback\n","import time\n","import gc\n","import re\n","from collections import defaultdict\n","from typing import Dict, Any, List\n","\n","import torch\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","# try:\n","#     from bitsandbytes.cextension import ColabNotSupportedError # From Cell 0\n","# except ImportError:\n","#     ColabNotSupportedError = None\n","\n","# --- Configuration for this Q1 Test Cell ---\n","Q1_TEST_BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # <<<--- ADJUST IF NEEDED\n","Q1_TEST_LOG_FILE = os.path.join(Q1_TEST_BASE_PROJECT_DIR, f'q1_targeted_qwen2_4b_nothink_v2_test_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","\n","# LLM Config for Qwen2-4B-Instruct\n","Q1_TEST_LLM_MODEL_ID = \"Qwen/Qwen3-4B\"\n","Q1_TEST_USE_QUANTIZATION = True\n","\n","# Thematic Analysis Parameters\n","Q1_TEST_NUM_THEMES_FROM_MAP_CHUNK_STR = \"up to 2\" # String for LLM prompt for map step\n","Q1_TEST_NUM_THEMES_FROM_MAP_CHUNK_INT = 2     # Integer for parsing map step output\n","Q1_TEST_FINAL_NUM_THEMES = 3                  # Final number of themes for REDUCE step (integer)\n","\n","Q1_TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 3000 # Content tokens for input\n","Q1_TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 1000     # << INCREASED for LLM to output JSON for map\n","Q1_TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 1000 # << INCREASED for LLM to output JSON for reduce\n","\n","# Input file from Stage 4a\n","Q1_TEST_INPUT_COLLATED_PDF_TEXTS_JSON = os.path.join(Q1_TEST_BASE_PROJECT_DIR, 'phase2_stage4a_collated_pdf_texts_20250518_233834.json') # <<<--- UPDATE\n","\n","# Source for actual question texts\n","Q1_TEST_QUESTION_TEXT_SOURCE_FILE = os.path.join(Q1_TEST_BASE_PROJECT_DIR, 'question_centric_aggregation_output.json')\n","\n","QID_TO_TEST = \"Q1\"\n","\n","# --- FileLogger ---\n","try:\n","    logger_q1_test = FileLogger(Q1_TEST_LOG_FILE)\n","    logger_q1_test.log(f\"INFO: Q1 Test (Qwen2-4B, NoThink, IncrTokens) - Logger Initialized for QID: {QID_TO_TEST}.\")\n","except NameError: # Fallback\n","    class PrintLogger:\n","        def log(self, msg): print(f\"{time.strftime('%Y%m%d_%H%M%S')}: {msg}\")\n","    logger_q1_test = PrintLogger()\n","    logger_q1_test.log(f\"WARN: Q1 Test (Qwen2-4B, NoThink, IncrTokens) - Using fallback PrintLogger for QID: {QID_TO_TEST}.\")\n","\n","# --- Helper: Batch Individual Items (batch_text_items_q1_test - same as before) ---\n","def batch_text_items_q1_test(text_items_list: List[str], tokenizer_for_batching, max_tokens_per_batch: int):\n","    batches = []\n","    current_batch_item_texts = []\n","    current_batch_concatenated_text = \"\"\n","    logger_q1_test.log(f\"DEBUG Q1_BATCHER: Batching {len(text_items_list)} items into max {max_tokens_per_batch} token batches.\")\n","    for item_idx, item_text_original in enumerate(text_items_list):\n","        if not item_text_original or not item_text_original.strip(): continue\n","        text_to_try_adding = item_text_original\n","        if current_batch_item_texts: text_to_try_adding = \"\\n\\n--- Next Excerpt ---\\n\" + item_text_original\n","        potential_full_batch_text = current_batch_concatenated_text + text_to_try_adding\n","        num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))\n","        if num_tokens <= max_tokens_per_batch:\n","            current_batch_item_texts.append(item_text_original)\n","            current_batch_concatenated_text = potential_full_batch_text\n","        else:\n","            if current_batch_item_texts: batches.append(current_batch_item_texts)\n","            single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))\n","            if single_item_tokens > max_tokens_per_batch:\n","                logger_q1_test.log(f\"  WARN Q1_BATCHER: Item {item_idx} ({single_item_tokens} tokens) > limit ({max_tokens_per_batch}). Own oversized batch.\")\n","                batches.append([item_text_original])\n","                current_batch_item_texts = []\n","                current_batch_concatenated_text = \"\"\n","            else:\n","                current_batch_item_texts = [item_text_original]\n","                current_batch_concatenated_text = item_text_original\n","    if current_batch_item_texts: batches.append(current_batch_item_texts)\n","    logger_q1_test.log(f\"INFO Q1_BATCHER: Created {len(batches)} batches of items.\")\n","    return batches\n","\n","# --- Helper: LLM Call (Adapted for Qwen2, enable_thinking=False) ---\n","def get_llm_json_themes_q1_qwen2_nothink_test(text_input_for_llm: str, question_text_for_prompt: str,\n","                                              num_themes_to_request_in_prompt: str, # String like \"up to 2\" or \"top 3\"\n","                                              llm_pipe: pipeline, max_new_tokens: int,\n","                                              log_prefix_qid_step: str, map_reduce_step_type: str):\n","    json_format_example = (\"{\\n  \\\"themes\\\": [\\n    {\\\"theme_label\\\": \\\"Label 1\\\", \\\"explanation\\\": \\\"Expl 1.\\\"},\\n\"\n","                           \"    {\\\"theme_label\\\": \\\"Label 2\\\", \\\"explanation\\\": \\\"Expl 2.\\\"}\\n  ]\\n}\") # Example for 2 themes\n","    system_prompt = \"You are an expert policy analyst.\"\n","    user_content = \"\"\n","\n","    if map_reduce_step_type == \"MAP_ON_BATCHED_ITEMS\":\n","        user_content = (\n","            f\"Carefully review the following BATCH of excerpts/summaries from PDF submissions, all responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on this provided BATCH, identify the most prominent and distinct points, arguments, or themes, {num_themes_to_request_in_prompt}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object formatted as follows, containing a list called 'themes'. Each object in the 'themes' list must have two string keys: 'theme_label' and 'explanation'. If no clear themes are present or fewer than requested are distinct, return only those that are clear within the JSON structure.\\n\"\n","            f\"Example JSON format (if requesting {num_themes_to_request_in_prompt}):\\n{json_format_example}\\n\\n\"\n","            f\"Batch of Excerpts/Summaries:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output:\"\n","        )\n","    elif map_reduce_step_type == \"REDUCE_INTERMEDIATE_THEMES\":\n","        user_content = (\n","            f\"The following are sets of preliminary themes/points ... question: '{question_text_for_prompt}'.\\n\"\n","            f\"Your task is to synthesize these preliminary themes into the overall most significant and distinct common themes, {num_themes_to_request_in_prompt}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object... (etc., ensure example matches num_themes_to_request_in_prompt)\\n\"\n","            f\"Example JSON format (if requesting {num_themes_to_request_in_prompt}):\\n{json_format_example}\\n\\n\"\n","            f\"Preliminary Themes/Points Provided:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output (Synthesized {num_themes_to_request_in_prompt} Themes):\"\n","        )\n","    else: # DIRECT_PROCESSING\n","        user_content = (\n","            f\"Carefully review all the following text ... question: '{question_text_for_prompt}'.\\n\"\n","            f\"Identify the most common and distinct points, arguments, or themes, {num_themes_to_request_in_prompt}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object... (etc.)\\n\"\n","            f\"Example JSON:\\n{json_format_example}\\n\\n\"\n","            f\"Provided Text:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output:\"\n","        )\n","\n","    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_content}]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(\n","            messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n","        )\n","        terminators = [llm_pipe.tokenizer.eos_token_id, llm_pipe.tokenizer.convert_tokens_to_ids(\"<|im_end|>\")]\n","        valid_terminators = [t for t in terminators if t is not None]\n","        generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": True,\n","                           \"temperature\": 0.7, \"top_p\": 0.8, \"top_k\": 20,\n","                           \"eos_token_id\": valid_terminators, \"pad_token_id\": llm_pipe.tokenizer.eos_token_id}\n","\n","        model_config = llm_pipe.model.config\n","        model_max_len = getattr(model_config, 'max_position_embeddings', None) or \\\n","                        getattr(model_config, 'max_sequence_length', None) or \\\n","                        getattr(model_config, 'sliding_window', None) or \\\n","                        getattr(model_config, 'window_size', None) or \\\n","                        getattr(llm_pipe.tokenizer, 'model_max_length', None) or 32768\n","        logger_q1_test.log(f\"DEBUG Q1_LLM_CALL (Qwen2-NoThink): {map_reduce_step_type} for {log_prefix_qid_step}. Effective model_max_len for check: {model_max_len}.\")\n","        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True)\n","        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95: # Check if prompt + output might exceed\n","             logger_q1_test.log(f\"WARN Q1_LLM_CALL (Qwen2-NoThink): {log_prefix_qid_step} - Prompt+Output may exceed context {model_max_len}.\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","        generated_text_full = outputs[0]['generated_text']\n","\n","        assistant_response_start_tag = \"<|im_start|>assistant\\n\"\n","        actual_response_text = generated_text_full\n","        last_assistant_tag_idx = generated_text_full.rfind(assistant_response_start_tag)\n","        if last_assistant_tag_idx != -1:\n","            actual_response_text = generated_text_full[last_assistant_tag_idx + len(assistant_response_start_tag):].strip()\n","        elif prompt_formatted in generated_text_full: # Less reliable fallback\n","            actual_response_text = generated_text_full[len(prompt_formatted):].strip()\n","\n","        if actual_response_text.endswith(\"<|im_end|>\"):\n","            actual_response_text = actual_response_text[:-len(\"<|im_end|>\")].strip()\n","\n","        llm_json_part = actual_response_text\n","        json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", actual_response_text, re.DOTALL)\n","        if json_match: llm_json_part = json_match.group(1).strip()\n","        else:\n","            # Try to find first '{' and last '}'\n","            first_brace = actual_response_text.find('{')\n","            last_brace = actual_response_text.rfind('}')\n","            if first_brace != -1 and last_brace != -1 and last_brace > first_brace:\n","                llm_json_part = actual_response_text[first_brace : last_brace+1].strip()\n","            else: logger_q1_test.log(f\"WARN Q1_LLM_CALL (Qwen2-NoThink): No clear JSON block for {log_prefix_qid_step}.\")\n","\n","        logger_q1_test.log(f\"    Q1_TEST {map_reduce_step_type.upper()} RAW LLM JSON-candidate PART for {log_prefix_qid_step}:\\n{llm_json_part}\")\n","        return llm_json_part\n","    except Exception as e:\n","        logger_q1_test.log(f\"ERROR Q1_LLM_CALL (Qwen2-NoThink): {map_reduce_step_type} LLM exception for {log_prefix_qid_step}: {e}\\n{traceback.format_exc()}\")\n","        return f\"LLM_ERROR: Exception - {str(e)}\"\n","\n","# --- Helper: Parse LLM JSON Thematic Output ---\n","def parse_llm_json_themes_q1_test(llm_json_output_text: str, max_themes_to_extract: int) -> List[Dict[str, str]]:\n","    # ... (Same robust JSON parser from previous, using logger_q1_test) ...\n","    logger_q1_test.log(f\"DEBUG Q1_JSON_PARSER: Attempting to parse up to {max_themes_to_extract} themes from: '{llm_json_output_text[:100]}...'\")\n","    parsed_themes = []\n","    error_entry = lambda expl: [{\"theme_label\": \"Parsing/LLM Error\", \"explanation\": str(expl)[:500] + \"...\"}]\n","    if not llm_json_output_text or llm_json_output_text.startswith(\"LLM_ERROR:\"):\n","        return error_entry(llm_json_output_text)\n","    try:\n","        data = json.loads(llm_json_output_text)\n","        if isinstance(data, dict) and \"themes\" in data and isinstance(data[\"themes\"], list):\n","            for theme_obj in data[\"themes\"]:\n","                if len(parsed_themes) >= max_themes_to_extract: break\n","                if isinstance(theme_obj, dict) and \"theme_label\" in theme_obj and \"explanation\" in theme_obj:\n","                    parsed_themes.append({\"theme_label\": str(theme_obj[\"theme_label\"]), \"explanation\": str(theme_obj[\"explanation\"])})\n","            if parsed_themes: logger_q1_test.log(f\"DEBUG Q1_JSON_PARSER: Parsed {len(parsed_themes)} themes.\"); return parsed_themes\n","            else: logger_q1_test.log(\"WARN Q1_JSON_PARSER: Valid JSON, 'themes' list empty/malformed.\"); return error_entry(f\"Valid JSON, themes empty. Raw: {llm_json_output_text[:200]}\")\n","        else: logger_q1_test.log(f\"WARN Q1_JSON_PARSER: LLM JSON not expected structure. Got: {str(data)[:200]}\"); return error_entry(f\"Valid JSON, unexpected struct. Raw: {llm_json_output_text[:200]}\")\n","    except json.JSONDecodeError as e:\n","        logger_q1_test.log(f\"ERROR Q1_JSON_PARSER: Not valid JSON. Error: {e}. Raw:\\n{llm_json_output_text}\")\n","        return error_entry(f\"JSONDecodeError: {e}. Raw: {llm_json_output_text[:200]}\")\n","    except Exception as e_gen:\n","        logger_q1_test.log(f\"ERROR Q1_JSON_PARSER: Unexpected error: {e_gen}\\n{traceback.format_exc()}\")\n","        return error_entry(f\"Unexpected Parsing Error: {e_gen}. Raw: {llm_json_output_text[:200]}\")\n","\n","# --- Main Test Execution Function ---\n","def run_q1_targeted_thematic_test_with_qwen2_nothink():\n","    logger_q1_test.log(f\"INFO: Q1 Targeted Test (Qwen2-4B, NoThink, IncrTokens) - Starting analysis for QID: {QID_TO_TEST}\")\n","\n","    # --- 0. Load Actual Question Text ---\n","    question_actual_text_q1 = f\"Question text for {QID_TO_TEST} not loaded\"\n","    try:\n","        # ... (Q-text loading logic using Q1_TEST_QUESTION_TEXT_SOURCE_FILE) ...\n","        if not os.path.exists(Q1_TEST_QUESTION_TEXT_SOURCE_FILE): raise FileNotFoundError(f\"File not found: {Q1_TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","        with open(Q1_TEST_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f: qtext_data = json.load(f)\n","        if \"question_data\" in qtext_data:\n","            q_data = qtext_data.get(\"question_data\", {}).get(QID_TO_TEST)\n","            if q_data and \"question_text\" in q_data: question_actual_text_q1 = q_data[\"question_text\"]\n","        logger_q1_test.log(f\"INFO Q1_TEST: Using Q text: '{question_actual_text_q1[:100]}...'\")\n","    except Exception as e: logger_q1_test.log(f\"ERROR Q1_TEST: Failed to load Q text: {e}\\n{traceback.format_exc()}\")\n","\n","    # --- 1. Load LLM (Qwen2-4B-Instruct) ---\n","    llm_pipe_q1_test = None; tokenizer_q1_test = None # For batcher\n","    # ... (Full LLM loading logic for Qwen2-4B from previous response, using Q1_TEST_... variables) ...\n","    try:\n","        logger_q1_test.log(f\"INFO Q1_TEST: Loading LLM: {Q1_TEST_LLM_MODEL_ID} (Quant: {Q1_TEST_USE_QUANTIZATION})\")\n","        tokenizer_q1_test = AutoTokenizer.from_pretrained(Q1_TEST_LLM_MODEL_ID, trust_remote_code=True)\n","        if getattr(tokenizer_q1_test, 'pad_token', None) is None : tokenizer_q1_test.pad_token = tokenizer_q1_test.eos_token\n","        bnb_config_q1 = None; effective_quant_q1 = Q1_TEST_USE_QUANTIZATION\n","        if Q1_TEST_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16\n","                bnb_config_q1 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n","            except ImportError: effective_quant_q1 = False; logger_q1_test.log(\"WARN Q1_TEST: bitsandbytes NI for Qwen quant.\")\n","            except Exception as e_bnb: effective_quant_q1 = False; logger_q1_test.log(f\"WARN Q1_TEST: BNB failed for Qwen: {e_bnb}.\")\n","        elif Q1_TEST_USE_QUANTIZATION: effective_quant_q1 = False; logger_q1_test.log(\"WARN Q1_TEST: Qwen Quant req, No CUDA.\")\n","        model_q1_test = AutoModelForCausalLM.from_pretrained(\n","            Q1_TEST_LLM_MODEL_ID, quantization_config=bnb_config_q1 if effective_quant_q1 else None,\n","            torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True )\n","        llm_pipe_q1_test = pipeline(\"text-generation\", model=model_q1_test, tokenizer=tokenizer_q1_test)\n","        logger_q1_test.log(\"INFO Q1_TEST: LLM Pipeline (Qwen2-4B) Loaded.\")\n","    except Exception as e: logger_q1_test.log(f\"CRITICAL Q1_TEST: Failed to load Qwen2-4B LLM: {e}\\n{traceback.format_exc()}\"); return\n","\n","    # --- 2. Load and Filter Data for QID_TO_TEST ---\n","    q1_items_for_analysis = []\n","    # ... (Data loading and filtering logic as before) ...\n","    try:\n","        with open(Q1_TEST_INPUT_COLLATED_PDF_TEXTS_JSON, 'r') as f: s4a_data = json.load(f)\n","        q1_all_s4a_items = s4a_data.get(\"aggregated_pdf_content_by_qid\",{}).get(QID_TO_TEST,[])\n","        q1_items_for_analysis = [item['text'] for item in q1_all_s4a_items if item.get('type')=='pdf_summary' and item.get('text')]\n","        if not q1_items_for_analysis:\n","            q1_items_for_analysis = [item['text'] for item in q1_all_s4a_items if item.get('type')=='pdf_passages' and item.get('text')]\n","            if q1_items_for_analysis: logger_q1_test.log(f\"INFO Q1_TEST: Using {len(q1_items_for_analysis)} PASSAGES for {QID_TO_TEST}.\")\n","        else: logger_q1_test.log(f\"INFO Q1_TEST: Using {len(q1_items_for_analysis)} SUMMARIES for {QID_TO_TEST}.\")\n","        if not q1_items_for_analysis: logger_q1_test.log(f\"ERROR Q1_TEST: No usable text for {QID_TO_TEST}.\"); return\n","    except Exception as e: logger_q1_test.log(f\"CRITICAL Q1_TEST: Failed data load for {QID_TO_TEST}: {e}\\n{traceback.format_exc()}\"); return\n","\n","    # --- Map-Reduce Thematic Analysis ---\n","    final_parsed_themes_q1 = []\n","    map_llm_outputs_raw_q1 = []\n","    reduce_llm_output_raw = \"N/A\"\n","\n","    map_batches_q1 = batch_text_items_q1_test(\n","        q1_items_for_analysis, llm_pipe_q1_test.tokenizer,\n","        Q1_TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK\n","    )\n","    # ... (Rest of Map-Reduce logic, ensuring correct variables are passed to LLM and parser)\n","    if not map_batches_q1: logger_q1_test.log(f\"WARN Q1_TEST: No map batches for {QID_TO_TEST}.\")\n","    elif len(map_batches_q1) == 1:\n","        direct_input = \"\\n\\n--- Next Excerpt ---\\n\".join(map_batches_q1[0])\n","        reduce_llm_output_raw = get_llm_json_themes_q1_qwen2_nothink_test( # Corrected function name\n","            direct_input, question_actual_text_q1, f\"top {Q1_TEST_FINAL_NUM_THEMES}\",\n","            llm_pipe_q1_test, Q1_TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","            f\"{QID_TO_TEST}_direct\", \"DIRECT_PROCESSING\" )\n","        final_parsed_themes_q1 = parse_llm_json_themes_q1_test(reduce_llm_output_raw, Q1_TEST_FINAL_NUM_THEMES)\n","        map_llm_outputs_raw_q1.append(reduce_llm_output_raw)\n","    else:\n","        intermediate_themes_q1_list = []\n","        for i, batch_strings in enumerate(map_batches_q1):\n","            batch_concat_text = \"\\n\\n--- Next Excerpt ---\\n\".join(batch_strings)\n","            # logger_q1_test.log(f\"  Q1_TEST MAP BATCH {i+1}/{len(map_batches_q1)} ...\") # Already in get_llm_json_themes\n","            map_out = get_llm_json_themes_q1_qwen2_nothink_test( # Corrected\n","                batch_concat_text, question_actual_text_q1,\n","                Q1_TEST_NUM_THEMES_FROM_MAP_CHUNK_STR, # Use string \"up to N\" for prompt\n","                llm_pipe_q1_test, Q1_TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                f\"{QID_TO_TEST}_map_batch{i+1}\", \"MAP_ON_BATCHED_ITEMS\")\n","            map_llm_outputs_raw_q1.append(map_out)\n","            if not map_out.startswith(\"LLM_ERROR:\"): intermediate_themes_q1_list.append(map_out)\n","            time.sleep(0.5)\n","\n","        if intermediate_themes_q1_list:\n","            parsed_intermediate_for_reduce_q1 = []\n","            for idx, raw_json_map_out in enumerate(intermediate_themes_q1_list):\n","                # Use INT for parsing map outputs\n","                parsed_map_themes = parse_llm_json_themes_q1_test(raw_json_map_out, Q1_TEST_NUM_THEMES_FROM_MAP_CHUNK_INT)\n","                logger_q1_test.log(f\"  Q1_TEST PARSED MAP OUTPUT for batch {idx+1}: {parsed_map_themes}\")\n","                for pt_map in parsed_map_themes:\n","                    if not pt_map.get(\"theme_label\",\"\").lower().startswith((\"llm error\", \"parsing/llm error\", \"unparsed\", \"valid json\")):\n","                        parsed_intermediate_for_reduce_q1.append(f\"Preliminary Theme ... Label: {pt_map.get('theme_label')}\\nExplanation: {pt_map.get('explanation')}\")\n","                    else: logger_q1_test.log(f\"  WARN Q1_TEST: Map batch {idx+1} parsing issue/LLM error for reduce input: {pt_map}\")\n","\n","            if parsed_intermediate_for_reduce_q1:\n","                combined_intermediate_text_q1 = \"\\n\\n---\\n\\n\".join(parsed_intermediate_for_reduce_q1)\n","                reduce_llm_output_raw = get_llm_json_themes_q1_qwen2_nothink_test( # Corrected\n","                    combined_intermediate_text_q1, question_actual_text_q1, f\"top {Q1_TEST_FINAL_NUM_THEMES}\",\n","                    llm_pipe_q1_test, Q1_TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                    f\"{QID_TO_TEST}_reduce\", \"REDUCE_INTERMEDIATE_THEMES\")\n","                final_parsed_themes_q1 = parse_llm_json_themes_q1_test(reduce_llm_output_raw, Q1_TEST_FINAL_NUM_THEMES)\n","            else: reduce_llm_output_raw = f\"ERROR: No successfully parsed intermediate themes for {QID_TO_TEST}.\"\n","        else: reduce_llm_output_raw = f\"ERROR: No valid intermediate themes from MAP step for {QID_TO_TEST}.\"\n","\n","    logger_q1_test.log(f\"\\n--- Q1 TARGETED TEST (Qwen2-4B, NoThink) - FINAL PARSED THEMES for {QID_TO_TEST} ---\")\n","    # ... (Logging final themes and raw outputs)\n","    if final_parsed_themes_q1:\n","        for i, theme in enumerate(final_parsed_themes_q1): logger_q1_test.log(f\"  Theme {i+1}: {theme.get('theme_label')}\\n     Explanation: {theme.get('explanation')}\")\n","    else: logger_q1_test.log(f\"  No final themes parsed for {QID_TO_TEST}.\")\n","    logger_q1_test.log(f\"  Raw MAP (first, max 500): {(map_llm_outputs_raw_q1[0][:500] + '...') if map_llm_outputs_raw_q1 and map_llm_outputs_raw_q1[0] else 'N/A'}\")\n","    logger_q1_test.log(f\"  Raw REDUCE (max 500): {(reduce_llm_output_raw[:500] + '...') if reduce_llm_output_raw else 'N/A'}\")\n","\n","    logger_q1_test.log(\"\\nINFO Q1_TEST: Cleaning up LLM resources...\")\n","    # ... (cleanup code)\n","    if 'llm_pipe_q1_test' in locals(): del llm_pipe_q1_test\n","    if 'model_q1_test' in locals(): del model_q1_test\n","    if 'tokenizer_q1_test' in locals(): del tokenizer_q1_test\n","    gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","    logger_q1_test.log(f\"INFO: Q1 Targeted Test (Qwen2-4B, NoThink) Finished for QID: {QID_TO_TEST}\")\n","\n","# --- Execute the Test ---\n","# print(\"REMINDER: Update Q1_TEST_... config variables. Ensure Cells 0 & 1 ran.\")\n","# print(\"REMINDER: To run, uncomment the line below.\")\n","# run_q1_targeted_thematic_test_with_qwen2_nothink()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JoUasRMAKJov"},"outputs":[],"source":["# @title Q1 TARGETED TEST CELL: Thematic Analysis (JSON Output, Map-Reduce)\n","\n","# --- Test Cell Imports ---\n","import json\n","import os\n","import sys # For FileLogger\n","import traceback # For FileLogger\n","import time\n","import gc\n","import re\n","from collections import defaultdict\n","from typing import Dict, Any, List\n","\n","import torch\n","import numpy as np # For FileLogger\n","# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline # From Cell 0\n","# try:\n","#     from bitsandbytes.cextension import ColabNotSupportedError # From Cell 0\n","# except ImportError:\n","#     ColabNotSupportedError = None\n","\n","# --- Configuration for this Q1 Test Cell ---\n","Q1_TEST_BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # <<<--- ADJUST IF NEEDED\n","Q1_TEST_LOG_FILE = os.path.join(Q1_TEST_BASE_PROJECT_DIR, f'q1_targeted_thematic_test_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","\n","# LLM Config\n","Q1_TEST_LLM_MODEL_ID = 'google/gemma-3-4b-it'  # <<<--- VERIFY/ADJUST (e.g., 'google/gemma-7b-it')\n","Q1_TEST_USE_QUANTIZATION = False            # <<<--- VERIFY/ADJUST (True/False)\n","\n","# Thematic Analysis Parameters\n","Q1_TEST_NUM_THEMES_FROM_MAP_CHUNK = 3   # How many themes to ask from each MAP chunk (integer)\n","Q1_TEST_FINAL_NUM_THEMES = 3            # Final number of themes to request from REDUCE step (integer)\n","Q1_TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2800\n","Q1_TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 500\n","Q1_TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 750\n","# Q1_TEST_MAPREDUCE_CHUNK_OVERLAP_WORDS = 30 # Not used if batching individual items\n","\n","# Input file from Stage 4a (Phase 2 output)\n","Q1_TEST_INPUT_COLLATED_PDF_TEXTS_JSON = os.path.join(Q1_TEST_BASE_PROJECT_DIR, 'phase2_stage4a_collated_pdf_texts_20250518_233834.json') # <<<--- UPDATE THIS FILENAME\n","\n","# Source for actual question texts\n","Q1_TEST_QUESTION_TEXT_SOURCE_FILE = os.path.join(Q1_TEST_BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # <<<--- ENSURE THIS IS CORRECT\n","\n","QID_TO_TEST = \"Q1\"\n","\n","# --- FileLogger (Assuming defined in Cell 1 or globally) ---\n","try:\n","    logger_q1_test = FileLogger(Q1_TEST_LOG_FILE)\n","    logger_q1_test.log(f\"INFO: Q1 Targeted Test - Logger Initialized for QID: {QID_TO_TEST}.\")\n","except NameError:\n","    print(\"CRITICAL: FileLogger class not defined. Please ensure Cell 1 has been executed.\")\n","    class PrintLogger: # Fallback\n","        def log(self, msg): print(f\"{time.strftime('%Y%m%d_%H%M%S')}: {msg}\")\n","    logger_q1_test = PrintLogger()\n","    logger_q1_test.log(f\"WARN: Q1 Targeted Test - Using fallback PrintLogger for QID: {QID_TO_TEST}.\")\n","\n","\n","# --- Helper: Batch Individual Items (for Map step on list of texts) ---\n","def batch_text_items_q1_test(text_items_list: List[str], tokenizer_for_batching, max_tokens_per_batch: int):\n","    batches = []\n","    current_batch_item_texts = []\n","    current_batch_concatenated_text = \"\"\n","    logger_q1_test.log(f\"DEBUG Q1_BATCHER: Batching {len(text_items_list)} items into max {max_tokens_per_batch} token batches.\")\n","    for item_idx, item_text_original in enumerate(text_items_list):\n","        if not item_text_original or not item_text_original.strip(): continue\n","        text_to_try_adding = item_text_original\n","        if current_batch_item_texts: text_to_try_adding = \"\\n\\n--- Next Excerpt ---\\n\" + item_text_original\n","        potential_full_batch_text = current_batch_concatenated_text + text_to_try_adding\n","        num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))\n","        if num_tokens <= max_tokens_per_batch:\n","            current_batch_item_texts.append(item_text_original)\n","            current_batch_concatenated_text = potential_full_batch_text\n","        else:\n","            if current_batch_item_texts: batches.append(current_batch_item_texts)\n","            single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))\n","            if single_item_tokens > max_tokens_per_batch:\n","                logger_q1_test.log(f\"  WARN Q1_BATCHER: Item {item_idx} ({single_item_tokens} tokens) > limit ({max_tokens_per_batch}). Own oversized batch.\")\n","                batches.append([item_text_original])\n","                current_batch_item_texts = []\n","                current_batch_concatenated_text = \"\"\n","            else:\n","                current_batch_item_texts = [item_text_original]\n","                current_batch_concatenated_text = item_text_original\n","    if current_batch_item_texts: batches.append(current_batch_item_texts)\n","    logger_q1_test.log(f\"INFO Q1_BATCHER: Created {len(batches)} batches of items.\")\n","    return batches\n","\n","# --- Helper: LLM Call for Thematic Points (Requests JSON Output) ---\n","def get_llm_json_themes_q1_test(text_input_for_llm: str, question_text_for_prompt: str,\n","                                num_themes_to_request_in_prompt: str, # e.g., \"up to 3\" - the string for the prompt\n","                                llm_pipe: pipeline, max_new_tokens: int,\n","                                log_prefix_qid_step: str, map_reduce_step_type: str):\n","    json_format_example = (\"{\\n  \\\"themes\\\": [\\n    {\\\"theme_label\\\": \\\"Label 1\\\", \\\"explanation\\\": \\\"Expl 1.\\\"},\\n\"\n","                           \"    {\\\"theme_label\\\": \\\"Label 2\\\", \\\"explanation\\\": \\\"Expl 2.\\\"}\\n  ]\\n}\")\n","    prompt_instruction = \"\"\n","    if map_reduce_step_type == \"MAP_ON_BATCHED_ITEMS\":\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Review the BATCH of excerpts/summaries from PDF submissions, all responding to: '{question_text_for_prompt}'.\\n\"\n","            f\"Identify the most prominent and distinct points from THIS BATCH, {num_themes_to_request_in_prompt}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object formatted as follows, with a list 'themes', each having 'theme_label' and 'explanation'.\\n\"\n","            f\"Example JSON:\\n{json_format_example}\\n\\n\"\n","            f\"Batch of Excerpts/Summaries:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output:\"\n","        )\n","    elif map_reduce_step_type == \"REDUCE_INTERMEDIATE_THEMES\":\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. The following are preliminary themes/points from different parts of responses to: '{question_text_for_prompt}'.\\n\"\n","            f\"Synthesize these into the overall most significant and distinct common themes, {num_themes_to_request_in_prompt}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object... (etc. as before)\\n\"\n","            f\"Example JSON:\\n{json_format_example}\\n\\n\"\n","            f\"Preliminary Themes/Points:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output (Synthesized Top Themes):\"\n","        )\n","    else: # Should not happen if types are managed, but a fallback\n","        logger_q1_test.log(f\"ERROR Q1_LLM_CALL: Unknown map_reduce_step_type: {map_reduce_step_type}\"); return \"LLM_ERROR: Unknown step type\"\n","\n","    messages = [{\"role\": \"user\", \"content\": prompt_instruction}]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": False,\n","                           \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id,\n","                           \"top_p\": None, \"top_k\": None} # To silence warnings\n","\n","        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True)\n","        input_text_tokens = llm_pipe.tokenizer.encode(text_input_for_llm, add_special_tokens=False)\n","        model_max_len = 8192\n","        model_config = llm_pipe.model.config\n","        if \"gemma\" in model_config.model_type.lower(): model_max_len = getattr(model_config, 'max_sequence_length', getattr(model_config, 'max_position_embeddings', 8192))\n","        elif hasattr(model_config, 'max_position_embeddings'): model_max_len = model_config.max_position_embeddings\n","        elif hasattr(model_config, 'max_sequence_length'): model_max_len = model_config.max_sequence_length\n","        if \"gemma\" in model_config.model_type.lower() and model_max_len > 8192 : model_max_len = 8192\n","\n","        logger_q1_test.log(f\"DEBUG Q1_LLM_CALL: {map_reduce_step_type} for {log_prefix_qid_step}. Prompt toks: {len(prompt_tokens)}. Content toks: {len(input_text_tokens)}. Max new: {max_new_tokens}. Model max_len: {model_max_len}.\")\n","        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.97:\n","             logger_q1_test.log(f\"WARN Q1_LLM_CALL: {log_prefix_qid_step} - Step {map_reduce_step_type} - Prompt+Output tokens might exceed context ({model_max_len}).\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","\n","        generated_text_full = outputs[0]['generated_text']\n","        llm_response_text = generated_text_full\n","        json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", generated_text_full, re.DOTALL)\n","        if json_match: llm_response_text = json_match.group(1).strip()\n","        else:\n","            json_match_alt = re.search(r\"(\\{.*?\\})\", generated_text_full, re.DOTALL)\n","            if json_match_alt: llm_response_text = json_match_alt.group(1).strip()\n","            else: logger_q1_test.log(f\"WARN Q1_LLM_CALL: {map_reduce_step_type} for {log_prefix_qid_step} - No JSON block found. Output may be non-JSON.\")\n","\n","        logger_q1_test.log(f\"    Q1_TEST {map_reduce_step_type.upper()} RAW LLM OUTPUT for {log_prefix_qid_step}:\\n{llm_response_text}\") # Log cleaned/extracted JSON part\n","        return llm_response_text\n","    except Exception as e:\n","        logger_q1_test.log(f\"ERROR Q1_LLM_CALL: {map_reduce_step_type} LLM exception for {log_prefix_qid_step}: {e}\\n{traceback.format_exc()}\")\n","        return f\"LLM_ERROR: Exception - {str(e)}\"\n","\n","# --- Helper: Parse LLM JSON Thematic Output ---\n","def parse_llm_json_themes_q1_test(llm_json_output_text: str, max_themes_to_extract: int) -> List[Dict[str, str]]: # Takes INT now\n","    logger_q1_test.log(f\"DEBUG Q1_JSON_PARSER: Attempting to parse up to {max_themes_to_extract} themes.\") # Now uses int\n","    parsed_themes = []\n","    error_entry = lambda expl: [{\"theme_label\": \"Parsing/LLM Error\", \"explanation\": str(expl)[:500] + \"...\"}]\n","\n","    if not llm_json_output_text or llm_json_output_text.startswith(\"LLM_ERROR:\"):\n","        logger_q1_test.log(f\"DEBUG Q1_JSON_PARSER: LLM Error or empty input: {llm_json_output_text[:100]}\")\n","        return error_entry(llm_json_output_text)\n","    try:\n","        data = json.loads(llm_json_output_text) # Expects cleaned JSON string\n","        if isinstance(data, dict) and \"themes\" in data and isinstance(data[\"themes\"], list):\n","            for theme_obj in data[\"themes\"]:\n","                if len(parsed_themes) >= max_themes_to_extract: break # Compare int with int\n","                if isinstance(theme_obj, dict) and \"theme_label\" in theme_obj and \"explanation\" in theme_obj:\n","                    parsed_themes.append({\"theme_label\": str(theme_obj[\"theme_label\"]), \"explanation\": str(theme_obj[\"explanation\"])})\n","            if parsed_themes:\n","                logger_q1_test.log(f\"DEBUG Q1_JSON_PARSER: Successfully parsed {len(parsed_themes)} themes.\")\n","                return parsed_themes\n","            else:\n","                 logger_q1_test.log(\"WARN Q1_JSON_PARSER: Valid JSON but 'themes' list yielded no usable themes.\")\n","                 return error_entry(f\"Valid JSON, but 'themes' list empty/malformed. Raw: {llm_json_output_text[:200]}\")\n","        else:\n","            logger_q1_test.log(f\"WARN Q1_JSON_PARSER: LLM JSON not expected structure {{'themes': [...]}}. Got: {str(data)[:200]}\")\n","            return error_entry(f\"Valid JSON, unexpected structure. Raw: {llm_json_output_text[:200]}\")\n","    except json.JSONDecodeError as e:\n","        logger_q1_test.log(f\"ERROR Q1_JSON_PARSER: Not valid JSON. Error: {e}. Raw:\\n{llm_json_output_text}\")\n","        return error_entry(f\"JSONDecodeError: {e}. Raw: {llm_json_output_text[:200]}\")\n","    except Exception as e_gen:\n","        logger_q1_test.log(f\"ERROR Q1_JSON_PARSER: Unexpected error: {e_gen}\\n{traceback.format_exc()}\")\n","        return error_entry(f\"Unexpected Parsing Error: {e_gen}. Raw: {llm_json_output_text[:200]}\")\n","\n","# --- Main Test Execution Function ---\n","def run_q1_targeted_thematic_test():\n","    logger_q1_test.log(f\"INFO: Q1 Targeted Test - Starting analysis for QID: {QID_TO_TEST}\")\n","\n","    # --- 0. Load Actual Question Text for QID_TO_TEST ---\n","    question_actual_text_q1 = f\"Question text for {QID_TO_TEST} not loaded (Default)\"\n","    try:\n","        logger_q1_test.log(f\"INFO Q1_TEST: Attempting to load question texts from: {Q1_TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","        if not os.path.exists(Q1_TEST_QUESTION_TEXT_SOURCE_FILE):\n","            raise FileNotFoundError(f\"File not found: {Q1_TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","        with open(Q1_TEST_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:\n","            qtext_source_data = json.load(f_qtext)\n","        if \"question_data\" in qtext_source_data:\n","            q_data_content = qtext_source_data.get(\"question_data\", {}).get(QID_TO_TEST)\n","            if q_data_content and \"question_text\" in q_data_content:\n","                question_actual_text_q1 = q_data_content[\"question_text\"]\n","                logger_q1_test.log(f\"INFO Q1_TEST: Loaded {QID_TO_TEST} text: '{question_actual_text_q1[:100]}...'\")\n","            else: logger_q1_test.log(f\"WARN Q1_TEST: {QID_TO_TEST} text not in 'question_data'.\")\n","        else: logger_q1_test.log(f\"WARN Q1_TEST: Expected 'question_data' key not in {Q1_TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","    except Exception as e: logger_q1_test.log(f\"ERROR Q1_TEST: Failed to load Q text for {QID_TO_TEST}: {e}\\n{traceback.format_exc()}\")\n","\n","    # --- 1. Load LLM ---\n","    llm_pipe_q1_test = None; tokenizer_q1_test = None\n","    try:\n","        logger_q1_test.log(f\"INFO Q1_TEST: Loading LLM: {Q1_TEST_LLM_MODEL_ID} (Quant: {Q1_TEST_USE_QUANTIZATION})\")\n","        tokenizer_q1_test = AutoTokenizer.from_pretrained(Q1_TEST_LLM_MODEL_ID)\n","        if getattr(tokenizer_q1_test, 'pad_token', None) is None : tokenizer_q1_test.pad_token = tokenizer_q1_test.eos_token\n","        bnb_config_q1 = None; effective_quant_q1 = Q1_TEST_USE_QUANTIZATION\n","        if Q1_TEST_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config_q1 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n","            except Exception as e_bnb: effective_quant_q1 = False; logger_q1_test.log(f\"WARN Q1_TEST: BNB failed: {e_bnb}.\")\n","        elif Q1_TEST_USE_QUANTIZATION: effective_quant_q1 = False; logger_q1_test.log(\"WARN Q1_TEST: Quant requested, No CUDA.\")\n","        model_q1_test = AutoModelForCausalLM.from_pretrained(\n","            Q1_TEST_LLM_MODEL_ID,\n","            quantization_config=bnb_config_q1 if effective_quant_q1 else None,\n","            torch_dtype=torch.bfloat16 if not effective_quant_q1 and torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (torch.float32 if not effective_quant_q1 else None),\n","            device_map=\"auto\", trust_remote_code=True )\n","        llm_pipe_q1_test = pipeline(\"text-generation\", model=model_q1_test, tokenizer=tokenizer_q1_test)\n","        logger_q1_test.log(\"INFO Q1_TEST: LLM Pipeline Loaded.\")\n","    except Exception as e: logger_q1_test.log(f\"CRITICAL Q1_TEST: Failed to load LLM: {e}\\n{traceback.format_exc()}\"); return\n","\n","    # --- 2. Load and Filter Data for QID_TO_TEST (e.g., Q1 Summaries for Scenario A) ---\n","    q1_items_from_stage4a = []\n","    try:\n","        with open(Q1_TEST_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:\n","            stage4a_data = json.load(f)\n","        q1_all_items_from_s4a = stage4a_data.get(\"aggregated_pdf_content_by_qid\", {}).get(QID_TO_TEST, [])\n","        # For this Q1 test, let's assume we're testing Scenario A (on summaries) if Q1 was summarized,\n","        # OR Scenario B (on passages) if Q1 was NOT summarized in Phase 1.\n","        # The logs show Q1 has 2 passages and many summaries (implicitly, as total is 113 for Q1).\n","        # Let's try Scenario A on its summaries for this targeted test.\n","        q1_items_from_stage4a = [item['text'] for item in q1_all_items_from_s4a if item.get('type') == 'pdf_summary' and item.get('text')]\n","        if not q1_items_from_stage4a:\n","            logger_q1_test.log(f\"WARN Q1_TEST: No 'pdf_summary' items found for {QID_TO_TEST}. Trying 'pdf_passages'.\")\n","            q1_items_from_stage4a = [item['text'] for item in q1_all_items_from_s4a if item.get('type') == 'pdf_passages' and item.get('text')]\n","            if q1_items_from_stage4a:\n","                 logger_q1_test.log(f\"INFO Q1_TEST: Using {len(q1_items_from_stage4a)} 'pdf_passages' for {QID_TO_TEST} instead.\")\n","            else:\n","                logger_q1_test.log(f\"ERROR Q1_TEST: No usable text (summary or passage) found for {QID_TO_TEST}. Aborting.\")\n","                return\n","        else:\n","             logger_q1_test.log(f\"INFO Q1_TEST: Loaded {len(q1_items_from_stage4a)} 'pdf_summary' items for {QID_TO_TEST}.\")\n","    except Exception as e:\n","        logger_q1_test.log(f\"CRITICAL Q1_TEST: Failed to load Stage 4a data for {QID_TO_TEST}: {e}\\n{traceback.format_exc()}\")\n","        return\n","\n","    # --- Map-Reduce Thematic Analysis for QID_TO_TEST ---\n","    final_parsed_themes_q1 = []\n","    map_llm_outputs_raw_q1 = []\n","    reduce_llm_output_raw = \"N/A\"\n","\n","    # Batch the selected items (summaries or passages)\n","    # The batch_text_items_q1_test function takes a list of strings.\n","    map_batches_q1 = batch_text_items_q1_test(\n","        q1_items_from_stage4a, # This is now List[str]\n","        llm_pipe_q1_test.tokenizer,\n","        Q1_TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK\n","    )\n","    logger_q1_test.log(f\"INFO Q1_TEST: Grouped items for {QID_TO_TEST} into {len(map_batches_q1)} batches for Map step.\")\n","\n","    if not map_batches_q1:\n","        logger_q1_test.log(f\"WARN Q1_TEST: No map batches created for {QID_TO_TEST}. Skipping LLM analysis.\")\n","    elif len(map_batches_q1) == 1: # Direct processing\n","        logger_q1_test.log(f\"INFO Q1_TEST: Single batch for {QID_TO_TEST}. Direct LLM call.\")\n","        direct_input_text_q1 = \"\\n\\n--- Next Excerpt ---\\n\".join(map_batches_q1[0])\n","        # Use Q1_TEST_FINAL_NUM_THEMES for num_themes_to_request_in_prompt for direct call\n","        reduce_llm_output_raw = get_llm_json_themes_q1_test(\n","            direct_input_text_q1, question_actual_text_q1, f\"top {Q1_TEST_FINAL_NUM_THEMES}\",\n","            llm_pipe_q1_test, Q1_TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS, # Using REDUCE tokens as it's final output\n","            f\"{QID_TO_TEST}_direct\", \"DIRECT_PROCESSING\" # A generic type for direct call\n","        )\n","        final_parsed_themes_q1 = parse_llm_json_themes_q1_test(reduce_llm_output_raw, Q1_TEST_FINAL_NUM_THEMES)\n","        map_llm_outputs_raw_q1.append(reduce_llm_output_raw)\n","    else: # Map-Reduce\n","        intermediate_themes_q1_list = [] # Store RAW JSON strings from map step\n","        for i, batch_of_item_strings in enumerate(map_batches_q1):\n","            current_batch_concatenated_text = \"\\n\\n--- Next Excerpt ---\\n\".join(batch_of_item_strings)\n","            logger_q1_test.log(f\"  Q1_TEST MAP BATCH {i+1}/{len(map_batches_q1)} (Items: {len(batch_of_item_strings)}, Chars: {len(current_batch_concatenated_text)})\")\n","            # Debug input text\n","            logger_q1_test.log(f\"DEBUG Q1_LLM_INPUT_CHECK for {QID_TO_TEST}_map_batch{i+1} (MAP_ON_BATCHED_ITEMS):\\n  Type of text_input_for_llm: {type(current_batch_concatenated_text)}\\n  Content snippet (first 300, last 100 chars): {current_batch_concatenated_text[:300]}...{current_batch_concatenated_text[-100:]}\")\n","\n","            map_out = get_llm_json_themes_q1_test(\n","                current_batch_concatenated_text, question_actual_text_q1,\n","                f\"up to {Q1_TEST_NUM_THEMES_FROM_MAP_CHUNK}\", # String instruction for map\n","                llm_pipe_q1_test, Q1_TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                f\"{QID_TO_TEST}_map_batch{i+1}\", \"MAP_ON_BATCHED_ITEMS\"\n","            )\n","            map_llm_outputs_raw_q1.append(map_out)\n","            if not map_out.startswith(\"LLM_ERROR:\"):\n","                intermediate_themes_q1_list.append(map_out) # Add raw JSON string\n","            time.sleep(0.5) # Slightly longer sleep between calls\n","\n","        if intermediate_themes_q1_list:\n","            parsed_intermediate_for_reduce_q1 = []\n","            logger_q1_test.log(f\"DEBUG Q1_TEST: Processing {len(intermediate_themes_q1_list)} intermediate map outputs for reduce step input.\")\n","            for idx, raw_json_map_out in enumerate(intermediate_themes_q1_list):\n","                logger_q1_test.log(f\"  DEBUG Q1_TEST: Parsing map output text for batch {idx+1}: '{raw_json_map_out[:100]}...'\")\n","                # Pass the INTEGER Q1_TEST_NUM_THEMES_FROM_MAP_CHUNK here\n","                parsed_map_themes = parse_llm_json_themes_q1_test(raw_json_map_out, Q1_TEST_NUM_THEMES_FROM_MAP_CHUNK)\n","                logger_q1_test.log(f\"  Q1_TEST PARSED MAP OUTPUT for batch {idx+1}: {parsed_map_themes}\")\n","\n","                for pt_map in parsed_map_themes:\n","                    if not pt_map.get(\"theme_label\",\"\").startswith((\"LLM Error\", \"Parsing/LLM Error\", \"Unparsed\", \"Valid JSON\")):\n","                        parsed_intermediate_for_reduce_q1.append(\n","                            f\"Preliminary Theme from Batch {idx+1}, Point {(len(parsed_intermediate_for_reduce_q1) % Q1_TEST_NUM_THEMES_FROM_MAP_CHUNK) + 1}:\\n\"\n","                            f\"Theme Label: {pt_map.get('theme_label')}\\n\"\n","                            f\"Explanation: {pt_map.get('explanation')}\" )\n","                    else:\n","                         logger_q1_test.log(f\"  WARN Q1_TEST: Map batch {idx+1} parsing issue/LLM error: {pt_map}\")\n","\n","            if parsed_intermediate_for_reduce_q1:\n","                combined_intermediate_text_q1 = \"\\n\\n---\\n\\n\".join(parsed_intermediate_for_reduce_q1)\n","                logger_q1_test.log(f\"INFO Q1_TEST: Combined intermediate themes for REDUCE step (Chars: {len(combined_intermediate_text_q1)}):\\n{combined_intermediate_text_q1[:1000]}...\") # Log snippet\n","                logger_q1_test.log(f\"INFO Q1_TEST: Starting REDUCE step for {QID_TO_TEST}.\")\n","                # Use Q1_TEST_FINAL_NUM_THEMES (int) for the prompt string for reduce\n","                reduce_llm_output_raw = get_llm_json_themes_q1_test(\n","                    combined_intermediate_text_q1, question_actual_text_q1,\n","                    f\"top {Q1_TEST_FINAL_NUM_THEMES}\", # String instruction for reduce\n","                    llm_pipe_q1_test, Q1_TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                    f\"{QID_TO_TEST}_reduce\", \"REDUCE_INTERMEDIATE_THEMES\"\n","                )\n","                final_parsed_themes_q1 = parse_llm_json_themes_q1_test(reduce_llm_output_raw, Q1_TEST_FINAL_NUM_THEMES)\n","            else:\n","                logger_q1_test.log(f\"ERROR Q1_TEST: No successfully parsed intermediate themes from any MAP step for {QID_TO_TEST}.\")\n","                reduce_llm_output_raw = f\"ERROR: No successfully parsed intermediate themes from MAP steps for {QID_TO_TEST}.\"\n","        else:\n","            logger_q1_test.log(f\"ERROR Q1_TEST: No valid (non-error) intermediate themes from MAP step for {QID_TO_TEST}.\")\n","            reduce_llm_output_raw = f\"ERROR: No valid intermediate themes from MAP step for {QID_TO_TEST}.\"\n","\n","    logger_q1_test.log(f\"\\n--- Q1 TARGETED TEST - FINAL PARSED THEMES for {QID_TO_TEST} ---\")\n","    if final_parsed_themes_q1:\n","        for i, theme in enumerate(final_parsed_themes_q1):\n","            logger_q1_test.log(f\"  Theme {i+1}: {theme.get('theme_label')}\\n     Explanation: {theme.get('explanation')}\")\n","    else:\n","        logger_q1_test.log(f\"  No final themes parsed for {QID_TO_TEST}.\")\n","\n","    logger_q1_test.log(f\"  Q1_TEST Raw MAP Outputs (first one, max 500 chars):\")\n","    logger_q1_test.log(f\"  {(map_llm_outputs_raw_q1[0][:500] + '...') if map_llm_outputs_raw_q1 and map_llm_outputs_raw_q1[0] else 'N/A'}\")\n","    logger_q1_test.log(f\"  Q1_TEST Raw REDUCE Output (max 500 chars):\")\n","    logger_q1_test.log(f\"  {(reduce_llm_output_raw[:500] + '...') if reduce_llm_output_raw else 'N/A'}\")\n","\n","    logger_q1_test.log(\"\\nINFO Q1_TEST: Cleaning up LLM resources...\")\n","    if 'llm_pipe_q1_test' in locals(): del llm_pipe_q1_test\n","    if 'model_q1_test' in locals(): del model_q1_test\n","    if 'tokenizer_q1_test' in locals(): del tokenizer_q1_test\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","    logger_q1_test.log(f\"INFO: Q1 Targeted Test Finished for QID: {QID_TO_TEST}\")\n","\n","# --- Execute the Test ---\n","# print(\"REMINDER: Update Q1_TEST_... config variables, especially file paths and LLM ID.\")\n","# print(\"REMINDER: Ensure FileLogger class is defined and prerequisite cells (0, 1) are run.\")\n","# print(\"REMINDER: To run the targeted Q1 test, uncomment the line below.\")\n","run_q1_targeted_thematic_test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKotHF5ebj_R"},"outputs":[],"source":["# @title Cell 2: Global Project Configuration\n","\n","import os\n","import time\n","\n","# --- Base Paths ---\n","# !!! CRITICAL: UPDATE THIS TO YOUR MAIN PROJECT FOLDER !!!\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n","\n","# --- Derived Output Directories (will be created if they don't exist) ---\n","PHASE1_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase1_PDF_Extraction_Outputs/')\n","PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n","PHASE3_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase3_LLM_Generated_Themes/')\n","PHASE4_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase4_Final_Thematic_Analysis/')\n","LOG_DIR = os.path.join(BASE_PROJECT_DIR, 'Pipeline_Logs/')\n","\n","for d_path in [PHASE1_OUTPUT_DIR, PHASE2_OUTPUT_DIR, PHASE3_OUTPUT_DIR, PHASE4_OUTPUT_DIR, LOG_DIR]:\n","    os.makedirs(d_path, exist_ok=True)\n","\n","# --- Common Input Files ---\n","# !!! CRITICAL: ENSURE THESE PATHS ARE CORRECT !!!\n","ORIGINAL_RESPONSES_JSONL = os.path.join(BASE_PROJECT_DIR, 'processed_responses.jsonl')\n","QUESTION_DEFINITIONS_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # Source of QID -> Question Text\n","\n","# --- Phase 1: PDF Extraction & Per-Response Summarization (Original Stage 4) ---\n","P1_LOG_FILE = os.path.join(LOG_DIR, \"phase1_pdf_extraction.log\")\n","# !!! CRITICAL: This should point to your ALREADY EXISTING, COMPLETE output from the long Stage 4 run !!!\n","P1_OUTPUT_FILE = os.path.join(PHASE1_OUTPUT_DIR, 'pdf_passage_analysis_by_response_FULL.json')\n","P1_STATE_FILE = os.path.join(PHASE1_OUTPUT_DIR, 'phase1_original_stage4_processing_state.json') # State file for Phase 1\n","P1_EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n","P1_LLM_ID_FOR_SUMMARIES = 'google/gemma-2b-it' # Smaller model for per-response summaries\n","P1_USE_QUANTIZATION = False # Set based on your preference/capability for this LLM\n","P1_MIN_PDF_TEXT_LENGTH = 100\n","P1_MIN_SIMILARITY_SCORE = 0.5\n","P1_CONTEXT_WINDOW_SIZE_SENTENCES = 1\n","P1_TOP_K_PASSAGES = 3\n","P1_NUM_SUMMARY_SENTENCES_PER_RESPONSE_QID = 3\n","P1_MIN_CHARS_FOR_SUMMARY = 150\n","P1_LLM_MAX_NEW_TOKENS_FOR_PER_RESPONSE_SUMMARY = 200 # Max new tokens for P1 summaries\n","\n","P1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE = {\n","    \"Q1\": True, \"Q2\": True, \"Q3\": True, \"Q4\": True, \"Q5\": True, \"Q6\": True, \"Q7\": True,\n","    \"Q8\": True, \"Q9\": True, \"Q10\": True,\"Q11\": True, \"Q12\": True, \"Q13\": True, \"Q14\": True,\n","    \"Q15\": True, \"Q16\": False,\"Q17\": True, \"Q18\": True, \"Q19\": True,\"Q20\": False, \"Q21\": True,\n","    \"Q22\": True, \"Q23\": True, \"Q24\": True, \"Q25\": True, \"Q26\": True, \"Q27\": True, \"Q28\": True,\n","    \"Q29\": True, \"Q30\": True, \"Q31\": True,\"Q32\": False,\"Q33\": False,\"Q34\": False,\"Q35\": False,\n","    \"Q36\": False,\n","}\n","P1_MAX_RUNTIME_MINUTES = 60 * 1 # Shorter for reruns if state exists, adjust if running from scratch\n","P1_SAVE_STATE_INTERVAL_SECONDS = 60 * 10\n","\n","# --- Phase 2: Collation of PDF Content (Original Stage 4a) ---\n","# Input P2_INPUT_FILE_OR_DIR will be set to P1_OUTPUT_FILE in orchestration\n","P2_LOG_FILE = os.path.join(LOG_DIR, f'phase2_text_collation_{time.strftime(\"%Y%m%d\")}.log')\n","P2_OUTPUT_COLLATED_JSON = os.path.join(PHASE2_OUTPUT_DIR, f'phase2_collated_pdf_texts_{time.strftime(\"%Y%m%d_%H%M%S\")}.json')\n","\n","# --- Phase 3: LLM Thematic Point Generation (Map-Reduce from your Test Cell) ---\n","P3_LOG_FILE = os.path.join(LOG_DIR, f'phase3_llm_themes_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","P3_INPUT_COLLATED_JSON = P2_OUTPUT_COLLATED_JSON # Output from Phase 2\n","P3_OUTPUT_LLM_THEMES_JSON = os.path.join(PHASE3_OUTPUT_DIR, f'phase3_llm_generated_themes_{time.strftime(\"%Y%m%d_%H%M%S\")}.json')\n","P3_LLM_ID_FOR_THEMES = 'google/gemma-3-4b-it' # Or your preferred model like 'google/gemma-7b-it'\n","P3_USE_QUANTIZATION = False # Set based on LLM choice and resources\n","P3_NUM_THEMES_TO_REQUEST_FROM_LLM = \"up to 3\"\n","P3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2800\n","P3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 500\n","P3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 750\n","P3_MAPREDUCE_CHUNK_OVERLAP_WORDS = 30\n","P3_QIDS_TO_PROCESS_THEMATICALLY = None # Set to None for all QIDs, or e.g., [\"Q4\", \"Q16\"] to test subset\n","\n","# --- Phase 4: Theme Validation, Quantification & Quotation Extraction ---\n","P4_LOG_FILE = os.path.join(LOG_DIR, f'phase4_theme_quantification_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","P4_INPUT_LLM_THEMES_JSON = P3_OUTPUT_LLM_THEMES_JSON # Output from Phase 3\n","P4_INPUT_COLLATED_TEXTS_JSON = P2_OUTPUT_COLLATED_JSON # Source texts from Phase 2\n","P4_OUTPUT_FINAL_ANALYSIS_JSON = os.path.join(PHASE4_OUTPUT_DIR, f'phase4_final_thematic_report_{time.strftime(\"%Y%m%d_%H%M%S\")}.json')\n","P4_HUMAN_DEFINED_KEYPHRASES_FILE = os.path.join(BASE_PROJECT_DIR, 'human_defined_keyphrases_for_themes.json')\n","P4_NUM_EXAMPLE_QUOTES_PER_THEME = 2\n","\n","# --- General ---\n","# This flag is used by helper functions within phases if they have detailed debug paths\n","ENABLE_DETAILED_DEBUG_LOGGING = False # Set True for very verbose logs within phases\n","\n","print(f\"INFO: Cell 2 - Global Configuration loaded. Base project directory: {BASE_PROJECT_DIR}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYpy3nrqcYqV"},"outputs":[],"source":["# @title Cell 3: Phase 1 - PDF Extraction & Per-Response Summarization (Original Stage 4)\n","\n","# This cell contains the full logic of your original Stage 4,\n","# which is now designated as Phase 1 of the PDF analysis pipeline.\n","# It reads raw responses, extracts PDF text, finds relevant passages,\n","# and (for configured QIDs) generates a summary for those passages per response.\n","\n","# Ensure logger_p1 is initialized here or that run_phase1_pdf_extraction handles it.\n","# logger_p1 = FileLogger(P1_LOG_FILE) # Defined in orchestration cell or here\n","\n","# --- Helper Functions for Phase 1 (Copied from your original Stage 4 / Cell 3 of full pipeline) ---\n","\n","def save_phase1_state(state_file, processed_response_ids_set, current_results_dict, logger_instance):\n","    logger_instance.log(f\"INFO: Saving Phase 1 processing state to {state_file}...\")\n","    try:\n","        state_data = {\n","            \"processed_responses_ids\": sorted(list(processed_response_ids_set)),\n","            \"results_so_far\": current_results_dict\n","        }\n","        temp_state_file = state_file + \".tmp\"\n","        with open(temp_state_file, 'w', encoding='utf-8') as f:\n","            json.dump(state_data, f, indent=2, default=logger_instance._get_serializer())\n","        os.replace(temp_state_file, state_file)\n","        logger_instance.log(f\"INFO: Phase 1 State saved ({len(processed_response_ids_set)} responses).\")\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR: Failed to save Phase 1 state to {state_file}: {e}\\n{traceback.format_exc()}\")\n","\n","def load_phase1_state(state_file, logger_instance):\n","    if os.path.exists(state_file):\n","        logger_instance.log(f\"INFO: Found Phase 1 state file {state_file}. Loading...\")\n","        try:\n","            with open(state_file, 'r', encoding='utf-8') as f: state_data = json.load(f)\n","            processed_ids = set(state_data.get(\"processed_responses_ids\", []))\n","            results = state_data.get(\"results_so_far\", {})\n","            if not isinstance(results, dict):\n","                logger_instance.log(\"ERROR: Loaded 'results_so_far' not a dict. Starting fresh.\")\n","                return set(), {}\n","            logger_instance.log(f\"INFO: Loaded Phase 1 state. Resuming after {len(processed_ids)} responses.\")\n","            return processed_ids, results\n","        except Exception as e:\n","            logger_instance.log(f\"ERROR: Failed to load/parse Phase 1 state {state_file}: {e}. Starting fresh.\")\n","            return set(), {}\n","    else:\n","        logger_instance.log(\"INFO: No Phase 1 state file found. Starting fresh.\")\n","        return set(), {}\n","\n","def get_phase1_local_embeddings(texts: List[str], model: SentenceTransformer, device: str, logger_instance, batch_size: int = 64):\n","    if not texts: return None\n","    try:\n","        if ENABLE_DETAILED_DEBUG_LOGGING: logger_instance.log(f\"DEBUG P1: Generating embeddings for {len(texts)} texts on {device}...\")\n","        model.to(device)\n","        embeddings_tensor = model.encode(texts, convert_to_tensor=True, show_progress_bar=ENABLE_DETAILED_DEBUG_LOGGING, batch_size=batch_size, device=device)\n","        return embeddings_tensor.cpu().numpy()\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P1: Failed local embeddings: {e}\\n{traceback.format_exc()}\")\n","        return None\n","\n","def retrieve_phase1_relevant_passages(question_embedding: np.ndarray, pdf_sentences: List[str],\n","                                     pdf_sentence_embeddings: np.ndarray, similarity_threshold: float,\n","                                     top_k: int, context_window_sentences: int, logger_instance):\n","    # ... (Full retrieve_relevant_passages logic from your Cell 3, using logger_instance)\n","    # ... (Ensure sbert_util is imported as from sentence_transformers import util as sbert_util in Cell 0)\n","    relevant_passages_texts = []\n","    highest_score_found = None\n","    if question_embedding is None or pdf_sentence_embeddings is None or not pdf_sentences:\n","        return relevant_passages_texts, highest_score_found\n","    if question_embedding.ndim == 1: question_embedding = question_embedding.reshape(1, -1)\n","    try:\n","        target_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Or pass device\n","        q_tensor = torch.from_numpy(question_embedding).to(dtype=torch.float32, device=target_device)\n","        s_tensor = torch.from_numpy(pdf_sentence_embeddings).to(dtype=torch.float32, device=target_device)\n","\n","        cosine_scores_tensor = sbert_util.cos_sim(q_tensor, s_tensor)[0]\n","        cosine_scores = cosine_scores_tensor.cpu().numpy()\n","        sorted_indices = np.argsort(cosine_scores)[::-1]\n","        added_passage_hashes = set()\n","        for idx in sorted_indices:\n","            score = cosine_scores[idx]\n","            if score < similarity_threshold: break\n","            if highest_score_found is None: highest_score_found = float(score)\n","            start_idx = max(0, idx - context_window_sentences)\n","            end_idx = min(len(pdf_sentences), idx + context_window_sentences + 1)\n","            if start_idx >= end_idx : continue # Should not happen if idx is valid\n","            context_passage_list = pdf_sentences[start_idx:end_idx]\n","            passage_text = \" \".join(context_passage_list).strip()\n","            passage_hash = hash(passage_text)\n","            if passage_text and passage_hash not in added_passage_hashes:\n","                relevant_passages_texts.append(passage_text)\n","                added_passage_hashes.add(passage_hash)\n","                if len(relevant_passages_texts) >= top_k: break\n","        return relevant_passages_texts, highest_score_found\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P1: Passage retrieval failed: {e}\\n{traceback.format_exc()}\")\n","        return [], None\n","\n","\n","def summarize_phase1_context_block(text_chunk: str, question_text_for_prompt: str, qid_for_log: str,\n","                                 llm_pipe: pipeline, num_target_sentences: int, max_new_tokens: int, logger_instance):\n","    # ... (Full summarize_context_block logic from your Cell 3, using logger_instance and ENABLE_DETAILED_DEBUG_LOGGING)\n","    # ... (Remember the prompt was for \"responses to the question\", if text_chunk is just one response's passages, adjust prompt)\n","    if not text_chunk or not llm_pipe: return None\n","    # Prompt for summarizing passages from a single PDF for a single question\n","    messages = [{\n","        \"role\": \"user\",\n","        \"content\": f\"Carefully review the following text excerpt extracted from a PDF submission, which is relevant to the question: \\\"{question_text_for_prompt}\\\". \"\n","                   f\"Concisely summarize the main points from THIS EXCERPT in approximately {num_target_sentences} distinct sentences. \"\n","                   f\"Focus on capturing the essence of this specific excerpt as it relates to the question.\\n\\nExcerpt:\\n\\\"\\\"\\\"\\n{text_chunk}\\n\\\"\\\"\\\"\\n\\nSummary of This Excerpt:\"\n","    }]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        generation_args = {\n","            \"max_new_tokens\": max_new_tokens, \"do_sample\": False,\n","            \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id,\n","        }\n","        if ENABLE_DETAILED_DEBUG_LOGGING: logger_instance.log(f\"DEBUG P1: LLM summary call for QID {qid_for_log} (Excerpt chars: {len(text_chunk)})...\")\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger_instance.log(f\"ERROR P1: LLM summary empty output for QID {qid_for_log}: {outputs}\")\n","            return None\n","        generated_text_full = outputs[0]['generated_text']\n","        summary_text = \"\" # Initialize\n","        if generated_text_full.startswith(prompt_formatted):\n","            summary_text = generated_text_full[len(prompt_formatted):].strip()\n","        else:\n","            response_marker = \"<start_of_turn>model\\n\"\n","            summary_start_index = generated_text_full.rfind(response_marker)\n","            if summary_start_index != -1:\n","                summary_text = generated_text_full[summary_start_index + len(response_marker):].strip()\n","            else:\n","                logger_instance.log(f\"WARN P1: No standard marker for QID {qid_for_log}. Using full output minus prompt attempt.\")\n","                if prompt_formatted in generated_text_full: summary_text = generated_text_full.split(prompt_formatted, 1)[-1].strip()\n","                else: summary_text = generated_text_full\n","        if not summary_text.strip() or summary_text.startswith(\"Error:\"):\n","            logger_instance.log(f\"WARN P1: Final summary for QID {qid_for_log} empty/error. Raw: {generated_text_full[:200]}\")\n","            return \"\"\n","        return summary_text\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P1: LLM summarization exception for QID {qid_for_log}: {e}\\n{traceback.format_exc()}\")\n","        return None\n","\n","# --- Main Execution Function for Phase 1 ---\n","def run_phase1_pdf_extraction(logger_p1_instance): # Pass logger\n","    # global logger_p1 # Not needed if passed as argument\n","    logger_p1 = logger_p1_instance # Use the passed logger\n","    logger_p1.log(\"=\" * 50); logger_p1.log(\" INFO: Starting Phase 1: PDF Content Extraction (Original Stage 4 Logic)\"); logger_p1.log(\"=\" * 50)\n","    # ... (Full logic from Cell 4 of your full pipeline, starting from \"phase1_device = ...\")\n","    # Make sure all config variables are P1_... (e.g., P1_EMBEDDING_MODEL, P1_LLM_ID_FOR_SUMMARIES)\n","    # Make sure helper functions are called with logger_p1\n","    # Example adaptation:\n","    # processed_response_ids_p1, results_p1 = load_phase1_state(P1_STATE_FILE, logger_p1)\n","    # ...\n","    # question_embeddings_all_p1 = get_phase1_local_embeddings(target_qtexts_p1, embedding_model_p1, phase1_device, logger_p1)\n","    # ...\n","    # save_phase1_state(P1_STATE_FILE, processed_response_ids_p1, results_p1, logger_p1)\n","    # ...\n","    # And the final save to P1_OUTPUT_FILE\n","    # Ensure it returns True on success, False on critical failure.\n","\n","    # --- This is a condensed version of your previous Cell 4 logic ----\n","    phase1_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    logger_p1.log(f\"INFO P1: Using device: {phase1_device}\")\n","\n","    embedding_model_p1 = None; llm_tokenizer_p1 = None; llm_model_p1 = None; llm_summarization_pipe_p1 = None\n","    try:\n","        logger_p1.log(f\"INFO P1: Loading embedding model: {P1_EMBEDDING_MODEL}...\")\n","        embedding_model_p1 = SentenceTransformer(P1_EMBEDDING_MODEL, device=phase1_device)\n","        logger_p1.log(\"INFO P1: Embedding model loaded.\")\n","        logger_p1.log(f\"INFO P1: Loading LLM tokenizer for: {P1_LLM_ID_FOR_SUMMARIES}...\")\n","        llm_tokenizer_p1 = AutoTokenizer.from_pretrained(P1_LLM_ID_FOR_SUMMARIES)\n","        if getattr(llm_tokenizer_p1, 'pad_token', None) is None: llm_tokenizer_p1.pad_token = llm_tokenizer_p1.eos_token\n","        logger_p1.log(\"INFO P1: LLM Tokenizer loaded.\")\n","        bnb_config_p1 = None; current_run_p1_quant = P1_USE_QUANTIZATION\n","        if P1_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config_p1 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n","            except Exception as qe: logger_p1.log(f\"WARN P1: BNB config failed: {qe}. Disabling quant.\"); current_run_p1_quant=False\n","        elif P1_USE_QUANTIZATION: current_run_p1_quant=False; logger_p1.log(\"WARN P1: Quant requested, no CUDA.\")\n","\n","        logger_p1.log(f\"INFO P1: Loading LLM model: {P1_LLM_ID_FOR_SUMMARIES} (Quant: {current_run_p1_quant})...\")\n","        llm_model_p1 = AutoModelForCausalLM.from_pretrained(\n","            P1_LLM_ID_FOR_SUMMARIES,\n","            quantization_config=bnb_config_p1 if current_run_p1_quant else None,\n","            torch_dtype=torch.bfloat16 if not current_run_p1_quant and torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (torch.float32 if not current_run_p1_quant else None),\n","            device_map=\"auto\", trust_remote_code=True)\n","        llm_summarization_pipe_p1 = pipeline(\"text-generation\", model=llm_model_p1, tokenizer=llm_tokenizer_p1)\n","        logger_p1.log(\"INFO P1: LLM models loaded.\")\n","    except Exception as le: logger_p1.log(f\"CRITICAL P1: Model loading failed: {le}\\n{traceback.format_exc()}\"); return False\n","\n","    qid_to_text_map_p1 = {}; qid_to_embedding_map_p1 = {}\n","    try: # Load QID map\n","        with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f: q_data = json.load(f)\n","        for qid, data in q_data.get(\"question_data\", {}).items(): qid_to_text_map_p1[qid] = data.get(\"question_text\")\n","        if not qid_to_text_map_p1: raise ValueError(\"No QID texts.\")\n","        logger_p1.log(f\"INFO P1: Loaded {len(qid_to_text_map_p1)} QID texts.\")\n","        target_qids_p1 = list(qid_to_text_map_p1.keys()); target_qtexts_p1 = list(qid_to_text_map_p1.values())\n","        q_embeds = get_phase1_local_embeddings(target_qtexts_p1, embedding_model_p1, phase1_device, logger_p1)\n","        if q_embeds is None: raise ValueError(\"Q embeds failed.\")\n","        qid_to_embedding_map_p1 = {qid: emb for qid, emb in zip(target_qids_p1, q_embeds)}\n","    except Exception as qle: logger_p1.log(f\"ERROR P1: QID map/embeds failed: {qle}\\n{traceback.format_exc()}\"); return False\n","\n","    processed_response_ids_p1, results_p1 = load_phase1_state(P1_STATE_FILE, logger_p1)\n","    initial_processed_count = len(processed_response_ids_p1)\n","\n","    processed_in_this_run_p1 = 0; pdfs_analyzed_this_run_p1 = 0; llm_sums_att_p1 = 0; llm_sums_succ_p1 = 0\n","    start_processing_time_p1 = time.time(); last_save_time_p1 = start_processing_time_p1\n","    MAX_RUNTIME_SECONDS_P1 = P1_MAX_RUNTIME_MINUTES * 60\n","\n","    total_lines_in_input = 0\n","    try:\n","        with open(ORIGINAL_RESPONSES_JSONL, 'r', encoding='utf-8') as infile_count: total_lines_in_input = sum(1 for _ in infile_count)\n","    except: total_lines_in_input = -1\n","    logger_p1.log(f\"INFO P1: Total lines in input: {total_lines_in_input}. Resuming from {initial_processed_count} processed.\")\n","\n","    try:\n","        with open(ORIGINAL_RESPONSES_JSONL, 'r', encoding='utf-8') as infile:\n","            for line_num, line in enumerate(infile, 1):\n","                if total_lines_in_input > 0 and line_num % (total_lines_in_input // 20 or 1) == 0:\n","                    logger_p1.log(f\"PROGRESS P1: Line {line_num}/{total_lines_in_input} ({(line_num/total_lines_in_input)*100:.1f}%).\")\n","                current_elapsed_time = time.time() - start_processing_time_p1\n","                if current_elapsed_time >= MAX_RUNTIME_SECONDS_P1 and MAX_RUNTIME_SECONDS_P1 > 0 : # Check if max runtime is set\n","                    logger_p1.log(f\"WARN P1: Runtime limit reached ({MAX_RUNTIME_SECONDS_P1}s). Stopping.\"); break\n","                if time.time() - last_save_time_p1 >= P1_SAVE_STATE_INTERVAL_SECONDS:\n","                    save_phase1_state(P1_STATE_FILE, processed_response_ids_p1, results_p1, logger_p1)\n","                    last_save_time_p1 = time.time()\n","\n","                response_id_p1 = f\"UnknownLine{line_num}\"\n","                try:\n","                    response_obj_p1 = json.loads(line); response_id_p1 = str(response_obj_p1.get(\"id\", response_id_p1))\n","                    if response_id_p1 in processed_response_ids_p1: continue\n","\n","                    t_response_start = time.time()\n","                    if processed_in_this_run_p1 < 5 or processed_in_this_run_p1 % 100 == 0 or ENABLE_DETAILED_DEBUG_LOGGING:\n","                        logger_p1.log(f\"INFO P1: BEGIN Response ID {response_id_p1} (Line: {line_num}, New: {processed_in_this_run_p1 + 1})\")\n","\n","                    results_p1[response_id_p1] = {}\n","                    pdf_text_p1 = response_obj_p1.get(\"pdf_text\")\n","                    if not pdf_text_p1 or len(pdf_text_p1) < P1_MIN_PDF_TEXT_LENGTH:\n","                        results_p1[response_id_p1][\"status\"] = \"pdf_missing_or_short\"\n","                    else:\n","                        _cleaned_pdf_text_p1 = re.sub(r'\\s+', ' ', pdf_text_p1).strip()\n","                        _pdf_sentences_p1 = nltk.sent_tokenize(_cleaned_pdf_text_p1)\n","                        _pdf_sentences_p1 = [s.strip() for s in _pdf_sentences_p1 if s.strip()]\n","                        _pdf_sentence_embeddings_p1 = None\n","                        if _pdf_sentences_p1:\n","                            _pdf_sentence_embeddings_p1 = get_phase1_local_embeddings(_pdf_sentences_p1, embedding_model_p1, phase1_device, logger_p1)\n","\n","                        if not _pdf_sentences_p1 or _pdf_sentence_embeddings_p1 is None:\n","                            results_p1[response_id_p1][\"status\"] = \"pdf_no_sentences_or_embedding_error\"\n","                        else:\n","                            pdfs_analyzed_this_run_p1 +=1\n","                            for qid_p1 in target_qids_p1:\n","                                q_text_p1 = qid_to_text_map_p1[qid_p1]; q_emb_p1 = qid_to_embedding_map_p1.get(qid_p1)\n","                                current_q_result_p1 = {\"question_text\": q_text_p1}\n","                                if q_emb_p1 is None: current_q_result_p1[\"status\"] = \"error_missing_q_embedding\"\n","                                else:\n","                                    passages_p1, top_score_p1 = retrieve_phase1_relevant_passages(\n","                                        q_emb_p1, _pdf_sentences_p1, _pdf_sentence_embeddings_p1,\n","                                        P1_MIN_SIMILARITY_SCORE, P1_TOP_K_PASSAGES, P1_CONTEXT_WINDOW_SIZE_SENTENCES, logger_p1\n","                                    )\n","                                    if passages_p1:\n","                                        current_q_result_p1.update({\"status\": \"passages_extracted\", \"extracted_passages\": passages_p1,\n","                                                                    \"top_passage_score\": float(top_score_p1) if top_score_p1 is not None else None})\n","                                        if llm_summarization_pipe_p1 and P1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE.get(qid_p1, False):\n","                                            combined_passage_text_p1 = \"\\n\\n\".join(passages_p1)\n","                                            if len(combined_passage_text_p1) >= P1_MIN_CHARS_FOR_SUMMARY:\n","                                                llm_sums_att_p1 += 1\n","                                                summary_p1 = summarize_phase1_context_block(combined_passage_text_p1, q_text_p1, qid_p1,\n","                                                                                            llm_summarization_pipe_p1, P1_NUM_SUMMARY_SENTENCES_PER_RESPONSE_QID,\n","                                                                                            P1_LLM_MAX_NEW_TOKENS_FOR_PER_RESPONSE_SUMMARY, logger_p1)\n","                                                if summary_p1 is not None:\n","                                                    current_q_result_p1[\"summary\"] = summary_p1\n","                                                    if summary_p1: current_q_result_p1[\"status\"] = \"success_summarized\"; llm_sums_succ_p1 +=1\n","                                                    else: current_q_result_p1[\"status\"] = \"summary_attempted_empty\"\n","                                                else: current_q_result_p1[\"status\"] = \"error_llm_summarization\"\n","                                            else: current_q_result_p1[\"status\"] = \"passages_too_short_for_summary\"\n","                                    else: current_q_result_p1[\"status\"] = \"no_relevant_passages_found\"\n","                                results_p1[response_id_p1][qid_p1] = current_q_result_p1\n","\n","                    processed_response_ids_p1.add(response_id_p1)\n","                    processed_in_this_run_p1 += 1\n","                    if processed_in_this_run_p1 < 5 or processed_in_this_run_p1 % 100 == 0 or ENABLE_DETAILED_DEBUG_LOGGING:\n","                        logger_p1.log(f\"  DONE Response ID {response_id_p1}. Time: {time.time() - t_response_start:.2f}s. New in run: {processed_in_this_run_p1}.\")\n","                    if processed_in_this_run_p1 % 200 == 0 : gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","                except json.JSONDecodeError: logger_p1.log(f\"WARN P1: JSON decode error line {line_num}. Skipping.\"); results_p1[response_id_p1]={\"status\":\"json_error\"}; processed_response_ids_p1.add(response_id_p1)\n","                except Exception as e_resp: logger_p1.log(f\"ERROR P1: Processing response {response_id_p1} failed: {e_resp}\\n{traceback.format_exc()}\"); results_p1[response_id_p1]={\"status\":\"processing_error\"}; processed_response_ids_p1.add(response_id_p1)\n","\n","    except FileNotFoundError: logger_p1.log(f\"ERROR P1: Input file {ORIGINAL_RESPONSES_JSONL} not found.\"); return False\n","    except Exception as e_main_loop: logger_p1.log(f\"CRITICAL P1: Main loop error: {e_main_loop}\\n{traceback.format_exc()}\"); return False\n","    finally:\n","        logger_p1.log(f\"INFO P1: Loop finished. New responses this run: {processed_in_this_run_p1}. PDFs analyzed: {pdfs_analyzed_this_run_p1}. Summaries Att: {llm_sums_att_p1}, Succ: {llm_sums_succ_p1}.\")\n","        save_phase1_state(P1_STATE_FILE, processed_response_ids_p1, results_p1, logger_p1)\n","        logger_p1.log(f\"INFO P1: Saving all {len(results_p1)} accumulated results to {P1_OUTPUT_FILE}...\")\n","        try:\n","            with open(P1_OUTPUT_FILE, 'w', encoding='utf-8') as f_final:\n","                json.dump({\"pdf_analysis_by_response\": results_p1}, f_final, indent=2, default=logger_p1._get_serializer())\n","            logger_p1.log(\"INFO P1: Final results saved.\")\n","        except Exception as e_final_save: logger_p1.log(f\"ERROR P1: Final save failed: {e_final_save}\\n{traceback.format_exc()}\")\n","\n","        logger_p1.log(\"INFO P1: Cleaning up models...\")\n","        del embedding_model_p1, llm_tokenizer_p1, llm_model_p1, llm_summarization_pipe_p1, qid_to_embedding_map_p1\n","        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","        logger_p1.log(\"INFO P1: Cleanup complete.\")\n","\n","    logger_p1.log(\"=\" * 50); logger_p1.log(\" INFO: Phase 1 Processing Finished \"); logger_p1.log(\"=\" * 50)\n","    return True\n","\n","print(\"INFO: Cell 3 - Phase 1 logic defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xtyd87uwcuh8"},"outputs":[],"source":["# @title Cell 4: Phase 2 - Collation of PDF Content (Original Stage 4a)\n","\n","# logger_p2 = FileLogger(P2_LOG_FILE) # Defined in orchestration cell or here\n","\n","def clean_text_p2(text: str) -> str: # Added type hint\n","    if not text or not isinstance(text, str): return \"\"\n","    text = re.sub(r'\\s*\\n\\s*', '\\n', text).strip() # Consolidate multiple newlines then strip\n","    text = re.sub(r'[ \\t]+', ' ', text).strip()    # Consolidate multiple spaces/tabs then strip\n","    return text\n","\n","def run_phase2_collate_pdf_texts(logger_p2_instance): # Pass logger\n","    # global logger_p2 # Not needed\n","    logger_p2 = logger_p2_instance\n","    logger_p2.log(\"=\" * 50); logger_p2.log(\" INFO: Starting Phase 2: PDF Text Collation\"); logger_p2.log(\"=\" * 50)\n","\n","    input_phase1_path = P1_OUTPUT_FILE # Corrected to use P1_OUTPUT_FILE from global config\n","    output_collated_json_file = P2_OUTPUT_COLLATED_JSON\n","\n","    if not os.path.exists(input_phase1_path):\n","        logger_p2.log(f\"ERROR P2: Input file from Phase 1 '{input_phase1_path}' does not exist. Phase 1 might need to be run or output path verified.\")\n","        return False\n","\n","    output_dir_p2 = os.path.dirname(output_collated_json_file)\n","    if output_dir_p2 and not os.path.exists(output_dir_p2): # Should have been created by global config\n","        os.makedirs(output_dir_p2, exist_ok=True)\n","        logger_p2.log(f\"INFO P2: Created output directory for Phase 2: '{output_dir_p2}'\")\n","\n","    # Phase 1 produces a single JSON file, so files_to_process_p2 will be just that one file.\n","    files_to_process_p2 = [input_phase1_path]\n","    logger_p2.log(f\"INFO P2: Processing Phase 1 output file: '{input_phase1_path}'\")\n","\n","    qid_collected_content_p2 = defaultdict(list)\n","    total_content_items_collated_p2 = 0\n","    summaries_collated_count_p2 = 0\n","    passages_collated_count_p2 = 0\n","    all_processed_response_ids_p2 = set()\n","\n","    for file_idx, file_path in enumerate(files_to_process_p2): # Loop will run once\n","        # logger_p2.log(f\"INFO P2: Processing file {file_idx + 1}/{len(files_to_process_p2)}: '{os.path.basename(file_path)}'...\") # Log less if only one file\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                phase1_output_data_full = json.load(f) # Load the entire JSON from Phase 1\n","\n","            pdf_analysis_by_response_p2 = phase1_output_data_full.get(\"pdf_analysis_by_response\")\n","            if not pdf_analysis_by_response_p2 or not isinstance(pdf_analysis_by_response_p2, dict) :\n","                logger_p2.log(f\"  WARN P2: 'pdf_analysis_by_response' key not found, empty, or not a dict in '{file_path}'.\")\n","                continue\n","\n","            logger_p2.log(f\"INFO P2: Processing {len(pdf_analysis_by_response_p2)} responses from '{os.path.basename(file_path)}'.\")\n","            for response_id, response_q_data_map in pdf_analysis_by_response_p2.items():\n","                all_processed_response_ids_p2.add(response_id)\n","                if not isinstance(response_q_data_map, dict): continue # Skip status entries like \"pdf_missing_or_short\"\n","\n","                for qid, q_content in response_q_data_map.items():\n","                    if not isinstance(q_content, dict): continue\n","\n","                    content_item_p2 = {\n","                        \"source_file\": os.path.basename(file_path), # From P1_OUTPUT_FILE\n","                        \"response_id\": response_id,\n","                        \"question_id\": qid,\n","                        \"type\": None, \"text\": None\n","                    }\n","                    text_to_add_p2 = None\n","                    summary_p2 = q_content.get(\"summary\") # Summary from Phase 1 output for this resp-QID\n","\n","                    if summary_p2 and isinstance(summary_p2, str) and summary_p2.strip():\n","                        text_to_add_p2 = clean_text_p2(summary_p2)\n","                        content_item_p2[\"type\"] = \"pdf_summary\"\n","                        summaries_collated_count_p2 +=1\n","                    # Fallback to passages if summary is not valid OR if it was explicitly skipped in Phase 1 (no \"summary\" key)\n","                    elif q_content.get(\"extracted_passages\"):\n","                        passages_p2 = q_content.get(\"extracted_passages\")\n","                        if passages_p2 and isinstance(passages_p2, list):\n","                            valid_passages_p2 = [p for p in passages_p2 if isinstance(p, str) and p.strip()]\n","                            if valid_passages_p2:\n","                                text_to_add_p2 = clean_text_p2(\"\\n\\n\".join(valid_passages_p2)) # Join all passages for this resp-QID\n","                                content_item_p2[\"type\"] = \"pdf_passages\"\n","                                passages_collated_count_p2 +=1\n","\n","                    if text_to_add_p2: # Only add if text was actually found (either summary or passages)\n","                        content_item_p2[\"text\"] = text_to_add_p2\n","                        qid_collected_content_p2[qid].append(content_item_p2)\n","                        total_content_items_collated_p2 += 1\n","        except Exception as e:\n","            logger_p2.log(f\"ERROR P2: Processing Phase 1 file '{file_path}' for Phase 2: {e}\")\n","            logger_p2.log(traceback.format_exc())\n","            return False # Critical error during file processing\n","\n","    final_output_data_p2 = {\n","        \"metadata\": {\n","            \"creation_timestamp\": time.strftime('%Y-%m-%d %H:%M:%S %Z', time.gmtime()),\n","            \"source_phase1_output_file\": input_phase1_path, # Log the specific file used\n","            \"total_unique_qids_found\": len(qid_collected_content_p2),\n","            \"total_content_items_collated\": total_content_items_collated_p2,\n","            \"pdf_summaries_collated\": summaries_collated_count_p2,\n","            \"pdf_passages_collated\": passages_collated_count_p2,\n","            \"total_unique_responses_covered\": len(all_processed_response_ids_p2)\n","        },\n","        \"aggregated_pdf_content_by_qid\": dict(qid_collected_content_p2)\n","    }\n","    logger_p2.log(f\"\\n--- Phase 2 Collation Summary ---\") # Changed from Aggregation to Collation\n","    for key, value in final_output_data_p2[\"metadata\"].items(): logger_p2.log(f\"  {key}: {value}\")\n","\n","    try:\n","        with open(output_collated_json_file, 'w', encoding='utf-8') as outfile:\n","            json.dump(final_output_data_p2, outfile, indent=2, default=logger_p2._get_serializer())\n","        logger_p2.log(f\"INFO P2: Successfully saved Phase 2 collated PDF content to '{output_collated_json_file}'.\")\n","    except Exception as e:\n","        logger_p2.log(f\"ERROR P2: Could not write Phase 2 output JSON: {e}\")\n","        logger_p2.log(traceback.format_exc())\n","        return False\n","\n","    logger_p2.log(\"=\" * 50); logger_p2.log(\" INFO: Phase 2 (PDF Text Collation) Finished \"); logger_p2.log(\"=\" * 50)\n","    return True\n","\n","print(\"INFO: Cell 4 - Phase 2 logic defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIrJnMtHdMx2"},"outputs":[],"source":["# @title Cell 5: Phase 3 - LLM Thematic Point Generation (Map-Reduce)\n","\n","# logger_p3 = FileLogger(P3_LOG_FILE) # Defined in orchestration cell or here\n","\n","# --- Helper: Text Chunker (for Scenario A type - concatenating then chunking) ---\n","def create_text_chunks_p3(full_text: str, tokenizer_for_chunking, max_tokens: int, overlap_words: int, logger_instance):\n","    # ... (Same as create_text_chunks_json_test or create_text_chunks_for_scenario_a, using logger_instance)\n","    words = full_text.split()\n","    if not words: return []\n","    chunks = []\n","    current_chunk_words = []\n","    logger_instance.log(f\"DEBUG P3_CHUNKER: Chunking {len(words)} words into max {max_tokens} token chunks.\")\n","    for word_idx, word in enumerate(words): # Simplified from test cell for brevity\n","        current_chunk_words.append(word)\n","        current_text = \" \".join(current_chunk_words)\n","        if len(tokenizer_for_chunking.encode(current_text, add_special_tokens=False)) > max_tokens:\n","            if len(current_chunk_words) > 1 : # If more than one word caused overflow\n","                 chunks.append(\" \".join(current_chunk_words[:-1]))\n","                 # Apply overlap from the chunk just added\n","                 prev_chunk_for_overlap = chunks[-1].split()\n","                 overlap_start = max(0, len(prev_chunk_for_overlap) - overlap_words)\n","                 current_chunk_words = prev_chunk_for_overlap[overlap_start:] + [current_chunk_words[-1]] # Overlap + overflowing word\n","            else: # Single word is too long\n","                 chunks.append(current_text) # Add it as is, will likely be truncated by LLM\n","                 current_chunk_words = []\n","    if current_chunk_words: chunks.append(\" \".join(current_chunk_words))\n","    logger_instance.log(f\"INFO P3_CHUNKER: Split text into {len(chunks)} chunks.\")\n","    return chunks\n","\n","\n","# --- Helper: Batch Individual Items (for Map step on list of texts) ---\n","def batch_text_items_p3(text_items_list: List[str], tokenizer_for_batching, max_tokens_per_batch: int, logger_instance):\n","    # ... (Same as batch_text_items_json_test, using logger_instance)\n","    batches = []\n","    current_batch_item_texts = []\n","    current_batch_concatenated_text = \"\"\n","    logger_instance.log(f\"DEBUG P3_BATCHER: Batching {len(text_items_list)} items into max {max_tokens_per_batch} token batches.\")\n","    for item_idx, item_text_original in enumerate(text_items_list): # text_items_list contains individual text strings\n","        if not item_text_original or not item_text_original.strip(): continue\n","\n","        # Tentatively add the new item (with separator if needed) and check token length\n","        text_for_next_item_in_batch = item_text_original\n","        if current_batch_item_texts: # If current batch is not empty, add separator\n","            text_for_next_item_in_batch = \"\\n\\n--- Next Excerpt ---\\n\" + item_text_original\n","\n","        potential_full_batch_text = current_batch_concatenated_text + text_for_next_item_in_batch\n","        num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))\n","\n","        if num_tokens <= max_tokens_per_batch:\n","            current_batch_item_texts.append(item_text_original) # Store original text\n","            current_batch_concatenated_text = potential_full_batch_text # Update the concatenated version for token check\n","        else: # Current batch + new item is too long\n","            if current_batch_item_texts: # Finalize current batch if it has items\n","                batches.append(current_batch_item_texts)\n","\n","            # Start new batch with the current item_text_original\n","            # Check if this single item itself is too long\n","            single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))\n","            if single_item_tokens > max_tokens_per_batch:\n","                logger_instance.log(f\"  WARN P3_BATCHER: Item {item_idx} (tokens: {single_item_tokens}) > limit ({max_tokens_per_batch}). Will be its own oversized batch.\")\n","                batches.append([item_text_original]) # Add as its own batch (might be truncated by LLM)\n","                current_batch_item_texts = [] # Reset for next\n","                current_batch_concatenated_text = \"\"\n","            else:\n","                current_batch_item_texts = [item_text_original]\n","                current_batch_concatenated_text = item_text_original\n","\n","    if current_batch_item_texts: batches.append(current_batch_item_texts) # Add any remaining batch\n","    logger_instance.log(f\"INFO P3_BATCHER: Created {len(batches)} batches of items.\")\n","    return batches\n","\n","# --- Helper: LLM Call for Thematic Points (Requests JSON Output) ---\n","def get_llm_json_themes_p3(text_input_for_llm: str, question_text_for_prompt: str,\n","                             num_themes_instruction: str, # e.g., \"up to 3\"\n","                             llm_pipe: pipeline, max_new_tokens: int,\n","                             log_prefix_qid_scenario: str, map_reduce_step_type: str, logger_instance):\n","    # ... (Same as get_llm_json_themes_test, using logger_instance and P3_NUM_THEMES_TO_REQUEST_FROM_LLM for instruction)\n","    # ... (Make sure model_max_len detection is robust)\n","    json_format_example = (\"{\\n  \\\"themes\\\": [\\n    {\\\"theme_label\\\": \\\"Label 1\\\", \\\"explanation\\\": \\\"Expl 1.\\\"},\\n\"\n","                           \"    {\\\"theme_label\\\": \\\"Label 2\\\", \\\"explanation\\\": \\\"Expl 2.\\\"}\\n  ]\\n}\")\n","    prompt_instruction = \"\"\n","    # Determine the number for the prompt, e.g. \"up to 3\" -> use 3, \"top 5\" -> use 5\n","    num_themes_for_prompt = 3 # Default\n","    if \"up to\" in num_themes_instruction:\n","        try: num_themes_for_prompt = int(num_themes_instruction.split(\"up to\")[-1].strip())\n","        except: pass\n","\n","    if map_reduce_step_type == \"MAP_ON_BATCHED_ITEMS\": # Processing batches of original items (summaries or passages)\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Review the following BATCH of excerpts/summaries from PDF submissions, all responding to: '{question_text_for_prompt}'.\\n\"\n","            f\"Identify the most prominent and distinct points, arguments, or themes from THIS BATCH, {num_themes_instruction}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object formatted as follows, with a list 'themes', each having 'theme_label' and 'explanation'.\\n\"\n","            f\"Example JSON:\\n{json_format_example}\\n\\n\"\n","            f\"Batch of Excerpts/Summaries:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output:\"\n","        )\n","    elif map_reduce_step_type == \"REDUCE_INTERMEDIATE_THEMES\":\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. The following are preliminary themes/points from different parts of responses to: '{question_text_for_prompt}'.\\n\"\n","            f\"Synthesize these into the overall most significant and distinct common themes, {num_themes_instruction}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object formatted as follows, with a list 'themes', each having 'theme_label' and 'explanation'. Consolidate similar ideas.\\n\"\n","            f\"Example JSON:\\n{json_format_example}\\n\\n\"\n","            f\"Preliminary Themes/Points:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output (Synthesized Top Themes):\"\n","        )\n","    else: # DIRECT_PROCESSING (single large chunk of concatenated summaries/passages)\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Review the following combined text from PDF submissions responding to: '{question_text_for_prompt}'.\\n\"\n","            f\"Identify the most common and distinct points, arguments, or themes, {num_themes_instruction}.\\n\"\n","            f\"Respond *only* with a single, valid JSON object formatted as follows, with a list 'themes', each having 'theme_label' and 'explanation'.\\n\"\n","            f\"Example JSON:\\n{json_format_example}\\n\\n\"\n","            f\"Provided Text:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\nJSON Output:\"\n","        )\n","    messages = [{\"role\": \"user\", \"content\": prompt_instruction}]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": False,\n","                           \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id}\n","        # ... (model_max_len detection and logging as in test cell, using logger_instance) ...\n","        model_config = llm_pipe.model.config\n","        model_max_len = 8192 # Default\n","        if \"gemma\" in model_config.model_type.lower(): model_max_len = getattr(model_config, 'max_sequence_length', getattr(model_config, 'max_position_embeddings', 8192))\n","        elif hasattr(model_config, 'max_position_embeddings'): model_max_len = model_config.max_position_embeddings\n","        elif hasattr(model_config, 'max_sequence_length'): model_max_len = model_config.max_sequence_length\n","        if model_max_len > 8192 and \"gemma\" in model_config.model_type.lower() : model_max_len = 8192 # Cap Gemma for safety\n","\n","        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True)\n","        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.97: # Slightly more aggressive buffer\n","             logger_instance.log(f\"WARN P3_LLM: {log_prefix_qid_scenario} - Step {map_reduce_step_type} - Prompt+Output tokens might exceed context ({model_max_len}).\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","        # ... (LLM output extraction logic as in test cell, using logger_instance) ...\n","        generated_text_full = outputs[0]['generated_text']\n","        llm_response_text = generated_text_full # Default\n","        json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", generated_text_full, re.DOTALL)\n","        if json_match: llm_response_text = json_match.group(1).strip()\n","        else:\n","            json_match_alt = re.search(r\"(\\{.*?\\})\", generated_text_full, re.DOTALL)\n","            if json_match_alt: llm_response_text = json_match_alt.group(1).strip()\n","            else: logger_instance.log(f\"WARN P3_LLM: {map_reduce_step_type} for {log_prefix_qid_scenario} - Could not isolate JSON block. Output may be non-JSON.\")\n","        return llm_response_text\n","    except Exception as e: # Catch all exceptions during LLM call\n","        logger_instance.log(f\"ERROR P3_LLM: {map_reduce_step_type} LLM exception for {log_prefix_qid_scenario}: {e}\\n{traceback.format_exc()}\")\n","        return f\"LLM_ERROR: Exception during generation - {str(e)}\"\n","\n","\n","# --- Helper: Parse LLM JSON Thematic Output ---\n","def parse_llm_json_themes_p3(llm_json_output_text: str, num_themes_instruction: str, logger_instance) -> List[Dict[str, str]]:\n","    # ... (Same as parse_llm_json_themes_test, using logger_instance) ...\n","    # Determine max themes from instruction like \"up to 3\"\n","    max_themes_to_parse = 3 # Default\n","    match_num = re.search(r'\\d+', num_themes_instruction)\n","    if match_num: max_themes_to_parse = int(match_num.group(0))\n","\n","    parsed_themes = []\n","    if not llm_json_output_text or llm_json_output_text.startswith(\"LLM_ERROR:\"):\n","        return [{\"theme_label\": \"LLM Error in Generation\", \"explanation\": llm_json_output_text}]\n","    try:\n","        data = json.loads(llm_json_output_text)\n","        if isinstance(data, dict) and \"themes\" in data and isinstance(data[\"themes\"], list):\n","            for theme_obj in data[\"themes\"]:\n","                if isinstance(theme_obj, dict) and \"theme_label\" in theme_obj and \"explanation\" in theme_obj:\n","                    parsed_themes.append({\"theme_label\": str(theme_obj[\"theme_label\"]), \"explanation\": str(theme_obj[\"explanation\"])})\n","                    if len(parsed_themes) >= max_themes_to_parse: break\n","            if parsed_themes: return parsed_themes\n","            else: return [{\"theme_label\": \"Valid JSON, themes list malformed/empty\", \"explanation\": llm_json_output_text[:200]+\"...\"}]\n","        else: return [{\"theme_label\": \"Valid JSON, unexpected structure\", \"explanation\": llm_json_output_text[:200]+\"...\"}]\n","    except json.JSONDecodeError as e:\n","        logger_instance.log(f\"ERROR P3_JSON_PARSER: LLM output not valid JSON. Error: {e}\\nRaw Output:\\n{llm_json_output_text}\")\n","        return [{\"theme_label\": \"Invalid JSON Output\", \"explanation\": f\"JSONDecodeError. Raw: {llm_json_output_text[:200]}...\"}]\n","    except Exception as e_gen:\n","        logger_instance.log(f\"ERROR P3_JSON_PARSER: Unexpected error: {e_gen}\\nRaw Output:\\n{llm_json_output_text}\")\n","        return [{\"theme_label\": \"Unexpected Parsing Error\", \"explanation\": f\"Error. Raw: {llm_json_output_text[:200]}...\"}]\n","\n","\n","# --- Main Function for Phase 3 ---\n","def run_phase3_llm_thematic_generation(logger_p3_instance): # Pass logger\n","    # global logger_p3 # Not needed\n","    logger_p3 = logger_p3_instance\n","    logger_p3.log(\"=\" * 50); logger_p3.log(\" INFO: Starting Phase 3: LLM Thematic Point Generation (Map-Reduce, JSON Output)\"); logger_p3.log(\"=\" * 50)\n","\n","    # --- Load LLM ---\n","    llm_thematic_pipe_p3 = None; tokenizer_p3 = None # For chunker/batcher\n","    try:\n","        logger_p3.log(f\"INFO P3: Loading LLM: {P3_LLM_ID_FOR_THEMES} (Quant: {P3_USE_QUANTIZATION})\")\n","        tokenizer_p3 = AutoTokenizer.from_pretrained(P3_LLM_ID_FOR_THEMES)\n","        if getattr(tokenizer_p3, 'pad_token', None) is None : tokenizer_p3.pad_token = tokenizer_p3.eos_token\n","        bnb_config_p3 = None; current_p3_quant = P3_USE_QUANTIZATION\n","        if P3_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config_p3 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n","            except Exception as qe: logger_p3.log(f\"WARN P3: BNB config failed: {qe}. Disabling quant.\"); current_p3_quant=False\n","        elif P3_USE_QUANTIZATION: current_p3_quant=False; logger_p3.log(\"WARN P3: Quant requested, no CUDA.\")\n","        model_p3 = AutoModelForCausalLM.from_pretrained( P3_LLM_ID_FOR_THEMES,\n","            quantization_config=bnb_config_p3 if current_p3_quant else None,\n","            torch_dtype=torch.bfloat16 if not current_p3_quant and torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (torch.float32 if not current_p3_quant else None),\n","            device_map=\"auto\", trust_remote_code=True)\n","        llm_thematic_pipe_p3 = pipeline(\"text-generation\", model=model_p3, tokenizer=tokenizer_p3)\n","        logger_p3.log(\"INFO P3: LLM Pipeline Loaded.\")\n","    except Exception as e: logger_p3.log(f\"CRITICAL P3: LLM loading failed: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    # --- Load Collated Texts (Phase 2 output) & Question Definitions ---\n","    try:\n","        with open(P3_INPUT_COLLATED_JSON, 'r', encoding='utf-8') as f: collated_data = json.load(f)\n","        collated_texts_by_qid_p3 = collated_data.get(\"aggregated_pdf_content_by_qid\", {})\n","        if not collated_texts_by_qid_p3 : raise ValueError(\"P2 output empty or malformed.\")\n","        with open(QUESTION_DEFINITIONS_FILE, 'r', encoding='utf-8') as f: q_defs = json.load(f)\n","        qid_to_actual_question_text_p3 = {k:v.get(\"question_text\",\"\") for k,v in q_defs.get(\"question_data\", {}).items()}\n","        logger_p3.log(f\"INFO P3: Loaded collated texts for {len(collated_texts_by_qid_p3)} QIDs and {len(qid_to_actual_question_text_p3)} Q-texts.\")\n","    except Exception as e: logger_p3.log(f\"FATAL P3: Data loading failed: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    overall_thematic_results_p3 = {}; # Load existing if P3_OUTPUT_LLM_THEMES_JSON exists\n","    if os.path.exists(P3_OUTPUT_LLM_THEMES_JSON):\n","        try:\n","            with open(P3_OUTPUT_LLM_THEMES_JSON,'r') as f_exist: overall_thematic_results_p3=json.load(f_exist)\n","            logger_p3.log(f\"INFO P3: Loaded {len(overall_thematic_results_p3)} existing results from {P3_OUTPUT_LLM_THEMES_JSON}\")\n","        except: logger_p3.log(f\"WARN P3: Could not load existing output {P3_OUTPUT_LLM_THEMES_JSON}, starting fresh.\")\n","\n","    qids_to_run = P3_QIDS_TO_PROCESS_THEMATICALLY if P3_QIDS_TO_PROCESS_THEMATICALLY else sorted(collated_texts_by_qid_p3.keys())\n","\n","    for qid_idx, qid in enumerate(qids_to_run):\n","        logger_p3.log(f\"\\n--- P3 Processing QID {qid_idx + 1}/{len(qids_to_run)}: {qid} ---\")\n","        if qid in overall_thematic_results_p3 and overall_thematic_results_p3[qid].get(\"status\", \"\").startswith(\"success\"):\n","            logger_p3.log(f\"INFO P3: QID {qid} already successfully processed. Skipping.\"); continue\n","\n","        question_actual_text = qid_to_actual_question_text_p3.get(qid, f\"Question {qid}\")\n","        # text_items are list of dicts: {'source_file':..., 'text':...}\n","        source_text_items_for_qid = [item for item in collated_texts_by_qid_p3.get(qid, []) if item.get(\"text\")]\n","        if not source_text_items_for_qid:\n","            logger_p3.log(f\"WARN P3: No source texts for QID {qid}. Skipping.\"); overall_thematic_results_p3[qid] = {\"question_text\": question_actual_text, \"status\":\"no_source_texts\", \"parsed_top_themes\":[]}; continue\n","\n","        # Decide strategy: if Q4, use concatenated summaries (Scenario A like), else batch items (Scenario B like)\n","        # This is a simplification. A more robust way is to check if ALL items are summaries for Scenario A.\n","        # For this main pipeline, let's default to Scenario B type: batching individual items from Stage4a.\n","        map_batches_p3 = batch_text_items_p3(\n","            [item['text'] for item in source_text_items_for_qid], # Pass list of text strings\n","            llm_thematic_pipe_p3.tokenizer, P3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK, logger_p3\n","        )\n","\n","        final_llm_output_str = \"N/A\"; raw_map_outputs = []; status_p3=\"init\"\n","\n","        if not map_batches_p3:\n","            logger_p3.log(f\"WARN P3: No map batches created for QID {qid}.\"); status_p3=\"no_map_batches\"\n","        elif len(map_batches_p3) == 1: # Direct processing\n","            logger_p3.log(f\"INFO P3: QID {qid} - Single batch ({len(map_batches_p3[0])} items). Direct LLM call.\")\n","            direct_input_text = \"\\n\\n--- Next Excerpt ---\\n\".join(map_batches_p3[0])\n","            final_llm_output_str = get_llm_json_themes_p3(direct_input_text, question_actual_text, P3_NUM_THEMES_TO_REQUEST_FROM_LLM,\n","                                                          llm_thematic_pipe_p3, P3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS, # Use reduce tokens for final output\n","                                                          f\"{qid}_direct\", \"DIRECT_PROCESSING\", logger_p3)\n","            raw_map_outputs.append(final_llm_output_str)\n","        else: # Map-Reduce\n","            logger_p3.log(f\"INFO P3: QID {qid} - Map-Reduce with {len(map_batches_p3)} map steps.\")\n","            intermediate_themes_text_p3 = []\n","            for i, batch_of_item_texts in enumerate(map_batches_p3):\n","                batch_concat_text = \"\\n\\n--- Next Excerpt ---\\n\".join(batch_of_item_texts)\n","                map_out = get_llm_json_themes_p3(batch_concat_text, question_actual_text, \"up to 3\", # map step gets up to 3\n","                                                 llm_thematic_pipe_p3, P3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                                                 f\"{qid}_map_batch{i+1}\", \"MAP_ON_BATCHED_ITEMS\", logger_p3)\n","                raw_map_outputs.append(map_out)\n","                if not map_out.startswith(\"LLM_ERROR:\"): intermediate_themes_text_p3.append(map_out) # Store raw JSON map output\n","                time.sleep(0.2)\n","\n","            if intermediate_themes_text_p3:\n","                # For reduce, combine the *raw JSON strings* from map outputs or *parsed text*\n","                # Let's try combining parsed text for cleaner reduce input\n","                parsed_intermediate_for_reduce = []\n","                for raw_json_map_out in intermediate_themes_text_p3:\n","                    parsed_map_themes = parse_llm_json_themes_p3(raw_json_map_out, 3, logger_p3)\n","                    for pt in parsed_map_themes:\n","                         if not pt.get(\"theme_label\",\"\").lower().startswith((\"llm error\", \"unparsed\", \"valid json\")):\n","                             parsed_intermediate_for_reduce.append(f\"Theme Label: {pt.get('theme_label')}\\nExplanation: {pt.get('explanation')}\")\n","\n","                if parsed_intermediate_for_reduce:\n","                    combined_intermediate_text = \"\\n\\n---\\n\\n\".join(parsed_intermediate_for_reduce)\n","                    final_llm_output_str = get_llm_json_themes_p3(combined_intermediate_text, question_actual_text, P3_NUM_THEMES_TO_REQUEST_FROM_LLM,\n","                                                                llm_thematic_pipe_p3, P3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                                                                f\"{qid}_reduce\", \"REDUCE_INTERMEDIATE_THEMES\", logger_p3)\n","                else: status_p3 = \"error_map_parsing_no_valid_themes_for_reduce\"; final_llm_output_str = \"Error: Could not parse any valid intermediate themes.\"\n","            else: status_p3 = \"error_map_step_all_failed\"; final_llm_output_str = \"Error: All map steps failed.\"\n","\n","        parsed_themes_list_p3 = parse_llm_json_themes_p3(final_llm_output_str, 3, logger_p3) # Assume asking up to 3\n","        if status_p3 == \"init\": # if not set by an error condition above\n","            status_p3 = \"success_thematic_analysis\" if not final_llm_output_str.startswith(\"LLM_ERROR:\") and not (len(parsed_themes_list_p3)==1 and parsed_themes_list_p3[0][\"theme_label\"].lower().startswith((\"llm error\", \"unparsed\", \"valid json\"))) else \"error_final_llm_or_parse\"\n","\n","        overall_thematic_results_p3[qid] = {\n","            \"question_text\": question_actual_text, \"status\": status_p3,\n","            \"total_source_items_for_qid\": len(source_text_items_for_qid),\n","            \"map_reduce_applied\": len(map_batches_p3) > 1 if map_batches_p3 else False,\n","            \"num_map_batches\": len(map_batches_p3) if map_batches_p3 else 0,\n","            \"parsed_top_themes\": parsed_themes_list_p3,\n","            \"llm_raw_output_final_synthesis\": final_llm_output_str,\n","            \"llm_raw_output_map_batches\": raw_map_outputs\n","        }\n","        try: # Save after each QID\n","            with open(P3_OUTPUT_LLM_THEMES_JSON, 'w', encoding='utf-8') as f_out:\n","                json.dump(overall_thematic_results_p3, f_out, indent=2, default=logger_p3._get_serializer())\n","            logger_p3.log(f\"INFO P3: Saved results to '{P3_OUTPUT_LLM_THEMES_JSON}' after QID {qid}.\")\n","        except Exception as es: logger_p3.log(f\"ERROR P3: Save failed for QID {qid}: {es}\")\n","        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","    logger_p3.log(\"=\" * 50); logger_p3.log(\" INFO: Phase 3 Finished \"); logger_p3.log(\"=\" * 50)\n","    return True\n","\n","print(\"INFO: Cell 5 - Phase 3 logic defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owBi-LlSdd4k"},"outputs":[],"source":["# @title Cell 6: Phase 4 - Theme Validation, Quantification & Quotation Extraction (NEW)\n","\n","# logger_p4 = FileLogger(P4_LOG_FILE) # Defined in orchestration cell or here\n","\n","def load_human_defined_keyphrases_p4(keyphrase_file_path: str, logger_instance) -> Dict[str, Dict[str, List[str]]]:\n","    # ... (Same as before, using logger_instance)\n","    if not os.path.exists(keyphrase_file_path):\n","        logger_instance.log(f\"WARN P4: Keyphrase file not found: {keyphrase_file_path}. Will create empty if orchestration runs Phase 4.\")\n","        return {} # Return empty, Phase 4 orchestration will create an empty one if needed.\n","    try:\n","        with open(keyphrase_file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","            if not isinstance(data, dict): # Basic validation\n","                logger_instance.log(f\"ERROR P4: Keyphrase file {keyphrase_file_path} is not a valid JSON dictionary.\")\n","                return {}\n","            return data\n","    except json.JSONDecodeError as e:\n","        logger_instance.log(f\"ERROR P4: Failed to decode JSON from keyphrase file {keyphrase_file_path}: {e}\")\n","        return {}\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR P4: Failed to load keyphrase file {keyphrase_file_path}: {e}\")\n","        return {}\n","\n","def count_themes_and_extract_quotes_p4(\n","    llm_themes_by_qid: Dict[str, Any], # Output from Phase 3 (QID -> P3 analysis dict)\n","    collated_source_texts_data: Dict[str, Any], # Output from Phase 2 (P2_OUTPUT_COLLATED_JSON)\n","    human_keyphrases_by_qid: Dict[str, Dict[str, List[str]]], # Loaded from P4_HUMAN_DEFINED_KEYPHRASES_FILE\n","    num_quotes_per_theme: int,\n","    logger_instance\n",") -> Dict[str, Any]:\n","    final_report_data_p4 = {}\n","\n","    source_texts_for_all_qids = collated_source_texts_data.get(\"aggregated_pdf_content_by_qid\", {})\n","\n","    for qid, p3_qid_analysis_data in llm_themes_by_qid.items():\n","        logger_instance.log(f\"INFO P4: Processing QID: {qid} for quantification and quoting.\")\n","\n","        question_text = p3_qid_analysis_data.get(\"question_text\", \"N/A from Phase 3\")\n","        llm_identified_themes = p3_qid_analysis_data.get(\"parsed_top_themes\", []) # List of {\"theme_label\": ..., \"explanation\": ...}\n","\n","        final_report_data_p4[qid] = {\n","            \"question_text\": question_text,\n","            \"llm_generated_themes_from_p3\": llm_identified_themes, # Store what LLM produced\n","            \"status_from_p3\": p3_qid_analysis_data.get(\"status\", \"N/A\"),\n","            \"quantified_points\": [] # This will hold points with counts and quotes\n","        }\n","\n","        # Keyphrases defined by humans for this QID.\n","        # The keys in qid_human_keyphrases are the \"human-validated/curated theme labels\".\n","        qid_human_keyphrases = human_keyphrases_by_qid.get(qid, {})\n","        source_text_items_for_this_qid = source_texts_for_all_qids.get(qid, [])\n","\n","        if not source_text_items_for_this_qid:\n","            logger_instance.log(f\"  WARN P4: No source texts found in collated data for QID {qid}. Cannot quantify or quote.\")\n","            # Add a note or empty quantified_points if needed\n","            final_report_data_p4[qid][\"quantified_points\"].append({\n","                \"point_label\": \"No source texts for quantification\",\n","                \"keyphrases_used\": [], \"unique_response_count\": 0, \"example_quotes\": []\n","            })\n","            continue\n","\n","        if not qid_human_keyphrases:\n","            logger_instance.log(f\"  WARN P4: No human-defined keyphrases for QID {qid}. Will list LLM themes with 0 count.\")\n","            # If no human keyphrases, list LLM's themes but with 0 count as we can't verify/count them\n","            for llm_theme in llm_identified_themes:\n","                if llm_theme.get(\"theme_label\",\"\").lower().startswith((\"llm error\", \"unparsed\", \"valid json\")): # Skip error entries\n","                    continue\n","                final_report_data_p4[qid][\"quantified_points\"].append({\n","                    \"point_label\": llm_theme.get(\"theme_label\") + \" (LLM Suggested - Keyphrases Needed for Count)\",\n","                    \"llm_explanation\": llm_theme.get(\"explanation\"),\n","                    \"keyphrases_used\": [\"N/A - Define keyphrases\"],\n","                    \"unique_response_count\": 0,\n","                    \"example_quotes\": []\n","                })\n","            continue # Move to next QID\n","\n","        # --- If we have human-defined keyphrases for this QID ---\n","        for human_point_label, phrases_for_point in qid_human_keyphrases.items():\n","            logger_instance.log(f\"  P4 Counting for QID {qid} - Human Point: '{human_point_label}'\")\n","            matching_response_ids = set()\n","            potential_quotes = []\n","            # Find corresponding LLM explanation if label matches (for context in output)\n","            corresponding_llm_explanation = \"N/A - Label not directly from LLM or LLM theme list empty.\"\n","            for llm_theme_obj in llm_identified_themes:\n","                if llm_theme_obj.get(\"theme_label\") == human_point_label: # Assuming human labels might align with LLM labels\n","                    corresponding_llm_explanation = llm_theme_obj.get(\"explanation\", \"N/A\")\n","                    break\n","\n","            for source_item in source_text_items_for_this_qid:\n","                item_text = source_item.get(\"text\", \"\")\n","                if not item_text: continue\n","                item_text_lower = item_text.lower()\n","                response_id = source_item.get(\"response_id\")\n","\n","                for phrase in phrases_for_point:\n","                    if phrase.lower() in item_text_lower:\n","                        if response_id: matching_response_ids.add(response_id) # Only add if response_id is valid\n","                        # Quote extraction logic (as before)\n","                        sentences = nltk.sent_tokenize(item_text) # Requires NLTK\n","                        for sent_idx, sentence in enumerate(sentences):\n","                            if phrase.lower() in sentence.lower():\n","                                quote_context_start = max(0, sent_idx - 1)\n","                                quote_context_end = min(len(sentences), sent_idx + 2)\n","                                context_snippet = \" \".join(sentences[quote_context_start:quote_context_end])\n","                                potential_quotes.append({\n","                                    \"response_id\": response_id,\n","                                    \"source_document_type\": source_item.get(\"type\"),\n","                                    \"matched_phrase\": phrase,\n","                                    \"quote\": sentence,\n","                                    \"broader_context\": context_snippet # Context of +/- 1 sentence\n","                                })\n","                        break # Count this source_item once for this human_point_label\n","\n","            selected_quotes_p4 = [] # Basic quote selection\n","            # ... (quote selection logic as before, ensuring diversity) ...\n","            seen_quote_responses = set()\n","            for pq_item in sorted(potential_quotes, key=lambda x: len(x.get(\"quote\",\"\")), reverse=True): # Sort by quote length\n","                if len(selected_quotes_p4) < num_quotes_per_theme:\n","                    if pq_item.get(\"response_id\") not in seen_quote_responses or len(selected_quotes_p4) < num_quotes_per_theme / 2 :\n","                         selected_quotes_p4.append(pq_item)\n","                         if pq_item.get(\"response_id\"): seen_quote_responses.add(pq_item.get(\"response_id\"))\n","                else: break\n","\n","            final_report_data_p4[qid][\"quantified_points\"].append({\n","                \"point_label\": human_point_label,\n","                \"llm_explanation_if_direct_match\": corresponding_llm_explanation if corresponding_llm_explanation != \"N/A - Label not directly from LLM or LLM theme list empty.\" else \"This point label was human-defined/refined.\",\n","                \"keyphrases_used_for_counting\": phrases_for_point,\n","                \"unique_response_count\": len(matching_response_ids),\n","                \"example_quotes\": selected_quotes_p4\n","            })\n","            logger_instance.log(f\"    P4 QID {qid} - Point '{human_point_label}': {len(matching_response_ids)} unique responses. Extracted {len(selected_quotes_p4)} quotes.\")\n","\n","    return final_report_data_p4\n","\n","def run_phase4_theme_quantification_and_quoting(logger_p4_instance): # Pass logger\n","    # global logger_p4 # Not needed\n","    logger_p4 = logger_p4_instance\n","    logger_p4.log(\"=\" * 50); logger_p4.log(\" INFO: Starting Phase 4: Theme Quantification & Quotation Extraction\"); logger_p4.log(\"=\" * 50)\n","\n","    # 1. Load LLM-generated themes from Phase 3\n","    llm_themes_output_from_p3 = {}\n","    try:\n","        with open(P4_INPUT_LLM_THEMES_JSON, 'r', encoding='utf-8') as f: # P4_INPUT_LLM_THEMES_JSON is P3_OUTPUT_LLM_THEMES_JSON\n","            llm_themes_output_from_p3 = json.load(f)\n","        if not llm_themes_output_from_p3 or not isinstance(llm_themes_output_from_p3, dict):\n","             logger_p4.log(f\"ERROR P4: Phase 3 output file {P4_INPUT_LLM_THEMES_JSON} is empty or not a dict.\")\n","             return False\n","        logger_p4.log(f\"INFO P4: Loaded {len(llm_themes_output_from_p3)} QIDs with LLM themes from {P4_INPUT_LLM_THEMES_JSON}\")\n","    except FileNotFoundError:\n","        logger_p4.log(f\"ERROR P4: Phase 3 output file {P4_INPUT_LLM_THEMES_JSON} not found. Run Phase 3 first.\")\n","        return False\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to load LLM themes from {P4_INPUT_LLM_THEMES_JSON}: {e}\\n{traceback.format_exc()}\")\n","        return False\n","\n","    # 2. Load collated source texts from Phase 2\n","    collated_texts_output_from_p2 = {}\n","    try:\n","        with open(P4_INPUT_COLLATED_TEXTS_JSON, 'r', encoding='utf-8') as f: # P4_INPUT_COLLATED_TEXTS_JSON is P2_OUTPUT_COLLATED_JSON\n","            collated_texts_output_from_p2 = json.load(f)\n","        if not collated_texts_output_from_p2 or not isinstance(collated_texts_output_from_p2.get(\"aggregated_pdf_content_by_qid\"), dict):\n","            logger_p4.log(f\"ERROR P4: Phase 2 output file {P4_INPUT_COLLATED_TEXTS_JSON} is empty or missing 'aggregated_pdf_content_by_qid'.\")\n","            return False\n","        logger_p4.log(f\"INFO P4: Loaded collated texts from {P4_INPUT_COLLATED_TEXTS_JSON}\")\n","    except FileNotFoundError:\n","        logger_p4.log(f\"ERROR P4: Phase 2 collated texts file {P4_INPUT_COLLATED_TEXTS_JSON} not found. Run Phase 2 first.\")\n","        return False\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to load collated texts from {P4_INPUT_COLLATED_TEXTS_JSON}: {e}\\n{traceback.format_exc()}\")\n","        return False\n","\n","    # 3. Load human-defined keyphrases\n","    human_keyphrases_for_themes = load_human_defined_keyphrases_p4(P4_HUMAN_DEFINED_KEYPHRASES_FILE, logger_p4)\n","    if not human_keyphrases_for_themes:\n","        logger_p4.log(\"WARN P4: No human-defined keyphrases loaded. Quantification will be based on LLM themes directly if possible, or be limited.\")\n","        # If this file is crucial and must exist, you might return False here.\n","        # For now, it will proceed and the count_themes function will handle empty keyphrases per QID.\n","\n","    # 4. Perform counting and quote extraction\n","    # Pass the actual dicts, not just paths\n","    final_thematic_report_content = count_themes_and_extract_quotes_p4(\n","        llm_themes_output_from_p3, # This is QID -> {P3 analysis data}\n","        collated_texts_output_from_p2, # This is QID -> list of text items, inside \"aggregated_pdf_content_by_qid\"\n","        human_keyphrases_for_themes,\n","        P4_NUM_EXAMPLE_QUOTES_PER_THEME,\n","        logger_p4\n","    )\n","\n","    # 5. Save the final analysis\n","    try:\n","        with open(P4_OUTPUT_FINAL_ANALYSIS_JSON, 'w', encoding='utf-8') as f:\n","            json.dump(final_thematic_report_content, f, indent=2, default=logger_p4._get_serializer())\n","        logger_p4.log(f\"INFO P4: Successfully saved final thematic report to {P4_OUTPUT_FINAL_ANALYSIS_JSON}\")\n","    except Exception as e:\n","        logger_p4.log(f\"ERROR P4: Failed to save final analysis: {e}\\n{traceback.format_exc()}\"); return False\n","\n","    logger_p4.log(\"=\" * 50); logger_p4.log(\" INFO: Phase 4 Finished \"); logger_p4.log(\"=\" * 50)\n","    return True\n","\n","print(\"INFO: Cell 6 - Phase 4 logic defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7otbiT9Id1i8"},"outputs":[],"source":["# @title Cell 7: Main Pipeline Orchestration\n","\n","# Initialize a master logger for orchestration steps\n","# Ensure LOG_DIR from Cell 2 is defined and created\n","master_logger_file = os.path.join(LOG_DIR, f\"pipeline_orchestration_master_{time.strftime('%Y%m%d_%H%M%S')}.log\")\n","master_logger = FileLogger(master_logger_file)\n","master_logger.log(\"INFO: Orchestration Cell Started.\")\n","\n","# --- Flags to control which phases to run ---\n","# !!! SET THESE BASED ON YOUR NEEDS AND PREVIOUSLY COMPLETED STEPS !!!\n","RUN_PHASE_1 = False # PDF Extraction & Per-Response Summarization (Set to False if P1_OUTPUT_FILE is complete)\n","RUN_PHASE_2 = True # Collation of PDF Content (Set to False if P2_OUTPUT_COLLATED_JSON is complete)\n","RUN_PHASE_3 = True # LLM Thematic Point Generation (Set to False if P3_OUTPUT_LLM_THEMES_JSON is complete)\n","RUN_PHASE_4 = True  # Theme Validation, Quantification & Quotation Extraction\n","\n","# --- NLTK Resource Check (using master_logger) ---\n","master_logger.log(\"INFO: Orchestrator - Checking NLTK resources...\")\n","# Assuming download_nltk_resource_with_logger is defined (e.g., from Cell 1)\n","# and NLTK resources list from Cell 1 is sufficient.\n","download_nltk_resource_with_logger(master_logger, 'tokenizers/punkt', 'punkt')\n","download_nltk_resource_with_logger(master_logger, 'corpora/stopwords', 'stopwords')\n","# Add 'sentiment/vader_lexicon' if any phase uses VADER directly (Phase 1 doesn't seem to anymore for PDF)\n","\n","# --- Execute Pipeline Phases ---\n","phase_success_flags = {\n","    \"phase1\": False if RUN_PHASE_1 else True, # Assume success if not run and output exists\n","    \"phase2\": False if RUN_PHASE_2 else True,\n","    \"phase3\": False if RUN_PHASE_3 else True,\n","    \"phase4\": False if RUN_PHASE_4 else True,\n","}\n","\n","# This guard is good practice if you intend to run this cell directly\n","# In Colab, cells run in order, so __name__ is usually \"__main__\" for the executed cell\n","if __name__ == \"__main__\":\n","    master_logger.log(\"INFO: Starting Pipeline Execution based on RUN_PHASE flags...\")\n","\n","    # --- Phase 1 ---\n","    if RUN_PHASE_1:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 1: PDF Extraction & Per-Response Summarization <<<\")\n","        logger_p1_orch = FileLogger(P1_LOG_FILE) # Initialize logger for the phase\n","        success_p1 = run_phase1_pdf_extraction(logger_p1_orch)\n","        phase_success_flags[\"phase1\"] = success_p1\n","        if not success_p1: master_logger.log(\"CRITICAL: Phase 1 failed. Consider halting dependent phases.\")\n","        else: master_logger.log(\">>> PHASE 1 COMPLETED SUCCESSFULLY <<<\")\n","    else:\n","        master_logger.log(\">>> SKIPPING PHASE 1 (RUN_PHASE_1 is False) <<<\")\n","        if not os.path.exists(P1_OUTPUT_FILE):\n","            master_logger.log(f\"ERROR: Phase 1 skipped, but required output {P1_OUTPUT_FILE} not found! Dependent phases may fail.\")\n","            phase_success_flags[\"phase1\"] = False # Mark as failed for dependency\n","        else:\n","            master_logger.log(f\"INFO: Assuming Phase 1 output {P1_OUTPUT_FILE} is valid.\")\n","            phase_success_flags[\"phase1\"] = True\n","\n","\n","    # --- Phase 2 ---\n","    if phase_success_flags[\"phase1\"] and RUN_PHASE_2:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 2: Collation of PDF Content <<<\")\n","        logger_p2_orch = FileLogger(P2_LOG_FILE)\n","        success_p2 = run_phase2_collate_pdf_texts(logger_p2_orch)\n","        phase_success_flags[\"phase2\"] = success_p2\n","        if not success_p2: master_logger.log(\"CRITICAL: Phase 2 failed. Consider halting dependent phases.\")\n","        else: master_logger.log(\">>> PHASE 2 COMPLETED SUCCESSFULLY <<<\")\n","    elif not phase_success_flags[\"phase1\"] and RUN_PHASE_2:\n","        master_logger.log(\"WARN: Phase 1 did not complete successfully or was skipped without valid output. Skipping Phase 2.\")\n","    else: # RUN_PHASE_2 is False\n","        master_logger.log(\">>> SKIPPING PHASE 2 <<<\")\n","        if not os.path.exists(P2_OUTPUT_COLLATED_JSON) and phase_success_flags[\"phase1\"]: # If P1 was ok but P2 output is needed later\n","            master_logger.log(f\"ERROR: Phase 2 skipped, but required output {P2_OUTPUT_COLLATED_JSON} not found! Dependent phases may fail.\")\n","            phase_success_flags[\"phase2\"] = False\n","        elif os.path.exists(P2_OUTPUT_COLLATED_JSON):\n","             phase_success_flags[\"phase2\"] = True # Assume existing output is fine\n","             master_logger.log(f\"INFO: Assuming Phase 2 output {P2_OUTPUT_COLLATED_JSON} is valid.\")\n","\n","\n","    # --- Phase 3 ---\n","    if phase_success_flags[\"phase2\"] and RUN_PHASE_3:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 3: LLM Thematic Point Generation <<<\")\n","        logger_p3_orch = FileLogger(P3_LOG_FILE)\n","        success_p3 = run_phase3_llm_thematic_generation(logger_p3_orch)\n","        phase_success_flags[\"phase3\"] = success_p3\n","        if not success_p3: master_logger.log(\"CRITICAL: Phase 3 failed. Consider halting dependent phases.\")\n","        else: master_logger.log(\">>> PHASE 3 COMPLETED SUCCESSFULLY <<<\")\n","    elif not phase_success_flags[\"phase2\"] and RUN_PHASE_3:\n","        master_logger.log(\"WARN: Phase 2 did not complete successfully or was skipped without valid output. Skipping Phase 3.\")\n","    else: # RUN_PHASE_3 is False\n","        master_logger.log(\">>> SKIPPING PHASE 3 <<<\")\n","        if not os.path.exists(P3_OUTPUT_LLM_THEMES_JSON) and phase_success_flags[\"phase2\"]:\n","            master_logger.log(f\"ERROR: Phase 3 skipped, but required output {P3_OUTPUT_LLM_THEMES_JSON} not found! Dependent phases may fail.\")\n","            phase_success_flags[\"phase3\"] = False\n","        elif os.path.exists(P3_OUTPUT_LLM_THEMES_JSON):\n","            phase_success_flags[\"phase3\"] = True\n","            master_logger.log(f\"INFO: Assuming Phase 3 output {P3_OUTPUT_LLM_THEMES_JSON} is valid.\")\n","\n","\n","    # --- Phase 4 ---\n","    if phase_success_flags[\"phase3\"] and RUN_PHASE_4:\n","        master_logger.log(\"\\n>>> EXECUTING PHASE 4: Theme Validation, Quantification & Quotation Extraction <<<\")\n","        logger_p4_orch = FileLogger(P4_LOG_FILE)\n","        # Crucial Step: Ensure P4_HUMAN_DEFINED_KEYPHRASES_FILE exists and is populated!\n","        if not os.path.exists(P4_HUMAN_DEFINED_KEYPHRASES_FILE):\n","            master_logger.log(f\"WARN: Phase 4 - Human defined keyphrases file not found at {P4_HUMAN_DEFINED_KEYPHRASES_FILE}. Creating empty file.\")\n","            try:\n","                with open(P4_HUMAN_DEFINED_KEYPHRASES_FILE, 'w') as f_empty_kp: json.dump({}, f_empty_kp)\n","                master_logger.log(f\"INFO: Created empty keyphrase file at {P4_HUMAN_DEFINED_KEYPHRASES_FILE}. Quantification will be limited until populated.\")\n","            except Exception as e_kp_create:\n","                 master_logger.log(f\"ERROR: Could not create empty keyphrase file: {e_kp_create}\")\n","\n","        success_p4 = run_phase4_theme_quantification_and_quoting(\n","            logger_p4_orch,\n","            P4_INPUT_LLM_THEMES_JSON,    # Defined in Cell 2, output of P3\n","            P4_INPUT_COLLATED_TEXTS_JSON,# Defined in Cell 2, output of P2\n","            P4_HUMAN_DEFINED_KEYPHRASES_FILE, # Defined in Cell 2\n","            P4_OUTPUT_FINAL_ANALYSIS_JSON,   # Defined in Cell 2\n","            P4_NUM_EXAMPLE_QUOTES_PER_THEME  # Defined in Cell 2\n","        )\n","        phase_success_flags[\"phase4\"] = success_p4\n","        if not success_p4: master_logger.log(\"ERROR: Phase 4 encountered issues.\")\n","        else: master_logger.log(\">>> PHASE 4 COMPLETED SUCCESSFULLY <<<\")\n","    elif not phase_success_flags[\"phase3\"] and RUN_PHASE_4:\n","        master_logger.log(\"WARN: Phase 3 did not complete successfully or was skipped without valid output. Skipping Phase 4.\")\n","    else: # RUN_PHASE_4 is False\n","        master_logger.log(\">>> SKIPPING PHASE 4 <<<\")\n","        # No direct dependency from Phase 4 output to another phase in this setup.\n","\n","    master_logger.log(\"\\n\" + \"=\"*50)\n","    master_logger.log(\"Overall Pipeline Orchestration Concluded.\")\n","    for phase_name_idx, (phase_key, was_run_flag) in enumerate(\n","        [(\"phase1\", RUN_PHASE_1), (\"phase2\", RUN_PHASE_2),\n","         (\"phase3\", RUN_PHASE_3), (\"phase4\", RUN_PHASE_4)]):\n","\n","        status_msg = \"SKIPPED (Output assumed valid if exists)\"\n","        if was_run_flag:\n","            status_msg = 'SUCCESS' if phase_success_flags[phase_key] else 'FAILED'\n","        elif not phase_success_flags[phase_key]: # Skipped but required output missing\n","            status_msg = 'SKIPPED (Output MISSING for dependent phase!)'\n","\n","        master_logger.log(f\"  {phase_key.upper()} Status: {status_msg}\")\n","\n","    master_logger.log(\"Please check individual phase log files for details.\")\n","    master_logger.log(\"=\"*50)\n","\n","print(\"INFO: Cell 7 - Main Orchestration Cell defined. Set RUN_PHASE_X flags and execute this cell to run the pipeline.\")"]},{"cell_type":"markdown","metadata":{"id":"9ebU61128fdW"},"source":["# 19th May Afternoon"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stieLjzzcuPE"},"outputs":[],"source":["# @title TEST CELL (JSON Output): Q4 (Scenario A - Summaries) & Q16 (Scenario B - Passages) Thematic Analysis\n","\n","# --- Test Cell Imports ---\n","import json\n","import os\n","import sys\n","import traceback\n","import time\n","import gc\n","import re\n","from collections import defaultdict\n","from typing import Dict, Any, List\n","\n","import torch\n","import numpy as np\n","# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline # In Cell 0\n","# try:\n","#     from bitsandbytes.cextension import ColabNotSupportedError # In Cell 0\n","# except ImportError:\n","#     ColabNotSupportedError = None\n","\n","# --- Configuration for this Test Cell ---\n","TEST_JSON_BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # <<<--- ADJUST IF NEEDED\n","TEST_JSON_LOG_FILE = os.path.join(TEST_JSON_BASE_PROJECT_DIR, f'test_q4a_q16b_json_themes_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","\n","# LLM Config\n","TEST_JSON_LLM_MODEL_ID = 'google/gemma-3-4b-it'  # <<<--- Using 2b-it. Adjust if needed.\n","TEST_JSON_USE_QUANTIZATION = False             # <<<--- Adjust if needed.\n","\n","# Thematic Analysis Parameters\n","TEST_JSON_NUM_THEMES_TO_REQUEST = 3\n","TEST_JSON_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2800\n","TEST_JSON_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 500 # Increased slightly for JSON structure\n","TEST_JSON_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 750 # Increased slightly for JSON structure\n","TEST_JSON_MAPREDUCE_CHUNK_OVERLAP_WORDS = 30\n","\n","# Input file from Stage 4a (Phase 2 output)\n","TEST_JSON_INPUT_COLLATED_PDF_TEXTS_JSON = os.path.join(TEST_JSON_BASE_PROJECT_DIR, 'phase2_stage4a_collated_pdf_texts_20250518_233834.json') # <<<--- UPDATE THIS FILENAME\n","\n","# Source for actual question texts\n","TEST_JSON_QUESTION_TEXT_SOURCE_FILE = os.path.join(TEST_JSON_BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # <<<--- ENSURE THIS IS CORRECT\n","\n","# --- FileLogger (Assuming defined in Cell 1 or globally) ---\n","try:\n","    logger_test_json = FileLogger(TEST_JSON_LOG_FILE)\n","    logger_test_json.log(f\"INFO: Test Cell (JSON Themes) - Logger Initialized.\")\n","except NameError:\n","    print(\"CRITICAL: FileLogger class not defined. Please ensure Cell 1 has been executed.\")\n","    class PrintLogger: # Fallback\n","        def log(self, msg): print(f\"{time.strftime('%Y%m%d_%H%M%S')}: {msg}\")\n","    logger_test_json = PrintLogger()\n","    logger_test_json.log(\"WARN: Using fallback PrintLogger.\")\n","\n","# --- Helper: Text Chunker (for Scenario A) ---\n","def create_text_chunks_json_test(full_text: str, tokenizer_for_chunking, max_tokens: int, overlap_words: int = 30):\n","    # ... (Same as create_text_chunks_for_scenario_a, using logger_test_json) ...\n","    words = full_text.split()\n","    if not words: return []\n","    chunks = []\n","    current_chunk_words = []\n","    logger_test_json.log(f\"DEBUG CHUNKER_JSON: Starting to chunk text of {len(words)} words into max {max_tokens} token chunks.\")\n","    for word_idx, word in enumerate(words):\n","        potential_chunk_text = \" \".join(current_chunk_words + [word])\n","        num_tokens = len(tokenizer_for_chunking.encode(potential_chunk_text, add_special_tokens=False))\n","        if num_tokens <= max_tokens: current_chunk_words.append(word)\n","        else:\n","            if current_chunk_words: chunks.append(\" \".join(current_chunk_words))\n","            if chunks and overlap_words > 0:\n","                prev_chunk_words = chunks[-1].split()\n","                overlap_start_idx = max(0, len(prev_chunk_words) - overlap_words)\n","                current_chunk_words = prev_chunk_words[overlap_start_idx:] + [word]\n","            else: current_chunk_words = [word]\n","            new_chunk_text_check = \" \".join(current_chunk_words)\n","            new_chunk_tokens_check = len(tokenizer_for_chunking.encode(new_chunk_text_check, add_special_tokens=False))\n","            if new_chunk_tokens_check > max_tokens:\n","                if len(current_chunk_words) > 1 and chunks: current_chunk_words = [word]\n","                elif len(current_chunk_words) == 1: logger_test_json.log(f\"    WARN CHUNKER_JSON: Single word '{word}' oversized ({new_chunk_tokens_check} tokens).\")\n","    if current_chunk_words: chunks.append(\" \".join(current_chunk_words))\n","    logger_test_json.log(f\"INFO CHUNKER_JSON: Split text into {len(chunks)} chunks.\")\n","    return chunks\n","\n","# --- Helper: Batch Individual Items (for Scenario B map step) ---\n","def batch_text_items_json_test(text_items_list: List[str], tokenizer_for_batching, max_tokens_per_batch: int):\n","    # ... (Same as batch_text_items_for_scenario_b, using logger_test_json) ...\n","    batches = []\n","    current_batch_item_texts = []\n","    current_batch_concatenated_text = \"\"\n","    logger_test_json.log(f\"DEBUG BATCHER_JSON: Starting to batch {len(text_items_list)} items into max {max_tokens_per_batch} token batches.\")\n","    for item_idx, item_text_original in enumerate(text_items_list):\n","        if not item_text_original or not item_text_original.strip(): continue\n","        text_to_try_adding = item_text_original\n","        if current_batch_item_texts: text_to_try_adding = \"\\n\\n--- Next Excerpt ---\\n\" + item_text_original\n","        potential_full_batch_text = current_batch_concatenated_text + text_to_try_adding\n","        num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))\n","        if num_tokens <= max_tokens_per_batch:\n","            current_batch_item_texts.append(item_text_original)\n","            current_batch_concatenated_text = potential_full_batch_text\n","        else:\n","            if current_batch_item_texts: batches.append(current_batch_item_texts)\n","            single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))\n","            if single_item_tokens > max_tokens_per_batch:\n","                logger_test_json.log(f\"  WARN BATCHER_JSON: Item {item_idx} ({single_item_tokens} tokens) oversized. Own batch.\")\n","                batches.append([item_text_original])\n","                current_batch_item_texts = []\n","                current_batch_concatenated_text = \"\"\n","            else:\n","                current_batch_item_texts = [item_text_original]\n","                current_batch_concatenated_text = item_text_original\n","    if current_batch_item_texts: batches.append(current_batch_item_texts)\n","    logger_test_json.log(f\"INFO BATCHER_JSON: Created {len(batches)} batches of items.\")\n","    return batches\n","\n","\n","# --- Helper: LLM Call for Thematic Points (Requests JSON Output) ---\n","def get_llm_json_themes_test(text_input_for_llm: str, question_text_for_prompt: str,\n","                             num_themes_requested: int, llm_pipe: pipeline, max_new_tokens: int,\n","                             log_prefix_qid_scenario: str, map_reduce_step_type: str):\n","    json_format_example = (\n","        \"{\\n\"\n","        \"  \\\"themes\\\": [\\n\"\n","        \"    {\\n\"\n","        \"      \\\"theme_label\\\": \\\"Concise Label for Theme 1\\\",\\n\"\n","        \"      \\\"explanation\\\": \\\"A 1-2 sentence explanation of Theme 1 based on the provided text.\\\"\\n\"\n","        \"    },\\n\"\n","        \"    {\\n\"\n","        \"      \\\"theme_label\\\": \\\"Concise Label for Theme 2\\\",\\n\"\n","        \"      \\\"explanation\\\": \\\"A 1-2 sentence explanation of Theme 2 based on the provided text.\\\"\\n\"\n","        \"    }\\n\"\n","        \"    // ... up to \" + str(num_themes_requested) + \" themes ...\\n\"\n","        \"  ]\\n\"\n","        \"}\"\n","    )\n","    num_map_themes_to_ask = 3 # Ask for up to 3 themes from each map chunk/batch\n","\n","    prompt_instruction = \"\"\n","    if map_reduce_step_type == \"SCENARIO_A_MAP\": # Chunk of concatenated summaries\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. The following text is a CHUNK of concatenated PREVIOUSLY-GENERATED SUMMARIES from multiple PDF submissions responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on this provided CHUNK OF SUMMARIES, identify up to {num_map_themes_to_ask} most prominent and distinct points, arguments, or themes.\\n\"\n","            f\"Respond *only* with a single, valid JSON object formatted as follows, containing a list called 'themes'. Each object in the 'themes' list must have two string keys: 'theme_label' and 'explanation'.\\n\"\n","            f\"Example JSON format:\\n{json_format_example}\\n\\n\"\n","            f\"Chunk of Summaries:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"JSON Output:\"\n","        )\n","    elif map_reduce_step_type == \"SCENARIO_B_MAP\": # Batch of original passages\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Carefully review the following BATCH of original excerpts/passages from PDF submissions, all responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on this provided BATCH of original excerpts/passages, identify up to {num_map_themes_to_ask} prominent and distinct points, arguments, or themes.\\n\"\n","            f\"Respond *only* with a single, valid JSON object formatted as follows, containing a list called 'themes'. Each object in the 'themes' list must have two string keys: 'theme_label' and 'explanation'.\\n\"\n","            f\"Example JSON format:\\n{json_format_example}\\n\\n\"\n","            f\"Batch of Excerpts/Passages:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"JSON Output:\"\n","        )\n","    elif map_reduce_step_type == \"REDUCE_INTERMEDIATE_THEMES\":\n","        # Input `text_input_for_llm` for reduce is a string of preliminary JSON outputs or parsed themes\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. The following are sets of preliminary themes/points (potentially in JSON format or as text summaries of themes) that were individually extracted in previous steps from responses to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Your task is to synthesize these preliminary themes into the overall top {num_themes_requested} overarching and distinct common points or themes.\\n\"\n","            f\"Respond *only* with a single, valid JSON object formatted as follows, containing a list called 'themes'. Each object in the 'themes' list must have two string keys: 'theme_label' (a concise title) and 'explanation' (a 1-2 sentence summary).\\n\"\n","            f\"Example JSON format:\\n{json_format_example}\\n\\n\"\n","            f\"Preliminary Themes/Points Provided:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"JSON Output (Synthesized Top {num_themes_requested} Themes):\"\n","        )\n","    else: # DIRECT_PROCESSING\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Carefully review all the following text (which consists of combined PDF summaries OR a single batch of excerpts/passages) responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on the provided text, identify the top {num_themes_requested} most common and distinct points, arguments, or themes.\\n\"\n","            f\"Respond *only* with a single, valid JSON object formatted as follows, containing a list called 'themes'. Each object in the 'themes' list must have two string keys: 'theme_label' and 'explanation'.\\n\"\n","            f\"Example JSON format:\\n{json_format_example}\\n\\n\"\n","            f\"Provided Text:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"JSON Output (Top {num_themes_requested} Common Points):\"\n","        )\n","\n","    messages = [{\"role\": \"user\", \"content\": prompt_instruction}]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": False,\n","                           \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id}\n","\n","        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True)\n","        input_text_tokens = llm_pipe.tokenizer.encode(text_input_for_llm, add_special_tokens=False)\n","        model_max_len = 8192\n","        # ... (model_max_len detection logic from previous get_llm_thematic_points_ab_test, using logger_test_json)\n","        if hasattr(llm_pipe.model.config, 'max_position_embeddings'): model_max_len = llm_pipe.model.config.max_position_embeddings\n","        elif hasattr(llm_pipe.model.config, 'max_sequence_length'): model_max_len = llm_pipe.model.config.max_sequence_length\n","        elif hasattr(llm_pipe.tokenizer, 'model_max_length'): model_max_len = llm_pipe.tokenizer.model_max_length\n","        if \"gemma\" in llm_pipe.model.config.model_type.lower() and model_max_len > 8192 : model_max_len = 8192\n","\n","\n","        logger_test_json.log(f\"DEBUG LLM_JSON_CALL: {map_reduce_step_type} for {log_prefix_qid_scenario}. Prompt toks: {len(prompt_tokens)}. Content toks: {len(input_text_tokens)}. Max new: {max_new_tokens}. Model max_len: {model_max_len}.\")\n","        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95: # Check against determined model_max_len\n","             logger_test_json.log(f\"WARN LLM_JSON_CALL: {log_prefix_qid_scenario} - Step {map_reduce_step_type} - Prompt+Output tokens might exceed context ({model_max_len}).\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger_test_json.log(f\"ERROR LLM_JSON_CALL: {map_reduce_step_type} LLM unexpected output for {log_prefix_qid_scenario}.\")\n","            return \"LLM_ERROR: No output generated.\"\n","        generated_text_full = outputs[0]['generated_text']\n","\n","        llm_response_text = generated_text_full\n","        # Try to extract only the JSON part, be more aggressive\n","        json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", generated_text_full, re.DOTALL)\n","        if json_match:\n","            llm_response_text = json_match.group(1).strip()\n","        else:\n","            json_match_alt = re.search(r\"(\\{.*?\\})\", generated_text_full, re.DOTALL) # Look for any {...}\n","            if json_match_alt:\n","                llm_response_text = json_match_alt.group(1).strip()\n","                logger_test_json.log(f\"WARN LLM_JSON_CALL: {map_reduce_step_type} for {log_prefix_qid_scenario} - Used generic JSON block extraction. Review output.\")\n","            else: # Fallback: try stripping prompt if it's an exact match (less likely with JSON instructions)\n","                if generated_text_full.startswith(prompt_formatted):\n","                    llm_response_text = generated_text_full[len(prompt_formatted):].strip()\n","                else: # If no JSON found and prompt doesn't match, might be just model's text response\n","                    model_response_marker = \"<start_of_turn>model\\n\"\n","                    if model_response_marker in generated_text_full:\n","                         llm_response_text = generated_text_full.split(model_response_marker, 1)[-1].strip()\n","                    # else llm_response_text remains generated_text_full\n","                    logger_test_json.log(f\"WARN LLM_JSON_CALL: {map_reduce_step_type} for {log_prefix_qid_scenario} - Could not isolate JSON. Output may be non-JSON or include prompt.\")\n","\n","        return llm_response_text\n","    except Exception as e:\n","        logger_test_json.log(f\"ERROR LLM_JSON_CALL: {map_reduce_step_type} LLM exception for {log_prefix_qid_scenario}: {e}\")\n","        logger_test_json.log(traceback.format_exc())\n","        return f\"LLM_ERROR: Exception - {str(e)}\"\n","\n","# --- Helper: Parse LLM JSON Thematic Output ---\n","def parse_llm_json_themes_test(llm_json_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:\n","    parsed_themes = []\n","    if not llm_json_output_text or llm_json_output_text.startswith(\"LLM_ERROR:\"):\n","        return [{\"theme_label\": \"LLM Error in Generation\", \"explanation\": llm_json_output_text}]\n","\n","    try:\n","        # Basic cleanup that was in get_llm_json_themes_test is already done before this parser is called.\n","        # This parser now expects a string that *should* be JSON.\n","        data = json.loads(llm_json_output_text) # Expects cleaned JSON string\n","\n","        if isinstance(data, dict) and \"themes\" in data and isinstance(data[\"themes\"], list):\n","            for theme_obj in data[\"themes\"]:\n","                if isinstance(theme_obj, dict) and \"theme_label\" in theme_obj and \"explanation\" in theme_obj:\n","                    parsed_themes.append({\n","                        \"theme_label\": str(theme_obj[\"theme_label\"]),\n","                        \"explanation\": str(theme_obj[\"explanation\"])\n","                    })\n","                    if len(parsed_themes) >= num_expected_themes:\n","                        break\n","            if parsed_themes:\n","                return parsed_themes\n","            else:\n","                 logger_test_json.log(\"WARN JSON_PARSER: Valid JSON but 'themes' list malformed or empty.\")\n","                 return [{\"theme_label\": \"Valid JSON, themes list malformed/empty\", \"explanation\": llm_json_output_text[:200]+\"...\"}]\n","        else:\n","            logger_test_json.log(\"WARN JSON_PARSER: LLM output was valid JSON but not expected structure {'themes': [...]}.\" )\n","            return [{\"theme_label\": \"Valid JSON, unexpected structure\", \"explanation\": llm_json_output_text[:200]+\"...\"}]\n","    except json.JSONDecodeError as e:\n","        logger_test_json.log(f\"ERROR JSON_PARSER: LLM output not valid JSON. Error: {e}\")\n","        logger_test_json.log(f\"LLM Raw Output for JSON Parse Error:\\n{llm_json_output_text}\")\n","        return [{\"theme_label\": \"Invalid JSON Output\", \"explanation\": f\"JSONDecodeError: {e}. Raw: {llm_json_output_text[:200]}...\"}]\n","    except Exception as e_gen:\n","        logger_test_json.log(f\"ERROR JSON_PARSER: Unexpected error parsing: {e_gen}\")\n","        return [{\"theme_label\": \"Unexpected Parsing Error\", \"explanation\": f\"Error: {e_gen}. Raw: {llm_json_output_text[:200]}...\"}]\n","\n","\n","# --- Main Test Execution Function ---\n","def run_thematic_analysis_tests_with_json():\n","    logger_test_json.log(\"INFO: Test Cell (JSON) - Starting Thematic Analysis Scenarios Test (Q4-A, Q16-B).\")\n","\n","    # --- 0. Load LLM ---\n","    llm_thematic_pipe_test = None\n","    tokenizer_for_llm_test = None\n","    try:\n","        logger_test_json.log(f\"INFO TEST JSON: Loading LLM: {TEST_JSON_LLM_MODEL_ID} (Quant: {TEST_JSON_USE_QUANTIZATION})\")\n","        tokenizer_for_llm_test = AutoTokenizer.from_pretrained(TEST_JSON_LLM_MODEL_ID)\n","        if getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is not None:\n","            tokenizer_for_llm_test.pad_token = tokenizer_for_llm_test.eos_token\n","        elif getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is None:\n","             logger_test_json.log(f\"CRITICAL TEST JSON: Tokenizer for {TEST_JSON_LLM_MODEL_ID} has neither pad_token nor eos_token.\")\n","\n","        bnb_config_test = None; effective_quant_test = TEST_JSON_USE_QUANTIZATION\n","        if TEST_JSON_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config_test = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n","            except ImportError: effective_quant_test = False; logger_test_json.log(\"WARN TEST JSON: bitsandbytes not imported.\")\n","            except ColabNotSupportedError: effective_quant_test = False; logger_test_json.log(\"WARN TEST JSON: ColabNotSupportedError for bnb.\")\n","            except Exception as e_bnb: effective_quant_test = False; logger_test_json.log(f\"WARN TEST JSON: BNB config failed: {e_bnb}.\")\n","        elif TEST_JSON_USE_QUANTIZATION: effective_quant_test = False; logger_test_json.log(\"WARN TEST JSON: Quant requested, No CUDA.\")\n","\n","        model_for_llm_test = AutoModelForCausalLM.from_pretrained(\n","            TEST_JSON_LLM_MODEL_ID,\n","            quantization_config=bnb_config_test if effective_quant_test else None,\n","            torch_dtype=torch.bfloat16 if not effective_quant_test and torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (torch.float32 if not effective_quant_test else None),\n","            device_map=\"auto\", trust_remote_code=True\n","        )\n","        llm_thematic_pipe_test = pipeline(\"text-generation\", model=model_for_llm_test, tokenizer=tokenizer_for_llm_test)\n","        logger_test_json.log(\"INFO TEST JSON: LLM Pipeline Loaded.\")\n","    except Exception as e:\n","        logger_test_json.log(f\"CRITICAL TEST JSON: Failed to load LLM: {e}\\n{traceback.format_exc()}\")\n","        return\n","\n","    # --- 1. Load Full Stage 4a Data ---\n","    all_qids_collated_data = {}\n","    # ... (same loading logic as before, using TEST_JSON_INPUT_COLLATED_PDF_TEXTS_JSON and logger_test_json) ...\n","    try:\n","        with open(TEST_JSON_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:\n","            stage4a_data = json.load(f)\n","        all_qids_collated_data = stage4a_data.get(\"aggregated_pdf_content_by_qid\", {})\n","        if not all_qids_collated_data:\n","            logger_test_json.log(f\"ERROR TEST JSON: No 'aggregated_pdf_content_by_qid' in {TEST_JSON_INPUT_COLLATED_PDF_TEXTS_JSON}.\")\n","            return\n","        logger_test_json.log(f\"INFO TEST JSON: Loaded data for {len(all_qids_collated_data)} QIDs from {TEST_JSON_INPUT_COLLATED_PDF_TEXTS_JSON}.\")\n","    except Exception as e:\n","        logger_test_json.log(f\"CRITICAL TEST JSON: Failed to load Stage 4a data: {e}\\n{traceback.format_exc()}\")\n","        return\n","\n","    # --- 2. Load All Question Texts ---\n","    qid_to_actual_question_text_map = {}\n","    # ... (same loading logic as before, using TEST_JSON_QUESTION_TEXT_SOURCE_FILE and logger_test_json) ...\n","    try:\n","        logger_test_json.log(f\"INFO TEST JSON: Loading Q texts from: {TEST_JSON_QUESTION_TEXT_SOURCE_FILE}\")\n","        if not os.path.exists(TEST_JSON_QUESTION_TEXT_SOURCE_FILE):\n","             raise FileNotFoundError(f\"Q text source file not found: {TEST_JSON_QUESTION_TEXT_SOURCE_FILE}\")\n","        with open(TEST_JSON_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:\n","            qtext_source_data = json.load(f_qtext)\n","        if \"question_data\" in qtext_source_data:\n","            for qid_key, data_val in qtext_source_data.get(\"question_data\", {}).items():\n","                qid_to_actual_question_text_map[qid_key] = data_val.get(\"question_text\", f\"Text for {qid_key} NF\")\n","        logger_test_json.log(f\"INFO TEST JSON: Loaded {len(qid_to_actual_question_text_map)} Q texts.\")\n","    except Exception as e:\n","        logger_test_json.log(f\"ERROR TEST JSON: Failed to load Q texts: {e}\\n{traceback.format_exc()}\")\n","\n","\n","    # ==============================================================================\n","    # --- Scenario A: Map-Reduce on Stage 4 SUMMARIES for Q4 ---\n","    # ==============================================================================\n","    qid_for_scenario_a = \"Q4\"\n","    logger_test_json.log(\"\\n\" + \"=\"*30 + f\" SCENARIO A (JSON): Map-Reduce on PDF SUMMARIES for {qid_for_scenario_a} \" + \"=\"*30)\n","\n","    q_a_all_items = all_qids_collated_data.get(qid_for_scenario_a, [])\n","    q_a_summaries = [item['text'] for item in q_a_all_items if item.get('type') == 'pdf_summary' and item.get('text')]\n","    question_text_for_q_a = qid_to_actual_question_text_map.get(qid_for_scenario_a, f\"Question {qid_for_scenario_a} text not loaded\")\n","\n","    scenario_a_final_themes = []\n","    scenario_a_map_outputs_raw = []\n","    scenario_a_reduce_output_raw = \"N/A\"\n","\n","    if not q_a_summaries:\n","        logger_test_json.log(f\"WARN TEST JSON SCENARIO A: No 'pdf_summary' items for QID {qid_for_scenario_a}.\")\n","    else:\n","        logger_test_json.log(f\"INFO TEST JSON SCENARIO A: Found {len(q_a_summaries)} PDF summaries for {qid_for_scenario_a}.\")\n","        concatenated_q_a_summaries = \"\\n\\n--- Next Summary Entry ---\\n\".join(q_a_summaries)\n","\n","        summary_chunks_A = create_text_chunks_json_test(\n","            concatenated_q_a_summaries, llm_thematic_pipe_test.tokenizer,\n","            TEST_JSON_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK, TEST_JSON_MAPREDUCE_CHUNK_OVERLAP_WORDS\n","        )\n","\n","        if len(summary_chunks_A) > 1:\n","            logger_test_json.log(f\"INFO TEST JSON SCENARIO A: {qid_for_scenario_a} summaries split into {len(summary_chunks_A)} chunks.\")\n","            intermediate_themes_A_list = []\n","            for i, chunk in enumerate(summary_chunks_A):\n","                logger_test_json.log(f\"  SCENARIO A - MAP CHUNK {i+1}/{len(summary_chunks_A)} (Chars: {len(chunk)})\")\n","                map_out = get_llm_json_themes_test(\n","                    chunk, question_text_for_q_a, 3, llm_thematic_pipe_test,\n","                    TEST_JSON_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                    f\"{qid_for_scenario_a}_A_map_chunk{i+1}\", \"SCENARIO_A_MAP\"\n","                )\n","                scenario_a_map_outputs_raw.append(map_out)\n","                if not map_out.startswith(\"LLM_ERROR:\"): intermediate_themes_A_list.append(map_out) # Store raw JSON output\n","                time.sleep(0.2)\n","\n","            if intermediate_themes_A_list:\n","                # For reduce step, we pass the raw JSON strings from map step, or parsed themes joined as string\n","                # Passing raw JSON strings might be too verbose. Let's try parsing them first.\n","                parsed_intermediate_themes_for_reduce_A = []\n","                for raw_json_map_out in intermediate_themes_A_list:\n","                    parsed_map_themes = parse_llm_json_themes_test(raw_json_map_out, 3)\n","                    for pt in parsed_map_themes: # Convert parsed themes to a string format for reduce prompt\n","                        if not pt.get(\"theme_label\",\"\").startswith(\"LLM Error\") and not pt.get(\"theme_label\",\"\").startswith(\"Unparsed\"):\n","                             parsed_intermediate_themes_for_reduce_A.append(f\"Theme Label: {pt.get('theme_label')}\\nExplanation: {pt.get('explanation')}\")\n","\n","                if parsed_intermediate_themes_for_reduce_A:\n","                    combined_intermediate_A = \"\\n\\n---\\n\\n\".join(parsed_intermediate_themes_for_reduce_A)\n","                    logger_test_json.log(f\"  SCENARIO A - REDUCE Step for {qid_for_scenario_a} (Input Chars for reduce: {len(combined_intermediate_A)})\")\n","                    scenario_a_reduce_output_raw = get_llm_json_themes_test(\n","                        combined_intermediate_A, question_text_for_q_a, TEST_JSON_NUM_THEMES_TO_REQUEST,\n","                        llm_thematic_pipe_test, TEST_JSON_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                        f\"{qid_for_scenario_a}_A_reduce\", \"REDUCE_INTERMEDIATE_THEMES\"\n","                    )\n","                    scenario_a_final_themes = parse_llm_json_themes_test(scenario_a_reduce_output_raw, TEST_JSON_NUM_THEMES_TO_REQUEST)\n","                else:\n","                    logger_test_json.log(f\"ERROR TEST_JSON SCENARIO A: No successfully parsed intermediate themes from MAP step for {qid_for_scenario_a}.\")\n","                    scenario_a_reduce_output_raw = f\"ERROR: No successfully parsed intermediate themes from MAP step for {qid_for_scenario_a} (Summaries).\"\n","            else:\n","                logger_test_json.log(f\"ERROR TEST_JSON SCENARIO A: No valid (non-error) intermediate themes from MAP step for {qid_for_scenario_a}.\")\n","                scenario_a_reduce_output_raw = f\"ERROR: No valid intermediate themes from MAP step for {qid_for_scenario_a} (Summaries).\"\n","\n","        elif summary_chunks_A:\n","             logger_test_json.log(f\"INFO TEST_JSON SCENARIO A: {qid_for_scenario_a} summaries fit in one chunk. Direct LLM call.\")\n","             scenario_a_reduce_output_raw = get_llm_json_themes_test(\n","                 summary_chunks_A[0], question_text_for_q_a, TEST_JSON_NUM_THEMES_TO_REQUEST,\n","                 llm_thematic_pipe_test, TEST_JSON_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                 f\"{qid_for_scenario_a}_A_direct\", \"DIRECT_PROCESSING\"\n","             )\n","             scenario_a_final_themes = parse_llm_json_themes_test(scenario_a_reduce_output_raw, TEST_JSON_NUM_THEMES_TO_REQUEST)\n","             scenario_a_map_outputs_raw.append(scenario_a_reduce_output_raw) # Store direct output\n","        else:\n","            logger_test_json.log(f\"WARN TEST_JSON SCENARIO A: No summary chunks created for {qid_for_scenario_a}.\")\n","\n","    logger_test_json.log(\"\\n--- SCENARIO A (Map-Reduce on SUMMARIES, JSON Output) - Q4 Final Themes ---\")\n","    # ... (logging of Scenario A results) ...\n","    if scenario_a_final_themes:\n","        for i, theme in enumerate(scenario_a_final_themes): logger_test_json.log(f\"  A Theme {i+1} for {qid_for_scenario_a}: {theme.get('theme_label')}\\n     Explanation: {theme.get('explanation')}\")\n","    else: logger_test_json.log(f\"  No final themes parsed for Scenario A on {qid_for_scenario_a}.\")\n","    logger_test_json.log(f\"  Scenario A - Raw Map Outputs for {qid_for_scenario_a} (first one, max 500 chars):\")\n","    logger_test_json.log(f\"  {(scenario_a_map_outputs_raw[0][:500] + '...') if scenario_a_map_outputs_raw and scenario_a_map_outputs_raw[0] else 'N/A'}\")\n","    logger_test_json.log(f\"  Scenario A - Raw Reduce Output for {qid_for_scenario_a} (max 500 chars):\")\n","    logger_test_json.log(f\"  {(scenario_a_reduce_output_raw[:500] + '...') if scenario_a_reduce_output_raw else 'N/A'}\")\n","\n","\n","    # ==============================================================================\n","    # --- Scenario B: Map-Reduce on Stage 4 PASSAGES for Q16 ---\n","    # ==============================================================================\n","    qid_for_scenario_b = \"Q16\"\n","    logger_test_json.log(\"\\n\" + \"=\"*30 + f\" SCENARIO B (JSON): Map-Reduce on PDF PASSAGES for {qid_for_scenario_b} \" + \"=\"*30)\n","\n","    q_b_all_items = all_qids_collated_data.get(qid_for_scenario_b, [])\n","    q_b_passages = [item['text'] for item in q_b_all_items if item.get('type') == 'pdf_passages' and item.get('text')]\n","    question_text_for_q_b = qid_to_actual_question_text_map.get(qid_for_scenario_b, f\"Question {qid_for_scenario_b} text not loaded\")\n","\n","    scenario_b_final_themes = []\n","    scenario_b_map_outputs_raw = []\n","    scenario_b_reduce_output_raw = \"N/A\"\n","\n","    if not q_b_passages:\n","        logger_test_json.log(f\"WARN TEST_JSON SCENARIO B: No 'pdf_passages' items found for QID {qid_for_scenario_b}.\")\n","    else:\n","        logger_test_json.log(f\"INFO TEST_JSON SCENARIO B: Found {len(q_b_passages)} PDF passage blocks for {qid_for_scenario_b}.\")\n","\n","        passage_batches_B = batch_text_items_json_test(\n","            q_b_passages, llm_thematic_pipe_test.tokenizer,\n","            TEST_JSON_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK\n","        )\n","        logger_test_json.log(f\"INFO TEST_JSON SCENARIO B: Grouped {qid_for_scenario_b} passages into {len(passage_batches_B)} batches.\")\n","\n","        if len(passage_batches_B) > 0:\n","            intermediate_themes_B_list = []\n","            for i, batch_of_passage_strings in enumerate(passage_batches_B):\n","                current_batch_concatenated_text = \"\\n\\n--- Next Excerpt ---\\n\".join(batch_of_passage_strings)\n","                logger_test_json.log(f\"  SCENARIO B - MAP BATCH {i+1}/{len(passage_batches_B)} for {qid_for_scenario_b} (Passage items: {len(batch_of_passage_strings)}, Chars: {len(current_batch_concatenated_text)})\")\n","                map_out = get_llm_json_themes_test(\n","                    current_batch_concatenated_text, question_text_for_q_b, 3,\n","                    llm_thematic_pipe_test, TEST_JSON_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                    f\"{qid_for_scenario_b}_B_map_batch{i+1}\", \"SCENARIO_B_MAP\"\n","                )\n","                scenario_b_map_outputs_raw.append(map_out)\n","                if not map_out.startswith(\"LLM_ERROR:\"): intermediate_themes_B_list.append(map_out) # Store raw JSON\n","                time.sleep(0.2)\n","\n","            if intermediate_themes_B_list:\n","                # Similar to Scenario A, parse intermediate themes before combining for reduce\n","                parsed_intermediate_themes_for_reduce_B = []\n","                for raw_json_map_out_B in intermediate_themes_B_list:\n","                    parsed_map_themes_B = parse_llm_json_themes_test(raw_json_map_out_B, 3)\n","                    for pt_B in parsed_map_themes_B:\n","                        if not pt_B.get(\"theme_label\",\"\").startswith(\"LLM Error\") and not pt_B.get(\"theme_label\",\"\").startswith(\"Unparsed\") and not pt_B.get(\"theme_label\",\"\").startswith(\"Valid JSON\"):\n","                             parsed_intermediate_themes_for_reduce_B.append(f\"Theme Label: {pt_B.get('theme_label')}\\nExplanation: {pt_B.get('explanation')}\")\n","\n","                if parsed_intermediate_themes_for_reduce_B:\n","                    combined_intermediate_B = \"\\n\\n---\\n\\n\".join(parsed_intermediate_themes_for_reduce_B)\n","                    logger_test_json.log(f\"  SCENARIO B - REDUCE Step for {qid_for_scenario_b} (Input Chars for reduce: {len(combined_intermediate_B)})\")\n","                    scenario_b_reduce_output_raw = get_llm_json_themes_test(\n","                        combined_intermediate_B, question_text_for_q_b, TEST_JSON_NUM_THEMES_TO_REQUEST,\n","                        llm_thematic_pipe_test, TEST_JSON_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                        f\"{qid_for_scenario_b}_B_reduce\", \"REDUCE_INTERMEDIATE_THEMES\"\n","                    )\n","                    scenario_b_final_themes = parse_llm_json_themes_test(scenario_b_reduce_output_raw, TEST_JSON_NUM_THEMES_TO_REQUEST)\n","                else:\n","                    logger_test_json.log(f\"ERROR TEST_JSON SCENARIO B: No successfully parsed intermediate themes from MAP step for {qid_for_scenario_b} (Passages).\")\n","                    scenario_b_reduce_output_raw = f\"ERROR: No successfully parsed intermediate themes from MAP step for {qid_for_scenario_b} (Passages).\"\n","            else:\n","                logger_test_json.log(f\"ERROR TEST_JSON SCENARIO B: No valid intermediate themes from MAP step for {qid_for_scenario_b} (Passages).\")\n","                scenario_b_reduce_output_raw = f\"ERROR: No valid intermediate themes from MAP step for {qid_for_scenario_b} (Passages).\"\n","        else:\n","            logger_test_json.log(f\"WARN TEST_JSON SCENARIO B: No passage batches created for {qid_for_scenario_b}.\")\n","\n","    logger_test_json.log(\"\\n--- SCENARIO B (Map-Reduce on PDF PASSAGES, JSON Output) - Q16 Final Themes ---\")\n","    # ... (logging of Scenario B results) ...\n","    if scenario_b_final_themes:\n","        for i, theme in enumerate(scenario_b_final_themes): logger_test_json.log(f\"  B Theme {i+1} for {qid_for_scenario_b}: {theme.get('theme_label')}\\n     Explanation: {theme.get('explanation')}\")\n","    else: logger_test_json.log(f\"  No final themes parsed for Scenario B on {qid_for_scenario_b}.\")\n","    logger_test_json.log(f\"  Scenario B - Raw Map Outputs for {qid_for_scenario_b} (first one, max 500 chars):\")\n","    logger_test_json.log(f\"  {(scenario_b_map_outputs_raw[0][:500] + '...') if scenario_b_map_outputs_raw and scenario_b_map_outputs_raw[0] else 'N/A'}\")\n","    logger_test_json.log(f\"  Scenario B - Raw Reduce Output for {qid_for_scenario_b} (max 500 chars):\")\n","    logger_test_json.log(f\"  {(scenario_b_reduce_output_raw[:500] + '...') if scenario_b_reduce_output_raw else 'N/A'}\")\n","\n","\n","    logger_test_json.log(\"\\nINFO: Test Cell (JSON) - All Thematic Analysis Scenarios Test Finished.\")\n","\n","    logger_test_json.log(\"INFO TEST JSON: Cleaning up test LLM resources...\")\n","    # ... (cleanup code as before) ...\n","    if 'llm_thematic_pipe_test' in locals() and llm_thematic_pipe_test is not None: del llm_thematic_pipe_test\n","    if 'model_for_llm_test' in locals() and model_for_llm_test is not None: del model_for_llm_test\n","    if 'tokenizer_for_llm_test' in locals() and tokenizer_for_llm_test is not None: del tokenizer_for_llm_test\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","    logger_test_json.log(\"INFO TEST JSON: Test LLM cleanup complete.\")\n","\n","# --- Execute the Test ---\n","# print(\"REMINDER: Update TEST_JSON_... config variables, especially file paths and LLM ID.\")\n","# print(\"REMINDER: Ensure FileLogger class is defined and prerequisite cells (0, 1) are run.\")\n","# print(\"REMINDER: To run the test, uncomment the line below.\")\n","# run_thematic_analysis_tests_with_json()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qkdpZpAMMW-"},"outputs":[],"source":["# @title TEST CELL: Q4 (Scenario A - Summaries) & Q16 (Scenario B - Passages) Thematic Analysis\n","\n","# --- Test Cell Imports (ensure these are run if not already globally available) ---\n","import json\n","import os\n","import sys\n","import traceback\n","import time\n","import gc\n","import re\n","from collections import defaultdict\n","from typing import Dict, Any, List\n","\n","import torch\n","import numpy as np\n","# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline # In Cell 0\n","# try:\n","#     from bitsandbytes.cextension import ColabNotSupportedError # In Cell 0\n","# except ImportError:\n","#     ColabNotSupportedError = None\n","\n","# --- Configuration for this Test Cell ---\n","TEST_BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # <<<--- ADJUST IF NEEDED\n","TEST_LOG_FILE = os.path.join(TEST_BASE_PROJECT_DIR, f'test_q4a_q16b_thematic_analysis_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","\n","# LLM Config\n","TEST_LLM_MODEL_ID = 'google/gemma-3-4b-it'  # <<<--- Using 2b-it. Adjust to your PHASE3_LLM_MODEL_ID if desired.\n","TEST_USE_QUANTIZATION = False             # <<<--- Set to your PHASE3_USE_QUANTIZATION.\n","\n","# Thematic Analysis Parameters\n","TEST_NUM_THEMES_TO_REQUEST = 3 # Final number of themes for both scenarios\n","TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2800\n","TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 400\n","TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 600\n","TEST_MAPREDUCE_CHUNK_OVERLAP_WORDS = 30\n","\n","# Input file from Stage 4a (Phase 2 output)\n","TEST_INPUT_COLLATED_PDF_TEXTS_JSON = os.path.join(TEST_BASE_PROJECT_DIR, 'phase2_stage4a_collated_pdf_texts_20250518_233834.json') # <<<--- UPDATE THIS FILENAME\n","\n","# Source for actual question texts\n","TEST_QUESTION_TEXT_SOURCE_FILE = os.path.join(TEST_BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # <<<--- ENSURE THIS IS CORRECT\n","\n","# --- FileLogger (Assuming defined in Cell 1 or globally) ---\n","try:\n","    logger_test_ab = FileLogger(TEST_LOG_FILE) # Changed logger name for this combined test\n","    logger_test_ab.log(\"INFO: Test Cell (Q4-A, Q16-B) - Logger Initialized.\")\n","except NameError:\n","    print(\"CRITICAL: FileLogger class not defined. Please ensure Cell 1 has been executed.\")\n","    class PrintLogger: # Fallback\n","        def log(self, msg): print(f\"{time.strftime('%Y%m%d_%H%M%S')}: {msg}\")\n","    logger_test_ab = PrintLogger()\n","    logger_test_ab.log(\"WARN: Using fallback PrintLogger.\")\n","\n","# --- Helper: Text Chunker (for Scenario A - chunks a single long string of concatenated summaries) ---\n","def create_text_chunks_for_scenario_a(full_text: str, tokenizer_for_chunking, max_tokens: int, overlap_words: int = 30):\n","    # ... (Same robust create_text_chunks_for_test / create_text_chunks_scenario_a from previous response)\n","    # ... (Ensure it uses logger_test_ab.log())\n","    words = full_text.split()\n","    if not words: return []\n","    chunks = []\n","    current_chunk_words = []\n","    logger_test_ab.log(f\"DEBUG CHUNKER_A: Starting to chunk text of {len(words)} words into max {max_tokens} token chunks.\")\n","    for word_idx, word in enumerate(words):\n","        potential_chunk_text = \" \".join(current_chunk_words + [word])\n","        num_tokens = len(tokenizer_for_chunking.encode(potential_chunk_text, add_special_tokens=False))\n","        if num_tokens <= max_tokens:\n","            current_chunk_words.append(word)\n","        else:\n","            if current_chunk_words:\n","                chunks.append(\" \".join(current_chunk_words))\n","            if chunks and overlap_words > 0:\n","                prev_chunk_words = chunks[-1].split()\n","                overlap_start_idx = max(0, len(prev_chunk_words) - overlap_words)\n","                current_chunk_words = prev_chunk_words[overlap_start_idx:] + [word]\n","            else:\n","                current_chunk_words = [word]\n","            new_chunk_text_check = \" \".join(current_chunk_words)\n","            new_chunk_tokens_check = len(tokenizer_for_chunking.encode(new_chunk_text_check, add_special_tokens=False))\n","            if new_chunk_tokens_check > max_tokens:\n","                if len(current_chunk_words) > 1 and chunks: current_chunk_words = [word]\n","                elif len(current_chunk_words) == 1: logger_test_ab.log(f\"    WARN CHUNKER_A: Single word '{word}' oversized ({new_chunk_tokens_check} tokens).\")\n","    if current_chunk_words: chunks.append(\" \".join(current_chunk_words))\n","    logger_test_ab.log(f\"INFO CHUNKER_A: Split text into {len(chunks)} chunks.\")\n","    return chunks\n","\n","# --- Helper: Batch Individual Items (for Scenario B map step - batches a list of text items) ---\n","def batch_text_items_for_scenario_b(text_items_list: List[str], tokenizer_for_batching, max_tokens_per_batch: int):\n","    # ... (Same robust batch_text_items_for_test from previous response)\n","    # ... (Ensure it uses logger_test_ab.log())\n","    batches = []\n","    current_batch_item_texts = []\n","    current_batch_concatenated_text = \"\"\n","    logger_test_ab.log(f\"DEBUG BATCHER_B: Starting to batch {len(text_items_list)} items into max {max_tokens_per_batch} token batches.\")\n","    for item_idx, item_text_original in enumerate(text_items_list):\n","        if not item_text_original or not item_text_original.strip(): continue\n","        text_to_try_adding = item_text_original\n","        if current_batch_item_texts: text_to_try_adding = \"\\n\\n--- Next Excerpt ---\\n\" + item_text_original\n","        potential_full_batch_text = current_batch_concatenated_text + text_to_try_adding\n","        num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))\n","        if num_tokens <= max_tokens_per_batch:\n","            current_batch_item_texts.append(item_text_original)\n","            current_batch_concatenated_text = potential_full_batch_text\n","        else:\n","            if current_batch_item_texts: batches.append(current_batch_item_texts)\n","            single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))\n","            if single_item_tokens > max_tokens_per_batch:\n","                logger_test_ab.log(f\"  WARN BATCHER_B: Item {item_idx} ({single_item_tokens} tokens) oversized. Own batch.\")\n","                batches.append([item_text_original])\n","                current_batch_item_texts = []\n","                current_batch_concatenated_text = \"\"\n","            else:\n","                current_batch_item_texts = [item_text_original]\n","                current_batch_concatenated_text = item_text_original\n","    if current_batch_item_texts: batches.append(current_batch_item_texts)\n","    logger_test_ab.log(f\"INFO BATCHER_B: Created {len(batches)} batches of items.\")\n","    return batches\n","\n","# --- Helper: LLM Call for Thematic Points ---\n","def get_llm_thematic_points_ab_test(text_input_for_llm: str, question_text_for_prompt: str,\n","                                 num_themes_requested: int, llm_pipe: pipeline, max_new_tokens: int,\n","                                 log_prefix_qid_scenario: str, map_reduce_step_type: str):\n","    # ... (Same get_llm_thematic_points_test function from previous test cell, including robust context window check)\n","    # ... (Ensure it uses logger_test_ab.log())\n","    prompt_instruction = \"\" # Create this based on map_reduce_step_type like before\n","    if map_reduce_step_type == \"SCENARIO_A_MAP\": # Chunk of concatenated summaries\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. The following text is a CHUNK of concatenated PREVIOUSLY-GENERATED SUMMARIES from multiple PDF submissions responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on this provided CHUNK OF SUMMARIES, identify up to 3 most prominent and distinct points, arguments, or themes they collectively raise.\\n\"\n","            f\"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 points are evident, list only those that are clear and distinct.\\n\\n\"\n","            f\"Chunk of Summaries:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Prominent Points from this Chunk of Summaries:\"\n","        )\n","    elif map_reduce_step_type == \"SCENARIO_B_MAP\": # Batch of original passages\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Carefully review the following BATCH of original excerpts/passages from PDF submissions, all responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on this provided BATCH of original excerpts/passages, identify up to 3 prominent and distinct points, arguments, or themes they collectively raise.\\n\"\n","            f\"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 points are evident, list only those that are clear and distinct.\\n\\n\"\n","            f\"Batch of Excerpts/Passages:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Prominent Points from this Batch:\"\n","        )\n","    elif map_reduce_step_type == \"REDUCE_INTERMEDIATE_THEMES\":\n","        prompt_instruction = (\n","             f\"You are an expert policy analyst. The following are sets of preliminary themes/points that were individually extracted in previous steps from responses to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Your task is to synthesize these preliminary themes into the overall top {num_themes_requested} overarching and distinct common points or themes.\\n\"\n","            f\"For each final synthesized point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). Consolidate similar preliminary themes. Ensure the final themes are distinct and well-supported by the range of preliminary themes provided.\\n\\n\"\n","            f\"Preliminary Themes/Points Provided:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Top {num_themes_requested} Synthesized Common Points:\"\n","        )\n","    else: # DIRECT_PROCESSING\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Carefully review all the following text (which consists of combined PDF summaries OR a single batch of excerpts/passages) responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on the provided text, identify the top {num_themes_requested} most common and distinct points, arguments, or themes raised by the respondents.\\n\"\n","            f\"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes_requested} points are evident, list only those that are clear.\\n\\n\"\n","            f\"Provided Text:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Top {num_themes_requested} Common Points:\"\n","        )\n","    messages = [{\"role\": \"user\", \"content\": prompt_instruction}]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": False,\n","                           \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id}\n","        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True)\n","        input_text_tokens = llm_pipe.tokenizer.encode(text_input_for_llm, add_special_tokens=False)\n","        model_max_len = 8192\n","        if hasattr(llm_pipe.model.config, 'max_position_embeddings'): model_max_len = llm_pipe.model.config.max_position_embeddings\n","        elif hasattr(llm_pipe.model.config, 'max_sequence_length'): model_max_len = llm_pipe.model.config.max_sequence_length\n","        elif hasattr(llm_pipe.tokenizer, 'model_max_length'): model_max_len = llm_pipe.tokenizer.model_max_length\n","        logger_test_ab.log(f\"DEBUG LLM_CALL: {map_reduce_step_type} for {log_prefix_qid_scenario}. Prompt toks: {len(prompt_tokens)}. Content toks: {len(input_text_tokens)}. Max new: {max_new_tokens}. Model max_len: {model_max_len}.\")\n","        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95:\n","             logger_test_ab.log(f\"WARN LLM_CALL: {log_prefix_qid_scenario} - Step {map_reduce_step_type} - Prompt+Output tokens might exceed context ({model_max_len}).\")\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger_test_ab.log(f\"ERROR LLM_CALL: {map_reduce_step_type} LLM unexpected output for {log_prefix_qid_scenario}.\")\n","            return \"LLM_ERROR: No output generated.\"\n","        generated_text_full = outputs[0]['generated_text']\n","        llm_response_text = generated_text_full[len(prompt_formatted):].strip() if generated_text_full.startswith(prompt_formatted) else generated_text_full.split(\"<start_of_turn>model\\n\", 1)[-1].strip() if \"<start_of_turn>model\\n\" in generated_text_full else generated_text_full\n","        return llm_response_text\n","    except Exception as e:\n","        logger_test_ab.log(f\"ERROR LLM_CALL: {map_reduce_step_type} LLM exception for {log_prefix_qid_scenario}: {e}\")\n","        logger_test_ab.log(traceback.format_exc())\n","        return f\"LLM_ERROR: Exception - {str(e)}\"\n","\n","# --- Helper: Parse LLM Thematic Output ---\n","def parse_llm_themes_ab_test(llm_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:\n","    # ... (Same robust parse_llm_themes_test function from previous test cell, ensure it uses logger_test_ab) ...\n","    parsed_themes = []\n","    if not llm_output_text or llm_output_text.startswith(\"LLM_ERROR:\"):\n","        return [{\"theme_label\": \"LLM Error or Parsing Error\", \"explanation\": llm_output_text}]\n","    theme_pattern = re.compile(r\"Theme Label \\d+[:\\-\\s]*(.*?)\\s*Explanation \\d+[:\\-\\s]*(.*?)(?=(Theme Label \\d+|$|\\Z))\", re.DOTALL | re.IGNORECASE) # Added \\Z for end of string\n","    matches = theme_pattern.findall(llm_output_text)\n","    for match_idx, match in enumerate(matches):\n","        if len(parsed_themes) < num_expected_themes:\n","            parsed_themes.append({\"theme_label\": match[0].strip(), \"explanation\": match[1].strip()})\n","        else: break\n","    if not parsed_themes and llm_output_text.strip() and not llm_output_text.startswith(\"LLM_ERROR:\"):\n","        potential_blocks = re.split(r'\\n\\s*(?=\\d+[\\.\\)\\-])', \"\\n\" + llm_output_text.strip())\n","        for block in potential_blocks:\n","            block = block.strip()\n","            if not block: continue\n","            if len(parsed_themes) >= num_expected_themes: break\n","            label_part = block.split('\\n')[0]\n","            explanation_part = block\n","            parsed_themes.append({\"theme_label\": re.sub(r\"^\\d+[\\.\\)\\-]\\s*\", \"\", label_part).strip()[:150], \"explanation\": explanation_part})\n","    if not parsed_themes and llm_output_text.strip() and not llm_output_text.startswith(\"LLM_ERROR:\"):\n","        parsed_themes.append({\"theme_label\": \"Unparsed LLM Output Block\", \"explanation\": llm_output_text.strip()})\n","    return parsed_themes[:num_expected_themes]\n","\n","\n","# --- Main Test Execution Function ---\n","def run_thematic_analysis_tests():\n","    logger_test_ab.log(\"INFO: Test Cell - Starting Thematic Analysis Scenarios Test (Q4-A, Q16-B).\")\n","\n","    # --- 0. Load LLM (once for all tests in this cell) ---\n","    llm_thematic_pipe_test = None\n","    tokenizer_for_llm_test = None # For chunkers/batchers\n","    try:\n","        logger_test_ab.log(f\"INFO TEST: Loading LLM: {TEST_LLM_MODEL_ID} (Quant: {TEST_USE_QUANTIZATION})\")\n","        tokenizer_for_llm_test = AutoTokenizer.from_pretrained(TEST_LLM_MODEL_ID)\n","        if getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is not None:\n","            tokenizer_for_llm_test.pad_token = tokenizer_for_llm_test.eos_token\n","        elif getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is None:\n","             logger_test_ab.log(f\"CRITICAL TEST: Tokenizer for {TEST_LLM_MODEL_ID} has neither pad_token nor eos_token.\")\n","\n","        bnb_config_test = None\n","        effective_quant_test = TEST_USE_QUANTIZATION\n","        if TEST_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config_test = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n","                logger_test_ab.log(f\"INFO TEST: BNB config with compute_dtype: {compute_dtype}\")\n","            except ImportError: # Specific for bitsandbytes\n","                 logger_test_ab.log(f\"WARN TEST: bitsandbytes not imported. Disabling quantization.\")\n","                 effective_quant_test = False\n","            except ColabNotSupportedError: # Specific error from bnb\n","                 logger_test_ab.log(f\"WARN TEST: Bitsandbytes ColabNotSupportedError. Disabling quantization.\")\n","                 effective_quant_test = False\n","            except Exception as e_bnb:\n","                logger_test_ab.log(f\"WARN TEST: BNB config failed: {e_bnb}. Disabling quantization.\")\n","                effective_quant_test = False\n","        elif TEST_USE_QUANTIZATION and not torch.cuda.is_available():\n","            logger_test_ab.log(\"WARN TEST: Quantization requested, No CUDA. Disabling quantization.\")\n","            effective_quant_test = False\n","\n","        model_for_llm_test = AutoModelForCausalLM.from_pretrained(\n","            TEST_LLM_MODEL_ID,\n","            quantization_config=bnb_config_test if effective_quant_test else None,\n","            torch_dtype=torch.bfloat16 if not effective_quant_test and torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (torch.float32 if not effective_quant_test else None),\n","            device_map=\"auto\", trust_remote_code=True\n","        )\n","        llm_thematic_pipe_test = pipeline(\"text-generation\", model=model_for_llm_test, tokenizer=tokenizer_for_llm_test)\n","        logger_test_ab.log(\"INFO TEST: LLM Pipeline Loaded.\")\n","    except Exception as e:\n","        logger_test_ab.log(f\"CRITICAL TEST: Failed to load LLM: {e}\")\n","        logger_test_ab.log(traceback.format_exc())\n","        return # Cannot proceed without LLM\n","\n","    # --- 1. Load Full Stage 4a Data (once) ---\n","    all_qids_collated_data = {}\n","    try:\n","        with open(TEST_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:\n","            stage4a_data = json.load(f)\n","        all_qids_collated_data = stage4a_data.get(\"aggregated_pdf_content_by_qid\", {})\n","        if not all_qids_collated_data:\n","            logger_test_ab.log(\"ERROR TEST: No 'aggregated_pdf_content_by_qid' found in Stage 4a output. Aborting.\")\n","            return\n","        logger_test_ab.log(f\"INFO TEST: Loaded collated data for {len(all_qids_collated_data)} QIDs from {TEST_INPUT_COLLATED_PDF_TEXTS_JSON}.\")\n","    except Exception as e:\n","        logger_test_ab.log(f\"CRITICAL TEST: Failed to load Stage 4a data from {TEST_INPUT_COLLATED_PDF_TEXTS_JSON}: {e}\")\n","        logger_test_ab.log(traceback.format_exc())\n","        return\n","\n","    # --- 2. Load All Question Texts (once) ---\n","    qid_to_actual_question_text_map = {}\n","    try:\n","        logger_test_ab.log(f\"INFO TEST: Attempting to load question texts from: {TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","        if not os.path.exists(TEST_QUESTION_TEXT_SOURCE_FILE):\n","             raise FileNotFoundError(f\"Question text source file not found: {TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","        with open(TEST_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:\n","            qtext_source_data = json.load(f_qtext)\n","        if \"question_data\" in qtext_source_data:\n","            for qid_key, data_val in qtext_source_data.get(\"question_data\", {}).items():\n","                qid_to_actual_question_text_map[qid_key] = data_val.get(\"question_text\", f\"Text for {qid_key} not found\")\n","            logger_test_ab.log(f\"INFO TEST: Loaded {len(qid_to_actual_question_text_map)} question texts (from 'question_data' structure).\")\n","        elif \"analysis_by_question\" in qtext_source_data: # Fallback for CAR_35 like structure\n","             for qid_key, data_val in qtext_source_data.get(\"analysis_by_question\", {}).items():\n","                qid_to_actual_question_text_map[qid_key] = data_val.get(\"question_text\", f\"Text for {qid_key} not found\")\n","             logger_test_ab.log(f\"INFO TEST: Loaded {len(qid_to_actual_question_text_map)} question texts (from 'analysis_by_question' structure).\")\n","        else:\n","            logger_test_ab.log(f\"WARN TEST: Neither 'question_data' nor 'analysis_by_question' key found in {TEST_QUESTION_TEXT_SOURCE_FILE}. Question texts may be missing.\")\n","    except Exception as e:\n","        logger_test_ab.log(f\"ERROR TEST: Failed to load question texts: {e}\")\n","        logger_test_ab.log(traceback.format_exc())\n","\n","\n","    # ==============================================================================\n","    # --- Scenario A: Map-Reduce on Stage 4 SUMMARIES for Q4 ---\n","    # ==============================================================================\n","    qid_for_scenario_a = \"Q4\"\n","    logger_test_ab.log(\"\\n\" + \"=\"*30 + f\" SCENARIO A: Map-Reduce on PDF SUMMARIES for {qid_for_scenario_a} \" + \"=\"*30)\n","\n","    q_a_all_items = all_qids_collated_data.get(qid_for_scenario_a, [])\n","    q_a_summaries = [item['text'] for item in q_a_all_items if item.get('type') == 'pdf_summary' and item.get('text')]\n","    question_text_for_q_a = qid_to_actual_question_text_map.get(qid_for_scenario_a, f\"Question {qid_for_scenario_a} text not loaded\")\n","\n","    scenario_a_final_themes = []\n","    scenario_a_map_outputs_raw = []\n","    scenario_a_reduce_output_raw = \"N/A\"\n","\n","    if not q_a_summaries:\n","        logger_test_ab.log(f\"WARN TEST SCENARIO A: No 'pdf_summary' items found for QID {qid_for_scenario_a}. Scenario A cannot proceed for this QID.\")\n","    else:\n","        logger_test_ab.log(f\"INFO TEST SCENARIO A: Found {len(q_a_summaries)} PDF summaries for {qid_for_scenario_a}.\")\n","        concatenated_q_a_summaries = \"\\n\\n--- Next Summary Entry ---\\n\".join(q_a_summaries)\n","        logger_test_ab.log(f\"INFO TEST SCENARIO A: Concatenated summaries char length for {qid_for_scenario_a}: {len(concatenated_q_a_summaries)}\")\n","\n","        summary_chunks_A = create_text_chunks_for_scenario_a(\n","            concatenated_q_a_summaries,\n","            llm_thematic_pipe_test.tokenizer,\n","            TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK,\n","            TEST_MAPREDUCE_CHUNK_OVERLAP_WORDS\n","        )\n","\n","        if len(summary_chunks_A) > 1:\n","            logger_test_ab.log(f\"INFO TEST SCENARIO A: {qid_for_scenario_a} summaries split into {len(summary_chunks_A)} chunks for Map step.\")\n","            intermediate_themes_A_list = []\n","            for i, chunk in enumerate(summary_chunks_A):\n","                logger_test_ab.log(f\"  SCENARIO A - MAP CHUNK {i+1}/{len(summary_chunks_A)} for {qid_for_scenario_a} (Chars: {len(chunk)})\")\n","                map_out = get_llm_thematic_points_ab_test(\n","                    chunk, question_text_for_q_a, 3,\n","                    llm_thematic_pipe_test, TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                    f\"{qid_for_scenario_a}_A_map_chunk{i+1}\", \"SCENARIO_A_MAP\"\n","                )\n","                scenario_a_map_outputs_raw.append(map_out)\n","                if not map_out.startswith(\"LLM_ERROR:\"): intermediate_themes_A_list.append(f\"Themes from Chunk {i+1}:\\n{map_out}\")\n","                time.sleep(0.2)\n","\n","            if intermediate_themes_A_list:\n","                combined_intermediate_A = \"\\n\\n\".join(intermediate_themes_A_list)\n","                logger_test_ab.log(f\"  SCENARIO A - REDUCE Step for {qid_for_scenario_a} (Input Chars for reduce: {len(combined_intermediate_A)})\")\n","                scenario_a_reduce_output_raw = get_llm_thematic_points_ab_test(\n","                    combined_intermediate_A, question_text_for_q_a, TEST_NUM_THEMES_TO_REQUEST,\n","                    llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                    f\"{qid_for_scenario_a}_A_reduce\", \"REDUCE_INTERMEDIATE_THEMES\"\n","                )\n","                scenario_a_final_themes = parse_llm_themes_ab_test(scenario_a_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)\n","            else:\n","                logger_test_ab.log(f\"ERROR TEST SCENARIO A: No intermediate themes from MAP step for {qid_for_scenario_a}.\")\n","                scenario_a_reduce_output_raw = f\"ERROR: No intermediate themes from MAP step for {qid_for_scenario_a} (Summaries).\"\n","        elif summary_chunks_A:\n","             logger_test_ab.log(f\"INFO TEST SCENARIO A: {qid_for_scenario_a} summaries fit in one chunk. Direct LLM call.\")\n","             scenario_a_reduce_output_raw = get_llm_thematic_points_ab_test(\n","                 summary_chunks_A[0], question_text_for_q_a, TEST_NUM_THEMES_TO_REQUEST,\n","                 llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                 f\"{qid_for_scenario_a}_A_direct\", \"DIRECT_PROCESSING\"\n","             )\n","             scenario_a_final_themes = parse_llm_themes_ab_test(scenario_a_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)\n","             scenario_a_map_outputs_raw.append(scenario_a_reduce_output_raw)\n","        else:\n","            logger_test_ab.log(f\"WARN TEST SCENARIO A: No summary chunks created for {qid_for_scenario_a}.\")\n","\n","    logger_test_ab.log(\"\\n--- SCENARIO A (Map-Reduce on SUMMARIES) - Q4 Final Themes ---\")\n","    if scenario_a_final_themes:\n","        for i, theme in enumerate(scenario_a_final_themes): logger_test_ab.log(f\"  A Theme {i+1} for {qid_for_scenario_a}: {theme.get('theme_label')}\\n     Explanation: {theme.get('explanation')}\")\n","    else: logger_test_ab.log(f\"  No final themes parsed for Scenario A on {qid_for_scenario_a}.\")\n","    logger_test_ab.log(f\"  Scenario A - Raw Map Outputs for {qid_for_scenario_a} (first one, max 500 chars):\")\n","    logger_test_ab.log(f\"  {(scenario_a_map_outputs_raw[0][:500] + '...') if scenario_a_map_outputs_raw and scenario_a_map_outputs_raw[0] else 'N/A'}\")\n","    logger_test_ab.log(f\"  Scenario A - Raw Reduce Output for {qid_for_scenario_a} (max 500 chars):\")\n","    logger_test_ab.log(f\"  {(scenario_a_reduce_output_raw[:500] + '...') if scenario_a_reduce_output_raw else 'N/A'}\")\n","\n","\n","    # ==============================================================================\n","    # --- Scenario B: Map-Reduce on Stage 4 PASSAGES for Q16 ---\n","    # ==============================================================================\n","    qid_for_scenario_b = \"Q16\"\n","    logger_test_ab.log(\"\\n\" + \"=\"*30 + f\" SCENARIO B: Map-Reduce on PDF PASSAGES for {qid_for_scenario_b} \" + \"=\"*30)\n","\n","    q_b_all_items = all_qids_collated_data.get(qid_for_scenario_b, [])\n","    q_b_passages = [item['text'] for item in q_b_all_items if item.get('type') == 'pdf_passages' and item.get('text')]\n","    question_text_for_q_b = qid_to_actual_question_text_map.get(qid_for_scenario_b, f\"Question {qid_for_scenario_b} text not loaded\")\n","\n","    scenario_b_final_themes = []\n","    scenario_b_map_outputs_raw = []\n","    scenario_b_reduce_output_raw = \"N/A\"\n","\n","    if not q_b_passages:\n","        logger_test_ab.log(f\"WARN TEST SCENARIO B: No 'pdf_passages' items found for QID {qid_for_scenario_b}. Scenario B cannot proceed for this QID.\")\n","    else:\n","        logger_test_ab.log(f\"INFO TEST SCENARIO B: Found {len(q_b_passages)} PDF passage blocks for {qid_for_scenario_b}.\")\n","\n","        passage_batches_B = batch_text_items_for_scenario_b( # Renamed to avoid conflict\n","            q_b_passages,\n","            llm_thematic_pipe_test.tokenizer,\n","            TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK\n","        )\n","        logger_test_ab.log(f\"INFO TEST SCENARIO B: Grouped {qid_for_scenario_b} passages into {len(passage_batches_B)} batches for Map step.\")\n","\n","        if len(passage_batches_B) > 0:\n","            intermediate_themes_B_list = []\n","            for i, batch_of_passage_strings in enumerate(passage_batches_B): # Each item is a list of passage strings\n","                current_batch_concatenated_text = \"\\n\\n--- Next Excerpt ---\\n\".join(batch_of_passage_strings)\n","                logger_test_ab.log(f\"  SCENARIO B - MAP BATCH {i+1}/{len(passage_batches_B)} for {qid_for_scenario_b} (Passage items: {len(batch_of_passage_strings)}, Chars: {len(current_batch_concatenated_text)})\")\n","                map_out = get_llm_thematic_points_ab_test(\n","                    current_batch_concatenated_text, question_text_for_q_b, 3,\n","                    llm_thematic_pipe_test, TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                    f\"{qid_for_scenario_b}_B_map_batch{i+1}\", \"SCENARIO_B_MAP\"\n","                )\n","                scenario_b_map_outputs_raw.append(map_out)\n","                if not map_out.startswith(\"LLM_ERROR:\"): intermediate_themes_B_list.append(f\"Themes from Batch {i+1}:\\n{map_out}\")\n","                time.sleep(0.2)\n","\n","            if intermediate_themes_B_list:\n","                combined_intermediate_B = \"\\n\\n\".join(intermediate_themes_B_list)\n","                logger_test_ab.log(f\"  SCENARIO B - REDUCE Step for {qid_for_scenario_b} (Input Chars: {len(combined_intermediate_B)})\")\n","                scenario_b_reduce_output_raw = get_llm_thematic_points_ab_test(\n","                    combined_intermediate_B, question_text_for_q_b, TEST_NUM_THEMES_TO_REQUEST,\n","                    llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                    f\"{qid_for_scenario_b}_B_reduce\", \"REDUCE_INTERMEDIATE_THEMES\"\n","                )\n","                scenario_b_final_themes = parse_llm_themes_ab_test(scenario_b_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)\n","            else:\n","                logger_test_ab.log(f\"ERROR TEST SCENARIO B: No intermediate themes from MAP step for {qid_for_scenario_b} (Passages).\")\n","                scenario_b_reduce_output_raw = f\"ERROR: No intermediate themes from MAP step for {qid_for_scenario_b} (Passages).\"\n","        else:\n","            logger_test_ab.log(f\"WARN TEST SCENARIO B: No passage batches created for {qid_for_scenario_b}.\")\n","\n","    logger_test_ab.log(\"\\n--- SCENARIO B (Map-Reduce on PDF PASSAGES) - Q16 Final Themes ---\")\n","    if scenario_b_final_themes:\n","        for i, theme in enumerate(scenario_b_final_themes): logger_test_ab.log(f\"  B Theme {i+1} for {qid_for_scenario_b}: {theme.get('theme_label')}\\n     Explanation: {theme.get('explanation')}\")\n","    else: logger_test_ab.log(f\"  No final themes parsed for Scenario B on {qid_for_scenario_b}.\")\n","    logger_test_ab.log(f\"  Scenario B - Raw Map Outputs for {qid_for_scenario_b} (first one, max 500 chars):\")\n","    logger_test_ab.log(f\"  {(scenario_b_map_outputs_raw[0][:500] + '...') if scenario_b_map_outputs_raw and scenario_b_map_outputs_raw[0] else 'N/A'}\")\n","    logger_test_ab.log(f\"  Scenario B - Raw Reduce Output for {qid_for_scenario_b} (max 500 chars):\")\n","    logger_test_ab.log(f\"  {(scenario_b_reduce_output_raw[:500] + '...') if scenario_b_reduce_output_raw else 'N/A'}\")\n","\n","\n","    logger_test_ab.log(\"\\nINFO: Test Cell - All Thematic Analysis Scenarios Test Finished.\")\n","\n","    logger_test_ab.log(\"INFO TEST: Cleaning up test LLM resources...\")\n","    if 'llm_thematic_pipe_test' in locals() and llm_thematic_pipe_test is not None: del llm_thematic_pipe_test\n","    if 'model_for_llm_test' in locals() and model_for_llm_test is not None: del model_for_llm_test\n","    if 'tokenizer_for_llm_test' in locals() and tokenizer_for_llm_test is not None: del tokenizer_for_llm_test\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","    logger_test_ab.log(\"INFO TEST: Test LLM cleanup complete.\")\n","\n","# --- Execute the Test ---\n","# print(\"REMINDER: Update TEST_... config variables, especially file paths and LLM ID.\")\n","# print(\"REMINDER: Ensure FileLogger class is defined and prerequisite cells (0, 1) are run.\")\n","# print(\"REMINDER: To run the test, uncomment the line below.\")\n","# run_thematic_analysis_tests()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyBjHAMTFdpd"},"outputs":[],"source":["# @title Quick script to find QIDs with passage counts\n","import json\n","stage4a_file = '/content/drive/MyDrive/Colab Notebooks/Legal/phase2_stage4a_collated_pdf_texts_20250518_233834.json' # Your actual file\n","passage_counts_per_qid = {}\n","with open(stage4a_file, 'r') as f:\n","    data = json.load(f)\n","for qid, items in data.get(\"aggregated_pdf_content_by_qid\", {}).items():\n","    passage_counts_per_qid[qid] = sum(1 for item in items if item.get('type') == 'pdf_passages')\n","\n","sorted_passage_counts = sorted(passage_counts_per_qid.items(), key=lambda x: x[1], reverse=True)\n","print(\"QIDs with most 'pdf_passages' items:\")\n","for qid, count in sorted_passage_counts[:10]:\n","    print(f\"{qid}: {count} passages\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Af5vcH98jNB"},"outputs":[],"source":["# @title TEST CELL: Q4 Thematic Analysis - SCENARIO A (Map-Reduce on PDF Summaries) ONLY\n","\n","# --- Test Cell Imports (ensure these are run if not already globally available) ---\n","import json\n","import os\n","import sys\n","import traceback\n","import time\n","import gc\n","import re\n","from collections import defaultdict\n","from typing import Dict, Any, List\n","\n","import torch\n","import numpy as np\n","# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline # Already in Cell 0\n","# try:\n","#     from bitsandbytes.cextension import ColabNotSupportedError # Already in Cell 0\n","# except ImportError:\n","#     ColabNotSupportedError = None\n","\n","# --- Configuration for this Test Cell ---\n","TEST_A_BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # <<<--- ADJUST IF NEEDED\n","TEST_A_LOG_FILE = os.path.join(TEST_A_BASE_PROJECT_DIR, f'test_q4_scenario_A_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","\n","# LLM Config\n","TEST_A_LLM_MODEL_ID = 'google/gemma-3-4b-it'  # <<<--- Using 2b-it for test. Adjust to your PHASE3_LLM_MODEL_ID (e.g., 'google/gemma-7b-it') if desired.\n","TEST_A_USE_QUANTIZATION = False             # <<<--- Set to your PHASE3_USE_QUANTIZATION\n","\n","# Thematic Analysis Parameters\n","TEST_A_NUM_THEMES_TO_REQUEST = 3\n","TEST_A_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2800 # Max *content* tokens for map step input (for chunks of concatenated summaries)\n","TEST_A_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 400\n","TEST_A_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 600\n","TEST_A_MAPREDUCE_CHUNK_OVERLAP_WORDS = 30\n","\n","# Input file from Stage 4a (Phase 2 output)\n","TEST_A_INPUT_COLLATED_PDF_TEXTS_JSON = os.path.join(TEST_A_BASE_PROJECT_DIR, 'phase2_stage4a_collated_pdf_texts_20250518_233834.json') # <<<--- UPDATE THIS FILENAME\n","\n","# Source for actual question texts\n","TEST_A_QUESTION_TEXT_SOURCE_FILE = os.path.join(TEST_A_BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # <<<--- ENSURE THIS IS CORRECT\n","\n","# Specific QID for this test\n","TEST_A_QID_TO_ANALYZE = \"Q4\"\n","\n","# --- FileLogger (Assuming defined in Cell 1 or globally) ---\n","try:\n","    logger_test_A = FileLogger(TEST_A_LOG_FILE)\n","    logger_test_A.log(f\"INFO: Test Scenario A - Logger Initialized for QID: {TEST_A_QID_TO_ANALYZE}.\")\n","except NameError:\n","    print(\"CRITICAL: FileLogger class not defined. Please ensure Cell 1 has been executed.\")\n","    class PrintLogger: # Fallback\n","        def log(self, msg): print(f\"{time.strftime('%Y%m%d_%H%M%S')}: {msg}\")\n","    logger_test_A = PrintLogger()\n","    logger_test_A.log(\"WARN: Using fallback PrintLogger.\")\n","\n","# --- Helper: Text Chunker (for Scenario A - chunks a single long string) ---\n","def create_text_chunks_scenario_a(full_text: str, tokenizer_for_chunking, max_tokens: int, overlap_words: int = 30):\n","    # ... (Using the create_text_chunks_for_test from previous response, ensure it uses logger_test_A.log())\n","    words = full_text.split()\n","    if not words: return []\n","    chunks = []\n","    current_chunk_words = []\n","    logger_test_A.log(f\"DEBUG CHUNKER_A: Starting to chunk text of {len(words)} words into max {max_tokens} token chunks.\")\n","    for word_idx, word in enumerate(words):\n","        potential_chunk_text = \" \".join(current_chunk_words + [word])\n","        num_tokens = len(tokenizer_for_chunking.encode(potential_chunk_text, add_special_tokens=False))\n","        if num_tokens <= max_tokens:\n","            current_chunk_words.append(word)\n","        else:\n","            if current_chunk_words:\n","                chunks.append(\" \".join(current_chunk_words))\n","                # logger_test_A.log(f\"  DEBUG CHUNKER_A: Added chunk {len(chunks)} with {len(current_chunk_words)} words, {len(tokenizer_for_chunking.encode(chunks[-1], add_special_tokens=False))} tokens.\")\n","            if chunks and overlap_words > 0:\n","                prev_chunk_words = chunks[-1].split()\n","                overlap_start_idx = max(0, len(prev_chunk_words) - overlap_words)\n","                current_chunk_words = prev_chunk_words[overlap_start_idx:] + [word]\n","            else:\n","                current_chunk_words = [word]\n","            new_chunk_text_check = \" \".join(current_chunk_words)\n","            new_chunk_tokens_check = len(tokenizer_for_chunking.encode(new_chunk_text_check, add_special_tokens=False))\n","            if new_chunk_tokens_check > max_tokens:\n","                if len(current_chunk_words) > 1 and chunks:\n","                    current_chunk_words = [word]\n","                elif len(current_chunk_words) == 1:\n","                     logger_test_A.log(f\"    WARN CHUNKER_A: Single word '{word}' oversized ({new_chunk_tokens_check} tokens).\")\n","    if current_chunk_words:\n","        chunks.append(\" \".join(current_chunk_words))\n","        # logger_test_A.log(f\"  DEBUG CHUNKER_A: Added final chunk {len(chunks)} with {len(current_chunk_words)} words, {len(tokenizer_for_chunking.encode(chunks[-1], add_special_tokens=False))} tokens.\")\n","    logger_test_A.log(f\"INFO CHUNKER_A: Split text into {len(chunks)} chunks.\")\n","    return chunks\n","\n","# --- Helper: LLM Call for Thematic Points ---\n","def get_llm_thematic_points_scenario_a(text_input_for_llm: str, question_text_for_prompt: str,\n","                                       num_themes_requested: int, llm_pipe: pipeline, max_new_tokens: int,\n","                                       log_prefix_qid: str, map_reduce_step_type: str): # \"SCENARIO_A_MAP\", \"REDUCE_INTERMEDIATE_THEMES\", \"DIRECT_PROCESSING\"\n","    prompt_instruction = \"\"\n","    if map_reduce_step_type == \"SCENARIO_A_MAP\":\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. The following text is a CHUNK of concatenated PREVIOUSLY-GENERATED SUMMARIES from multiple PDF submissions responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on this provided CHUNK OF SUMMARIES, identify up to 3 most prominent and distinct points, arguments, or themes they collectively raise.\\n\"\n","            f\"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 points are evident, list only those that are clear and distinct.\\n\\n\"\n","            f\"Chunk of Summaries:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Prominent Points from this Chunk of Summaries:\"\n","        )\n","    elif map_reduce_step_type == \"REDUCE_INTERMEDIATE_THEMES\":\n","        prompt_instruction = ( # Same reduce prompt as before\n","            f\"You are an expert policy analyst. The following are sets of preliminary themes/points that were individually extracted in previous steps from responses to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Your task is to synthesize these preliminary themes into the overall top {num_themes_requested} overarching and distinct common points or themes.\\n\"\n","            # ... (rest of reduce prompt as in your full Cell 8)\n","            f\"For each final synthesized point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). Consolidate similar preliminary themes. Ensure the final themes are distinct and well-supported by the range of preliminary themes provided.\\n\\n\"\n","            f\"Preliminary Themes/Points Provided:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Top {num_themes_requested} Synthesized Common Points:\"\n","        )\n","    else: # DIRECT_PROCESSING (if concatenated summaries fit in one go)\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Carefully review all the following text (which consists of combined PDF summaries) responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on the provided text, identify the top {num_themes_requested} most common and distinct points, arguments, or themes raised by the respondents.\\n\"\n","            # ... (rest of direct prompt as in your full Cell 8)\n","            f\"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes_requested} distinct common points are evident, list only those that are clear.\\n\\n\"\n","            f\"Provided Text (Combined PDF Summaries):\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Top {num_themes_requested} Common Points:\"\n","        )\n","    messages = [{\"role\": \"user\", \"content\": prompt_instruction}]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": False,\n","                           \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id}\n","\n","        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True)\n","        input_text_tokens = llm_pipe.tokenizer.encode(text_input_for_llm, add_special_tokens=False)\n","\n","        model_max_len = 8192 # Default\n","        if hasattr(llm_pipe.model.config, 'max_position_embeddings'): model_max_len = llm_pipe.model.config.max_position_embeddings\n","        elif hasattr(llm_pipe.model.config, 'max_sequence_length'): model_max_len = llm_pipe.model.config.max_sequence_length\n","        elif hasattr(llm_pipe.tokenizer, 'model_max_length'): model_max_len = llm_pipe.tokenizer.model_max_length\n","\n","        logger_test_A.log(f\"DEBUG SCENARIO_A: {map_reduce_step_type} LLM call for {log_prefix_qid}. Prompt toks: {len(prompt_tokens)}. Content toks: {len(input_text_tokens)}. Max new: {max_new_tokens}. Model max_len: {model_max_len}.\")\n","\n","        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95:\n","             logger_test_A.log(f\"WARN SCENARIO_A: {log_prefix_qid} - Step {map_reduce_step_type} - Prompt+Output tokens might exceed model context ({model_max_len}).\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger_test_A.log(f\"ERROR SCENARIO_A: {map_reduce_step_type} LLM returned unexpected for {log_prefix_qid}.\")\n","            return \"LLM_ERROR: No output generated.\"\n","        generated_text_full = outputs[0]['generated_text']\n","        llm_response_text = generated_text_full[len(prompt_formatted):].strip() if generated_text_full.startswith(prompt_formatted) else generated_text_full.split(\"<start_of_turn>model\\n\", 1)[-1].strip() if \"<start_of_turn>model\\n\" in generated_text_full else generated_text_full\n","        return llm_response_text\n","    except Exception as e:\n","        logger_test_A.log(f\"ERROR SCENARIO_A: {map_reduce_step_type} LLM exception for {log_prefix_qid}: {e}\")\n","        logger_test_A.log(traceback.format_exc())\n","        return f\"LLM_ERROR: Exception - {str(e)}\"\n","\n","# --- Helper: Parse LLM Thematic Output ---\n","def parse_llm_themes_scenario_a(llm_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:\n","    # ... (Use the same parse_llm_themes_test function from the previous test cell, ensure it uses logger_test_A) ...\n","    parsed_themes = []\n","    if not llm_output_text or llm_output_text.startswith(\"LLM_ERROR:\"):\n","        return [{\"theme_label\": \"LLM Error or Parsing Error\", \"explanation\": llm_output_text}]\n","    theme_pattern = re.compile(r\"Theme Label \\d+[:\\-\\s]*(.*?)\\s*Explanation \\d+[:\\-\\s]*(.*?)(?=(Theme Label \\d+|$))\", re.DOTALL | re.IGNORECASE)\n","    matches = theme_pattern.findall(llm_output_text)\n","    for match_idx, match in enumerate(matches):\n","        if len(parsed_themes) < num_expected_themes:\n","            parsed_themes.append({\"theme_label\": match[0].strip(), \"explanation\": match[1].strip()})\n","        else: break\n","    if not parsed_themes and llm_output_text.strip() and not llm_output_text.startswith(\"LLM_ERROR:\"):\n","        potential_blocks = re.split(r'\\n\\s*(?=\\d+[\\.\\)\\-])', \"\\n\" + llm_output_text.strip())\n","        for block in potential_blocks:\n","            block = block.strip()\n","            if not block: continue\n","            if len(parsed_themes) >= num_expected_themes: break\n","            label_part = block.split('\\n')[0] # Simplistic grab\n","            explanation_part = block\n","            parsed_themes.append({\"theme_label\": re.sub(r\"^\\d+[\\.\\)\\-]\\s*\", \"\", label_part).strip()[:150], \"explanation\": explanation_part})\n","    if not parsed_themes and llm_output_text.strip() and not llm_output_text.startswith(\"LLM_ERROR:\"):\n","        parsed_themes.append({\"theme_label\": \"Unparsed LLM Output Block\", \"explanation\": llm_output_text.strip()})\n","    return parsed_themes[:num_expected_themes]\n","\n","\n","# --- Main Test Execution Function for SCENARIO A ---\n","def run_q4_scenario_a_test():\n","    logger_test_A.log(\"INFO: Test Scenario A - Starting Q4 Thematic Analysis on SUMMARIES.\")\n","\n","    # --- 0. Load Actual Question Text for Q4 ---\n","    question_actual_text_q4 = f\"Question text for {TEST_A_QID_TO_ANALYZE} not loaded (Default)\"\n","    try:\n","        # ... (Same question text loading logic as in your previous test cell, using TEST_A_QUESTION_TEXT_SOURCE_FILE)\n","        logger_test_A.log(f\"INFO TEST_A: Attempting to load question texts from: {TEST_A_QUESTION_TEXT_SOURCE_FILE}\")\n","        if not os.path.exists(TEST_A_QUESTION_TEXT_SOURCE_FILE):\n","            raise FileNotFoundError(f\"File not found: {TEST_A_QUESTION_TEXT_SOURCE_FILE}\")\n","        with open(TEST_A_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:\n","            qtext_source_data = json.load(f_qtext)\n","        if \"question_data\" in qtext_source_data:\n","            q4_data_content = qtext_source_data.get(\"question_data\", {}).get(TEST_A_QID_TO_ANALYZE)\n","            if q4_data_content and \"question_text\" in q4_data_content:\n","                question_actual_text_q4 = q4_data_content[\"question_text\"]\n","                logger_test_A.log(f\"INFO TEST_A: Loaded Q4 text: '{question_actual_text_q4[:100]}...'\")\n","            else: logger_test_A.log(f\"WARN TEST_A: Q4 text not in 'question_data'.\")\n","        # Add elif for \"analysis_by_question\" if needed\n","        else: logger_test_A.log(f\"WARN TEST_A: Expected keys for Q texts not in {TEST_A_QUESTION_TEXT_SOURCE_FILE}\")\n","    except Exception as e:\n","        logger_test_A.log(f\"ERROR TEST_A: Failed to load Q4 text: {e}\")\n","        logger_test_A.log(traceback.format_exc())\n","\n","    # --- 1. Load LLM ---\n","    llm_pipe_scenario_a = None\n","    tokenizer_scenario_a = None\n","    try:\n","        # ... (LLM loading logic as in your previous test cell, using TEST_A_LLM_MODEL_ID, TEST_A_USE_QUANTIZATION)\n","        logger_test_A.log(f\"INFO TEST_A: Loading LLM: {TEST_A_LLM_MODEL_ID} (Quant: {TEST_A_USE_QUANTIZATION})\")\n","        tokenizer_scenario_a = AutoTokenizer.from_pretrained(TEST_A_LLM_MODEL_ID)\n","        if getattr(tokenizer_scenario_a, 'pad_token', None) is None : tokenizer_scenario_a.pad_token = tokenizer_scenario_a.eos_token\n","\n","        bnb_config_A = None\n","        effective_quant_A = TEST_A_USE_QUANTIZATION\n","        if TEST_A_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config_A = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n","            except Exception as e_bnb: effective_quant_A = False; logger_test_A.log(f\"WARN TEST_A: BNB failed: {e_bnb}\")\n","        elif TEST_A_USE_QUANTIZATION: effective_quant_A = False; logger_test_A.log(\"WARN TEST_A: Quant requested, No CUDA.\")\n","\n","        model_A = AutoModelForCausalLM.from_pretrained(\n","            TEST_A_LLM_MODEL_ID,\n","            quantization_config=bnb_config_A if effective_quant_A else None,\n","            torch_dtype=torch.bfloat16 if not effective_quant_A and torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (torch.float32 if not effective_quant_A else None),\n","            device_map=\"auto\", trust_remote_code=True\n","        )\n","        llm_pipe_scenario_a = pipeline(\"text-generation\", model=model_A, tokenizer=tokenizer_scenario_a)\n","        logger_test_A.log(\"INFO TEST_A: LLM Pipeline Loaded.\")\n","    except Exception as e:\n","        logger_test_A.log(f\"CRITICAL TEST_A: Failed to load LLM: {e}\")\n","        logger_test_A.log(traceback.format_exc())\n","        return\n","\n","    # --- 2. Load and Filter Q4 Summaries from Stage 4a Data ---\n","    q4_summaries_from_stage4a = []\n","    try:\n","        with open(TEST_A_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:\n","            stage4a_data = json.load(f)\n","        q4_all_items = stage4a_data.get(\"aggregated_pdf_content_by_qid\", {}).get(TEST_A_QID_TO_ANALYZE, [])\n","        q4_summaries_from_stage4a = [item['text'] for item in q4_all_items if item.get('type') == 'pdf_summary' and item.get('text')]\n","        if not q4_summaries_from_stage4a:\n","            logger_test_A.log(f\"ERROR TEST_A: No 'pdf_summary' items found for QID {TEST_A_QID_TO_ANALYZE}. Aborting Scenario A test.\")\n","            return\n","        logger_test_A.log(f\"INFO TEST_A: Loaded {len(q4_summaries_from_stage4a)} PDF summaries for {TEST_A_QID_TO_ANALYZE}.\")\n","    except Exception as e:\n","        logger_test_A.log(f\"CRITICAL TEST_A: Failed to load Stage 4a data: {e}\")\n","        logger_test_A.log(traceback.format_exc())\n","        return\n","\n","    # --- Scenario A: Map-Reduce on Concatenated Stage 4 Summaries ---\n","    concatenated_q4_summaries = \"\\n\\n--- Next Summary Entry ---\\n\".join(q4_summaries_from_stage4a) # Use a clear separator\n","    logger_test_A.log(f\"INFO TEST_A: Concatenated summaries char length: {len(concatenated_q4_summaries)}\")\n","\n","    final_themes_A = []\n","    map_outputs_raw_A = []\n","    reduce_output_raw_A = \"N/A\"\n","\n","    # Chunk the concatenated string of summaries\n","    summary_chunks = create_text_chunks_scenario_a(\n","        concatenated_q4_summaries,\n","        llm_pipe_scenario_a.tokenizer, # Pass the loaded tokenizer\n","        TEST_A_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK,\n","        TEST_A_MAPREDUCE_CHUNK_OVERLAP_WORDS\n","    )\n","\n","    if len(summary_chunks) > 1: # Map-Reduce needed\n","        logger_test_A.log(f\"INFO TEST_A: Concatenated summaries for Q4 split into {len(summary_chunks)} chunks for Map step.\")\n","        intermediate_themes_text_A = []\n","        for i, chunk in enumerate(summary_chunks):\n","            logger_test_A.log(f\"  SCENARIO A - MAP CHUNK {i+1}/{len(summary_chunks)} (Chars: {len(chunk)})\")\n","            map_out = get_llm_thematic_points_scenario_a(\n","                chunk, question_actual_text_q4, 3, # Request e.g. 3 themes per chunk\n","                llm_pipe_scenario_a, TEST_A_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                f\"{TEST_A_QID_TO_ANALYZE}_A_map_chunk{i+1}\", \"SCENARIO_A_MAP\"\n","            )\n","            map_outputs_raw_A.append(map_out)\n","            if not map_out.startswith(\"LLM_ERROR:\"):\n","                intermediate_themes_text_A.append(f\"Themes from Chunk {i+1}:\\n{map_out}\")\n","            time.sleep(0.5) # Small delay\n","\n","        if intermediate_themes_text_A:\n","            combined_intermediate_A = \"\\n\\n\".join(intermediate_themes_text_A)\n","            logger_test_A.log(f\"  SCENARIO A - REDUCE Step on intermediate themes (Input Chars: {len(combined_intermediate_A)})\")\n","            reduce_output_raw_A = get_llm_thematic_points_scenario_a(\n","                combined_intermediate_A, question_actual_text_q4, TEST_A_NUM_THEMES_TO_REQUEST,\n","                llm_pipe_scenario_a, TEST_A_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                f\"{TEST_A_QID_TO_ANALYZE}_A_reduce\", \"REDUCE_INTERMEDIATE_THEMES\"\n","            )\n","            final_themes_A = parse_llm_themes_scenario_a(reduce_output_raw_A, TEST_A_NUM_THEMES_TO_REQUEST)\n","        else:\n","            logger_test_A.log(\"ERROR TEST_A: No valid intermediate themes from MAP step.\")\n","            reduce_output_raw_A = \"ERROR: No intermediate themes from MAP step (Summaries).\"\n","\n","    elif summary_chunks: # Only one chunk, direct processing\n","         logger_test_A.log(f\"INFO TEST_A: Concatenated summaries fit in one chunk. Direct LLM call on summaries.\")\n","         reduce_output_raw_A = get_llm_thematic_points_scenario_a(\n","             summary_chunks[0], question_actual_text_q4, TEST_A_NUM_THEMES_TO_REQUEST,\n","             llm_pipe_scenario_a, TEST_A_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS, # Use reduce tokens for final output\n","             f\"{TEST_A_QID_TO_ANALYZE}_A_direct\", \"DIRECT_PROCESSING\"\n","         )\n","         final_themes_A = parse_llm_themes_scenario_a(reduce_output_raw_A, TEST_A_NUM_THEMES_TO_REQUEST)\n","         map_outputs_raw_A.append(reduce_output_raw_A) # Store direct output as if it was a map output\n","    else:\n","        logger_test_A.log(\"WARN TEST_A: No summary chunks created (input likely empty after join/filter).\")\n","\n","    logger_test_A.log(\"\\n\" + \"-\"*15 + f\" SCENARIO A (Map-Reduce on SUMMARIES) - {TEST_A_QID_TO_ANALYZE} Final Themes \" + \"-\"*15)\n","    if final_themes_A:\n","        for i, theme in enumerate(final_themes_A):\n","            logger_test_A.log(f\"  A Theme {i+1}: {theme.get('theme_label')}\\n     Explanation: {theme.get('explanation')}\")\n","    else:\n","        logger_test_A.log(\"  No final themes parsed for Scenario A.\")\n","    logger_test_A.log(\"  Scenario A - Raw Map Outputs (first one if many, max 500 chars):\")\n","    logger_test_A.log(f\"  {(map_outputs_raw_A[0][:500] + '...') if map_outputs_raw_A and map_outputs_raw_A[0] else 'N/A'}\")\n","    logger_test_A.log(\"  Scenario A - Raw Reduce Output (max 500 chars):\")\n","    logger_test_A.log(f\"  {(reduce_output_raw_A[:500] + '...') if reduce_output_raw_A else 'N/A'}\")\n","\n","    logger_test_A.log(\"\\nINFO: Test Cell - Scenario A for Q4 Finished.\")\n","\n","    # Cleanup\n","    logger_test_A.log(\"INFO TEST_A: Cleaning up LLM resources for Scenario A...\")\n","    if 'llm_pipe_scenario_a' in locals() and llm_pipe_scenario_a is not None: del llm_pipe_scenario_a\n","    if 'model_A' in locals() and model_A is not None: del model_A\n","    if 'tokenizer_scenario_a' in locals() and tokenizer_scenario_a is not None: del tokenizer_scenario_a\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","    logger_test_A.log(\"INFO TEST_A: LLM cleanup complete.\")\n","\n","# --- Execute the Test for Scenario A ---\n","# run_q4_scenario_a_test()\n","# print(\"REMINDER: Uncomment 'run_q4_scenario_a_test()' to execute SCENARIO A test for Q4.\")"]},{"cell_type":"markdown","metadata":{"id":"Q5D5QFN4281U"},"source":["# 19th May"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JJ4wHEzMS0nE"},"outputs":[],"source":["# @title TEST CELL: Q4 Thematic Analysis - Scenario A (Summaries) vs. Scenario B (Passages)\n","\n","# --- Test Cell Imports (ensure these are run if not already globally available) ---\n","import json\n","import os\n","import sys # Used by FileLogger if not already imported\n","import traceback # Used by FileLogger if not already imported\n","import time\n","import gc\n","import re\n","from collections import defaultdict\n","from typing import Dict, Any, List # Used by FileLogger if not already imported\n","\n","# Assuming ML/NLP Core Imports, Hugging Face, BitsAndBytes are done in Cell 0\n","# Ensure torch, numpy, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline are available\n","# For FileLogger, ensure numpy and torch are available if you expect to log such objects,\n","# though this specific test cell doesn't directly create them for logging.\n","import torch\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","try:\n","    from bitsandbytes.cextension import ColabNotSupportedError\n","except ImportError:\n","    ColabNotSupportedError = None\n","\n","\n","# --- Configuration for this Test Cell ---\n","TEST_BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # <<<--- ADJUST IF NEEDED\n","TEST_LOG_FILE = os.path.join(TEST_BASE_PROJECT_DIR, f'test_q4_thematic_analysis_{time.strftime(\"%Y%m%d_%H%M%S\")}.log')\n","\n","# LLM Config\n","TEST_LLM_MODEL_ID = 'google/gemma-3-4b-it'  # <<<--- IMPORTANT: Using 2b-it. Adjust to your PHASE3_LLM_MODEL_ID (e.g., 'google/gemma-7b-it') if different and you have resources.\n","TEST_USE_QUANTIZATION = False             # <<<--- IMPORTANT: Set to your PHASE3_USE_QUANTIZATION (True/False)\n","\n","# Thematic Analysis Parameters\n","TEST_NUM_THEMES_TO_REQUEST = 3 # Final number of themes\n","TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2800 # Max *content* tokens for map step input items (text part only)\n","                                                 # LLM tokenizer.encode(text) to check. Needs room for prompt.\n","TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 400     # Max tokens LLM generates for themes *from one sub-chunk*.\n","TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 600  # Max tokens LLM generates for *final* themes.\n","TEST_MAPREDUCE_CHUNK_OVERLAP_WORDS = 30          # For create_text_chunks_for_test if used.\n","\n","# Input file from Stage 4a (Phase 2 output)\n","# Replace with the *actual filename* generated by your Stage 4a / Phase 2 script\n","TEST_INPUT_COLLATED_PDF_TEXTS_JSON = os.path.join(TEST_BASE_PROJECT_DIR, 'phase2_stage4a_collated_pdf_texts_20250518_233834.json') # <<<--- UPDATE THIS FILENAME (Example from your log)\n","\n","# Source for actual question texts\n","TEST_QUESTION_TEXT_SOURCE_FILE = os.path.join(TEST_BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # <<<--- ENSURE THIS IS CORRECT\n","\n","# --- FileLogger Class (Assuming defined in Cell 1 or globally) ---\n","# If not, you would paste the FileLogger class definition here.\n","# For this example, I'll assume it's available and works as you provided.\n","try:\n","    logger_phase3_test = FileLogger(TEST_LOG_FILE)\n","    logger_phase3_test.log(\"INFO: Test Cell - Logger Initialized.\")\n","except NameError:\n","    print(\"CRITICAL: FileLogger class not defined. Please ensure Cell 1 has been executed.\")\n","    # Fallback to print if logger fails, to allow some output\n","    class PrintLogger:\n","        def log(self, msg): print(f\"{time.strftime('%Y%m%d_%H%M%S')}: {msg}\")\n","        def log_json(self, data, prefix=\"JSON\"): print(f\"{time.strftime('%Y%m%d_%H%M%S')}: {prefix} - {json.dumps(data, indent=2)}\")\n","    logger_phase3_test = PrintLogger()\n","    logger_phase3_test.log(\"WARN: Using fallback PrintLogger as FileLogger was not found.\")\n","\n","\n","# --- Helper: Text Chunker (for Scenario A if concatenated summaries are too long) ---\n","def create_text_chunks_for_test(full_text: str, tokenizer_for_chunking, max_tokens: int, overlap_words: int = 30):\n","    words = full_text.split() # Simple split, consider nltk.word_tokenize for more complex text\n","    if not words: return []\n","\n","    chunks = []\n","    current_chunk_words = []\n","\n","    logger_phase3_test.log(f\"DEBUG CHUNKER: Starting to chunk text of {len(words)} words into max {max_tokens} token chunks.\")\n","\n","    for word_idx, word in enumerate(words):\n","        # Test adding the new word\n","        potential_chunk_text = \" \".join(current_chunk_words + [word])\n","        num_tokens = len(tokenizer_for_chunking.encode(potential_chunk_text, add_special_tokens=False))\n","\n","        if num_tokens <= max_tokens:\n","            current_chunk_words.append(word)\n","        else:\n","            # Current chunk plus new word is too long. Finalize current_chunk_words.\n","            if current_chunk_words: # Add current chunk if it has content\n","                chunks.append(\" \".join(current_chunk_words))\n","                logger_phase3_test.log(f\"  DEBUG CHUNKER: Added chunk {len(chunks)} with {len(current_chunk_words)} words, {len(tokenizer_for_chunking.encode(chunks[-1], add_special_tokens=False))} tokens.\")\n","\n","            # Start new chunk. Apply overlap from the *finalized* previous chunk.\n","            if chunks and overlap_words > 0:\n","                 # Get words from the *actual last chunk added* for overlap\n","                prev_chunk_words = chunks[-1].split()\n","                overlap_start_idx = max(0, len(prev_chunk_words) - overlap_words)\n","                current_chunk_words = prev_chunk_words[overlap_start_idx:] + [word]\n","            else: # No previous chunk or no overlap\n","                current_chunk_words = [word]\n","\n","            # If the new chunk (overlap + new word, or just new word) is ALREADY too long\n","            new_chunk_text_check = \" \".join(current_chunk_words)\n","            new_chunk_tokens_check = len(tokenizer_for_chunking.encode(new_chunk_text_check, add_special_tokens=False))\n","            if new_chunk_tokens_check > max_tokens:\n","                logger_phase3_test.log(f\"  WARN CHUNKER: Word '{word}' or (overlap + word) resulted in an oversized new chunk ({new_chunk_tokens_check} tokens).\")\n","                if len(current_chunk_words) > 1 and chunks: # If it was overlap + word that was too long\n","                    # Discard overlap for this problematic word, just start with the word\n","                    logger_phase3_test.log(f\"    DEBUG CHUNKER: Discarding overlap, starting new chunk with just '{word}'.\")\n","                    current_chunk_words = [word]\n","                    # Check again if the single word is too long\n","                    if len(tokenizer_for_chunking.encode(word, add_special_tokens=False)) > max_tokens:\n","                         logger_phase3_test.log(f\"    WARN CHUNKER: Single word '{word}' itself is too long ({len(tokenizer_for_chunking.encode(word, add_special_tokens=False))} tokens). Adding as oversized chunk.\")\n","                         # It will be added as its own chunk in the next 'if current_chunk_words'\n","                elif len(current_chunk_words) == 1: # Single word is too long\n","                     logger_phase3_test.log(f\"    WARN CHUNKER: Single word '{word}' itself is too long ({new_chunk_tokens_check} tokens). Adding as oversized chunk.\")\n","                     # Will be added as its own chunk below\n","\n","    if current_chunk_words: # Add the last remaining chunk\n","        chunks.append(\" \".join(current_chunk_words))\n","        logger_phase3_test.log(f\"  DEBUG CHUNKER: Added final chunk {len(chunks)} with {len(current_chunk_words)} words, {len(tokenizer_for_chunking.encode(chunks[-1], add_special_tokens=False))} tokens.\")\n","\n","    logger_phase3_test.log(f\"INFO CHUNKER: Split text into {len(chunks)} chunks.\")\n","    return chunks\n","\n","\n","# --- Helper: Batch Individual Items (for Scenario B map step) ---\n","def batch_text_items_for_test(text_items_list: List[str], tokenizer_for_batching, max_tokens_per_batch: int):\n","    batches = [] # List of lists of strings (each inner list is a batch of item texts)\n","    current_batch_item_texts = [] # List of strings for the current batch\n","    current_batch_concatenated_text = \"\" # For token checking\n","\n","    logger_phase3_test.log(f\"DEBUG BATCHER: Starting to batch {len(text_items_list)} items into max {max_tokens_per_batch} token batches.\")\n","\n","    for item_idx, item_text_original in enumerate(text_items_list):\n","        if not item_text_original or not item_text_original.strip():\n","            logger_phase3_test.log(f\"  DEBUG BATCHER: Item {item_idx} is empty, skipping.\")\n","            continue\n","\n","        # Text for next item to potentially add, with separator if needed\n","        text_to_try_adding = item_text_original\n","        if current_batch_item_texts: # If batch is not empty, add separator before next item\n","            text_to_try_adding = \"\\n\\n--- Next Excerpt ---\\n\" + item_text_original\n","\n","        potential_full_batch_text = current_batch_concatenated_text + text_to_try_adding\n","        num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))\n","\n","        if num_tokens <= max_tokens_per_batch:\n","            current_batch_item_texts.append(item_text_original) # Store original item text\n","            current_batch_concatenated_text = potential_full_batch_text # Update concatenated text for next check\n","        else:\n","            # Current batch + new item is too long. Finalize current_batch_item_texts.\n","            if current_batch_item_texts:\n","                batches.append(current_batch_item_texts)\n","                logger_phase3_test.log(f\"  DEBUG BATCHER: Added batch {len(batches)} with {len(current_batch_item_texts)} items, {len(tokenizer_for_batching.encode(current_batch_concatenated_text, add_special_tokens=False))} tokens.\")\n","\n","            # Start new batch with the current item_text_original\n","            # Check if the item_text_original itself is too long\n","            single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))\n","            if single_item_tokens > max_tokens_per_batch:\n","                logger_phase3_test.log(f\"  WARN BATCHER: Item {item_idx} (len {len(item_text_original)} chars, {single_item_tokens} tokens) is too long for batch limit {max_tokens_per_batch}. Will be its own oversized batch.\")\n","                batches.append([item_text_original]) # Add as its own batch\n","                logger_phase3_test.log(f\"  DEBUG BATCHER: Added oversized item as batch {len(batches)}.\")\n","                current_batch_item_texts = [] # Reset for next iteration\n","                current_batch_concatenated_text = \"\"\n","            else:\n","                current_batch_item_texts = [item_text_original]\n","                current_batch_concatenated_text = item_text_original\n","\n","    if current_batch_item_texts: # Add any remaining batch\n","        batches.append(current_batch_item_texts)\n","        logger_phase3_test.log(f\"  DEBUG BATCHER: Added final batch {len(batches)} with {len(current_batch_item_texts)} items, {len(tokenizer_for_batching.encode(current_batch_concatenated_text, add_special_tokens=False))} tokens.\")\n","\n","    logger_phase3_test.log(f\"INFO BATCHER: Created {len(batches)} batches of items.\")\n","    return batches\n","\n","\n","# --- Helper: LLM Call for Thematic Points ---\n","def get_llm_thematic_points_test(text_input_for_llm: str,\n","                                 question_text_for_prompt: str,\n","                                 num_themes_requested: int,\n","                                 llm_pipe: pipeline,\n","                                 max_new_tokens: int,\n","                                 qid_for_log: str, # For logging purposes\n","                                 map_reduce_step_type: str): # \"SCENARIO_A_MAP\", \"SCENARIO_B_MAP\", \"REDUCE_INTERMEDIATE_THEMES\", \"DIRECT_PROCESSING\"\n","    prompt_instruction = \"\"\n","    if map_reduce_step_type == \"SCENARIO_A_MAP\":\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. The following text is a CHUNK of concatenated PREVIOUSLY-GENERATED SUMMARIES from multiple PDF submissions responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on this provided CHUNK OF SUMMARIES, identify up to 3 most prominent and distinct points, arguments, or themes they collectively raise.\\n\"\n","            f\"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 points are evident, list only those that are clear and distinct.\\n\\n\"\n","            f\"Chunk of Summaries:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Prominent Points from this Chunk of Summaries:\"\n","        )\n","    elif map_reduce_step_type == \"SCENARIO_B_MAP\":\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Carefully review the following BATCH of original excerpts/passages from PDF submissions, all responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on this provided BATCH of original excerpts/passages, identify up to 3 prominent and distinct points, arguments, or themes they collectively raise.\\n\"\n","            f\"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 points are evident, list only those that are clear and distinct.\\n\\n\"\n","            f\"Batch of Excerpts/Passages:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Prominent Points from this Batch:\"\n","        )\n","    elif map_reduce_step_type == \"REDUCE_INTERMEDIATE_THEMES\":\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. The following are sets of preliminary themes/points that were individually extracted in previous steps from responses to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Your task is to synthesize these preliminary themes into the overall top {num_themes_requested} overarching and distinct common points or themes.\\n\"\n","            f\"For each final synthesized point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). Consolidate similar preliminary themes. Ensure the final themes are distinct and well-supported by the range of preliminary themes provided.\\n\\n\"\n","            f\"Preliminary Themes/Points Provided:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Top {num_themes_requested} Synthesized Common Points:\"\n","        )\n","    else: # Direct processing (e.g. Scenario A if all summaries fit, or if only one batch in Scenario B)\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Carefully review all the following text (which consists of combined summaries OR a single batch of excerpts/passages) from PDF submissions responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on the provided text, identify the top {num_themes_requested} most common and distinct points, arguments, or themes raised by the respondents.\\n\"\n","            f\"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes_requested} points are evident, list only those that are clear.\\n\\n\"\n","            f\"Provided Text:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Top {num_themes_requested} Common Points:\"\n","        )\n","    messages = [{\"role\": \"user\", \"content\": prompt_instruction}]\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": False,\n","                           \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id}\n","\n","        # Debug logging for prompt length and input length\n","        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True) # Full prompt tokens\n","        input_text_tokens = llm_pipe.tokenizer.encode(text_input_for_llm, add_special_tokens=False) # Just the content part\n","        logger_phase3_test.log(f\"DEBUG TEST: {map_reduce_step_type} LLM call for {qid_for_log}. Prompt tokens: {len(prompt_tokens)}. Input content tokens: {len(input_text_tokens)}. Max new: {max_new_tokens}.\")\n","\n","        # if len(prompt_tokens) + max_new_tokens > (llm_pipe.model.config.max_position_embeddings or 8192) * 0.95: # Check against model's actual max context (use 95% as buffer)\n","        #      logger_phase3_test.log(f\"WARN TEST: QID {qid_for_log} - Step {map_reduce_step_type} - Combined prompt tokens ({len(prompt_tokens)}) + max_new_tokens ({max_new_tokens}) might exceed model's context window. LLM might truncate or error.\")\n","\n","# Inside get_llm_thematic_points_test function\n","\n","        # --- Determine model's max context window ---\n","        model_max_len = 8192 # Default if not found\n","        if hasattr(llm_pipe.model.config, 'max_position_embeddings'):\n","            model_max_len = llm_pipe.model.config.max_position_embeddings\n","        elif hasattr(llm_pipe.model.config, 'max_sequence_length'): # Common for Gemma\n","            model_max_len = llm_pipe.model.config.max_sequence_length\n","        elif hasattr(llm_pipe.tokenizer, 'model_max_length'): # Sometimes tokenizer has it\n","             model_max_len = llm_pipe.tokenizer.model_max_length\n","        logger_phase3_test.log(f\"DEBUG TEST: Determined model_max_len: {model_max_len}\")\n","\n","\n","        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95: # Check against determined model_max_len\n","             logger_phase3_test.log(f\"WARN TEST: QID {qid_for_log} - Step {map_reduce_step_type} - Combined prompt tokens ({len(prompt_tokens)}) + max_new_tokens ({max_new_tokens}) might exceed model's context window ({model_max_len}). LLM might truncate or error.\")\n","        # --- End context window check modification ---\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger_phase3_test.log(f\"ERROR TEST: {map_reduce_step_type} LLM returned unexpected for QID {qid_for_log}.\")\n","            return \"LLM_ERROR: No output generated.\"\n","\n","        generated_text_full = outputs[0]['generated_text']\n","\n","        # Attempt to strip the prompt part from the generated text\n","        llm_response_text = generated_text_full\n","        if generated_text_full.startswith(prompt_formatted):\n","            llm_response_text = generated_text_full[len(prompt_formatted):].strip()\n","        else:\n","            # Gemma specific, or general model response start\n","            model_response_marker = \"<start_of_turn>model\\n\"\n","            if model_response_marker in generated_text_full:\n","                llm_response_text = generated_text_full.split(model_response_marker, 1)[-1].strip()\n","            else: # If no clear marker and no prompt match, log a warning and return the full text\n","                logger_phase3_test.log(f\"WARN TEST: {map_reduce_step_type} for QID {qid_for_log} - Could not reliably strip prompt. LLM output may contain prompt residue.\")\n","\n","        return llm_response_text\n","    except Exception as e:\n","        logger_phase3_test.log(f\"ERROR TEST: {map_reduce_step_type} LLM call exception for QID {qid_for_log}: {e}\")\n","        logger_phase3_test.log(traceback.format_exc())\n","        return f\"LLM_ERROR: Exception - {str(e)}\"\n","\n","# --- Helper: Parse LLM Thematic Output ---\n","def parse_llm_themes_test(llm_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:\n","    parsed_themes = []\n","    if not llm_output_text or llm_output_text.startswith(\"LLM_ERROR:\"):\n","        return [{\"theme_label\": \"LLM Error or Parsing Error\", \"explanation\": llm_output_text}]\n","\n","    # Improved regex to capture \"X. Theme Label: [Label]\" and \"Explanation: [Explanation]\"\n","    # It looks for a number, then optionally \"Theme Label:\", then captures the label text.\n","    # Then it looks for \"Explanation:\" and captures the explanation text until the next theme number or end of string.\n","    # This pattern is designed to be more flexible.\n","\n","    # Split the output into potential theme blocks based on numbering (e.g., \"1.\", \"2.\", etc.)\n","    # Add a newline at the beginning to help regex catch the first item if not starting with \\n\n","    potential_blocks = re.split(r'\\n\\s*(?=\\d+[\\.\\)\\-])', \"\\n\" + llm_output_text.strip())\n","\n","    for block_idx, block in enumerate(potential_blocks):\n","        block = block.strip()\n","        if not block:\n","            continue\n","\n","        theme_label = f\"Theme {len(parsed_themes) + 1} (Label not distinctly parsed)\" # Default label\n","        explanation = block # Default explanation is the whole block initially\n","\n","        # Try to find \"Theme Label:\" or similar, then \"Explanation:\"\n","        # Case-insensitive search for \"Theme Label\" and \"Explanation\"\n","        label_match = re.search(r\"^(?:\\d+[\\.\\)\\-]\\s*)?(?:Theme Label\\s*[:\\-]?\\s*)(.*?)(?:\\n|$)\", block, re.IGNORECASE | re.MULTILINE)\n","        expl_match = re.search(r\"Explanation\\s*[:\\-]?\\s*(.*)\", block, re.IGNORECASE | re.DOTALL)\n","\n","        if label_match:\n","            theme_label = label_match.group(1).strip()\n","            if expl_match: # If explanation keyword is also found\n","                explanation = expl_match.group(1).strip()\n","            else: # If label found, but no \"Explanation:\" keyword, assume rest of block after label is explanation\n","                # This part is tricky; LLM might not always use \"Explanation:\"\n","                # Try to take text after the found label\n","                explanation_start_index = block.lower().find(theme_label.lower()) + len(theme_label)\n","                # Look for a newline or just take the rest\n","                temp_expl = block[explanation_start_index:].strip()\n","                if temp_expl.startswith(\":\") or temp_expl.startswith(\"-\"):\n","                    temp_expl = temp_expl[1:].strip()\n","                if temp_expl: # Only if there's something left after the label\n","                     explanation = temp_expl\n","                # else explanation remains the full block or needs better logic\n","\n","        elif expl_match: # Found explanation but no distinct label before it\n","            explanation = expl_match.group(1).strip()\n","            # Try to infer label from text before \"Explanation:\"\n","            label_candidate_text = block[:expl_match.start()].strip()\n","            # Remove potential numbering like \"1. \" from the inferred label\n","            label_candidate_text = re.sub(r\"^\\d+[\\.\\)\\-]\\s*\", \"\", label_candidate_text).strip()\n","            if label_candidate_text:\n","                theme_label = label_candidate_text\n","\n","        # Only add if we have something meaningful\n","        if theme_label != f\"Theme {len(parsed_themes) + 1} (Label not distinctly parsed)\" or explanation != block :\n","             if theme_label.strip() or explanation.strip(): # Ensure at least one part has content\n","                parsed_themes.append({\"theme_label\": theme_label.strip(), \"explanation\": explanation.strip()})\n","\n","        if len(parsed_themes) >= num_expected_themes:\n","            break\n","\n","    if not parsed_themes and llm_output_text.strip() and not llm_output_text.startswith(\"LLM_ERROR:\"): # Fallback for completely unparsed\n","        parsed_themes.append({\"theme_label\": \"Unparsed LLM Output Block\", \"explanation\": llm_output_text.strip()})\n","\n","    return parsed_themes[:num_expected_themes]\n","\n","\n","# --- Main Test Execution Function ---\n","def run_q4_thematic_test_scenarios():\n","    logger_phase3_test.log(\"INFO: Test Cell - Starting Q4 Thematic Analysis Scenarios Test.\")\n","\n","    # --- 0. Load Actual Question Text for Q4 ---\n","    question_actual_text_q4_for_test = f\"Question text for Q4 not loaded (Default)\"\n","    try:\n","        logger_phase3_test.log(f\"INFO TEST: Attempting to load question texts from: {TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","        if not os.path.exists(TEST_QUESTION_TEXT_SOURCE_FILE):\n","            raise FileNotFoundError(f\"File not found: {TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","\n","        with open(TEST_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:\n","            qtext_source_data = json.load(f_qtext)\n","\n","        if \"question_data\" in qtext_source_data:\n","            q4_data = qtext_source_data.get(\"question_data\", {}).get(\"Q4\")\n","            if q4_data and \"question_text\" in q4_data:\n","                question_actual_text_q4_for_test = q4_data[\"question_text\"]\n","                logger_phase3_test.log(f\"INFO TEST: Successfully loaded Q4 text (Stage 2 struct): '{question_actual_text_q4_for_test[:100]}...'\")\n","            else:\n","                logger_phase3_test.log(f\"WARN TEST: Q4 or 'question_text' not in 'question_data' in {TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","        elif \"analysis_by_question\" in qtext_source_data: # Fallback to CAR_35 like structure\n","            q4_data = qtext_source_data.get(\"analysis_by_question\", {}).get(\"Q4\")\n","            if q4_data and \"question_text\" in q4_data:\n","                question_actual_text_q4_for_test = q4_data[\"question_text\"]\n","                logger_phase3_test.log(f\"INFO TEST: Successfully loaded Q4 text (CAR_35 struct): '{question_actual_text_q4_for_test[:100]}...'\")\n","            else:\n","                logger_phase3_test.log(f\"WARN TEST: Q4 or 'question_text' not in 'analysis_by_question' in {TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","        else:\n","            logger_phase3_test.log(f\"WARN TEST: Expected keys for question texts not found in {TEST_QUESTION_TEXT_SOURCE_FILE}\")\n","    except Exception as e:\n","        logger_phase3_test.log(f\"ERROR TEST: Failed to load question text for Q4: {e}\")\n","        logger_phase3_test.log(traceback.format_exc())\n","\n","    # --- 1. Load LLM ---\n","    llm_thematic_pipe_test = None\n","    tokenizer_for_llm_test = None # To be used by chunkers\n","    try:\n","        logger_phase3_test.log(f\"INFO TEST: Loading LLM: {TEST_LLM_MODEL_ID} (Quant: {TEST_USE_QUANTIZATION})\")\n","        tokenizer_for_llm_test = AutoTokenizer.from_pretrained(TEST_LLM_MODEL_ID) # Assign to variable\n","        if getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is not None:\n","            tokenizer_for_llm_test.pad_token = tokenizer_for_llm_test.eos_token\n","        elif getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is None:\n","             logger_phase3_test.log(f\"CRITICAL TEST: Tokenizer for {TEST_LLM_MODEL_ID} has neither pad_token nor eos_token.\")\n","\n","        bnb_config_test = None\n","        effective_quant_test = TEST_USE_QUANTIZATION\n","        if TEST_USE_QUANTIZATION and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config_test = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype)\n","                logger_phase3_test.log(f\"INFO TEST: BNB config with compute_dtype: {compute_dtype}\")\n","            except Exception as e_bnb:\n","                logger_phase3_test.log(f\"WARN TEST: BNB config failed: {e_bnb}. Disabling quantization.\")\n","                effective_quant_test = False\n","        elif TEST_USE_QUANTIZATION and not torch.cuda.is_available(): # Check if CUDA is available for quantization\n","            logger_phase3_test.log(\"WARN TEST: Quantization requested but CUDA not available. Disabling quantization.\")\n","            effective_quant_test = False # Ensure quantization is disabled\n","\n","        model_for_llm_test = AutoModelForCausalLM.from_pretrained( # Assign to variable\n","            TEST_LLM_MODEL_ID,\n","            quantization_config=bnb_config_test if effective_quant_test else None,\n","            torch_dtype=torch.bfloat16 if not effective_quant_test and torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (torch.float32 if not effective_quant_test else None), # Conditional dtype\n","            device_map=\"auto\", trust_remote_code=True\n","        )\n","        llm_thematic_pipe_test = pipeline(\"text-generation\", model=model_for_llm_test, tokenizer=tokenizer_for_llm_test)\n","        logger_phase3_test.log(\"INFO TEST: LLM Pipeline Loaded.\")\n","    except Exception as e:\n","        logger_phase3_test.log(f\"CRITICAL TEST: Failed to load LLM for test: {e}\")\n","        logger_phase3_test.log(traceback.format_exc())\n","        return\n","\n","    # --- 2. Load Stage 4a Data for Q4 ---\n","    q4_all_items = []\n","    try:\n","        with open(TEST_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:\n","            stage4a_data = json.load(f)\n","        q4_all_items = stage4a_data.get(\"aggregated_pdf_content_by_qid\", {}).get(\"Q4\", [])\n","        if not q4_all_items:\n","            logger_phase3_test.log(\"ERROR TEST: No data found for Q4 in Stage 4a output. Aborting test.\")\n","            return\n","        logger_phase3_test.log(f\"INFO TEST: Loaded {len(q4_all_items)} text items for Q4 from Stage 4a output.\")\n","    except Exception as e:\n","        logger_phase3_test.log(f\"CRITICAL TEST: Failed to load Stage 4a data: {e}\")\n","        logger_phase3_test.log(traceback.format_exc())\n","        return\n","\n","    # --- Scenario A: Map-Reduce on Stage 4 SUMMARIES for Q4 ---\n","    logger_phase3_test.log(\"\\n\" + \"=\"*30 + \" SCENARIO A: Map-Reduce on Q4 PDF SUMMARIES \" + \"=\"*30)\n","    q4_summaries_from_stage4a = [item['text'] for item in q4_all_items if item.get('type') == 'pdf_summary' and item.get('text')]\n","\n","    scenario_a_final_themes = []\n","    scenario_a_map_outputs_raw = []\n","    scenario_a_reduce_output_raw = \"N/A\"\n","\n","    if not q4_summaries_from_stage4a:\n","        logger_phase3_test.log(\"WARN TEST SCENARIO A: No Stage 4 PDF summaries found for Q4. Scenario A cannot proceed as intended based on summaries.\")\n","    else:\n","        logger_phase3_test.log(f\"INFO TEST SCENARIO A: Found {len(q4_summaries_from_stage4a)} Stage 4 PDF summaries for Q4.\")\n","        concatenated_q4_summaries = \"\\n\\n--- Next Summary ---\\n\".join(q4_summaries_from_stage4a)\n","        logger_phase3_test.log(f\"INFO TEST SCENARIO A: Concatenated summaries char length: {len(concatenated_q4_summaries)}\")\n","\n","        summary_chunks = create_text_chunks_for_test(\n","            concatenated_q4_summaries,\n","            llm_thematic_pipe_test.tokenizer,\n","            TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK,\n","            TEST_MAPREDUCE_CHUNK_OVERLAP_WORDS\n","        )\n","\n","        if len(summary_chunks) > 1:\n","            logger_phase3_test.log(f\"INFO TEST SCENARIO A: Concatenated summaries split into {len(summary_chunks)} chunks for Map step.\")\n","            intermediate_themes_A = []\n","            for i, chunk in enumerate(summary_chunks):\n","                logger_phase3_test.log(f\"  SCENARIO A - MAP CHUNK {i+1}/{len(summary_chunks)}\")\n","                map_out = get_llm_thematic_points_test(chunk, question_actual_text_q4_for_test, 3, llm_thematic_pipe_test, TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS, \"Q4_A_map_chunk\", \"SCENARIO_A_MAP\")\n","                scenario_a_map_outputs_raw.append(map_out)\n","                if not map_out.startswith(\"LLM_ERROR:\"): intermediate_themes_A.append(map_out)\n","                time.sleep(0.2) # Shorter sleep for faster test\n","\n","            if intermediate_themes_A:\n","                combined_intermediate_A = \"\\n\\n--- Next Set of Preliminary Themes ---\\n\".join(intermediate_themes_A)\n","                logger_phase3_test.log(f\"  SCENARIO A - REDUCE Step (Input chars for reduce: {len(combined_intermediate_A)})\")\n","                scenario_a_reduce_output_raw = get_llm_thematic_points_test(combined_intermediate_A, question_actual_text_q4_for_test, TEST_NUM_THEMES_TO_REQUEST, llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS, \"Q4_A_reduce\", \"REDUCE_INTERMEDIATE_THEMES\")\n","                scenario_a_final_themes = parse_llm_themes_test(scenario_a_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)\n","            else:\n","                logger_phase3_test.log(\"ERROR TEST SCENARIO A: No intermediate themes from MAP step for summaries.\")\n","                scenario_a_reduce_output_raw = \"ERROR: No intermediate themes from MAP step (Summaries).\"\n","        elif summary_chunks:\n","             logger_phase3_test.log(f\"INFO TEST SCENARIO A: Concatenated summaries fit in one chunk. Direct LLM call on summaries.\")\n","             scenario_a_reduce_output_raw = get_llm_thematic_points_test(summary_chunks[0], question_actual_text_q4_for_test, TEST_NUM_THEMES_TO_REQUEST, llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS, \"Q4_A_direct_summaries\", \"DIRECT_PROCESSING\")\n","             scenario_a_final_themes = parse_llm_themes_test(scenario_a_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)\n","             scenario_a_map_outputs_raw.append(scenario_a_reduce_output_raw)\n","        else:\n","            logger_phase3_test.log(\"WARN TEST SCENARIO A: No summary chunks created (input likely empty after join/filter).\")\n","\n","    logger_phase3_test.log(\"\\n--- SCENARIO A (Map-Reduce on PDF SUMMARIES) - Q4 Final Themes ---\")\n","    if scenario_a_final_themes:\n","        for i, theme in enumerate(scenario_a_final_themes): logger_phase3_test.log(f\"  A Theme {i+1}: {theme.get('theme_label')}\\n     Explanation: {theme.get('explanation')}\")\n","    else: logger_phase3_test.log(\"  No final themes parsed for Scenario A.\")\n","    logger_phase3_test.log(\"  Scenario A - Raw Map Outputs (first one if many):\")\n","    logger_phase3_test.log(f\"  {scenario_a_map_outputs_raw[0][:500] if scenario_a_map_outputs_raw else 'N/A'}...\")\n","    logger_phase3_test.log(\"  Scenario A - Raw Reduce Output:\")\n","    logger_phase3_test.log(f\"  {scenario_a_reduce_output_raw[:500]}...\")\n","\n","\n","    # --- Scenario B: Map-Reduce on Stage 4 PASSAGES for Q4 ---\n","    logger_phase3_test.log(\"\\n\" + \"=\"*30 + \" SCENARIO B: Map-Reduce on Q4 PDF PASSAGES \" + \"=\"*30)\n","    q4_passages_from_stage4a = [item['text'] for item in q4_all_items if item.get('type') == 'pdf_passages' and item.get('text')]\n","\n","    scenario_b_final_themes = []\n","    scenario_b_map_outputs_raw = []\n","    scenario_b_reduce_output_raw = \"N/A\"\n","\n","    if not q4_passages_from_stage4a:\n","        logger_phase3_test.log(\"WARN TEST SCENARIO B: No Stage 4 PDF passages found for Q4. Scenario B cannot proceed.\")\n","    else:\n","        logger_phase3_test.log(f\"INFO TEST SCENARIO B: Found {len(q4_passages_from_stage4a)} Stage 4 PDF passage blocks for Q4.\")\n","\n","        passage_batches = batch_text_items_for_test(\n","            q4_passages_from_stage4a,\n","            llm_thematic_pipe_test.tokenizer,\n","            TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK\n","        )\n","        logger_phase3_test.log(f\"INFO TEST SCENARIO B: Grouped passages into {len(passage_batches)} batches for Map step.\")\n","\n","        if len(passage_batches) > 0:\n","            intermediate_themes_B = []\n","            for i, batch_of_passage_items_text in enumerate(passage_batches): # batch_of_passage_items_text is List[str]\n","                current_batch_concatenated_text = \"\\n\\n--- Next Excerpt ---\\n\".join(batch_of_passage_items_text)\n","                logger_phase3_test.log(f\"  SCENARIO B - MAP BATCH {i+1}/{len(passage_batches)} (Passage items in batch: {len(batch_of_passage_items_text)}, Chars: {len(current_batch_concatenated_text)})\")\n","                map_out = get_llm_thematic_points_test(current_batch_concatenated_text, question_actual_text_q4_for_test, 3, llm_thematic_pipe_test, TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS, \"Q4_B_map_batch\", \"SCENARIO_B_MAP\")\n","                scenario_b_map_outputs_raw.append(map_out)\n","                if not map_out.startswith(\"LLM_ERROR:\"): intermediate_themes_B.append(map_out)\n","                time.sleep(0.2)\n","\n","            if intermediate_themes_B:\n","                combined_intermediate_B = \"\\n\\n--- Next Set of Preliminary Themes ---\\n\".join(intermediate_themes_B)\n","                logger_phase3_test.log(f\"  SCENARIO B - REDUCE Step (Input chars for reduce: {len(combined_intermediate_B)})\")\n","                scenario_b_reduce_output_raw = get_llm_thematic_points_test(combined_intermediate_B, question_actual_text_q4_for_test, TEST_NUM_THEMES_TO_REQUEST, llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS, \"Q4_B_reduce\", \"REDUCE_INTERMEDIATE_THEMES\")\n","                scenario_b_final_themes = parse_llm_themes_test(scenario_b_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)\n","            else:\n","                logger_phase3_test.log(\"ERROR TEST SCENARIO B: No intermediate themes from MAP step for passages.\")\n","                scenario_b_reduce_output_raw = \"ERROR: No intermediate themes from MAP step (Passages).\"\n","        else: # Should not happen if q4_passages_from_stage4a was not empty\n","            logger_phase3_test.log(\"WARN TEST SCENARIO B: No passage batches created (input passage list might have been empty or all items filtered).\")\n","\n","\n","    logger_phase3_test.log(\"\\n--- SCENARIO B (Map-Reduce on PDF PASSAGES) - Q4 Final Themes ---\")\n","    if scenario_b_final_themes:\n","        for i, theme in enumerate(scenario_b_final_themes): logger_phase3_test.log(f\"  B Theme {i+1}: {theme.get('theme_label')}\\n     Explanation: {theme.get('explanation')}\")\n","    else: logger_phase3_test.log(\"  No final themes parsed for Scenario B.\")\n","    logger_phase3_test.log(\"  Scenario B - Raw Map Outputs (first one if many):\")\n","    logger_phase3_test.log(f\"  {scenario_b_map_outputs_raw[0][:500] if scenario_b_map_outputs_raw else 'N/A'}...\")\n","    logger_phase3_test.log(\"  Scenario B - Raw Reduce Output:\")\n","    logger_phase3_test.log(f\"  {scenario_b_reduce_output_raw[:500]}...\")\n","\n","    logger_phase3_test.log(\"\\nINFO: Test Cell - Q4 Thematic Analysis Scenarios Test Finished.\")\n","\n","    # Cleanup test LLM\n","    logger_phase3_test.log(\"INFO TEST: Cleaning up test LLM resources...\")\n","    if 'llm_thematic_pipe_test' in locals() and llm_thematic_pipe_test is not None: del llm_thematic_pipe_test\n","    if 'model_for_llm_test' in locals() and model_for_llm_test is not None: del model_for_llm_test # Changed variable name\n","    if 'tokenizer_for_llm_test' in locals() and tokenizer_for_llm_test is not None: del tokenizer_for_llm_test # Changed variable name\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","    logger_phase3_test.log(\"INFO TEST: Test LLM cleanup complete.\")\n","\n","# --- Execute the Test ---\n","# Ensure prerequisite cells (0 for imports, 1 for FileLogger) are run.\n","# Update paths and model IDs in the configuration section at the top of THIS cell.\n","# Then, uncomment the line below to run:\n","run_q4_thematic_test_scenarios()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3YJRlXe7hVy"},"outputs":[],"source":["# @title Cell 7: Configuration for Phase 3 (LLM Thematic Analysis)\n","\n","# Input for Phase 3 is the output from Phase 2\n","PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON = PHASE2_OUTPUT_COLLATED_JSON\n","\n","# Output file for Phase 3\n","PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON = os.path.join(BASE_PROJECT_DIR, f'phase3_pdf_thematic_points_{time.strftime(\"%Y%m%d_%H%M%S\")}.json')\n","PHASE3_LOG_FILE = os.path.join(BASE_PROJECT_DIR, f\"phase3_llm_thematic_analysis_{time.strftime('%Y%m%d_%H%M%S')}.log\")\n","\n","# LLM and Tokenizer for Thematic Analysis\n","PHASE3_LLM_MODEL_ID = 'google/gemma-3-4b-it' # Your chosen model\n","PHASE3_USE_QUANTIZATION = False # Set to False as per your Phase 1 config, or True if desired for Phase 3\n","\n","# Thematic Analysis Parameters\n","PHASE3_NUM_THEMES_TO_REQUEST = 3 # Final number of themes desired\n","\n","# --- Parameters for Map-Reduce Chunking ---\n","PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2500 # Max *content* tokens per sub-chunk for the MAP step.\n","                                                  # LLM (e.g. Gemma 8k context) tokenizer.encode(text) to check.\n","                                                  # Needs to leave room for prompt and LLM's map-step output.\n","PHASE3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 500    # Max tokens for LLM to generate themes *from one sub-chunk*.\n","PHASE3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 700 # Max tokens for LLM to generate *final* themes from intermediate themes.\n","PHASE3_MAPREDUCE_CHUNK_OVERLAP_WORDS = 50         # Optional: Overlap words between chunks to maintain context.\n","\n","# Optional: QID selection for thematic analysis (if not processing all)\n","PHASE3_QIDS_TO_PROCESS_THEMATICALLY = None # [\"Q4\"] or None for all\n","\n","# Source for actual question texts\n","PHASE3_QUESTION_TEXT_SOURCE_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # From original Stage 2 for question text\n","# OR: os.path.join(BASE_PROJECT_DIR, 'comprehensive_analysis_report.json') if CAR_35 is preferred source for Q texts\n","\n","print(\"INFO: Cell 7 - Configuration for Phase 3 (LLM Thematic Analysis) loaded with Map-Reduce params.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mEvxu-y272dJ"},"outputs":[],"source":["# @title Cell 8: Helper Functions & Main Logic for Phase 3 (LLM Thematic Analysis)\n","\n","logger_phase3 = FileLogger(PHASE3_LOG_FILE)\n","\n","# --- Helper: Text Chunker ---\n","def create_text_chunks(full_text: str, tokenizer, max_tokens_per_chunk: int, overlap_words: int = 50):\n","    \"\"\"\n","    Splits a long text into smaller chunks based on token count, with optional word overlap.\n","    \"\"\"\n","    # Simple word-based chunking as a proxy for token-based, then refine with tokenizer.\n","    # A more precise method would tokenize the whole text and then split token IDs.\n","    # This is a practical approximation for now.\n","\n","    words = full_text.split() # Split by space, not ideal for all languages but okay for English\n","    chunks = []\n","    current_chunk_words = []\n","    current_token_count = 0\n","\n","    # Estimate tokens per word (very rough, depends on tokenizer and text)\n","    # For a more accurate approach, tokenize word by word or sentence by sentence.\n","    # avg_tokens_per_word = 1.5 # Highly approximate, adjust based on observation\n","\n","    for word_idx, word in enumerate(words):\n","        # Estimate token length of current word + space\n","        # word_token_len = len(tokenizer.encode(word + \" \", add_special_tokens=False))\n","\n","        # Simpler: check combined current_chunk_words with the new word\n","        potential_chunk_text = \" \".join(current_chunk_words + [word])\n","        potential_chunk_tokens = tokenizer.encode(potential_chunk_text, add_special_tokens=False) # Don't add BOS/EOS here\n","\n","        if len(potential_chunk_tokens) <= max_tokens_per_chunk:\n","            current_chunk_words.append(word)\n","        else:\n","            # Current chunk is full (or adding the word makes it full)\n","            if current_chunk_words: # Add current chunk if it has content\n","                chunks.append(\" \".join(current_chunk_words))\n","\n","            # Start new chunk, potentially with overlap\n","            overlap_start_index = max(0, len(current_chunk_words) - overlap_words) if chunks else 0\n","            current_chunk_words = current_chunk_words[overlap_start_index:] + [word] # Start new with overlap + current word\n","\n","            # If even a single word is too long (after overlap logic), this needs more advanced splitting\n","            # For now, assume words are not excessively long token-wise.\n","            new_chunk_test_tokens = tokenizer.encode(\" \".join(current_chunk_words), add_special_tokens=False)\n","            if len(new_chunk_test_tokens) > max_tokens_per_chunk and len(current_chunk_words) > 1 : # if just one word made it too long, it will be handled by next if\n","                 # if the overlap made it too long, reset current_chunk_words to just the new word\n","                 current_chunk_words = [word]\n","\n","\n","    # Add the last remaining chunk\n","    if current_chunk_words:\n","        chunks.append(\" \".join(current_chunk_words))\n","\n","    # Final check on chunk token lengths (can be slightly off due to word splitting)\n","    # This step is optional but good for ensuring compliance\n","    # It might require re-chunking or truncating, which adds complexity.\n","    # For now, we assume the above gives reasonably sized chunks.\n","    # A more robust chunker would tokenize the full text and split based on token IDs.\n","\n","    logger_phase3.log(f\"INFO: Split text into {len(chunks)} chunks for Map-Reduce.\")\n","    if chunks:\n","        for i, chunk_text in enumerate(chunks[:3]): # Log first few\n","            logger_phase3.log(f\"  DEBUG: Chunk {i} approx char length: {len(chunk_text)}, approx token length: {len(tokenizer.encode(chunk_text, add_special_tokens=False))}\")\n","    return chunks\n","\n","\n","# --- Helper: LLM Call for Thematic Points (get_llm_thematic_points - same as before) ---\n","def get_llm_thematic_points(text_chunk_for_llm: str,\n","                            question_text_for_prompt: str,\n","                            num_themes: int, # Can be num_themes for sub-chunk or final num_themes\n","                            llm_pipe: pipeline,\n","                            max_new_tokens: int,\n","                            qid_for_log: str,\n","                            is_reduce_step: bool = False): # New flag\n","    \"\"\"\n","    Sends a text chunk to the LLM to get thematic points.\n","    Returns the raw text output from the LLM.\n","    \"\"\"\n","    prompt_instruction = \"\"\n","    if is_reduce_step:\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. The following are sets of preliminary themes/points extracted from different parts of a larger collection of responses to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Your task is to synthesize these preliminary themes into the top {num_themes} overarching and distinct common points or themes that represent the entire collection.\\n\"\n","            f\"For each final synthesized point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). Consolidate similar preliminary themes into a single, more general point.\\n\\n\"\n","            f\"Preliminary Themes/Points Provided:\\n\\\"\\\"\\\"\\n{text_chunk_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Top {num_themes} Synthesized Common Points:\"\n","        )\n","    else: # Map step prompt\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Carefully review the following excerpt from PDF submissions responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on this provided excerpt, identify up to {num_themes} most prominent and distinct points, arguments, or themes raised.\\n\" # \"up to\" for map step\n","            f\"For each point, provide a concise label for the point (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes} distinct common points are evident, list only those that are clear.\\n\\n\"\n","            f\"Excerpt:\\n\\\"\\\"\\\"\\n{text_chunk_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Prominent Points from this Excerpt:\"\n","        )\n","\n","    messages = [{\"role\": \"user\", \"content\": prompt_instruction}]\n","\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(\n","            messages, tokenize=False, add_generation_prompt=True\n","        )\n","        # Token check for prompt length\n","        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=False)\n","        if len(prompt_tokens) + max_new_tokens > (llm_pipe.model.config.max_position_embeddings or 8192) : # Check against model's actual max context\n","            logger_phase3.log(f\"WARN: Phase 3 - QID {qid_for_log} - Prompt tokens ({len(prompt_tokens)}) + max_new_tokens ({max_new_tokens}) might exceed model's context window. Truncating input text chunk if this is not reduce step.\")\n","            # For map step, the input `text_chunk_for_llm` should have already been managed by `create_text_chunks`.\n","            # For reduce step, if `text_chunk_for_llm` (combined intermediate themes) is too long, this is an issue.\n","\n","        generation_args = {\n","            \"max_new_tokens\": max_new_tokens,\n","            \"do_sample\": False,\n","            \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id,\n","        }\n","\n","        step_type = \"Reduce\" if is_reduce_step else \"Map\"\n","        if PHASE1_ENABLE_DEBUG_LOGGING:\n","             logger_phase3.log(f\"DEBUG: Phase 3 - {step_type} Step LLM call for QID {qid_for_log}. Input text char length: {len(text_chunk_for_llm)}\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","\n","        # ... (rest of LLM output parsing, same as before) ...\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger_phase3.log(f\"ERROR: Phase 3 - {step_type} Step LLM pipeline returned unexpected/empty output for QID {qid_for_log}.\")\n","            return \"LLM_ERROR: No output generated.\"\n","        generated_text_full = outputs[0]['generated_text']\n","        llm_response_text = \"\"\n","        if generated_text_full.startswith(prompt_formatted):\n","            llm_response_text = generated_text_full[len(prompt_formatted):].strip()\n","        else:\n","            response_marker = \"<start_of_turn>model\\n\"\n","            summary_start_index = generated_text_full.rfind(response_marker)\n","            if summary_start_index != -1:\n","                llm_response_text = generated_text_full[summary_start_index + len(response_marker):].strip()\n","            else:\n","                llm_response_text = generated_text_full\n","                logger_phase3.log(f\"WARN: Phase 3 - {step_type} Step - Could not reliably strip prompt for QID {qid_for_log}.\")\n","        return llm_response_text\n","\n","    except Exception as e:\n","        step_type = \"Reduce\" if is_reduce_step else \"Map\"\n","        logger_phase3.log(f\"ERROR: Phase 3 - {step_type} Step - Exception during LLM call for QID {qid_for_log}: {e}\")\n","        logger_phase3.log(traceback.format_exc())\n","        return f\"LLM_ERROR: Exception during generation - {str(e)}\"\n","\n","\n","# --- Helper: Parse LLM Thematic Output (parse_llm_themes - same as before) ---\n","def parse_llm_themes(llm_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:\n","    # ... (parser logic from previous script, ensure it's robust) ...\n","    # ... (same as in your Cell 8)\n","    parsed_themes = []\n","    if not llm_output_text or llm_output_text.startswith(\"LLM_ERROR:\"):\n","        return [{\"theme_label\": \"Parsing Error\", \"explanation\": llm_output_text}]\n","    current_theme_label = None\n","    current_explanation = \"\"\n","    lines = llm_output_text.splitlines()\n","    theme_item = {}\n","\n","    for line in lines:\n","        line_stripped = line.strip()\n","        if not line_stripped: continue\n","        match_new_theme = re.match(r\"^\\s*(\\d+[\\.\\)\\-]?)\\s*(?:Theme Label\\s*[:\\-]?\\s*)?(.*)\", line_stripped, re.IGNORECASE)\n","        if match_new_theme:\n","            if theme_item.get(\"theme_label\"):\n","                if not theme_item.get(\"explanation\"): theme_item[\"explanation\"] = current_explanation.strip()\n","                if theme_item.get(\"theme_label\") or theme_item.get(\"explanation\"):\n","                    parsed_themes.append(theme_item)\n","            theme_item = {}\n","            current_explanation = \"\"\n","            label_candidate = match_new_theme.group(2).strip()\n","            if \"explanation:\" in label_candidate.lower():\n","                parts = re.split(r\"explanation\\s*[:\\-]\", label_candidate, maxsplit=1, flags=re.IGNORECASE)\n","                theme_item[\"theme_label\"] = parts[0].strip()\n","                current_explanation = parts[1].strip() if len(parts) > 1 else \"\"\n","            else:\n","                theme_item[\"theme_label\"] = label_candidate\n","        elif \"explanation:\" in line_stripped.lower():\n","            if theme_item and not theme_item.get(\"theme_label\") and current_explanation.strip():\n","                theme_item[\"theme_label\"] = current_explanation.strip()\n","                current_explanation = \"\"\n","            current_explanation += re.split(r\"explanation\\s*[:\\-]\", line_stripped, maxsplit=1, flags=re.IGNORECASE)[-1].strip() + \" \"\n","        elif theme_item:\n","            current_explanation += line_stripped + \" \"\n","    if theme_item.get(\"theme_label\"):\n","        if not theme_item.get(\"explanation\"): theme_item[\"explanation\"] = current_explanation.strip()\n","        if theme_item.get(\"theme_label\") or theme_item.get(\"explanation\"):\n","            parsed_themes.append(theme_item)\n","    elif current_explanation.strip() and not parsed_themes:\n","        parsed_themes.append({\"theme_label\": \"General Theme\", \"explanation\": current_explanation.strip()})\n","    if not parsed_themes and llm_output_text:\n","        parsed_themes.append({\"theme_label\": \"Unparsed LLM Output\", \"explanation\": llm_output_text})\n","    return parsed_themes[:num_expected_themes]\n","\n","\n","# --- Main Function for Phase 3 (Modified for Map-Reduce) ---\n","def run_phase3_llm_thematic_analysis():\n","    global logger_phase3\n","    logger_phase3.log(\"=\" * 50); logger_phase3.log(\" INFO: Starting Phase 3: LLM Thematic Analysis per QID (with Map-Reduce)\"); logger_phase3.log(\"=\" * 50)\n","    # ... (initial logging of configs, LLM loading, Phase 2 data loading, QID text loading - same as before) ...\n","    # ... ENSURE llm_thematic_pipe_p3 is loaded and available ...\n","    # --- Load LLM for Thematic Analysis ---\n","    llm_tokenizer_p3 = None\n","    llm_model_p3 = None\n","    llm_thematic_pipe_p3 = None # This is our Hugging Face pipeline\n","    try:\n","        logger_phase3.log(f\"INFO: Loading LLM tokenizer for Phase 3: {PHASE3_LLM_MODEL_ID}...\")\n","        llm_tokenizer_p3 = AutoTokenizer.from_pretrained(PHASE3_LLM_MODEL_ID)\n","        if getattr(llm_tokenizer_p3, 'pad_token', None) is None and getattr(llm_tokenizer_p3, 'eos_token', None):\n","            llm_tokenizer_p3.pad_token = llm_tokenizer_p3.eos_token\n","            llm_tokenizer_p3.padding_side = \"left\"\n","        logger_phase3.log(\"INFO: Phase 3 LLM Tokenizer loaded.\")\n","\n","        bnb_config_p3 = None\n","        current_use_quantization_p3 = PHASE3_USE_QUANTIZATION\n","        if current_use_quantization_p3 and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config_p3 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n","                logger_phase3.log(f\"INFO: Phase 3 - 4-bit quantization configured (Compute dtype: {compute_dtype}).\")\n","            except Exception as q_err:\n","                logger_phase3.log(f\"ERROR: Phase 3 - Failed BitsAndBytesConfig: {q_err}. Disabling quantization.\")\n","                current_use_quantization_p3 = False\n","\n","        logger_phase3.log(f\"INFO: Loading Phase 3 LLM model: {PHASE3_LLM_MODEL_ID} (Quantization: {current_use_quantization_p3})...\")\n","        llm_model_p3 = AutoModelForCausalLM.from_pretrained(\n","            PHASE3_LLM_MODEL_ID,\n","            quantization_config=bnb_config_p3 if current_use_quantization_p3 else None,\n","            torch_dtype=torch.bfloat16 if not current_use_quantization_p3 else None, # Ensure correct dtype\n","            device_map=\"auto\", trust_remote_code=True\n","        )\n","        logger_phase3.log(\"INFO: Phase 3 LLM model loaded.\")\n","\n","        llm_thematic_pipe_p3 = pipeline(\"text-generation\", model=llm_model_p3, tokenizer=llm_tokenizer_p3)\n","        logger_phase3.log(\"INFO: Phase 3 LLM text-generation pipeline created successfully.\")\n","    except Exception as load_err:\n","        logger_phase3.log(f\"CRITICAL: Failed to load LLM/pipeline for Phase 3: {load_err}\")\n","        logger_phase3.log(traceback.format_exc())\n","        return False\n","\n","    # --- Load Collated Texts from Phase 2 ---\n","    try:\n","        with open(PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:\n","            phase2_data = json.load(f)\n","        collated_texts_by_qid_p3 = phase2_data.get(\"aggregated_pdf_content_by_qid\", {})\n","        if not collated_texts_by_qid_p3: # Check if the key itself is missing or the dict is empty\n","            logger_phase3.log(f\"ERROR: 'aggregated_pdf_content_by_qid' key not found or no data in {PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON}\")\n","            return False\n","        logger_phase3.log(f\"INFO: Loaded collated PDF texts for {len(collated_texts_by_qid_p3)} QIDs from Phase 2 output.\")\n","    except Exception as e:\n","        logger_phase3.log(f\"FATAL: Could not load Phase 2 collated texts file: {PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON}. Error: {e}\")\n","        return False\n","\n","    # --- Load Question Texts ---\n","    qid_to_actual_question_text_p3 = {}\n","    # ... (same question text loading logic as before) ...\n","    try:\n","        with open(PHASE3_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:\n","            qtext_source_data = json.load(f_qtext)\n","        if \"analysis_by_question\" in qtext_source_data: # CAR_35 structure\n","            for qid_key, data_val in qtext_source_data.get(\"analysis_by_question\", {}).items():\n","                qid_to_actual_question_text_p3[qid_key] = data_val.get(\"question_text\", f\"Unknown text for {qid_key}\")\n","        elif \"question_data\" in qtext_source_data: # Stage 2 form output structure\n","             for qid_key, data_val in qtext_source_data.get(\"question_data\", {}).items():\n","                qid_to_actual_question_text_p3[qid_key] = data_val.get(\"question_text\", f\"Unknown text for {qid_key}\")\n","        logger_phase3.log(f\"INFO: Loaded actual question texts for {len(qid_to_actual_question_text_p3)} QIDs from {PHASE3_QUESTION_TEXT_SOURCE_FILE}.\")\n","    except Exception as e:\n","        logger_phase3.log(f\"WARN: Could not load question texts from {PHASE3_QUESTION_TEXT_SOURCE_FILE}. Prompts will use QID only. Error: {e}\")\n","\n","\n","    overall_thematic_results_p3 = {}\n","    # ... (load existing results logic - same as before) ...\n","    if os.path.exists(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON):\n","        try:\n","            with open(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON, 'r', encoding='utf-8') as f_exist:\n","                overall_thematic_results_p3 = json.load(f_exist)\n","            logger_phase3.log(f\"INFO: Loaded {len(overall_thematic_results_p3)} existing thematic results from {PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}\")\n","        except Exception as e_load_exist:\n","            logger_phase3.log(f\"WARN: Could not load existing results file {PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}. Starting fresh. Error: {e_load_exist}\")\n","\n","\n","    qids_for_processing_p3 = PHASE3_QIDS_TO_PROCESS_THEMATICALLY if PHASE3_QIDS_TO_PROCESS_THEMATICALLY else sorted(collated_texts_by_qid_p3.keys())\n","\n","    for qid_idx, qid in enumerate(qids_for_processing_p3):\n","        logger_phase3.log(f\"\\n--- Processing Thematic Analysis for QID {qid_idx + 1}/{len(qids_for_processing_p3)}: {qid} ---\")\n","        if qid in overall_thematic_results_p3 and overall_thematic_results_p3[qid].get(\"status\", \"\").startswith(\"success\"):\n","            logger_phase3.log(f\"INFO: QID {qid} already successfully processed. Skipping.\")\n","            continue\n","\n","        question_actual_text_p3 = qid_to_actual_question_text_p3.get(qid, f\"Question {qid}\") # Get actual question text\n","        text_items_for_this_qid_p3 = collated_texts_by_qid_p3.get(qid, [])\n","        # ... (check for empty text_items_for_this_qid_p3 - same as before) ...\n","        if not text_items_for_this_qid_p3:\n","            logger_phase3.log(f\"WARN: No collated texts found for QID {qid}. Skipping thematic analysis.\")\n","            overall_thematic_results_p3[qid] = {\"question_text\": question_actual_text_p3, \"status\": \"no_collated_text_for_qid\", \"themes\": [], \"llm_raw_output_map\": [], \"llm_raw_output_reduce\": \"N/A\"}\n","            continue\n","\n","        texts_to_combine_p3 = [item['text'] for item in text_items_for_this_qid_p3 if item.get('text')]\n","        if not texts_to_combine_p3:\n","             logger_phase3.log(f\"WARN: All text items for QID {qid} were empty after extraction. Skipping.\")\n","             overall_thematic_results_p3[qid] = {\"question_text\": question_actual_text_p3, \"status\": \"all_texts_empty_for_qid\", \"themes\": [], \"llm_raw_output_map\": [], \"llm_raw_output_reduce\": \"N/A\"}\n","             continue\n","\n","        current_qid_full_text = \"\\n\\n--- New Excerpt ---\\n\".join(texts_to_combine_p3)\n","        logger_phase3.log(f\"INFO: QID {qid} - Total combined character length for LLM input: {len(current_qid_full_text)}\")\n","\n","        # --- MAP-REDUCE LOGIC ---\n","        all_intermediate_themes_text = []\n","        raw_map_outputs = []\n","\n","        # Tokenize once for length check for the whole document, then chunk based on words as an approximation.\n","        # A more accurate way is to tokenize the whole document and split based on token IDs.\n","        # This is a simplification.\n","        full_text_tokens = llm_thematic_pipe_p3.tokenizer.encode(current_qid_full_text, add_special_tokens=False)\n","\n","        if len(full_text_tokens) > PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK:\n","            logger_phase3.log(f\"INFO: QID {qid} - Full text ({len(full_text_tokens)} tokens) exceeds chunk limit ({PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK}). Applying Map-Reduce.\")\n","\n","            text_sub_chunks = create_text_chunks(\n","                current_qid_full_text,\n","                llm_thematic_pipe_p3.tokenizer, # Pass the tokenizer\n","                PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK,\n","                PHASE3_MAPREDUCE_CHUNK_OVERLAP_WORDS\n","            )\n","            logger_phase3.log(f\"INFO: QID {qid} - Split into {len(text_sub_chunks)} sub-chunks for Map step.\")\n","\n","            for i, sub_chunk in enumerate(text_sub_chunks):\n","                logger_phase3.log(f\"  MAP Step for QID {qid}, Chunk {i+1}/{len(text_sub_chunks)}...\")\n","                map_llm_output = get_llm_thematic_points(\n","                    sub_chunk,\n","                    question_actual_text_p3,\n","                    PHASE3_NUM_THEMES_TO_REQUEST, # Ask for same number of themes from each chunk\n","                    llm_thematic_pipe_p3,\n","                    PHASE3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                    f\"{qid}_map_chunk_{i+1}\",\n","                    is_reduce_step=False\n","                )\n","                raw_map_outputs.append(map_llm_output)\n","                if not map_llm_output.startswith(\"LLM_ERROR:\"):\n","                    all_intermediate_themes_text.append(map_llm_output)\n","                time.sleep(1) # Small delay between LLM calls if needed\n","\n","            if not all_intermediate_themes_text:\n","                logger_phase3.log(f\"ERROR: QID {qid} - Map step produced no valid intermediate themes.\")\n","                overall_thematic_results_p3[qid] = {\n","                    \"question_text\": question_actual_text_p3, \"status\": \"error_map_step_no_themes\",\n","                    \"themes\": [], \"llm_raw_output_map\": raw_map_outputs, \"llm_raw_output_reduce\": \"N/A - Map failed\"\n","                }\n","                # ... (save and continue to next qid) ...\n","                continue # to next QID\n","\n","            # Combine intermediate themes for the Reduce step\n","            combined_intermediate_themes = \"\\n\\n--- Next Set of Preliminary Themes ---\\n\\n\".join(all_intermediate_themes_text)\n","            logger_phase3.log(f\"INFO: QID {qid} - REDUCE Step: Processing {len(all_intermediate_themes_text)} sets of intermediate themes.\")\n","\n","            # Check if combined_intermediate_themes itself is too long for Reduce step context\n","            reduce_input_tokens = llm_thematic_pipe_p3.tokenizer.encode(combined_intermediate_themes, add_special_tokens=False)\n","            if len(reduce_input_tokens) > PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK * 1.5 : # Heuristic, allow reduce input to be a bit larger\n","                logger_phase3.log(f\"WARN: QID {qid} - Combined intermediate themes ({len(reduce_input_tokens)} tokens) might be too long for Reduce step. Truncating.\")\n","                # Simple truncation (could be smarter, e.g. summarize intermediate themes first)\n","                # This is a complex problem: if intermediate themes are too many, need recursive reduce or different strategy.\n","                # For now, simple truncation:\n","                # How to truncate text based on token count without splitting mid-token is tricky.\n","                # One way: join, tokenize, then decode a slice of tokens.\n","                # For simplicity here, we'll just send it and rely on LLM or see if it errors.\n","                # A better way: if combined_intermediate_themes is too long, run create_text_chunks on IT,\n","                # then run reduce on EACH chunk of intermediate_themes, then another reduce layer.\n","                # This makes it truly recursive map-reduce.\n","                # For now, we'll just note the risk.\n","                pass\n","\n","\n","            final_llm_themes_output = get_llm_thematic_points(\n","                combined_intermediate_themes,\n","                question_actual_text_p3,\n","                PHASE3_NUM_THEMES_TO_REQUEST, # Final number of themes\n","                llm_thematic_pipe_p3,\n","                PHASE3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                f\"{qid}_reduce_step\",\n","                is_reduce_step=True\n","            )\n","            llm_raw_output_for_storage = final_llm_themes_output # This is the output of the reduce step\n","        else:\n","            # Full text is small enough, process directly (no map-reduce needed)\n","            logger_phase3.log(f\"INFO: QID {qid} - Full text ({len(full_text_tokens)} tokens) is within limit. Processing directly.\")\n","            final_llm_themes_output = get_llm_thematic_points(\n","                current_qid_full_text,\n","                question_actual_text_p3,\n","                PHASE3_NUM_THEMES_TO_REQUEST,\n","                llm_thematic_pipe_p3,\n","                PHASE3_LLM_MAX_NEW_TOKENS_FOR_THEMES, # Use the general theme token limit\n","                qid,\n","                is_reduce_step=False # Effectively a single map step\n","            )\n","            llm_raw_output_for_storage = final_llm_themes_output\n","            raw_map_outputs.append(final_llm_themes_output) # Store the single output as if it was a map output\n","\n","        # Parse the final themes output (either from direct call or reduce step)\n","        parsed_themes_list = parse_llm_themes(final_llm_themes_output, PHASE3_NUM_THEMES_TO_REQUEST)\n","        # ... (status setting and saving overall_thematic_results_p3 - same as before) ...\n","        status_p3 = \"success_thematic_analysis\"\n","        if final_llm_themes_output.startswith(\"LLM_ERROR:\"):\n","            status_p3 = \"error_llm_theme_generation\"\n","            if not all_intermediate_themes_text: status_p3 = \"error_direct_llm_theme_generation\" # if not map-reduce\n","            elif len(text_sub_chunks) > 0 : status_p3 = \"error_reduce_step_llm_theme_generation\"\n","\n","        elif not parsed_themes_list or (len(parsed_themes_list) == 1 and parsed_themes_list[0][\"theme_label\"] == \"Unparsed LLM Output\"):\n","            status_p3 = \"llm_output_parsing_failed_or_empty\"\n","            if len(text_sub_chunks) > 0 : status_p3 = \"reduce_step_llm_output_parsing_failed\"\n","            else: status_p3 = \"direct_llm_output_parsing_failed\"\n","\n","\n","        overall_thematic_results_p3[qid] = {\n","            \"question_text\": question_actual_text_p3,\n","            \"status\": status_p3,\n","            \"total_pdf_text_items_for_qid\": len(texts_to_combine_p3),\n","            \"approx_char_length_fed_to_llm_initial\": len(current_qid_full_text),\n","            \"map_reduce_applied\": len(text_sub_chunks) > 0 if 'text_sub_chunks' in locals() else False,\n","            \"num_map_chunks\": len(text_sub_chunks) if 'text_sub_chunks' in locals() else 1,\n","            \"llm_raw_output_map_chunks\": raw_map_outputs, # List of raw outputs from map steps\n","            \"llm_raw_output_final\": llm_raw_output_for_storage, # Raw output from reduce or direct step\n","            \"parsed_top_themes\": parsed_themes_list\n","        }\n","        # ... (save results after each QID logic - same as before) ...\n","        try:\n","            with open(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON, 'w', encoding='utf-8') as f_out_p3:\n","                json.dump(overall_thematic_results_p3, f_out_p3, indent=2, default=logger_phase3._get_serializer())\n","            logger_phase3.log(f\"INFO: Saved thematic analysis results to '{PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}' after QID {qid}.\")\n","        except Exception as e_save_p3:\n","            logger_phase3.log(f\"ERROR: Could not save intermediate results for Phase 3: {e_save_p3}\")\n","\n","        gc.collect()\n","        if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","\n","    logger_phase3.log(\"=\" * 50); logger_phase3.log(\" INFO: Phase 3 (LLM Thematic Analysis with Map-Reduce) Finished \"); logger_phase3.log(\"=\" * 50)\n","    return True\n","\n","# print(\"INFO: Cell 8 - Logic for Phase 3 (LLM Thematic Analysis) defined with Map-Reduce.\")\n","# To run:\n","# Make sure configurations in Cell 7 are set, especially PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK\n","# run_phase3_llm_thematic_analysis()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1UtLhKT-swx"},"outputs":[],"source":["# @title Cell 8: Helper Functions & Main Logic for Phase 3 (LLM Thematic Analysis) - REFINED MAP-REDUCE\n","\n","logger_phase3 = FileLogger(PHASE3_LOG_FILE) # Assuming this is initialized\n","\n","# --- Helper: LLM Call for Thematic Points (get_llm_thematic_points - modified prompt logic) ---\n","def get_llm_thematic_points(text_input_for_llm: str, # Can be a single excerpt or combined intermediate themes\n","                            question_text_for_prompt: str,\n","                            num_themes_requested: int,\n","                            llm_pipe: pipeline,\n","                            max_new_tokens: int,\n","                            qid_for_log: str,\n","                            map_reduce_step_type: str): # \"MAP_INDIVIDUAL_ITEMS\", \"REDUCE_INTERMEDIATE_THEMES\"\n","    \"\"\"\n","    Sends text to the LLM to get thematic points.\n","    Returns the raw text output from the LLM.\n","    \"\"\"\n","    prompt_instruction = \"\"\n","    if map_reduce_step_type == \"REDUCE_INTERMEDIATE_THEMES\":\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. The following are sets of preliminary themes/points that were individually extracted from different batches of responses to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Your task is to synthesize these preliminary themes into the overall top {num_themes_requested} overarching and distinct common points or themes that represent the entire collection of original responses.\\n\"\n","            f\"For each final synthesized point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). Consolidate similar preliminary themes. Ensure the final themes are distinct and well-supported by the range of preliminary themes provided.\\n\\n\"\n","            f\"Preliminary Themes/Points Provided:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Top {num_themes_requested} Synthesized Common Points:\"\n","        )\n","    elif map_reduce_step_type == \"MAP_INDIVIDUAL_ITEMS\":\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Carefully review the following batch of {num_themes_requested} excerpts/summaries from PDF submissions, all responding to the question: '{question_text_for_prompt}'.\\n\" # Assuming num_themes_requested for map might be # of items in batch\n","            f\"Based *only* on this provided batch of excerpts/summaries, identify up to 3 prominent and distinct points, arguments, or themes they collectively raise.\\n\"\n","            f\"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 distinct common points are evident, list only those that are clear.\\n\\n\"\n","            f\"Batch of Excerpts/Summaries:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Prominent Points from this Batch:\"\n","        )\n","    else: # Direct processing if no map-reduce\n","        prompt_instruction = (\n","            f\"You are an expert policy analyst. Carefully review all the following excerpts/summaries from PDF submissions responding to the question: '{question_text_for_prompt}'.\\n\"\n","            f\"Based *only* on the provided excerpts/summaries, identify the top {num_themes_requested} most common and distinct points, arguments, or themes raised by the respondents.\\n\"\n","            f\"For each point, provide a concise label for the point (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\\n\"\n","            f\"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes_requested} distinct common points are evident, list only those that are clear.\\n\\n\"\n","            f\"Excerpts/Summaries:\\n\\\"\\\"\\\"\\n{text_input_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","            f\"Top {num_themes_requested} Common Points:\"\n","        )\n","\n","\n","    messages = [{\"role\": \"user\", \"content\": prompt_instruction}]\n","    # ... (rest of the get_llm_thematic_points function: try-except, LLM call, output parsing - remains similar)\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(\n","            messages, tokenize=False, add_generation_prompt=True\n","        )\n","        generation_args = {\n","            \"max_new_tokens\": max_new_tokens, \"do_sample\": False,\n","            \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id,\n","        }\n","        if PHASE1_ENABLE_DEBUG_LOGGING:\n","             logger_phase3.log(f\"DEBUG: Phase 3 - {map_reduce_step_type} LLM call for QID {qid_for_log}. Input char len: {len(text_input_for_llm)}\")\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","        # ... (output extraction logic) ...\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger_phase3.log(f\"ERROR: Phase 3 - {map_reduce_step_type} LLM pipeline returned unexpected/empty output for QID {qid_for_log}.\")\n","            return \"LLM_ERROR: No output generated.\"\n","        generated_text_full = outputs[0]['generated_text']\n","        llm_response_text = \"\"\n","        if generated_text_full.startswith(prompt_formatted):\n","            llm_response_text = generated_text_full[len(prompt_formatted):].strip()\n","        else: # Fallback extraction\n","            response_marker = \"<start_of_turn>model\\n\"\n","            summary_start_index = generated_text_full.rfind(response_marker)\n","            if summary_start_index != -1:\n","                llm_response_text = generated_text_full[summary_start_index + len(response_marker):].strip()\n","            else:\n","                llm_response_text = generated_text_full\n","                logger_phase3.log(f\"WARN: Phase 3 - {map_reduce_step_type} - Could not reliably strip prompt for QID {qid_for_log}.\")\n","        return llm_response_text\n","    except Exception as e:\n","        logger_phase3.log(f\"ERROR: Phase 3 - {map_reduce_step_type} - Exception during LLM call for QID {qid_for_log}: {e}\")\n","        logger_phase3.log(traceback.format_exc())\n","        return f\"LLM_ERROR: Exception during generation - {str(e)}\"\n","\n","\n","# --- Helper: Parse LLM Thematic Output (parse_llm_themes - same as before) ---\n","# ... (ensure this function is robust for your LLM's output format) ...\n","\n","# --- Main Function for Phase 3 (Modified for BETTER Map-Reduce) ---\n","def run_phase3_llm_thematic_analysis():\n","    global logger_phase3\n","    logger_phase3.log(\"=\" * 50); logger_phase3.log(\" INFO: Starting Phase 3: LLM Thematic Analysis per QID (Refined Map-Reduce)\"); logger_phase3.log(\"=\" * 50)\n","    # ... (LLM loading, Phase 2 data loading, QID text loading - same as before) ...\n","    # ... Ensure llm_thematic_pipe_p3 (the pipeline object) is loaded and available ...\n","    # ... Ensure collated_texts_by_qid_p3 and qid_to_actual_question_text_p3 are loaded ...\n","\n","    overall_thematic_results_p3 = {} # To store final results\n","    # ... (load existing results logic - same as before) ...\n","\n","    qids_for_processing_p3 = PHASE3_QIDS_TO_PROCESS_THEMATICALLY if PHASE3_QIDS_TO_PROCESS_THEMATICALLY else sorted(collated_texts_by_qid_p3.keys())\n","\n","    for qid_idx, qid in enumerate(qids_for_processing_p3):\n","        logger_phase3.log(f\"\\n--- Processing Thematic Analysis for QID {qid_idx + 1}/{len(qids_for_processing_p3)}: {qid} ---\")\n","        # ... (skip if already processed logic - same as before) ...\n","\n","        question_actual_text_p3 = qid_to_actual_question_text_p3.get(qid, f\"Question {qid}\")\n","        # text_items_for_this_qid is a list of dicts: [{'source_file': ..., 'text': ...}, ...]\n","        text_items_for_this_qid_p3 = collated_texts_by_qid_p3.get(qid, [])\n","        # ... (check for empty text_items_for_this_qid_p3 - same as before) ...\n","\n","        if not text_items_for_this_qid_p3: # Handle empty list for QID\n","            logger_phase3.log(f\"WARN: No collated texts found for QID {qid}. Skipping thematic analysis.\")\n","            overall_thematic_results_p3[qid] = {\"question_text\": question_actual_text_p3, \"status\": \"no_collated_text_for_qid\", \"parsed_top_themes\": []}\n","            continue\n","\n","        # --- REFINED MAP-REDUCE LOGIC ---\n","        all_intermediate_themes_text_list = [] # Store raw text outputs of map step\n","        raw_map_outputs_list = [] # Store raw LLM response for each map call for debugging\n","\n","        # Group individual text items (summaries/passages from Stage 4a) into batches for the MAP step\n","        # The goal is for the *concatenated text of items in a batch* to be within LLM limits\n","        map_batches = []\n","        current_batch_texts = []\n","        current_batch_token_count = 0 # Approximation by char length initially, refine with tokenizer\n","\n","        # Heuristic: average tokens per character (very rough)\n","        # For a more robust approach, tokenize each item's text and sum tokens.\n","        # avg_chars_per_token = 4\n","        # max_chars_per_map_batch = PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK * avg_chars_per_token\n","\n","        # More robust batching based on tokenizing the combined text of the batch:\n","        temp_batch_texts = []\n","        for item_idx, item in enumerate(text_items_for_this_qid_p3):\n","            item_text = item.get('text')\n","            if not item_text: continue\n","\n","            temp_batch_texts.append(item_text)\n","            # Check token length of the current temp_batch_texts combined\n","            combined_text_for_batch_check = \"\\n\\n--- Next Excerpt ---\\n\".join(temp_batch_texts)\n","            tokens_for_batch_check = llm_thematic_pipe_p3.tokenizer.encode(combined_text_for_batch_check, add_special_tokens=False)\n","\n","            if len(tokens_for_batch_check) > PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK:\n","                if len(temp_batch_texts) > 1: # If more than one item made it too long\n","                    map_batches.append(temp_batch_texts[:-1]) # Add batch without the last item\n","                    temp_batch_texts = [temp_batch_texts[-1]] # Start new batch with the last item\n","                else: # Single item itself is too long\n","                    logger_phase3.log(f\"WARN: QID {qid}, Item {item_idx} text is too long ({len(tokens_for_batch_check)} tokens) even for a single map chunk. Truncating this item.\")\n","                    # Simple truncation for this item (could be smarter)\n","                    # Re-tokenize the single item and slice tokens\n","                    single_item_tokens = llm_thematic_pipe_p3.tokenizer.encode(item_text, add_special_tokens=False)\n","                    truncated_item_tokens = single_item_tokens[:PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK]\n","                    truncated_item_text = llm_thematic_pipe_p3.tokenizer.decode(truncated_item_tokens, skip_special_tokens=True)\n","                    map_batches.append([truncated_item_text])\n","                    temp_batch_texts = [] # Reset\n","\n","        if temp_batch_texts: # Add any remaining batch\n","            map_batches.append(temp_batch_texts)\n","\n","        logger_phase3.log(f\"INFO: QID {qid} - Created {len(map_batches)} batches of text items for MAP step.\")\n","\n","        if not map_batches: # Should not happen if text_items_for_this_qid_p3 was not empty\n","             logger_phase3.log(f\"WARN: QID {qid} - No map batches created. Skipping LLM analysis.\")\n","             overall_thematic_results_p3[qid] = {\"question_text\": question_actual_text_p3, \"status\": \"no_map_batches_created\", \"parsed_top_themes\": []}\n","             continue\n","\n","\n","        if len(map_batches) > 1 : # Apply Map-Reduce\n","            logger_phase3.log(f\"INFO: QID {qid} - Applying Map-Reduce with {len(map_batches)} map steps.\")\n","            for i, batch_of_texts in enumerate(map_batches):\n","                batch_combined_text = \"\\n\\n--- Next Excerpt ---\\n\".join(batch_of_texts)\n","                logger_phase3.log(f\"  MAP Step for QID {qid}, Batch {i+1}/{len(map_batches)} (Items: {len(batch_of_texts)}, Chars: {len(batch_combined_text)})...\")\n","\n","                map_llm_output = get_llm_thematic_points(\n","                    batch_combined_text,\n","                    question_actual_text_p3,\n","                    3, # Request e.g., 3 themes from each small batch\n","                    llm_thematic_pipe_p3,\n","                    PHASE3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,\n","                    f\"{qid}_map_batch_{i+1}\",\n","                    map_reduce_step_type=\"MAP_INDIVIDUAL_ITEMS\"\n","                )\n","                raw_map_outputs_list.append({\"batch_index\": i+1, \"raw_output\": map_llm_output})\n","                if not map_llm_output.startswith(\"LLM_ERROR:\"):\n","                    all_intermediate_themes_text_list.append(f\"Preliminary Themes from Batch {i+1}:\\n{map_llm_output}\")\n","                time.sleep(1) # Optional delay\n","\n","            if not all_intermediate_themes_text_list:\n","                logger_phase3.log(f\"ERROR: QID {qid} - Map step produced no valid intermediate themes.\")\n","                # ... (store error status and continue) ...\n","                overall_thematic_results_p3[qid] = {\n","                    \"question_text\": question_actual_text_p3, \"status\": \"error_map_step_no_themes\",\n","                    \"parsed_top_themes\": [], \"llm_raw_output_map_chunks\": raw_map_outputs_list, \"llm_raw_output_final\": \"N/A - Map failed\"\n","                }\n","                # ... (save and continue logic from before) ...\n","                continue\n","\n","\n","            combined_intermediate_themes_for_reduce = \"\\n\\n\".join(all_intermediate_themes_text_list)\n","            logger_phase3.log(f\"INFO: QID {qid} - REDUCE Step: Processing {len(all_intermediate_themes_text_list)} sets of intermediate themes (Total chars: {len(combined_intermediate_themes_for_reduce)}).\")\n","\n","            # **CRITICAL**: Check if combined_intermediate_themes_for_reduce is too long for the reduce LLM call\n","            reduce_input_tokens = llm_thematic_pipe_p3.tokenizer.encode(combined_intermediate_themes_for_reduce, add_special_tokens=False)\n","            if len(reduce_input_tokens) > PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK * 2.0 : # Allow reduce input to be larger, e.g. 2x map chunk, but still limited\n","                logger_phase3.log(f\"WARN: QID {qid} - Combined intermediate themes ({len(reduce_input_tokens)} tokens) too long for a single Reduce step. Further recursive reduction or summarization of intermediate themes would be needed for optimal results. Proceeding with potentially truncated/problematic input for now.\")\n","                # Implement more robust handling here if this becomes common:\n","                # e.g., chunk combined_intermediate_themes_for_reduce and do multiple reduce calls, then a final reduce.\n","                # For now, we pass it as is, LLM might truncate or error.\n","\n","            final_llm_themes_output = get_llm_thematic_points(\n","                combined_intermediate_themes_for_reduce,\n","                question_actual_text_p3,\n","                PHASE3_NUM_THEMES_TO_REQUEST, # Final number of themes\n","                llm_thematic_pipe_p3,\n","                PHASE3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,\n","                f\"{qid}_reduce_step\",\n","                map_reduce_step_type=\"REDUCE_INTERMEDIATE_THEMES\"\n","            )\n","            llm_raw_final_output_for_storage = final_llm_themes_output\n","        else: # Only one batch, so effectively no reduce step needed, direct processing.\n","            logger_phase3.log(f\"INFO: QID {qid} - Only one batch ({len(map_batches[0])} items). Processing directly (no separate reduce step).\")\n","            direct_input_text = \"\\n\\n--- Next Excerpt ---\\n\".join(map_batches[0])\n","            final_llm_themes_output = get_llm_thematic_points(\n","                direct_input_text,\n","                question_actual_text_p3,\n","                PHASE3_NUM_THEMES_TO_REQUEST,\n","                llm_thematic_pipe_p3,\n","                PHASE3_LLM_MAX_NEW_TOKENS_FOR_THEMES, # Max new tokens for final output\n","                qid,\n","                map_reduce_step_type=\"DIRECT_SINGLE_BATCH\" # Or a unique type\n","            )\n","            llm_raw_final_output_for_storage = final_llm_themes_output\n","            raw_map_outputs_list.append({\"batch_index\": 0, \"raw_output\": final_llm_themes_output})\n","\n","\n","        parsed_themes_list = parse_llm_themes(final_llm_themes_output, PHASE3_NUM_THEMES_TO_REQUEST)\n","        # ... (status setting and saving overall_thematic_results_p3, adapted for map-reduce logging)\n","        status_p3 = \"success_thematic_analysis\"\n","        if final_llm_themes_output.startswith(\"LLM_ERROR:\"): status_p3 = \"error_llm_theme_generation_final\"\n","        elif not parsed_themes_list or (len(parsed_themes_list) == 1 and parsed_themes_list[0][\"theme_label\"] == \"Unparsed LLM Output\"):\n","            status_p3 = \"llm_final_output_parsing_failed_or_empty\"\n","\n","        overall_thematic_results_p3[qid] = {\n","            \"question_text\": question_actual_text_p3,\n","            \"status\": status_p3,\n","            \"total_source_text_items_for_qid\": len(text_items_for_this_qid_p3), # From Stage 4a\n","            \"map_reduce_applied\": len(map_batches) > 1,\n","            \"num_map_batches\": len(map_batches),\n","            \"llm_raw_output_map_batches\": raw_map_outputs_list,\n","            \"llm_raw_output_final_synthesis\": llm_raw_final_output_for_storage,\n","            \"parsed_top_themes\": parsed_themes_list\n","        }\n","        # ... (save intermediate results to JSON logic) ...\n","        try:\n","            with open(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON, 'w', encoding='utf-8') as f_out_p3:\n","                json.dump(overall_thematic_results_p3, f_out_p3, indent=2, default=logger_phase3._get_serializer()) # Use logger's serializer\n","            logger_phase3.log(f\"INFO: Saved/Updated thematic analysis results to '{PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}' after QID {qid}.\")\n","        except Exception as e_save_p3:\n","            logger_phase3.log(f\"ERROR: Could not save intermediate results for Phase 3 (QID {qid}): {e_save_p3}\")\n","\n","\n","        gc.collect()\n","        if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","    logger_phase3.log(\"=\" * 50); logger_phase3.log(\" INFO: Phase 3 (LLM Thematic Analysis - Refined Map-Reduce) Finished \"); logger_phase3.log(\"=\" * 50)\n","    return True\n","\n","# print(\"INFO: Cell 8 - Logic for Phase 3 (LLM Thematic Analysis) with Refined Map-Reduce defined.\")\n","# To run Phase 3:\n","# run_phase3_llm_thematic_analysis()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CG8gGscA3AEw"},"outputs":[],"source":["# @title Get the config of LLM\n","from transformers import AutoConfig\n","\n","model_id = \"google/gemma-3-4b-it\" # Or your actual model ID\n","try:\n","    config = AutoConfig.from_pretrained(model_id)\n","\n","    # Common attribute names for max sequence length / context window:\n","    max_len_attrs = [\n","        \"max_position_embeddings\",\n","        \"n_positions\",\n","        \"max_sequence_length\",\n","        \"model_max_length\" # Often found in tokenizer_config.json too\n","    ]\n","\n","    context_window = None\n","    for attr in max_len_attrs:\n","        if hasattr(config, attr):\n","            context_window = getattr(config, attr)\n","            print(f\"Found context window attribute '{attr}': {context_window}\")\n","            break\n","\n","    if context_window is None:\n","        print(f\"Could not definitively determine context window from config attributes: {max_len_attrs}\")\n","        print(\"Full config:\", config) # Print full config to inspect manually\n","\n","except Exception as e:\n","    print(f\"Error loading config for {model_id}: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"A7KDjjdzV2c6"},"source":["# 16th May"]},{"cell_type":"markdown","metadata":{"id":"d6pm3QS0V_ks"},"source":["## New Stage 4: Question-Centric PDF Thematic Summarization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QEmIpNoSV9jc"},"outputs":[],"source":["# @title Cell 0: Initial Setup & Global Imports\n","\n","# --- Basic Python and System ---\n","import json\n","import os\n","import sys\n","import traceback\n","import time\n","import gc\n","import re\n","from collections import defaultdict, Counter\n","from typing import Dict, Any, List, Tuple, Optional\n","import warnings\n","import argparse # Kept for potential future use\n","import tempfile # Needed for test mode\n","\n","# --- ML/NLP Core Imports ---\n","import torch\n","import numpy as np\n","import nltk\n","\n","# --- Hugging Face Transformers & Related ---\n","from sentence_transformers import SentenceTransformer, util as sbert_util\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","\n","# --- Suppress Warnings (Optional) ---\n","warnings.filterwarnings(\"ignore\", message=\".*Using the latest cached version.*\")\n","warnings.filterwarnings(\"ignore\", message=\".*torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.*\")\n","\n","# --- BitsAndBytes (Optional, for Quantization) ---\n","try:\n","    from bitsandbytes.cextension import ColabNotSupportedError\n","except ImportError:\n","    ColabNotSupportedError = None # Define dummy class if not importable\n","    print(\"WARN: bitsandbytes library not found. Quantization will be disabled if requested.\")\n","    pass\n","\n","print(\"INFO: Cell 0 - Initial imports complete.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snOueyCFkgzx"},"outputs":[],"source":["# @title Cell 1: FileLogger Class & NLTK Downloads\n","\n","# =====================================================\n","#                Custom FileLogger Class\n","# =====================================================\n","class FileLogger:\n","    def __init__(self, log_file_path): # Renamed log_file to log_file_path for clarity\n","        self.log_file_path = log_file_path\n","        log_dir = os.path.dirname(log_file_path)\n","        if log_dir and not os.path.exists(log_dir):\n","            os.makedirs(log_dir, exist_ok=True)\n","        try:\n","            with open(self.log_file_path, 'w', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: Starting new log at {self.log_file_path}\\n\")\n","            print(f\"INFO: Logger initialized. Log file: {self.log_file_path}\")\n","        except IOError as e:\n","            print(f\"CRITICAL: Failed to initialize log file {self.log_file_path}: {e}\")\n","\n","    def _get_timestamp(self):\n","        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","\n","    def log(self, message: str):\n","        try:\n","            with open(self.log_file_path, 'a', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","        except Exception as e:\n","            print(f\"!!! LOGGER FAILED TO WRITE TO {self.log_file_path}: {e} !!!\")\n","            print(f\"{self._get_timestamp()}: {message}\") # Fallback print\n","\n","    def _get_serializer(self):\n","        def default_serializer(obj):\n","            if isinstance(obj, (np.integer, np.floating)): return obj.item()\n","            if isinstance(obj, np.ndarray): return obj.tolist()\n","            if isinstance(obj, set): return sorted(list(obj))\n","            if 'torch' in sys.modules and isinstance(obj, torch.Tensor):\n","                return obj.detach().cpu().numpy().tolist()\n","            try:\n","                return json.JSONEncoder().default(obj)\n","            except TypeError:\n","                return f\"UNSERIALIZABLE:{type(obj).__name__}\"\n","        return default_serializer\n","\n","    def log_json(self, data: Dict[str, Any], prefix=\"JSON_DATA\"):\n","        serializer = self._get_serializer()\n","        log_entry = {\"timestamp\": self._get_timestamp(), \"type\": prefix, \"data\": data}\n","        try:\n","            json_string = json.dumps(log_entry, default=serializer, indent=2)\n","            with open(self.log_file_path, 'a', encoding='utf-8') as f:\n","                f.write(json_string + \"\\n\")\n","        except Exception as e:\n","            self.log(f\"ERROR: Unexpected error during log_json (prefix={prefix}): {e}\")\n","            self.log(f\"TRACEBACK for log_json error:\\n{traceback.format_exc()}\")\n","\n","\n","# =====================================================\n","#         NLTK Setup\n","# =====================================================\n","def download_nltk_resource_with_logger(logger_instance, resource_id, download_name):\n","    \"\"\"Checks for an NLTK resource and downloads it if missing, using provided logger.\"\"\"\n","    try:\n","        nltk.data.find(resource_id)\n","    except LookupError:\n","        logger_instance.log(f\"INFO: NLTK resource '{download_name}' not found. Downloading...\")\n","        try:\n","            nltk.download(download_name, quiet=True)\n","            logger_instance.log(f\"INFO: Downloaded NLTK resource: {download_name}\")\n","        except Exception as dl_e:\n","            logger_instance.log(f\"ERROR: Failed NLTK download {download_name}: {dl_e}\")\n","    except Exception as e:\n","        logger_instance.log(f\"ERROR: NLTK check error {download_name}: {e}\")\n","\n","# Initialize a temporary logger for NLTK downloads if main logger isn't ready\n","# Or, ensure this cell runs after the main logger for a stage is initialized.\n","# For now, using print for this self-contained NLTK setup.\n","print(\"INFO: Checking/Downloading NLTK resources (using print for this setup block)...\")\n","def temp_nltk_download(resource_id, download_name):\n","    try: nltk.data.find(resource_id)\n","    except LookupError:\n","        print(f\"INFO: NLTK (temp): Downloading {download_name}...\")\n","        nltk.download(download_name, quiet=True)\n","temp_nltk_download('tokenizers/punkt', 'punkt')\n","temp_nltk_download('tokenizers/punkt_tab', 'punkt_tab')\n","temp_nltk_download('corpora/stopwords', 'stopwords')\n","print(\"INFO: NLTK resource check complete for Cell 1.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7kDfphUx8gF"},"outputs":[],"source":["# @title Verify environment\n","import torch\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"CUDA version: {torch.version.cuda}\")\n","    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n","\n","try:\n","    import bitsandbytes\n","    print(\"Successfully imported bitsandbytes!\")\n","    # You can now try using a feature that requires GPU bitsandbytes,\n","    # e.g., loading a model with load_in_8bit=True from Hugging Face transformers\n","except Exception as e:\n","    print(f\"Error importing bitsandbytes or bitsandbytes GPU features not available: {e}\")\n","    print(\"Make sure you have:\")\n","    print(\"1. Enabled GPU in Runtime > Change runtime type.\")\n","    print(\"2. Installed bitsandbytes with !pip install bitsandbytes.\")\n","    print(\"3. RESTARTED THE RUNTIME after installation.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1rFpXGek8OB"},"outputs":[],"source":["# @title Cell 2: Configuration for Phase 1 (Original Stage 4)\n","\n","# --- Global Path Configuration (UPDATE THESE!) ---\n","# Assuming Colab environment and Google Drive mounted at /content/drive\n","BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # Main project folder\n","\n","# --- Input Files for Phase 1 ---\n","PHASE1_ORIGINAL_RESPONSES_FILE = os.path.join(BASE_PROJECT_DIR, 'processed_responses.jsonl') # Source for raw data + PDF text\n","PHASE1_STAGE2_AGGREGATION_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # Source for QID->Text map (ensure this exists or is created by your form-only Stage 2)\n","\n","# --- Output & State Files for Phase 1 ---\n","# Directory to store chunked outputs of Phase 1 if it processes in chunks\n","PHASE1_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase1_OriginalStage4_Outputs/')\n","os.makedirs(PHASE1_OUTPUT_DIR, exist_ok=True)\n","# Example: PHASE1_PDF_ANALYSIS_BASE_FILENAME = 'pdf_passage_analysis_chunk' # Will be PHASE1_OUTPUT_DIR/pdf_passage_analysis_chunk_N.json\n","# For simplicity, if your original Stage 4 outputs to one potentially large file that it appends to or manages state for:\n","PHASE1_SINGLE_OUTPUT_FILE = os.path.join(PHASE1_OUTPUT_DIR, 'pdf_passage_analysis_by_response_FULL.json') # If it produces ONE file\n","PHASE1_STATE_FILE = os.path.join(BASE_PROJECT_DIR, 'phase1_original_stage4_processing_state.json')\n","PHASE1_LOG_FILE = os.path.join(BASE_PROJECT_DIR, \"phase1_original_stage4_pdf_extraction.log\")\n","\n","# --- Model Configuration for Phase 1 ---\n","PHASE1_EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n","PHASE1_LLM_MODEL_ID = 'google/gemma-3-4b-it' # Changed from 3-4b-it for wider compatibility, adjust if needed\n","PHASE1_USE_QUANTIZATION = False\n","\n","# --- Retrieval & Summarization Parameters for Phase 1 ---\n","PHASE1_MIN_SIMILARITY_SCORE = 0.5\n","PHASE1_CONTEXT_WINDOW_SIZE_SENTENCES = 1 # sentences before/after for passage\n","PHASE1_TOP_K_PASSAGES = 3\n","PHASE1_NUM_SUMMARY_SENTENCES_PER_RESPONSE_QID = 3\n","PHASE1_MIN_CHARS_FOR_SUMMARY = 150\n","PHASE1_LLM_MAX_NEW_TOKENS = 250 # Adjusted for 3 sentences from Gemma 2B\n","\n","# --- PDF Processing for Phase 1 ---\n","PHASE1_MIN_PDF_TEXT_LENGTH = 100\n","\n","# --- Runtime & Logging for Phase 1 ---\n","PHASE1_MAX_RUNTIME_MINUTES = 60 * 60 * 8 # e.g., 8 hours (adjust per run)\n","PHASE1_SAVE_STATE_INTERVAL_SECONDS = 60 * 20 # Save progress every 20 minutes\n","PHASE1_ENABLE_DEBUG_LOGGING = True # Set True for very detailed logs, False for production runs\n","PHASE1_TEST_SINGLE_RESPONSE_ID = None # Set to an ID for testing, None for full run\n","\n","# --- Question Selection for Per-Response Summarization in Phase 1 ---\n","# This determines which QIDs will have summaries generated *within each response's PDF analysis*\n","PHASE1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE = {\n","    \"Q1\": True, \"Q2\": True, \"Q3\": True, \"Q4\": True, \"Q5\": True, \"Q6\": True, \"Q7\": True,\n","    \"Q8\": True, \"Q9\": True, \"Q10\": True,\"Q11\": True, \"Q12\": True, \"Q13\": True, \"Q14\": True,\n","    \"Q15\": True, \"Q16\": False,\"Q17\": True, \"Q18\": True, \"Q19\": True,\"Q20\": False, \"Q21\": True,\n","    \"Q22\": True, \"Q23\": True, \"Q24\": True, \"Q25\": True, \"Q26\": True, \"Q27\": True, \"Q28\": True,\n","    \"Q29\": True, \"Q30\": True, \"Q31\": True,\"Q32\": False,\"Q33\": False,\"Q34\": False,\"Q35\": False,\n","    \"Q36\": False,\n","}\n","\n","print(\"INFO: Cell 2 - Configuration for Phase 1 (Original Stage 4) loaded.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyZcb3uFmnRd"},"outputs":[],"source":["# @title Cell 3: Helper Functions for Phase 1 (Original Stage 4)\n","# These are adapted from your unstructured_text_response_centric_uds_v2_py.txt\n","\n","# (Global logger for this Phase)\n","logger_phase1 = FileLogger(PHASE1_LOG_FILE)\n","\n","def save_phase1_output_chunk(output_dir, chunk_index, output_data):\n","    \"\"\"Saves a chunk of Phase 1 output.\"\"\"\n","    # This function needs to be designed based on how Original Stage 4 chunks.\n","    # If it produces one large resumable file, this isn't needed.\n","    # If it processes N responses and saves, this is where it saves.\n","    # For now, assuming the main loop of Original Stage 4 will call a general save function.\n","    # This function is more conceptual if your original Stage 4 saves its full state.\n","    filename = os.path.join(output_dir, f\"pdf_passage_analysis_chunk_{chunk_index}.json\")\n","    logger_phase1.log(f\"INFO: Saving Phase 1 output chunk to {filename}...\")\n","    serializer = logger_phase1._get_serializer() # Use logger's serializer\n","    try:\n","        final_output = {\"pdf_analysis_by_response\": output_data}\n","        with open(filename, 'w', encoding='utf-8') as f:\n","            json.dump(final_output, f, indent=2, default=serializer)\n","        logger_phase1.log(f\"INFO: Successfully saved Phase 1 chunk output.\")\n","    except Exception as e:\n","        logger_phase1.log(f\"ERROR: Saving Phase 1 chunk output failed: {e}\")\n","        logger_phase1.log(traceback.format_exc())\n","\n","def save_phase1_state(state_file, processed_response_ids_set, current_results_dict):\n","    \"\"\"Saves the current processing state for Phase 1.\"\"\"\n","    logger_phase1.log(f\"INFO: Saving Phase 1 processing state to {state_file}...\")\n","    try:\n","        state_data = {\n","            \"processed_responses_ids\": sorted(list(processed_response_ids_set)),\n","            \"results_so_far\": current_results_dict # This could be large if not careful\n","        }\n","        # Atomic save: write to temp then rename\n","        temp_state_file = state_file + \".tmp\"\n","        with open(temp_state_file, 'w', encoding='utf-8') as f:\n","            json.dump(state_data, f, indent=2, default=logger_phase1._get_serializer())\n","        os.replace(temp_state_file, state_file)\n","        logger_phase1.log(f\"INFO: Phase 1 State saved successfully ({len(processed_response_ids_set)} responses processed).\")\n","    except Exception as e:\n","        logger_phase1.log(f\"ERROR: Failed to save Phase 1 state to {state_file}: {e}\")\n","        logger_phase1.log(traceback.format_exc())\n","\n","def load_phase1_state(state_file):\n","    \"\"\"Loads Phase 1 processing state from a file.\"\"\"\n","    if os.path.exists(state_file):\n","        logger_phase1.log(f\"INFO: Found Phase 1 state file {state_file}. Attempting to load state...\")\n","        try:\n","            with open(state_file, 'r', encoding='utf-8') as f:\n","                state_data = json.load(f)\n","            processed_ids = set(state_data.get(\"processed_responses_ids\", []))\n","            results = state_data.get(\"results_so_far\", {}) # Results loaded from previous run\n","            if not isinstance(results, dict):\n","                logger_phase1.log(\"ERROR: Loaded 'results_so_far' is not a dictionary. Starting fresh.\")\n","                return set(), {}\n","            logger_phase1.log(f\"INFO: Successfully loaded Phase 1 state. Resuming after {len(processed_ids)} processed responses.\")\n","            return processed_ids, results\n","        except Exception as e:\n","            logger_phase1.log(f\"ERROR: Failed to load/parse Phase 1 state from {state_file}: {e}. Starting fresh.\")\n","            return set(), {}\n","    else:\n","        logger_phase1.log(\"INFO: No Phase 1 state file found. Starting fresh.\")\n","        return set(), {}\n","\n","def get_phase1_local_embeddings(texts: List[str], model: SentenceTransformer, device: str, batch_size: int = 32):\n","    if not texts: return None\n","    try:\n","        if PHASE1_ENABLE_DEBUG_LOGGING: logger_phase1.log(f\"DEBUG: Phase 1 - Generating local embeddings for {len(texts)} texts on {device}...\")\n","        model.to(device) # Ensure model is on the correct device\n","        # embeddings = model.encode(texts, convert_to_tensor=True, device=device, show_progress_bar=False, batch_size=batch_size)\n","        # Detach from graph and move to CPU before converting to numpy\n","        # return embeddings.detach().cpu().numpy()\n","\n","        # Corrected sbert usage\n","        embeddings_tensor = model.encode(texts, convert_to_tensor=True, show_progress_bar=PHASE1_ENABLE_DEBUG_LOGGING, batch_size=batch_size, device=device)\n","        return embeddings_tensor.cpu().numpy() # No need to detach if not part of a computation graph being tracked\n","    except Exception as e:\n","        logger_phase1.log(f\"ERROR: Phase 1 - Failed to generate local embeddings: {e}\")\n","        logger_phase1.log(traceback.format_exc())\n","        return None\n","\n","def retrieve_phase1_relevant_passages(question_embedding: np.ndarray,\n","                                     pdf_sentences: List[str],\n","                                     pdf_sentence_embeddings: np.ndarray,\n","                                     similarity_threshold: float,\n","                                     top_k: int,\n","                                     context_window_sentences: int):\n","    relevant_passages_texts = []\n","    highest_score_found = None\n","    if question_embedding is None or pdf_sentence_embeddings is None or not pdf_sentences:\n","        return relevant_passages_texts, highest_score_found\n","\n","    # Ensure question_embedding is 2D\n","    if question_embedding.ndim == 1:\n","        question_embedding = question_embedding.reshape(1, -1)\n","\n","    try:\n","        # Convert numpy arrays to PyTorch tensors for sbert_util.cos_sim\n","        q_tensor = torch.from_numpy(question_embedding).to(dtype=torch.float32)\n","        s_tensor = torch.from_numpy(pdf_sentence_embeddings).to(dtype=torch.float32)\n","\n","        # Move tensors to the same device (e.g., CPU if one is on CPU and other on GPU, or consistent GPU)\n","        # Assuming q_tensor and s_tensor might be on different devices or need to be moved to where SentenceTransformer model ran\n","        target_device = s_tensor.device # Assume sentence embeddings are on the target device\n","        q_tensor = q_tensor.to(target_device)\n","\n","        cosine_scores_tensor = sbert_util.cos_sim(q_tensor, s_tensor)[0] # Get the first row of scores\n","        cosine_scores = cosine_scores_tensor.cpu().numpy() # Move to CPU for numpy operations\n","\n","        # Sort scores in descending order and get indices\n","        sorted_indices = np.argsort(cosine_scores)[::-1]\n","\n","        added_passage_hashes = set() # To avoid duplicate passages from overlapping contexts\n","\n","        for idx in sorted_indices:\n","            score = cosine_scores[idx]\n","            if score < similarity_threshold:\n","                break # Scores are sorted, no need to check further\n","\n","            if highest_score_found is None: # Capture the top score\n","                highest_score_found = float(score)\n","\n","            start_idx = max(0, idx - context_window_sentences)\n","            end_idx = min(len(pdf_sentences), idx + context_window_sentences + 1)\n","\n","            context_passage_list = pdf_sentences[start_idx:end_idx]\n","            passage_text = \" \".join(context_passage_list).strip()\n","            passage_hash = hash(passage_text)\n","\n","            if passage_text and passage_hash not in added_passage_hashes:\n","                relevant_passages_texts.append(passage_text)\n","                added_passage_hashes.add(passage_hash)\n","                if len(relevant_passages_texts) >= top_k:\n","                    break\n","\n","        return relevant_passages_texts, highest_score_found\n","\n","    except Exception as e:\n","        logger_phase1.log(f\"ERROR: Phase 1 - Failed during passage retrieval: {e}\")\n","        logger_phase1.log(traceback.format_exc())\n","        return [], None\n","\n","\n","def summarize_phase1_context_block(text_chunk: str, question_text_for_prompt: str, qid_for_log: str,\n","                                 llm_pipe: pipeline, num_target_sentences: int, max_new_tokens_for_summary: int):\n","    if not text_chunk or not llm_pipe: return None\n","\n","    messages = [{\n","        \"role\": \"user\",\n","        \"content\": f\"Carefully review the following text excerpt which is in response to the question: \\\"{question_text_for_prompt}\\\". \"\n","                   f\"Concisely summarize the main points from this excerpt in approximately {num_target_sentences} distinct sentences. \"\n","                   f\"Focus on capturing the essence of the input text relevant to the question.\\n\\nExcerpt:\\n\\\"\\\"\\\"\\n{text_chunk}\\n\\\"\\\"\\\"\"\n","    }]\n","\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(\n","            messages, tokenize=False, add_generation_prompt=True\n","        )\n","\n","        generation_args = {\n","            \"max_new_tokens\": max_new_tokens_for_summary,\n","            \"do_sample\": False, # For deterministic summaries\n","            \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id,\n","        }\n","\n","        if PHASE1_ENABLE_DEBUG_LOGGING:\n","            logger_phase1.log(f\"DEBUG: Phase 1 - Calling LLM for per-response summary (QID {qid_for_log}, {len(text_chunk)} chars input)...\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger_phase1.log(f\"ERROR: Phase 1 - LLM pipeline returned unexpected/empty output for QID {qid_for_log}: {outputs}\")\n","            return None\n","\n","        generated_text_full = outputs[0]['generated_text']\n","###\n","        if PHASE1_ENABLE_DEBUG_LOGGING or not summary_text.strip(): # Log if debug or if summary ends up empty\n","            logger_phase1.log(f\"DEBUG/WARN: QID {qid_for_log} - LLM Raw Output (generated_text_full):\\nSOF<<<<<\\n{generated_text_full}\\n>>>>>EOF\")\n","###\n","        # Extract summary (logic from your original Stage 4)\n","        summary_text = \"\"\n","        if generated_text_full.startswith(prompt_formatted):\n","            summary_text = generated_text_full[len(prompt_formatted):].strip()\n","        else:\n","            response_marker = \"<start_of_turn>model\\n\" # Specific to Gemma instruct\n","            summary_start_index = generated_text_full.rfind(response_marker)\n","            if summary_start_index != -1:\n","                summary_text = generated_text_full[summary_start_index + len(response_marker):].strip()\n","            else:\n","                logger_phase1.log(f\"WARN: Phase 1 - Could not find standard model response marker for QID {qid_for_log}. Using full output minus prompt attempt.\")\n","                # Fallback: try to remove the known prompt string if it's not an exact start match (less reliable)\n","                # This might not be perfect if the LLM slightly alters the prompt start in its echo.\n","                if prompt_formatted in generated_text_full:\n","                    summary_text = generated_text_full.split(prompt_formatted, 1)[-1].strip()\n","                else:\n","                    summary_text = generated_text_full # Best guess\n","\n","        if summary_text.startswith(\"Error:\") or not summary_text.strip():\n","            logger_phase1.log(f\"WARN: Phase 1 - Final extracted summary for QID {qid_for_log} is empty or indicates error.\")\n","            return \"\"\n","        return summary_text\n","\n","    except Exception as e:\n","        logger_phase1.log(f\"ERROR: Phase 1 - Exception during LLM summarization for QID {qid_for_log}: {e}\")\n","        logger_phase1.log(traceback.format_exc())\n","        return None\n","\n","print(\"INFO: Cell 3 - Helper functions for Phase 1 (Original Stage 4) defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-nz6Kksn6-E"},"outputs":[],"source":["# @title Cell 4: Main Execution Logic for Phase 1 (Original Stage 4)\n","\n","def run_phase1_pdf_extraction():\n","    global logger_phase1 # Use the logger initialized for this phase\n","    logger_phase1.log(\"=\" * 50)\n","    logger_phase1.log(\" INFO: Starting Phase 1: PDF Content Extraction (Original Stage 4 Logic)\")\n","    logger_phase1.log(\"=\" * 50)\n","    # Log key configurations\n","    logger_phase1.log(f\"INFO: Input Original Responses (JSONL): {PHASE1_ORIGINAL_RESPONSES_FILE}\")\n","    logger_phase1.log(f\"INFO: Input QID Map (Stage 2 Form Output): {PHASE1_STAGE2_AGGREGATION_FILE}\")\n","    logger_phase1.log(f\"INFO: Output Directory for Chunks: {PHASE1_OUTPUT_DIR} (or single file: {PHASE1_SINGLE_OUTPUT_FILE})\") # Adjust log based on actual save strategy\n","    logger_phase1.log(f\"INFO: State File: {PHASE1_STATE_FILE}\")\n","    logger_phase1.log(f\"INFO: Embedding Model: {PHASE1_EMBEDDING_MODEL_NAME}\")\n","    logger_phase1.log(f\"INFO: LLM Model for Per-Response Summaries: {PHASE1_LLM_MODEL_ID}\")\n","    logger_phase1.log(f\"INFO: Use Quantization: {PHASE1_USE_QUANTIZATION}\")\n","\n","    # --- Determine Device ---\n","    phase1_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    logger_phase1.log(f\"INFO: Phase 1 using device: {phase1_device}\")\n","\n","    # --- Load Models ---\n","    embedding_model_p1 = None\n","    llm_tokenizer_p1 = None\n","    llm_model_p1 = None\n","    llm_summarization_pipe_p1 = None\n","    models_loaded_successfully_p1 = False\n","\n","    # Create a local variable to store the *effective* quantization state for this run.\n","    # Initialize it with the global configuration value.\n","    current_run_use_quantization = PHASE1_USE_QUANTIZATION # Local variable for this function's scope\n","\n","    try:\n","        logger_phase1.log(f\"INFO: Loading embedding model: {PHASE1_EMBEDDING_MODEL_NAME}...\")\n","        embedding_model_p1 = SentenceTransformer(PHASE1_EMBEDDING_MODEL_NAME, device=phase1_device)\n","        logger_phase1.log(\"INFO: Embedding model loaded.\")\n","\n","        logger_phase1.log(f\"INFO: Loading LLM tokenizer for: {PHASE1_LLM_MODEL_ID}...\")\n","        llm_tokenizer_p1 = AutoTokenizer.from_pretrained(PHASE1_LLM_MODEL_ID)\n","        if getattr(llm_tokenizer_p1, 'pad_token', None) is None and getattr(llm_tokenizer_p1, 'eos_token', None):\n","            llm_tokenizer_p1.pad_token = llm_tokenizer_p1.eos_token\n","            llm_tokenizer_p1.padding_side = \"left\" # Typical for Causal LMs\n","        logger_phase1.log(\"INFO: LLM Tokenizer loaded.\")\n","\n","        bnb_config_p1 = None\n","        if current_run_use_quantization and torch.cuda.is_available():\n","            logger_phase1.log(\"INFO: Setting up 4-bit quantization for LLM...\")\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config_p1 = BitsAndBytesConfig(\n","                    load_in_4bit=True,\n","                    bnb_4bit_quant_type=\"nf4\",\n","                    bnb_4bit_compute_dtype=compute_dtype,\n","                    bnb_4bit_use_double_quant=True\n","                )\n","                logger_phase1.log(f\"INFO: 4-bit quantization configured (Compute dtype: {compute_dtype}).\")\n","            except Exception as q_err:\n","                logger_phase1.log(f\"ERROR: Failed BitsAndBytesConfig: {q_err}. Disabling quantization for LLM.\")\n","                current_run_use_quantization = False # Modify ONLY the local variable\n","\n","        logger_phase1.log(f\"INFO: Loading LLM model: {PHASE1_LLM_MODEL_ID} (Quantization: {current_run_use_quantization})...\")\n","        llm_model_p1 = AutoModelForCausalLM.from_pretrained(\n","            PHASE1_LLM_MODEL_ID,\n","            quantization_config=bnb_config_p1 if PHASE1_USE_QUANTIZATION else None,\n","            torch_dtype=torch.bfloat16 if not PHASE1_USE_QUANTIZATION else None, # Gemma prefers bfloat16\n","            device_map=\"auto\", # Handles multi-GPU or CPU if CUDA not available for model\n","            trust_remote_code=True # Often needed for Gemma\n","        )\n","        logger_phase1.log(\"INFO: LLM model loaded.\")\n","\n","        llm_summarization_pipe_p1 = pipeline(\n","            \"text-generation\",\n","            model=llm_model_p1,\n","            tokenizer=llm_tokenizer_p1,\n","            # device handled by device_map\n","        )\n","        logger_phase1.log(\"INFO: LLM text-generation pipeline created successfully.\")\n","        models_loaded_successfully_p1 = True\n","\n","    except Exception as load_err:\n","        logger_phase1.log(f\"CRITICAL: Failed to load models/pipeline for Phase 1: {load_err}\")\n","        logger_phase1.log(traceback.format_exc())\n","        # Depending on requirements, you might exit or try to proceed without LLM summarization\n","        # For now, let's assume if LLM fails, we can't do per-response summaries but can still extract passages.\n","        # The main loop will need to handle llm_summarization_pipe_p1 being None.\n","        llm_summarization_pipe_p1 = None # Ensure it's None if loading failed\n","\n","    # --- Load QID to Text map and Pre-compute Question Embeddings ---\n","    qid_to_text_map_p1 = {}\n","    qid_to_embedding_map_p1 = {}\n","    try:\n","        with open(PHASE1_STAGE2_AGGREGATION_FILE, 'r', encoding='utf-8') as f_s2:\n","            stage2_data = json.load(f_s2)\n","        for qid, data in stage2_data.get(\"question_data\", {}).items():\n","            if \"question_text\" in data:\n","                qid_to_text_map_p1[qid] = data[\"question_text\"]\n","        if not qid_to_text_map_p1: raise ValueError(\"No question texts found in Stage 2 aggregation file.\")\n","        logger_phase1.log(f\"INFO: Loaded text for {len(qid_to_text_map_p1)} questions.\")\n","\n","        target_qids_p1 = list(qid_to_text_map_p1.keys())\n","        target_qtexts_p1 = list(qid_to_text_map_p1.values())\n","        if embedding_model_p1: # Only if embedding model loaded\n","            question_embeddings_all_p1 = get_phase1_local_embeddings(target_qtexts_p1, embedding_model_p1, phase1_device)\n","            if question_embeddings_all_p1 is None:\n","                raise ValueError(\"Failed to generate question embeddings.\")\n","            qid_to_embedding_map_p1 = {qid: emb for qid, emb in zip(target_qids_p1, question_embeddings_all_p1)}\n","            logger_phase1.log(f\"INFO: Generated embeddings for {len(qid_to_embedding_map_p1)} questions.\")\n","        else: # Critical if embedding model failed\n","            logger_phase1.log(\"CRITICAL: Embedding model not loaded. Cannot perform PDF passage retrieval. Exiting Phase 1.\")\n","            return False # Indicate failure\n","\n","    except Exception as e_qload:\n","        logger_phase1.log(f\"ERROR: Failed loading question texts/embeddings for Phase 1: {e_qload}\")\n","        logger_phase1.log(traceback.format_exc())\n","        return False # Indicate failure\n","\n","    # --- Load State for Resumption ---\n","    # `results_p1` will store {\"response_id\": {\"QID\": {\"summary\": ..., \"passages\": ...}}}\n","    processed_response_ids_p1, results_p1 = load_phase1_state(PHASE1_STATE_FILE)\n","\n","    # --- Determine Input File for Iteration (Original or Temp for Test) ---\n","    input_file_for_iteration_p1 = PHASE1_ORIGINAL_RESPONSES_FILE\n","    # Logic for TEST_SINGLE_RESPONSE_ID (creates temp file)\n","    # This part of your original Stage 4 can be reused here if needed for testing.\n","    # For brevity, assuming full run for now. If TEST_SINGLE_RESPONSE_ID is set,\n","    # input_file_for_iteration_p1 would point to the temp file with that single response.\n","    # Ensure this temp file creation uses PHASE1_ORIGINAL_RESPONSES_FILE as its source.\n","\n","    # --- Main Processing Loop (Iterating through responses) ---\n","    processed_in_this_run_p1 = 0\n","    pdfs_analyzed_this_run_p1 = 0\n","    start_processing_time_p1 = time.time()\n","    last_save_time_p1 = start_processing_time_p1\n","    MAX_RUNTIME_SECONDS_P1 = PHASE1_MAX_RUNTIME_MINUTES * 60\n","\n","    logger_phase1.log(f\"INFO: Starting Phase 1 processing loop for {input_file_for_iteration_p1}...\")\n","    try:\n","        # It's often better to read large JSONL files line by line\n","        # rather than loading all responses into memory at once.\n","        with open(input_file_for_iteration_p1, 'r', encoding='utf-8') as infile:\n","            for line_num, line in enumerate(infile, 1):\n","                # Runtime Limit Check\n","                if time.time() - start_processing_time_p1 >= MAX_RUNTIME_SECONDS_P1:\n","                    logger_phase1.log(f\"WARN: Phase 1 reached runtime limit. Stopping.\")\n","                    break\n","                # Periodic State Saving\n","                if time.time() - last_save_time_p1 >= PHASE1_SAVE_STATE_INTERVAL_SECONDS:\n","                    save_phase1_state(PHASE1_STATE_FILE, processed_response_ids_p1, results_p1)\n","                    last_save_time_p1 = time.time()\n","\n","                try:\n","                    response_obj_p1 = json.loads(line)\n","                    response_id_p1 = str(response_obj_p1.get(\"id\", f\"UnknownLine{line_num}\"))\n","\n","                    if response_id_p1 in processed_response_ids_p1:\n","                        continue # Skip already processed\n","\n","                    results_p1[response_id_p1] = {} # Initialize dict for this response's QID results\n","\n","                    pdf_text_p1 = response_obj_p1.get(\"pdf_text\")\n","                    if not pdf_text_p1 or len(pdf_text_p1) < PHASE1_MIN_PDF_TEXT_LENGTH:\n","                        results_p1[response_id_p1] = {\"status\": \"pdf_missing_or_short\"}\n","                        processed_response_ids_p1.add(response_id_p1)\n","                        continue\n","\n","                    # Get or process PDF sentences and embeddings\n","                    # Reusing the caching function from your \"Revised Stage 4\" idea is good here\n","                    # but original Stage 4 processes one PDF fully then moves to next.\n","                    # So, no cross-PDF caching needed *within this Stage 4 response loop itself*.\n","                    # The caching is beneficial if Stage 4 itself is restarted for the same PDF.\n","\n","                    _cleaned_pdf_text_p1 = re.sub(r'\\s+', ' ', pdf_text_p1).strip()\n","                    _pdf_sentences_p1 = nltk.sent_tokenize(_cleaned_pdf_text_p1)\n","                    _pdf_sentences_p1 = [s.strip() for s in _pdf_sentences_p1 if s.strip()]\n","\n","                    _pdf_sentence_embeddings_p1 = None\n","                    if _pdf_sentences_p1 and embedding_model_p1:\n","                         _pdf_sentence_embeddings_p1 = get_phase1_local_embeddings(\n","                             _pdf_sentences_p1, embedding_model_p1, phase1_device\n","                         )\n","\n","                    if not _pdf_sentences_p1 or _pdf_sentence_embeddings_p1 is None:\n","                        results_p1[response_id_p1] = {\"status\": \"pdf_no_sentences_or_embedding_error\"}\n","                        processed_response_ids_p1.add(response_id_p1)\n","                        continue\n","\n","                    pdfs_analyzed_this_run_p1 +=1\n","\n","                    # Inner loop for QIDs for this response's PDF\n","                    for qid_p1 in target_qids_p1:\n","                        q_text_p1 = qid_to_text_map_p1[qid_p1]\n","                        q_emb_p1 = qid_to_embedding_map_p1.get(qid_p1)\n","\n","                        current_q_result_p1 = {\"question_text\": q_text_p1}\n","                        if q_emb_p1 is None:\n","                            current_q_result_p1[\"status\"] = \"error_missing_q_embedding\"\n","                            results_p1[response_id_p1][qid_p1] = current_q_result_p1\n","                            continue\n","\n","                        passages_p1, top_score_p1 = retrieve_phase1_relevant_passages(\n","                            q_emb_p1, _pdf_sentences_p1, _pdf_sentence_embeddings_p1,\n","                            PHASE1_MIN_SIMILARITY_SCORE, PHASE1_TOP_K_PASSAGES, PHASE1_CONTEXT_WINDOW_SIZE_SENTENCES\n","                        )\n","\n","                        if passages_p1:\n","                            current_q_result_p1[\"status\"] = \"passages_extracted\"\n","                            current_q_result_p1[\"extracted_passages\"] = passages_p1\n","                            current_q_result_p1[\"top_passage_score\"] = float(top_score_p1) if top_score_p1 is not None else None\n","\n","                            # Conditional Per-Response/Per-QID Summarization\n","                            if llm_summarization_pipe_p1 and PHASE1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE.get(qid_p1, False):\n","                                combined_passage_text_p1 = \"\\n\\n\".join(passages_p1)\n","                                if len(combined_passage_text_p1) >= PHASE1_MIN_CHARS_FOR_SUMMARY:\n","                                    summary_p1 = summarize_phase1_context_block(\n","                                        combined_passage_text_p1, q_text_p1, qid_p1,\n","                                        llm_summarization_pipe_p1,\n","                                        PHASE1_NUM_SUMMARY_SENTENCES_PER_RESPONSE_QID,\n","                                        PHASE1_LLM_MAX_NEW_TOKENS\n","                                    )\n","                                    if summary_p1 is not None: # Check for None from error\n","                                        current_q_result_p1[\"summary\"] = summary_p1\n","                                        if summary_p1: # If not empty string from LLM error\n","                                             current_q_result_p1[\"status\"] = \"success_summarized\"\n","                                        else: # Empty string often means LLM had an issue but didn't except\n","                                             current_q_result_p1[\"status\"] = \"summary_attempted_empty_result\"\n","                                    else: # Summarization function returned None (explicit error)\n","                                        current_q_result_p1[\"status\"] = \"error_llm_summarization_response_qid\"\n","                                else:\n","                                    current_q_result_p1[\"status\"] = \"passages_extracted_too_short_for_summary\"\n","                        else:\n","                            current_q_result_p1[\"status\"] = \"no_relevant_passages_found\"\n","\n","                        results_p1[response_id_p1][qid_p1] = current_q_result_p1\n","\n","                    processed_response_ids_p1.add(response_id_p1)\n","                    processed_in_this_run_p1 += 1\n","                    if processed_in_this_run_p1 % 50 == 0:\n","                        logger_phase1.log(f\"INFO: Phase 1 - Processed {processed_in_this_run_p1} new responses in this run...\")\n","                        gc.collect()\n","                        if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","\n","                except json.JSONDecodeError:\n","                    logger_phase1.log(f\"WARN: Phase 1 - Skipping line {line_num} due to JSON parse error.\")\n","                    results_p1[f\"ErrorLine{line_num}\"] = {\"status\": \"error_json_decode_line\"}\n","                except Exception as e_resp:\n","                    logger_phase1.log(f\"ERROR: Phase 1 - Processing response line {line_num} (ID attempt: {response_id_p1 if 'response_id_p1' in locals() else 'N/A'}) failed: {e_resp}\")\n","                    logger_phase1.log(traceback.format_exc())\n","                    results_p1[response_id_p1 if 'response_id_p1' in locals() else f\"ErrorRespLine{line_num}\"] = {\"status\": \"error_processing_response\", \"error_message\": str(e_resp)}\n","                    if 'response_id_p1' in locals() : processed_response_ids_p1.add(response_id_p1) # Mark as processed to avoid retrying broken response\n","\n","    except FileNotFoundError:\n","        logger_phase1.log(f\"ERROR: Phase 1 - Input file not found: {input_file_for_iteration_p1}\")\n","        return False\n","    except Exception as e_file_loop:\n","        logger_phase1.log(f\"ERROR: Phase 1 - Unhandled error during file processing loop: {e_file_loop}\")\n","        logger_phase1.log(traceback.format_exc())\n","        # Attempt to save state even on this error\n","        save_phase1_state(PHASE1_STATE_FILE, processed_response_ids_p1, results_p1)\n","        return False\n","    finally:\n","        # Final save of state and results\n","        logger_phase1.log(f\"INFO: Phase 1 - Loop finished. Total new responses processed in this run: {processed_in_this_run_p1}. Total PDFs analyzed: {pdfs_analyzed_this_run_p1}.\")\n","        save_phase1_state(PHASE1_STATE_FILE, processed_response_ids_p1, results_p1)\n","\n","        # Decide how to save the final \"results_p1\".\n","        # If it's one large file that \"load_phase1_state\" can handle, save it directly.\n","        # If it's meant to be chunked, this part needs to align with your original Stage 4's chunking save logic.\n","        # For now, let's save it as a single (potentially large) file.\n","        # This assumes PHASE1_SINGLE_OUTPUT_FILE is the target.\n","        logger_phase1.log(f\"INFO: Saving all Phase 1 accumulated results to {PHASE1_SINGLE_OUTPUT_FILE}...\")\n","        try:\n","            with open(PHASE1_SINGLE_OUTPUT_FILE, 'w', encoding='utf-8') as f_final:\n","                json.dump({\"pdf_analysis_by_response\": results_p1}, f_final, indent=2, default=logger_phase1._get_serializer())\n","            logger_phase1.log(\"INFO: Final Phase 1 results saved.\")\n","        except Exception as e_final_save:\n","            logger_phase1.log(f\"ERROR: Could not save final Phase 1 results: {e_final_save}\")\n","            logger_phase1.log(traceback.format_exc())\n","\n","        # Cleanup models\n","        logger_phase1.log(\"INFO: Phase 1 - Cleaning up models and resources...\")\n","        del embedding_model_p1, llm_tokenizer_p1, llm_model_p1, llm_summarization_pipe_p1\n","        del qid_to_embedding_map_p1 #, question_embeddings_all_p1 (if defined)\n","        gc.collect()\n","        if torch.cuda.is_available(): torch.cuda.empty_cache()\n","        logger_phase1.log(\"INFO: Phase 1 - Cleanup complete.\")\n","\n","    logger_phase1.log(\"=\" * 50); logger_phase1.log(\" INFO: Phase 1 Processing Finished \"); logger_phase1.log(\"=\" * 50)\n","    return True # Indicate success\n","\n","# print(\"INFO: Cell 4 - Main Execution Logic for Phase 1 defined.\")\n","# To run Phase 1:\n","run_phase1_pdf_extraction()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmCZzbc5okF7"},"outputs":[],"source":["# @title Cell 5: Configuration for Phase 2 (Stage 4a)\n","\n","# Input for Phase 2 is the output directory from Phase 1 (or the single file if Phase 1 produced that)\n","PHASE2_INPUT_PHASE1_OUTPUT_PATH = PHASE1_OUTPUT_DIR # If Phase 1 saved chunks in this dir\n","# Or if Phase 1 saved one big file:\n","# PHASE2_INPUT_PHASE1_OUTPUT_PATH = PHASE1_SINGLE_OUTPUT_FILE\n","\n","PHASE2_OUTPUT_COLLATED_JSON = os.path.join(BASE_PROJECT_DIR, f'phase2_stage4a_collated_pdf_texts_{time.strftime(\"%Y%m%d_%H%M%S\")}.json')\n","PHASE2_LOG_FILE = os.path.join(BASE_PROJECT_DIR, f\"phase2_stage4a_text_collation_{time.strftime('%Y%m%d_%H%M%S')}.log\")\n","\n","print(\"INFO: Cell 5 - Configuration for Phase 2 (Stage 4a) loaded.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_cEtk_4on9Q"},"outputs":[],"source":["# @title Cell 6: Helper Functions & Main Logic for Phase 2 (Stage 4a)\n","# This is the script for \"extract_and_aggregate_content_to_json\" we developed.\n","import glob # <--- ADD THIS LINE\n","\n","logger_phase2 = FileLogger(PHASE2_LOG_FILE) # Logger for this phase\n","\n","# Re-define clean_text or ensure it's available globally\n","def clean_text_phase2(text):\n","    if not text or not isinstance(text, str): return \"\"\n","    text = re.sub(r'\\s*\\n\\s*', '\\n', text)\n","    text = re.sub(r'[ \\t]+', ' ', text)\n","    return text.strip()\n","\n","def run_phase2_collate_pdf_texts(input_phase1_path, output_collated_json_file):\n","    global logger_phase2\n","    logger_phase2.log(\"=\" * 50); logger_phase2.log(\" INFO: Starting Phase 2: PDF Text Collation (Stage 4a Logic)\"); logger_phase2.log(\"=\" * 50)\n","\n","    # Copied and adapted from the script \"extract_and_aggregate_content_to_json\"\n","    if not os.path.exists(input_phase1_path):\n","        logger_phase2.log(f\"ERROR: Input path '{input_phase1_path}' for Phase 1 outputs does not exist.\")\n","        return False\n","\n","    output_dir_p2 = os.path.dirname(output_collated_json_file)\n","    if output_dir_p2 and not os.path.exists(output_dir_p2):\n","        os.makedirs(output_dir_p2, exist_ok=True)\n","        logger_phase2.log(f\"INFO: Created output directory for Phase 2: '{output_dir_p2}'\")\n","\n","    files_to_process_p2 = []\n","    if os.path.isfile(input_phase1_path):\n","        files_to_process_p2 = [input_phase1_path]\n","    elif os.path.isdir(input_phase1_path):\n","        # This glob should match the output file names from your Original Stage 4 if it chunks,\n","        # or the single file if it produces one.\n","        files_to_process_p2 = glob.glob(os.path.join(input_phase1_path, 'pdf_passage_analysis*.json'))\n","        if not files_to_process_p2: # Fallback\n","             files_to_process_p2 = glob.glob(os.path.join(input_phase1_path, '*.json'))\n","    else:\n","        logger_phase2.log(f\"ERROR: Input path '{input_phase1_path}' is neither a file nor a directory.\")\n","        return False\n","\n","    if not files_to_process_p2:\n","        logger_phase2.log(f\"INFO: No JSON files from Phase 1 found to process in '{input_phase1_path}'.\")\n","        return False\n","    logger_phase2.log(f\"INFO: Found {len(files_to_process_p2)} Phase 1 output file(s) to process.\")\n","\n","    qid_collected_content_p2 = defaultdict(list)\n","    # ... (Rest of the `extract_and_aggregate_content_to_json` logic from previous message) ...\n","    # ... ensuring it uses `logger_phase2` and `clean_text_phase2` ...\n","    # ... and saves to `output_collated_json_file` ...\n","    # Key part is the loop:\n","    total_content_items_collected_p2 = 0\n","    summaries_collected_count_p2 = 0\n","    passages_collected_count_p2 = 0\n","    all_processed_response_ids_p2 = set()\n","\n","\n","    for file_idx, file_path in enumerate(files_to_process_p2):\n","        logger_phase2.log(f\"INFO: Processing Phase 1 file {file_idx + 1}/{len(files_to_process_p2)}: '{os.path.basename(file_path)}'...\")\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                phase1_output_data = json.load(f)\n","\n","            pdf_analysis_by_response_p2 = phase1_output_data.get(\"pdf_analysis_by_response\")\n","            if not pdf_analysis_by_response_p2:\n","                logger_phase2.log(f\"  WARN: 'pdf_analysis_by_response' key not found or empty in '{file_path}'.\")\n","                continue\n","\n","            for response_id, response_q_data in pdf_analysis_by_response_p2.items():\n","                all_processed_response_ids_p2.add(response_id)\n","                if not isinstance(response_q_data, dict): continue\n","\n","                for qid, q_content in response_q_data.items():\n","                    if not isinstance(q_content, dict): continue # Skip question text if it's just a string here\n","\n","                    content_item_p2 = {\n","                        \"source_file\": os.path.basename(file_path),\n","                        \"response_id\": response_id,\n","                        \"question_id\": qid, # Assuming qid is the actual QID key\n","                        \"type\": None, \"text\": None\n","                    }\n","                    text_to_add_p2 = None\n","                    summary_p2 = q_content.get(\"summary\")\n","                    if summary_p2 and isinstance(summary_p2, str) and summary_p2.strip():\n","                        text_to_add_p2 = clean_text_phase2(summary_p2)\n","                        content_item_p2[\"type\"] = \"pdf_summary\"\n","                        summaries_collected_count_p2 +=1\n","                    elif q_content.get(\"extracted_passages\"):\n","                        passages_p2 = q_content.get(\"extracted_passages\")\n","                        if passages_p2 and isinstance(passages_p2, list):\n","                            valid_passages_p2 = [p for p in passages_p2 if isinstance(p, str) and p.strip()]\n","                            if valid_passages_p2:\n","                                text_to_add_p2 = clean_text_phase2(\"\\n\\n\".join(valid_passages_p2))\n","                                content_item_p2[\"type\"] = \"pdf_passages\"\n","                                passages_collected_count_p2 +=1\n","\n","                    if text_to_add_p2:\n","                        content_item_p2[\"text\"] = text_to_add_p2\n","                        qid_collected_content_p2[qid].append(content_item_p2)\n","                        total_content_items_collected_p2 += 1\n","        except Exception as e:\n","            logger_phase2.log(f\"ERROR: Processing Phase 1 file '{file_path}' for Phase 2: {e}\")\n","            logger_phase2.log(traceback.format_exc())\n","\n","    final_output_data_p2 = {\n","        \"metadata\": {\n","            \"creation_timestamp\": time.strftime('%Y-%m-%d %H:%M:%S %Z', time.gmtime()),\n","            \"source_phase1_output_path\": input_phase1_path,\n","            \"total_phase1_files_processed\": len(files_to_process_p2),\n","            \"total_unique_qids_found\": len(qid_collected_content_p2),\n","            \"total_content_items_collated\": total_content_items_collected_p2,\n","            \"pdf_summaries_collated\": summaries_collected_count_p2,\n","            \"pdf_passages_collated\": passages_collected_count_p2,\n","            \"total_unique_responses_covered\": len(all_processed_response_ids_p2)\n","        },\n","        \"aggregated_pdf_content_by_qid\": dict(qid_collected_content_p2)\n","    }\n","    logger_phase2.log(f\"\\n--- Phase 2 Aggregation Summary ---\")\n","    for key, value in final_output_data_p2[\"metadata\"].items(): logger_phase2.log(f\"  {key}: {value}\")\n","\n","    try:\n","        with open(output_collated_json_file, 'w', encoding='utf-8') as outfile:\n","            json.dump(final_output_data_p2, outfile, indent=2, default=logger_phase2._get_serializer())\n","        logger_phase2.log(f\"INFO: Successfully saved Phase 2 collated PDF content to '{output_collated_json_file}'.\")\n","    except Exception as e:\n","        logger_phase2.log(f\"ERROR: Could not write Phase 2 output JSON: {e}\")\n","        logger_phase2.log(traceback.format_exc())\n","\n","    logger_phase2.log(\"=\" * 50); logger_phase2.log(\" INFO: Phase 2 (PDF Text Collation) Finished \"); logger_phase2.log(\"=\" * 50)\n","    return True\n","\n","# print(\"INFO: Cell 6 - Logic for Phase 2 (Stage 4a) defined.\")\n","# To run Phase 2:\n","# Make sure PHASE1_OUTPUT_DIR (or PHASE1_SINGLE_OUTPUT_FILE) contains the COMPLETE output from Phase 1\n","run_phase2_collate_pdf_texts(PHASE2_INPUT_PHASE1_OUTPUT_PATH, PHASE2_OUTPUT_COLLATED_JSON)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dg0xillcowVj"},"outputs":[],"source":["# @title Cell 7: Configuration for Phase 3 (LLM Thematic Analysis)\n","\n","# Input for Phase 3 is the output from Phase 2\n","PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON = PHASE2_OUTPUT_COLLATED_JSON # Use the output file name from Phase 2 config\n","\n","# Output file for Phase 3\n","PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON = os.path.join(BASE_PROJECT_DIR, f'phase3_pdf_thematic_points_{time.strftime(\"%Y%m%d_%H%M%S\")}.json')\n","PHASE3_LOG_FILE = os.path.join(BASE_PROJECT_DIR, f\"phase3_llm_thematic_analysis_{time.strftime('%Y%m%d_%H%M%S')}.log\")\n","\n","# LLM and Tokenizer for Thematic Analysis (can be same as Phase 1's LLM, or different)\n","PHASE3_LLM_MODEL_ID = 'google/gemma-3-4b-it' # Or PHASE1_LLM_MODEL_ID\n","PHASE3_USE_QUANTIZATION = False                # Or PHASE1_USE_QUANTIZATION\n","\n","# Thematic Analysis Parameters\n","PHASE3_NUM_THEMES_TO_REQUEST = 3\n","PHASE3_MAX_TOKENS_FOR_LLM_INPUT_CHUNK = 3000 # Max tokens to feed LLM in one go for map-reduce sub-problem\n","                                            # This is for the *text content*. Prompt will add more.\n","PHASE3_LLM_MAX_NEW_TOKENS_FOR_THEMES = 700  # Max tokens for LLM to generate for themes (e.g., 3 themes * (label + 2 sentences explanation))\n","                                            # Adjust based on NUM_THEMES_TO_REQUEST and desired explanation length\n","\n","# Optional: QID selection for thematic analysis (if not processing all)\n","PHASE3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"] # None # Set to a list of QIDs like [\"Q1\", \"Q4\"] to process only those, or None for all\n","\n","# Source for actual question texts (e.g., your CAR_35 or Stage 2 form output)\n","PHASE3_QUESTION_TEXT_SOURCE_FILE = os.path.join(BASE_PROJECT_DIR, 'comprehensive_analysis_report.json') # Example: CAR_35\n","# OR use PHASE1_STAGE2_AGGREGATION_FILE if that's more reliable for question texts\n","\n","print(\"INFO: Cell 7 - Configuration for Phase 3 (LLM Thematic Analysis) loaded.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dDd5Prxpa_r"},"outputs":[],"source":["# @title Cell 8: Helper Functions & Main Logic for Phase 3 (LLM Thematic Analysis)\n","\n","logger_phase3 = FileLogger(PHASE3_LOG_FILE)\n","\n","# --- Helper: LLM Call for Thematic Points ---\n","def get_llm_thematic_points(text_chunk_for_llm: str,\n","                            question_text_for_prompt: str,\n","                            num_themes: int,\n","                            llm_pipe: pipeline,\n","                            max_new_tokens: int,\n","                            qid_for_log: str):\n","    \"\"\"\n","    Sends a text chunk to the LLM to get thematic points.\n","    Returns the raw text output from the LLM.\n","    \"\"\"\n","    messages = [{\n","        \"role\": \"user\",\n","        \"content\": f\"You are an expert policy analyst. Carefully review all the following excerpts from PDF submissions responding to the question: '{question_text_for_prompt}'.\\n\"\n","                   f\"Based *only* on the provided excerpts, identify the top {num_themes} (e.g., three) most common and distinct points, arguments, or themes raised by the respondents.\\n\"\n","                   f\"For each point, provide a concise label for the point (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\\n\"\n","                   f\"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes} distinct common points are evident, list only those that are clear.\\n\\n\"\n","                   f\"Excerpts:\\n\\\"\\\"\\\"\\n{text_chunk_for_llm}\\n\\\"\\\"\\\"\\n\\n\"\n","                   f\"Top {num_themes} Common Points:\"\n","    }]\n","\n","    try:\n","        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(\n","            messages, tokenize=False, add_generation_prompt=True\n","        )\n","\n","        generation_args = {\n","            \"max_new_tokens\": max_new_tokens,\n","            \"do_sample\": False,\n","            \"pad_token_id\": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id,\n","        }\n","        if PHASE1_ENABLE_DEBUG_LOGGING: # Use a global debug flag or pass one\n","             logger_phase3.log(f\"DEBUG: Phase 3 - LLM call for QID {qid_for_log}. Prompt length (chars): {len(prompt_formatted)}, Input text length (chars): {len(text_chunk_for_llm)}\")\n","\n","        outputs = llm_pipe(prompt_formatted, **generation_args)\n","\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger_phase3.log(f\"ERROR: Phase 3 - LLM pipeline returned unexpected/empty output for QID {qid_for_log}.\")\n","            return \"LLM_ERROR: No output generated.\"\n","\n","        generated_text_full = outputs[0]['generated_text']\n","\n","        llm_response_text = \"\"\n","        if generated_text_full.startswith(prompt_formatted):\n","            llm_response_text = generated_text_full[len(prompt_formatted):].strip()\n","        else:\n","            response_marker = \"<start_of_turn>model\\n\"\n","            summary_start_index = generated_text_full.rfind(response_marker)\n","            if summary_start_index != -1:\n","                llm_response_text = generated_text_full[summary_start_index + len(response_marker):].strip()\n","            else:\n","                llm_response_text = generated_text_full # Take raw if unsure\n","                logger_phase3.log(f\"WARN: Phase 3 - Could not reliably strip prompt for QID {qid_for_log}.\")\n","\n","        return llm_response_text\n","\n","    except Exception as e:\n","        logger_phase3.log(f\"ERROR: Phase 3 - Exception during LLM call for QID {qid_for_log}: {e}\")\n","        logger_phase3.log(traceback.format_exc())\n","        return f\"LLM_ERROR: Exception during generation - {str(e)}\"\n","\n","# --- Helper: Parse LLM Thematic Output ---\n","def parse_llm_themes(llm_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:\n","    \"\"\"\n","    Parses the LLM's text output to extract structured themes.\n","    This is a basic parser and might need significant refinement based on actual LLM output.\n","    \"\"\"\n","    parsed_themes = []\n","    if not llm_output_text or llm_output_text.startswith(\"LLM_ERROR:\"):\n","        return [{\"theme_label\": \"Parsing Error\", \"explanation\": llm_output_text}]\n","\n","    # Attempt to find numbered points\n","    # Regex to find \"X. Theme Label: [Label]\" and \"Explanation X: [Explanation]\"\n","    # This regex is complex and highly dependent on consistent LLM output.\n","    # It tries to capture: number, label, and explanation separately.\n","    # Pattern assumes \"Theme Label X:\" and \"Explanation X:\" might appear.\n","    # Simpler: look for \"1.\", \"2.\", \"3.\" and then try to find \"Theme Label:\" and \"Explanation:\" after each.\n","\n","    # Simpler approach: Split by common theme delimiters if LLM is consistent\n","    # For example, if LLM outputs \"1. Theme Label: ... Explanation: ...\" then \"2. Theme Label: ...\"\n","\n","    # Let's try a line-by-line approach looking for keywords\n","    current_theme_label = None\n","    current_explanation = \"\"\n","\n","    # Normalize potential list markers (e.g., \"1.\", \"1)\", \"1-\")\n","    lines = llm_output_text.splitlines()\n","    theme_item = {}\n","\n","    for line in lines:\n","        line_stripped = line.strip()\n","        if not line_stripped:\n","            continue\n","\n","        # Try to find a new theme item marker (e.g., \"1.\", \"2. Theme Label:\")\n","        # More robustly: if line starts with a number followed by . or )\n","        # and (contains \"Theme Label\" or we just assume it's a new theme if previous one had explanation)\n","        match_new_theme = re.match(r\"^\\s*(\\d+[\\.\\)\\-]?)\\s*(?:Theme Label\\s*[:\\-]?\\s*)?(.*)\", line_stripped, re.IGNORECASE)\n","\n","        if match_new_theme:\n","            if theme_item.get(\"theme_label\"): # Save previous theme if fully formed\n","                if not theme_item.get(\"explanation\"): theme_item[\"explanation\"] = current_explanation.strip()\n","                if theme_item.get(\"theme_label\") or theme_item.get(\"explanation\"): # Ensure it's not empty\n","                    parsed_themes.append(theme_item)\n","\n","            theme_item = {} # Start new theme\n","            current_explanation = \"\"\n","            label_candidate = match_new_theme.group(2).strip()\n","\n","            # If \"Explanation:\" is also on this line after the label\n","            if \"explanation:\" in label_candidate.lower():\n","                parts = re.split(r\"explanation\\s*[:\\-]\", label_candidate, maxsplit=1, flags=re.IGNORECASE)\n","                theme_item[\"theme_label\"] = parts[0].strip()\n","                current_explanation = parts[1].strip() if len(parts) > 1 else \"\"\n","            else:\n","                theme_item[\"theme_label\"] = label_candidate\n","\n","        elif \"explanation:\" in line_stripped.lower():\n","            # If we haven't captured a label yet for the current theme_item,\n","            # then previous non-explanation lines might constitute the label.\n","            if theme_item and not theme_item.get(\"theme_label\") and current_explanation.strip():\n","                theme_item[\"theme_label\"] = current_explanation.strip()\n","                current_explanation = \"\" # Reset current_explanation as it was actually the label\n","\n","            current_explanation += re.split(r\"explanation\\s*[:\\-]\", line_stripped, maxsplit=1, flags=re.IGNORECASE)[-1].strip() + \" \"\n","\n","        elif theme_item: # If we are in a theme_item (label started) and it's not an explanation line start\n","            current_explanation += line_stripped + \" \" # Append to current explanation or potential label\n","\n","    # Add the last processed theme\n","    if theme_item.get(\"theme_label\"):\n","        if not theme_item.get(\"explanation\"): theme_item[\"explanation\"] = current_explanation.strip()\n","        if theme_item.get(\"theme_label\") or theme_item.get(\"explanation\"):\n","            parsed_themes.append(theme_item)\n","    elif current_explanation.strip() and not parsed_themes: # case where output is just a single block of text\n","        parsed_themes.append({\"theme_label\": \"General Theme\", \"explanation\": current_explanation.strip()})\n","\n","\n","    if not parsed_themes and llm_output_text: # Fallback if no structured themes found\n","        parsed_themes.append({\"theme_label\": \"Unparsed LLM Output\", \"explanation\": llm_output_text})\n","\n","    return parsed_themes[:num_expected_themes] # Return up to the number of expected themes\n","\n","# --- Main Function for Phase 3 ---\n","def run_phase3_llm_thematic_analysis():\n","    global logger_phase3\n","    logger_phase3.log(\"=\" * 50); logger_phase3.log(\" INFO: Starting Phase 3: LLM Thematic Analysis per QID\"); logger_phase3.log(\"=\" * 50)\n","    logger_phase3.log(f\"INFO: Input Collated PDF Texts (from Phase 2/Stage 4a): {PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON}\")\n","    logger_phase3.log(f\"INFO: LLM Model for Thematic Analysis: {PHASE3_LLM_MODEL_ID}\")\n","    logger_phase3.log(f\"INFO: Output Thematic Analysis File: {PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}\")\n","\n","    # --- Load LLM for Thematic Analysis ---\n","    llm_tokenizer_p3 = None\n","    llm_model_p3 = None\n","    llm_thematic_pipe_p3 = None\n","    # (Reuse LLM loading logic from Phase 1, adapting variable names)\n","    try:\n","        logger_phase3.log(f\"INFO: Loading LLM tokenizer for Phase 3: {PHASE3_LLM_MODEL_ID}...\")\n","        llm_tokenizer_p3 = AutoTokenizer.from_pretrained(PHASE3_LLM_MODEL_ID)\n","        if getattr(llm_tokenizer_p3, 'pad_token', None) is None and getattr(llm_tokenizer_p3, 'eos_token', None):\n","            llm_tokenizer_p3.pad_token = llm_tokenizer_p3.eos_token\n","            llm_tokenizer_p3.padding_side = \"left\"\n","        logger_phase3.log(\"INFO: Phase 3 LLM Tokenizer loaded.\")\n","\n","        bnb_config_p3 = None\n","        current_use_quantization_p3 = PHASE3_USE_QUANTIZATION # Use phase-specific config\n","        if current_use_quantization_p3 and torch.cuda.is_available():\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config_p3 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n","                logger_phase3.log(f\"INFO: Phase 3 - 4-bit quantization configured (Compute dtype: {compute_dtype}).\")\n","            except Exception as q_err:\n","                logger_phase3.log(f\"ERROR: Phase 3 - Failed BitsAndBytesConfig: {q_err}. Disabling quantization.\")\n","                current_use_quantization_p3 = False\n","\n","        logger_phase3.log(f\"INFO: Loading Phase 3 LLM model: {PHASE3_LLM_MODEL_ID} (Quantization: {current_use_quantization_p3})...\")\n","        llm_model_p3 = AutoModelForCausalLM.from_pretrained(\n","            PHASE3_LLM_MODEL_ID,\n","            quantization_config=bnb_config_p3 if current_use_quantization_p3 else None,\n","            torch_dtype=torch.bfloat16 if not current_use_quantization_p3 else None,\n","            device_map=\"auto\", trust_remote_code=True\n","        )\n","        logger_phase3.log(\"INFO: Phase 3 LLM model loaded.\")\n","\n","        llm_thematic_pipe_p3 = pipeline(\"text-generation\", model=llm_model_p3, tokenizer=llm_tokenizer_p3)\n","        logger_phase3.log(\"INFO: Phase 3 LLM text-generation pipeline created successfully.\")\n","    except Exception as load_err:\n","        logger_phase3.log(f\"CRITICAL: Failed to load LLM/pipeline for Phase 3: {load_err}\")\n","        logger_phase3.log(traceback.format_exc())\n","        return False # Cannot proceed\n","\n","    # --- Load Collated Texts from Phase 2 (Stage 4a) ---\n","    try:\n","        with open(PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:\n","            phase2_data = json.load(f)\n","        collated_texts_by_qid_p3 = phase2_data.get(\"aggregated_pdf_content_by_qid\", {}) # Ensure correct key\n","        if not collated_texts_by_qid_p3:\n","            logger_phase3.log(f\"ERROR: No 'aggregated_pdf_content_by_qid' found in {PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON}\")\n","            return False\n","        logger_phase3.log(f\"INFO: Loaded collated PDF texts for {len(collated_texts_by_qid_p3)} QIDs from Phase 2 output.\")\n","    except Exception as e:\n","        logger_phase3.log(f\"FATAL: Could not load Phase 2 collated texts file: {PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON}. Error: {e}\")\n","        return False\n","\n","    # --- Load Question Texts for Prompts ---\n","    qid_to_actual_question_text_p3 = {}\n","    try:\n","        with open(PHASE3_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:\n","            qtext_source_data = json.load(f_qtext)\n","        # Adjust key based on the source file (CAR_35 or Stage 2 form output)\n","        if \"analysis_by_question\" in qtext_source_data: # From CAR_35\n","            for qid, data in qtext_source_data.get(\"analysis_by_question\", {}).items():\n","                qid_to_actual_question_text_p3[qid] = data.get(\"question_text\", f\"Unknown text for {qid}\")\n","        elif \"question_data\" in qtext_source_data: # From Stage 2 form output\n","             for qid, data in qtext_source_data.get(\"question_data\", {}).items():\n","                qid_to_actual_question_text_p3[qid] = data.get(\"question_text\", f\"Unknown text for {qid}\")\n","        logger_phase3.log(f\"INFO: Loaded actual question texts for {len(qid_to_actual_question_text_p3)} QIDs.\")\n","    except Exception as e:\n","        logger_phase3.log(f\"WARN: Could not load question texts from {PHASE3_QUESTION_TEXT_SOURCE_FILE}. Prompts will use QID only. Error: {e}\")\n","\n","\n","    # --- Main Loop: Iterate QID by QID for Thematic Analysis ---\n","    overall_thematic_results_p3 = {}\n","    if os.path.exists(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON):\n","        try:\n","            with open(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON, 'r', encoding='utf-8') as f_exist:\n","                overall_thematic_results_p3 = json.load(f_exist)\n","            logger_phase3.log(f\"INFO: Loaded {len(overall_thematic_results_p3)} existing thematic results from {PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}\")\n","        except Exception as e_load_exist:\n","            logger_phase3.log(f\"WARN: Could not load existing results file {PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}. Starting fresh. Error: {e_load_exist}\")\n","\n","    qids_for_processing_p3 = PHASE3_QIDS_TO_PROCESS_THEMATICALLY if PHASE3_QIDS_TO_PROCESS_THEMATICALLY else sorted(collated_texts_by_qid_p3.keys())\n","\n","    for qid_idx, qid in enumerate(qids_for_processing_p3):\n","        logger_phase3.log(f\"\\n--- Processing Thematic Analysis for QID {qid_idx + 1}/{len(qids_for_processing_p3)}: {qid} ---\")\n","\n","        if qid in overall_thematic_results_p3 and overall_thematic_results_p3[qid].get(\"status\", \"\").startswith(\"success\"):\n","            logger_phase3.log(f\"INFO: QID {qid} already successfully processed. Skipping.\")\n","            continue\n","\n","        question_actual_text_p3 = qid_to_actual_question_text_p3.get(qid, f\"Question {qid}\")\n","\n","        text_items_for_this_qid_p3 = collated_texts_by_qid_p3.get(qid, [])\n","        if not text_items_for_this_qid_p3:\n","            logger_phase3.log(f\"WARN: No collated texts found for QID {qid}. Skipping thematic analysis for it.\")\n","            overall_thematic_results_p3[qid] = {\"question_text\": question_actual_text_p3, \"status\": \"no_collated_text_for_qid\", \"themes\": []}\n","            continue\n","\n","        # Combine texts for this QID\n","        # Each item in text_items_for_this_qid_p3 is {'source_file': ..., 'response_id': ..., 'question_id': ..., 'type': ..., 'text': ...}\n","        texts_to_combine_p3 = [item['text'] for item in text_items_for_this_qid_p3 if item.get('text')]\n","        if not texts_to_combine_p3:\n","            logger_phase3.log(f\"WARN: All text items for QID {qid} were empty. Skipping.\")\n","            overall_thematic_results_p3[qid] = {\"question_text\": question_actual_text_p3, \"status\": \"all_texts_empty_for_qid\", \"themes\": []}\n","            continue\n","\n","        current_qid_full_text_chunk = \"\\n\\n--- New Excerpt ---\\n\".join(texts_to_combine_p3)\n","\n","        # --- Context Window Management & LLM Call ---\n","        # This is where the map-reduce logic would be implemented if current_qid_full_text_chunk is too large\n","        # For now, simplified: if too large, it will likely error or truncate in the LLM call.\n","        # A robust implementation needs chunking of current_qid_full_text_chunk,\n","        # then map (LLM themes per chunk), then reduce (LLM themes of themes).\n","\n","        llm_raw_themes_output = get_llm_thematic_points(\n","            current_qid_full_text_chunk,\n","            question_actual_text_p3,\n","            PHASE3_NUM_THEMES_TO_REQUEST,\n","            llm_thematic_pipe_p3,\n","            PHASE3_LLM_MAX_NEW_TOKENS_FOR_THEMES,\n","            qid\n","        )\n","\n","        parsed_themes_list = parse_llm_themes(llm_raw_themes_output, PHASE3_NUM_THEMES_TO_REQUEST)\n","\n","        status_p3 = \"success_thematic_analysis\"\n","        if llm_raw_themes_output.startswith(\"LLM_ERROR:\"):\n","            status_p3 = \"error_llm_theme_generation\"\n","        elif not parsed_themes_list or (len(parsed_themes_list) == 1 and parsed_themes_list[0][\"theme_label\"] == \"Unparsed LLM Output\"):\n","            status_p3 = \"llm_output_parsing_failed_or_empty\"\n","\n","        overall_thematic_results_p3[qid] = {\n","            \"question_text\": question_actual_text_p3,\n","            \"status\": status_p3,\n","            \"total_pdf_text_items_for_qid\": len(texts_to_combine_p3),\n","            \"approx_char_length_fed_to_llm\": len(current_qid_full_text_chunk), # Add this\n","            \"llm_raw_output\": llm_raw_themes_output,\n","            \"parsed_top_themes\": parsed_themes_list\n","        }\n","\n","        # Save after each QID\n","        try:\n","            with open(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON, 'w', encoding='utf-8') as f_out_p3:\n","                json.dump(overall_thematic_results_p3, f_out_p3, indent=2, default=logger_phase3._get_serializer())\n","            logger_phase3.log(f\"INFO: Saved thematic analysis results to '{PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}' after QID {qid}.\")\n","        except Exception as e_save_p3:\n","            logger_phase3.log(f\"ERROR: Could not save intermediate results for Phase 3: {e_save_p3}\")\n","\n","        gc.collect()\n","        if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","    # --- Final Save (redundant if saving after each QID, but good for final confirmation) ---\n","    # ... (same save logic as above) ...\n","    logger_phase3.log(\"=\" * 50); logger_phase3.log(\" INFO: Phase 3 (LLM Thematic Analysis) Finished \"); logger_phase3.log(\"=\" * 50)\n","    return True\n","\n","\n","# print(\"INFO: Cell 8 - Logic for Phase 3 (LLM Thematic Analysis) defined.\")\n","# To run Phase 3:\n","# Make sure PHASE2_OUTPUT_COLLATED_JSON points to the COMPLETE output from Phase 2\n","# run_phase3_llm_thematic_analysis()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlJahb4mpk0R"},"outputs":[],"source":["# @title Cell 9: Main Orchestration Cell\n","\n","if __name__ == \"__main__\": # Standard guard\n","    # --- Phase 1 Execution ---\n","    # IMPORTANT: This phase needs to run to completion for ALL ~485 responses.\n","    # Due to its length, you might run it separately or ensure its state file\n","    # (PHASE1_STATE_FILE) is robustly managed for resumption.\n","    # If PHASE1_SINGLE_OUTPUT_FILE already exists and is complete, you can optionally skip.\n","\n","    PHASE1_COMPLETE = False # Set to True if you know Phase 1 output is complete and up-to-date\n","    if os.path.exists(PHASE1_SINGLE_OUTPUT_FILE): # Or check based on your chunking strategy\n","        # Add a more robust check if possible, e.g., number of responses in state file\n","        # For now, simple existence check\n","        # user_confirms_complete = input(f\"INFO: Phase 1 output file '{PHASE1_SINGLE_OUTPUT_FILE}' exists. Assume it's complete for all responses? (yes/no): \")\n","        # if user_confirms_complete.lower() == 'yes':\n","        #     PHASE1_COMPLETE = True\n","        #     logger_master.log(\"INFO: User confirmed Phase 1 output is complete. Skipping Phase 1 execution.\")\n","        # For automated runs, you might need a flag or a more sophisticated check.\n","        # For now, if the main output file exists, let's assume we might not need to run it,\n","        # BUT IT'S SAFER TO RUN IT if you're unsure, as it uses a state file to resume.\n","        # For this example, we'll assume it needs to be run or resumed.\n","        print(f\"INFO: Phase 1 output file '{PHASE1_SINGLE_OUTPUT_FILE}' target. Script will use state to resume/run.\")\n","\n","\n","    print(\"\\n>>> EXECUTING PHASE 1: PDF Content Extraction (Original Stage 4 Logic) <<<\")\n","    # Ensure logger_phase1 is re-initialized if you run this cell multiple times without kernel restart\n","    logger_phase1 = FileLogger(PHASE1_LOG_FILE)\n","    phase1_success = run_phase1_pdf_extraction()\n","    if not phase1_success:\n","        logger_phase1.log(\"CRITICAL: Phase 1 failed or did not complete. Subsequent phases may use incomplete data or fail.\")\n","        # Decide whether to stop the entire pipeline here\n","        # sys.exit(\"Phase 1 failed.\")\n","        print(\"CRITICAL: Phase 1 failed. Please check logs. Stopping further execution in this orchestration.\")\n","    else:\n","        print(\">>> PHASE 1 COMPLETED SUCCESSFULLY <<<\\n\")\n","\n","        # --- Phase 2 Execution ---\n","        # This runs only if Phase 1 was successful (or deemed complete)\n","        print(\"\\n>>> EXECUTING PHASE 2: PDF Text Collation (Stage 4a Logic) <<<\")\n","        logger_phase2 = FileLogger(PHASE2_LOG_FILE)\n","        # Determine the correct input for Phase 2\n","        # If Phase 1 produces one file, use PHASE1_SINGLE_OUTPUT_FILE\n","        # If Phase 1 produces chunks into PHASE1_OUTPUT_DIR, use that directory\n","        input_for_phase2 = PHASE1_SINGLE_OUTPUT_FILE if os.path.exists(PHASE1_SINGLE_OUTPUT_FILE) else PHASE1_OUTPUT_DIR\n","\n","        phase2_success = run_phase2_collate_pdf_texts(input_for_phase2, PHASE2_OUTPUT_COLLATED_JSON)\n","        if not phase2_success:\n","            logger_phase2.log(\"CRITICAL: Phase 2 failed. Cannot proceed to Phase 3.\")\n","            print(\"CRITICAL: Phase 2 failed. Please check logs. Stopping further execution.\")\n","        else:\n","            print(\">>> PHASE 2 COMPLETED SUCCESSFULLY <<<\\n\")\n","\n","            # --- Phase 3 Execution ---\n","            # This runs only if Phase 2 was successful\n","            print(\"\\n>>> EXECUTING PHASE 3: LLM Thematic Analysis per QID <<<\")\n","            logger_phase3 = FileLogger(PHASE3_LOG_FILE)\n","            phase3_success = run_phase3_llm_thematic_analysis()\n","            if not phase3_success:\n","                logger_phase3.log(\"ERROR: Phase 3 thematic analysis encountered issues.\")\n","                print(\"ERROR: Phase 3 thematic analysis encountered issues. Please check logs.\")\n","            else:\n","                print(\">>> PHASE 3 COMPLETED SUCCESSFULLY <<<\\n\")\n","                print(\"PIPELINE EXECUTION FINISHED.\")\n","\n","    # Final message\n","    master_log_file_for_orchestration = os.path.join(BASE_PROJECT_DIR, \"pipeline_orchestration_main.log\")\n","    master_logger = FileLogger(master_log_file_for_orchestration) # For overall pipeline status\n","    master_logger.log(\"=\"*50)\n","    master_logger.log(\"Overall Pipeline Orchestration Attempt Concluded.\")\n","    master_logger.log(f\"Phase 1 Success: {phase1_success if 'phase1_success' in locals() else 'Not Run or Failed Early'}\")\n","    master_logger.log(f\"Phase 2 Success: {phase2_success if 'phase2_success' in locals() else 'Not Run'}\")\n","    master_logger.log(f\"Phase 3 Success: {phase3_success if 'phase3_success' in locals() else 'Not Run'}\")\n","    master_logger.log(\"Please check individual phase log files for details.\")\n","    master_logger.log(\"=\"*50)\n","\n","print(\"INFO: Cell 9 - Main Orchestration Cell defined. Execute this cell to run the pipeline.\")"]},{"cell_type":"markdown","metadata":{"id":"7HZI3TRiYsdO"},"source":["# 15th May"]},{"cell_type":"markdown","metadata":{"id":"phsmYf8A8tJ4"},"source":["## Stage 4a-Helper - Draft Keyphrase Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SUMW0XVq8f0I"},"outputs":[],"source":["import json\n","import os\n","import time\n","from collections import Counter\n","import re\n","# For more advanced NLP if you want to try it:\n","# import spacy\n","# from nltk.corpus import stopwords\n","# from sklearn.feature_extraction.text import CountVectorizer\n","\n","# (Assume FileLogger class is defined and initialized as 'logger')\n","# Example:\n","# SCRIPT_LOG_FILE_KP_HELPER = 'stage4a_keyphrase_helper.log'\n","# logger = FileLogger(SCRIPT_LOG_FILE_KP_HELPER)\n","\n","\n","def preprocess_for_candidates(text, remove_stopwords=True):\n","    text = text.lower()\n","    # Remove typical summarization intro phrases more aggressively\n","    text = re.sub(r\"here.s a \\d-sentence summary of the responses?,? capturing the core themes:\", \"\", text)\n","    text = re.sub(r\"here.s a \\d-sentence summary of the response, capturing the core themes:\", \"\", text)\n","    text = re.sub(r\"here.s a \\d-sentence summary, capturing the core points:\", \"\", text)\n","    text = re.sub(r\"this response doesn't address the specific question.*\", \"\", text, flags=re.IGNORECASE)\n","    text = re.sub(r\"please provide the responses.*\", \"\", text, flags=re.IGNORECASE)\n","\n","    text = re.sub(r'[^\\w\\s-]', '', text) # Keep hyphens for compound words, remove other punctuation\n","    text = re.sub(r'\\d+', '', text)     # Remove numbers\n","\n","    # More aggressive stopword removal for keyphrase candidates\n","    # if remove_stopwords:\n","    #     # Consider a domain-specific stopword list + standard English\n","    #     stop_words = set(stopwords.words('english'))\n","    #     # Add more common but uninformative words from your domain if needed\n","    #     # e.g., stop_words.update([\"proposal\", \"act\", \"privacy\", \"individual\", \"rights\"])\n","    #     # - be careful not to remove actual keywords\n","    #     words = text.split()\n","    #     words = [word for word in words if word not in stop_words and len(word) > 2]\n","    #     text = \" \".join(words)\n","\n","    text = re.sub(r'\\s+', ' ', text).strip() # Normalize whitespace\n","    return text\n","\n","def extract_candidate_phrases_ngrams(texts, n_values=(2, 3, 4), top_k=15):\n","    \"\"\"Extracts top n-grams as candidate phrases.\"\"\"\n","    candidate_phrases = defaultdict(list)\n","    for n in n_values:\n","        all_n_grams = []\n","        for text in texts:\n","            processed_text = preprocess_for_candidates(text, remove_stopwords=False) # Keep stopwords for natural phrases initially\n","            words = processed_text.split()\n","            if len(words) < n:\n","                continue\n","            for i in range(len(words) - n + 1):\n","                ngram = \" \".join(words[i:i+n])\n","                # Filter out n-grams that are likely just common function words or too short\n","                if len(ngram.replace(\" \", \"\")) > 5 : # Heuristic: at least 5 non-space characters\n","                    all_n_grams.append(ngram)\n","\n","        # Filter n-grams that are substrings of longer, more frequent n-grams (simple version)\n","        counts = Counter(all_n_grams)\n","        top_phrases_for_n = []\n","        # Sort by count, then by length (longer preferred if count is same)\n","        sorted_phrases = sorted(counts.items(), key=lambda item: (-item[1], -len(item[0])))\n","\n","        added_phrases_set = set() # To avoid adding shorter versions if a longer one covers it\n","        for phrase, count in sorted_phrases:\n","            is_substring = False\n","            for added_p in added_phrases_set:\n","                if phrase in added_p:\n","                    is_substring = True\n","                    break\n","            if not is_substring:\n","                top_phrases_for_n.append((phrase, count))\n","                added_phrases_set.add(phrase)\n","            if len(top_phrases_for_n) >= top_k:\n","                break\n","        candidate_phrases[f'{n}-grams'] = top_phrases_for_n\n","    return candidate_phrases\n","\n","# If using spaCy for Noun Phrase Chunks (more advanced, requires spaCy installation and model)\n","# def extract_candidate_phrases_spacy_np(texts, nlp_spacy, top_k=20):\n","#     all_noun_phrases = []\n","#     for text in texts:\n","#         # Limit text length for spaCy if it's very long\n","#         doc = nlp_spacy(text[:5000]) # Process first 5000 chars\n","#         for chunk in doc.noun_chunks:\n","#             # Filter noun chunks - e.g., not too short, not just pronouns/determiners\n","#             if len(chunk.text.split()) > 1 and len(chunk.text) > 3:\n","#                 all_noun_phrases.append(chunk.text.lower().strip())\n","#     return Counter(all_noun_phrases).most_common(top_k)\n","\n","\n","def generate_draft_keyphrases_for_qid(qid, texts_for_qid, sample_size=20, nlp_spacy_model=None):\n","    \"\"\"\n","    Generates draft keyphrases for a single QID.\n","    - Provides a sample of texts for human/LLM review.\n","    - Extracts candidate n-grams.\n","    - Optionally, extracts noun phrases if spaCy is used.\n","    \"\"\"\n","    output = {\n","        \"qid\": qid,\n","        \"sample_texts_for_review\": [],\n","        \"candidate_ngrams\": {},\n","        # \"candidate_noun_phrases_spacy\": [] # If using spaCy\n","    }\n","\n","    if not texts_for_qid:\n","        return output\n","\n","    # Take a sample of texts (e.g., first N, or random N)\n","    # Prioritize summaries in the sample if type information is available\n","    summaries_sample = [item['text'] for item in texts_for_qid if item.get('type') == 'summary' and item.get('text')]\n","    passages_sample = [item['text'] for item in texts_for_qid if item.get('type') == 'passages' and item.get('text')]\n","\n","    combined_sample = summaries_sample[:sample_size//2] # Prioritize summaries\n","    remaining_needed = sample_size - len(combined_sample)\n","    if remaining_needed > 0:\n","        combined_sample.extend(passages_sample[:remaining_needed])\n","\n","    if not combined_sample and texts_for_qid: # Fallback if no type info or no summaries\n","        combined_sample = [item['text'] for item in texts_for_qid[:sample_size] if item.get('text')]\n","\n","    output[\"sample_texts_for_review\"] = combined_sample\n","\n","    # Candidate N-grams\n","    if combined_sample: # Use the sample for n-gram generation to speed things up\n","        output[\"candidate_ngrams\"] = extract_candidate_phrases_ngrams(combined_sample, n_values=(2,3,4), top_k=15)\n","\n","    # Candidate Noun Phrases (Optional, using spaCy)\n","    # if nlp_spacy_model and combined_sample:\n","    #     output[\"candidate_noun_phrases_spacy\"] = extract_candidate_phrases_spacy_np(combined_sample, nlp_spacy_model, top_k=20)\n","\n","    return output\n","\n","\n","# --- Main execution block for the Keyphrase Helper Script ---\n","if __name__ == \"__main__\":\n","    # --- Configuration ---\n","    # Ensure FileLogger is initialized if you uncomment logger lines\n","    SCRIPT_LOG_FILE_KP_HELPER = 'stage4a_keyphrase_helper.log'\n","    logger = FileLogger(SCRIPT_LOG_FILE_KP_HELPER)\n","\n","    INPUT_COLLATED_TEXTS_JSON = 'stage4a_collated_texts_for_thematic_analysis_20250515_060005.json' # From Stage 4a\n","    OUTPUT_DRAFT_KEYPHRASES_JSON = 'stage4a_helper_draft_keyphrases.json'\n","\n","    SAMPLE_SIZE_PER_QID = 10 # Number of text samples to include for review for each QID\n","\n","    # Optional: spaCy for Noun Phrase chunking (more advanced)\n","    # try:\n","    #     import spacy\n","    #     # You might need to download the model if running for the first time in Colab:\n","    #     # !python -m spacy download en_core_web_sm\n","    #     nlp_spacy = spacy.load(\"en_core_web_sm\")\n","    #     logger.log(\"INFO: spaCy model loaded for Noun Phrase extraction.\")\n","    # except ImportError:\n","    #     logger.log(\"WARN: spaCy not installed or model not found. Noun Phrase extraction will be skipped.\")\n","    #     nlp_spacy = None\n","    nlp_spacy = None # Keeping it simple for now\n","\n","    # --- Load data from Stage 4a ---\n","    try:\n","        with open(INPUT_COLLATED_TEXTS_JSON, 'r', encoding='utf-8') as f:\n","            stage4a_data = json.load(f)\n","        collated_content_by_qid = stage4a_data.get(\"aggregated_content_by_qid\", {})\n","        # metadata_from_4a = stage4a_data.get(\"metadata\", {}) # For question texts if stored there\n","    except Exception as e:\n","        logger.log(f\"FATAL: Could not load Stage 4a input JSON '{INPUT_COLLATED_TEXTS_JSON}': {e}\")\n","        print(f\"FATAL: Could not load Stage 4a input JSON '{INPUT_COLLATED_TEXTS_JSON}': {e}\")\n","        # import sys; sys.exit(1) # if running as standalone script\n","        raise # if running in Colab, let the error stop the cell\n","\n","    if not collated_content_by_qid:\n","        logger.log(f\"FATAL: No 'aggregated_content_by_qid' found in {INPUT_COLLATED_TEXTS_JSON}. Cannot proceed.\")\n","        print(f\"FATAL: No 'aggregated_content_by_qid' found in {INPUT_COLLATED_TEXTS_JSON}. Cannot proceed.\")\n","        raise ValueError(\"Missing aggregated content in input file.\")\n","\n","\n","    all_qids_draft_keyphrases = {}\n","    qids_to_process = sorted(collated_content_by_qid.keys())\n","\n","    logger.log(f\"INFO: Starting draft keyphrase generation for {len(qids_to_process)} QIDs.\")\n","\n","    for i, qid in enumerate(qids_to_process):\n","        logger.log(f\"INFO: Generating draft keyphrases for QID {i+1}/{len(qids_to_process)}: {qid}...\")\n","        texts_for_this_qid = collated_content_by_qid.get(qid, [])\n","\n","        # Each item in texts_for_this_qid is like:\n","        # {'source_file': 'file1.json', 'response_id': 'id_A', 'type': 'summary', 'text': '...'}\n","\n","        draft_output_for_qid = generate_draft_keyphrases_for_qid(\n","            qid,\n","            texts_for_this_qid,\n","            sample_size=SAMPLE_SIZE_PER_QID,\n","            nlp_spacy_model=nlp_spacy\n","        )\n","        all_qids_draft_keyphrases[qid] = draft_output_for_qid\n","        logger.log(f\"INFO: Draft keyphrases generated for QID: {qid}. Sample texts: {len(draft_output_for_qid['sample_texts_for_review'])}, N-grams generated.\")\n","\n","    # --- Save the draft keyphrases ---\n","    logger.log(f\"\\nINFO: Saving all draft keyphrases to '{OUTPUT_DRAFT_KEYPHRASES_JSON}'...\")\n","    try:\n","        final_output_structure = {\n","            \"metadata\": {\n","                \"generation_timestamp\": time.strftime('%Y-%m-%d %H:%M:%S %Z', time.gmtime()),\n","                \"source_collated_texts_file\": INPUT_COLLATED_TEXTS_JSON,\n","                \"sample_size_per_qid_for_review_texts\": SAMPLE_SIZE_PER_QID,\n","                \"notes\": \"This file contains DRAFT keyphrase suggestions and sample texts. MANUAL REVIEW AND REFINEMENT IS ESSENTIAL before using these in the main thematic analysis.\"\n","            },\n","            \"draft_keyphrases_by_qid\": all_qids_draft_keyphrases\n","        }\n","        with open(OUTPUT_DRAFT_KEYPHRASES_JSON, 'w', encoding='utf-8') as f_out:\n","            json.dump(final_output_structure, f_out, indent=2, default=logger._get_serializer()) # Using logger's serializer\n","        logger.log(f\"INFO: Successfully saved draft keyphrases to '{OUTPUT_DRAFT_KEYPHRASES_JSON}'.\")\n","    except Exception as e:\n","        logger.log(f\"ERROR: Could not write draft keyphrases JSON output: {e}\")\n","        logger.log(f\"TRACEBACK:\\n{traceback.format_exc()}\")\n","\n","    logger.log(\"\\nINFO: Draft keyphrase generation process complete.\")\n","\n","# Remember to update paths and ensure FileLogger is available when running in Colab."]},{"cell_type":"markdown","metadata":{"id":"O274pCiZxztx"},"source":["## Stage 4b: Thematic Analysis\n","\n","You are absolutely right to think that parts of what I described can be automated or at least semi-automated in another script! This would be a logical \"Stage 4b\" or \"Stage 5 - Thematic Analysis Module.\"\n","\n","Let's break down which parts are more amenable to automation and which still benefit greatly from human oversight, especially for a nuanced task like identifying \"common points\" in qualitative data.\n","\n","**Parts That Can Be Automated or Semi-Automated in a New Script:**\n","\n","1.  **Loading Data:**\n","    *   Loading the `stage4a_collated_texts_for_thematic_analysis_...json` file.\n","    *   Selecting a specific QID to analyze.\n","    *   Extracting the list of text items for that QID.\n","\n","2.  **Initial Filtering (Automated with Rules/Keywords):**\n","    *   **Removing Placeholder/Error Summaries:**\n","        *   Script can automatically filter out text items where `text` matches known placeholders like \"Please provide the responses...\" or \"This response doesn't address the specific question...\"\n","    *   **Filtering Potentially Off-Topic Summaries (Heuristic):**\n","        *   This is trickier but can be attempted. For Q4 (\"...exceptions...in the employment context?\"), you could define:\n","            *   **Positive keywords:** `employ`, `worker`, `staff`, `workplace`, `HR`, `personnel`, `exception`, `right`, `APP12`, `APP13`, `proposal 18`\n","            *   **Negative keywords (for Q4):** `small business support`, `child`, `age verification` (if these consistently indicate a summary meant for another question).\n","        *   The script could flag summaries that have a low count of positive keywords or a high count of negative keywords for a given QID for manual review, or even automatically set them aside. This is a heuristic and needs care.\n","\n","3.  **N-gram Frequency Analysis (Automated):**\n","    *   For the filtered texts of a given QID, the script can:\n","        *   Preprocess the text (lowercase, remove punctuation, potentially remove common stop words, though be careful with domain-specific stop words).\n","        *   Calculate and list the most frequent unigrams (single words), bi-grams (2-word phrases), and tri-grams (3-word phrases).\n","    *   **Output:** A ranked list of common phrases. This doesn't give \"points\" directly but provides strong clues to human reviewers about recurring terminology and potential themes.\n","\n","4.  **Keyword/Keyphrase Counting (Semi-Automated):**\n","    *   **After initial human review identifies potential \"points\" or keyphrases representing those points:**\n","    *   The script can take a list of these keyphrases (e.g., \"employee records exemption,\" \"balance privacy public interest,\" \"international standards alignment\").\n","    *   It can then search for these exact phrases (or variants using regular expressions) within the text items for the QID.\n","    *   **Counting (with de-duplication):**\n","        *   For each keyphrase found in a text item, record the `response_id`.\n","        *   The count for the \"point\" represented by that keyphrase would be the number of unique `response_id`s associated with it.\n","\n","5.  **Basic Topic Modeling (Automated, but requires human interpretation):**\n","    *   As discussed before, you could run LDA or NMF on the collected texts for a QID.\n","    *   The script automates the model training and outputting the top words for each topic.\n","    *   **Human step:** Someone still needs to look at the word lists for each topic and assign a meaningful \"point\" or \"theme\" label to it. The script can then count how many documents (and unique `response_id`s) are strongly associated with each (now labeled) topic.\n","\n","6.  **LLM-based Point Extraction and Clustering (More Complex Automation):**\n","    *   This is the most advanced automation.\n","    *   **Automated parts:** Sending texts to an LLM for point extraction, embedding those points, running clustering algorithms.\n","    *   **Human/Semi-Automated parts:** Crafting the initial LLM prompt for point extraction, interpreting/labeling the resulting clusters. The script could then count items per (labeled) cluster.\n","\n","**What Remains Primarily Human-Driven (even with automation support):**\n","\n","1.  **Defining \"Points\" / Creating the Codebook:** Even if n-grams or topic models suggest themes, a human needs to synthesize these into coherent, distinct \"points\" or arguments relevant to the policy question. For example, \"employee records\" and \"exemption\" might be frequent bi-grams; a human decides the \"point\" is \"Discussion around the employee records exemption.\"\n","2.  **Interpreting Nuance and Context:** Language is complex. Automated methods might miss sarcasm, implied meanings, or the subtle ways different respondents frame similar ideas.\n","3.  **Handling Ambiguity:** Some responses might be vague or touch on multiple points. Humans are better at disambiguating these.\n","4.  **Validating Automated Outputs:** If you use topic modeling or LLM clustering, a human needs to review the results to ensure they make sense and accurately reflect the data.\n","5.  **Deciding on Thresholds for \"Common\":** What constitutes \"common\"? Top 3 by count? Or points mentioned by >X% of unique respondents? This is a human judgment call.\n","\n","**How this Script Automates and What it Achieves:**\n","\n","1.  **Loads Stage 4a Output:** Takes the JSON from the previous collation step.\n","2.  **Focuses per QID:** Designed to be run (or looped) for each QID you want to analyze.\n","3.  **Automated Filtering:** Includes basic filtering for known placeholder summaries.\n","4.  **Automated N-gram Generation:** Provides top bi-grams and tri-grams as exploratory data to *aid human theme identification*.\n","5.  **Semi-Automated Keyphrase Counting:**\n","    *   The script includes `count_keyphrases`. This function is automated.\n","    *   **The human input needed:** Defining the `keyphrases_dict`. This dictionary maps a \"Point Label\" (e.g., \"Concerns about Employee Records Exemption\") to a list of actual phrases/keywords that indicate this point (e.g., `[\"employee records exemption\", \"record exemption\"]`). This dictionary would be built iteratively by humans looking at the texts, n-grams, etc.\n","    *   It counts unique `response_id`s for each point.\n","6.  **Structured Output:** Saves the n-grams and the keyphrase counts into a new JSON file, structured by QID.\n","\n","**Iterative Workflow for this New Script:**\n","\n","1.  **Run Stage 4a** (the script from our previous discussion) to get `stage4a_collated_texts_for_thematic_analysis_...json`.\n","2.  **Run this new \"Thematic Analysis\" script for a target QID (e.g., Q4).**\n","    *   Initially, the `keyphrases_dict` might be empty or have just a few guesses.\n","3.  **Review the output:**\n","    *   Look at the `top_bigrams` and `top_trigrams`. Do they suggest common themes?\n","    *   Manually read a sample of the actual texts for Q4 from the Stage 4a output.\n","4.  **Refine `keyphrases_dict`:** Based on your review, update the `keyphrases_dict` within the script (or load it from an external config file for better management) with more accurate and comprehensive phrases for the points you're identifying for Q4.\n","5.  **Re-run this \"Thematic Analysis\" script for Q4.** The `identified_points_and_counts` will now be based on your refined keyphrases.\n","6.  Repeat steps 3-5 until you are satisfied with the identified points and their counts for Q4.\n","7.  Once you have good counts, select the top 3 (or more) for your report.\n","8.  Move to the next QID and repeat the process.\n","\n","This script provides a framework. The keyphrase definition is the human-in-the-loop part that makes the \"point\" identification accurate. The automation helps with processing the text once those definitions are established. You can make the keyphrase definition more sophisticated (e.g., using regex for variations) as needed.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZlHIBlbx2zY"},"outputs":[],"source":["import json\n","from collections import Counter, defaultdict\n","import re\n","# Potentially: from sklearn.feature_extraction.text import CountVectorizer (for n-grams)\n","# Potentially: from nltk.corpus import stopwords (if doing stopword removal)\n","# Potentially: LLM libraries if going that route\n","\n","# (Assume FileLogger is available and initialized as 'logger')\n","\n","def preprocess_text_for_ngram(text, custom_stopwords=None):\n","    text = text.lower()\n","    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n","    text = re.sub(r'\\d+', '', text) # Remove numbers\n","    # Optional: stopword removal\n","    if custom_stopwords:\n","        words = text.split()\n","        words = [word for word in words if word not in custom_stopwords]\n","        text = \" \".join(words)\n","    return text\n","\n","def get_ngrams(text_list, n, top_k=20):\n","    # Simplified n-gram counter (sklearn's CountVectorizer is more robust)\n","    all_ngrams = []\n","    for text in text_list:\n","        processed_text = preprocess_text_for_ngram(text) # Basic preprocessing\n","        words = processed_text.split()\n","        if len(words) < n:\n","            continue\n","        for i in range(len(words) - n + 1):\n","            all_ngrams.append(\" \".join(words[i:i+n]))\n","    return Counter(all_ngrams).most_common(top_k)\n","\n","def count_keyphrases(text_list_with_ids, keyphrases_dict):\n","    \"\"\"\n","    Counts unique response_ids associated with each keyphrase.\n","    keyphrases_dict: {'Point Label 1': ['keyword1a', 'keyword1b'], 'Point Label 2': ['keyword2a']}\n","    text_list_with_ids: [{'response_id': 'id1', 'text': '...'}, ...]\n","    \"\"\"\n","    point_counts = defaultdict(lambda: {\"count\": 0, \"response_ids\": set()})\n","    for item in text_list_with_ids:\n","        text_lower = item['text'].lower()\n","        response_id = item['response_id']\n","        for point_label, phrases in keyphrases_dict.items():\n","            for phrase in phrases:\n","                if phrase.lower() in text_lower: # Simple substring match\n","                    point_counts[point_label][\"response_ids\"].add(response_id)\n","                    break # Count response once per point label, even if multiple phrases match\n","\n","    # Finalize counts\n","    for point_label in point_counts:\n","        point_counts[point_label][\"count\"] = len(point_counts[point_label][\"response_ids\"])\n","    return point_counts\n","\n","\n","def thematic_analysis_stage(input_collated_texts_json, output_analysis_json, qid_to_analyze):\n","    logger.log(f\"INFO: Starting Thematic Analysis for QID: {qid_to_analyze}\")\n","\n","    try:\n","        with open(input_collated_texts_json, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","    except Exception as e:\n","        logger.log(f\"ERROR: Could not load input JSON '{input_collated_texts_json}': {e}\")\n","        return\n","\n","    aggregated_content = data.get(\"aggregated_content_by_qid\", {})\n","    if qid_to_analyze not in aggregated_content:\n","        logger.log(f\"ERROR: QID '{qid_to_analyze}' not found in the input JSON.\")\n","        return\n","\n","    text_items_for_qid = aggregated_content[qid_to_analyze]\n","    logger.log(f\"INFO: Found {len(text_items_for_qid)} text items for QID '{qid_to_analyze}'.\")\n","\n","    # 1. Automated Filtering (Example)\n","    filtered_text_items = []\n","    placeholders_to_remove = [\n","        \"please provide the responses\",\n","        \"doesn't address the specific question\",\n","        \"i need the text of the responses\"\n","    ]\n","    for item in text_items_for_qid:\n","        text_lower = item['text'].lower()\n","        if not any(placeholder in text_lower for placeholder in placeholders_to_remove):\n","            filtered_text_items.append(item)\n","    logger.log(f\"INFO: After filtering placeholders, {len(filtered_text_items)} items remain for QID '{qid_to_analyze}'.\")\n","\n","    # Extract just the text for n-gram analysis\n","    texts_for_ngram = [item['text'] for item in filtered_text_items]\n","\n","    # 2. N-gram Frequency Analysis (Automated)\n","    top_bigrams = get_ngrams(texts_for_ngram, 2, top_k=30)\n","    top_trigrams = get_ngrams(texts_for_ngram, 3, top_k=30)\n","    logger.log(f\"INFO: Top Bigrams for {qid_to_analyze}: {top_bigrams}\")\n","    logger.log(f\"INFO: Top Trigrams for {qid_to_analyze}: {top_trigrams}\")\n","\n","    # 3. Keyphrase Counting (Semi-Automated - requires human-defined keyphrases)\n","    # This part would typically be iterative.\n","    # Iteration 1: Human reviews n-grams, CAR_35, and a sample of texts, then defines initial keyphrases.\n","    # Example keyphrases for Q4 (these would come from human analysis):\n","    q4_keyphrases_from_human_analysis = {\n","        \"Employee Records Exemption Discussion\": [\"employee records exemption\", \"employee record exemption\"],\n","        \"Balancing Privacy with Public/Business Interests\": [\"balance privacy\", \"competing public interest\", \"business practicalit\", \"operational necessit\"],\n","        \"Alignment with International Standards/GDPR\": [\"international standard\", \"gdpr\", \"global data protection\", \"harmonisation\"],\n","        \"Exceptions for Legal Obligations/Investigations\": [\"legal obligation\", \"law enforcement\", \"investigation\"],\n","        \"Concerns about Small Business Impact\": [\"small business\", \"burden on smes\"],\n","        \"Protection for Children/Vulnerable Individuals\": [\"children\", \"vulnerable individuals\", \"under 18\"] # Check relevance to Q4 context\n","    }\n","    # For a generic script, you might load these keyphrases from a config file per QID\n","\n","    # Use keyphrases relevant to the qid_to_analyze\n","    # For this example, let's assume qid_to_analyze is Q4\n","    current_qid_keyphrases = {}\n","    if qid_to_analyze == \"Q4\":\n","        current_qid_keyphrases = q4_keyphrases_from_human_analysis\n","    else:\n","        logger.log(f\"WARN: No predefined keyphrases for {qid_to_analyze}. Keyphrase counting will be skipped or use a generic set.\")\n","        # You could have a default empty dict or a mechanism to load them dynamically\n","\n","    counted_points = {}\n","    if current_qid_keyphrases:\n","        # Prepare list of dicts with 'response_id' and 'text' for count_keyphrases\n","        text_list_with_ids_for_counting = [{'response_id': item['response_id'], 'text': item['text']} for item in filtered_text_items]\n","        counted_points = count_keyphrases(text_list_with_ids_for_counting, current_qid_keyphrases)\n","        logger.log(f\"INFO: Keyphrase Counts (unique responses) for {qid_to_analyze}:\")\n","        for point, data in sorted(counted_points.items(), key=lambda item: item[1][\"count\"], reverse=True):\n","            logger.log(f\"  - \\\"{point}\\\": {data['count']} unique responses\")\n","\n","    # 4. Prepare Output\n","    analysis_output = {\n","        \"qid\": qid_to_analyze,\n","        \"question_text\": \"Get this from CAR_35 or Stage4a output metadata if available\", # TODO: Get actual Q text\n","        \"total_items_analyzed_for_qid\": len(filtered_text_items),\n","        \"top_bigrams\": top_bigrams,\n","        \"top_trigrams\": top_trigrams,\n","        \"identified_points_and_counts\": dict(counted_points), # Convert defaultdict\n","        \"notes\": \"Counts for 'identified_points_and_counts' are based on unique response_ids mentioning predefined keyphrases. Keyphrases require human definition and iteration.\"\n","    }\n","\n","    # Append to a JSONL file for multiple QIDs, or save one JSON per QID\n","    # For simplicity, let's assume one main output JSON that can be updated\n","    # This part needs refinement based on how you want to store results for ALL QIDs\n","\n","    # For now, let's just log the intended output for this QID\n","    logger.log_json(analysis_output, prefix=f\"ThematicAnalysis_{qid_to_analyze}\")\n","\n","    # If saving to a single aggregated output file:\n","    try:\n","        # Load existing data if file exists, otherwise start fresh\n","        all_qids_analysis_data = {}\n","        if os.path.exists(output_analysis_json):\n","            with open(output_analysis_json, 'r', encoding='utf-8') as f_in:\n","                all_qids_analysis_data = json.load(f_in)\n","\n","        all_qids_analysis_data[qid_to_analyze] = analysis_output # Add/update analysis for current QID\n","\n","        with open(output_analysis_json, 'w', encoding='utf-8') as f_out:\n","            json.dump(all_qids_analysis_data, f_out, indent=2, default=logger._get_serializer())\n","        logger.log(f\"INFO: Successfully saved/updated thematic analysis to '{output_analysis_json}' for QID {qid_to_analyze}\")\n","    except Exception as e:\n","        logger.log(f\"ERROR: Could not write thematic analysis output for {qid_to_analyze} to '{output_analysis_json}': {e}\")\n","\n","    logger.log(f\"INFO: Thematic Analysis for QID: {qid_to_analyze} complete.\")\n","\n","\n","# --- Main execution block for the new Thematic Analysis Stage ---\n","# (Assume FileLogger class is defined and 'logger' instance is created as in previous script)\n","\n","# Example usage:\n","SCRIPT_LOG_FILE_THEMATIC = 'stage4b_thematic_analysis.log'\n","logger = FileLogger(SCRIPT_LOG_FILE_THEMATIC) # Assuming logger is re-initialized or passed\n","\n","INPUT_COLLATED_TEXTS_JSON = 'stage4a_collated_texts_for_thematic_analysis_20250515_060005.json' # From Stage 4a\n","OUTPUT_THEMATIC_ANALYSIS_JSON = 'stage4b_thematic_analysis_results.json'\n","\n","QID_TO_PROCESS = \"Q4\" # Example: Process Q4\n","\n","if 'logger' not in globals() or not isinstance(logger, FileLogger):\n","  print(\"CRITICAL: 'logger' is not defined as a FileLogger instance. Please initialize it.\")\n","else:\n","  if not os.path.exists(INPUT_COLLATED_TEXTS_JSON):\n","    logger.log(f\"ERROR: Input file for thematic analysis not found: {INPUT_COLLATED_TEXTS_JSON}\")\n","  else:\n","    thematic_analysis_stage(INPUT_COLLATED_TEXTS_JSON, OUTPUT_THEMATIC_ANALYSIS_JSON, QID_TO_PROCESS)\n","# You would loop or call this for multiple QIDs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHX7RnLyyvKh"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Mx8rnPcoa6fe"},"source":["## Stage 4a: Text Collation for Thematic Analysis\n","\n","This stage processes the output of Stage 4 (pdf_passage_analysis_by_response_minilm_gemma.json) to create consolidated textual datasets for each consultation question, prioritizing the AI-generated summaries. The output of Stage 4a, stage4a_collated_texts_for_thematic_analysis.json, will serve as the primary input for the subsequent manual and semi-automated thematic analysis aimed at identifying common points..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pzhQpmlYyKL"},"outputs":[],"source":["# Extract summaries or passages from Stage 4 output file\n","import json\n","import os\n","import glob\n","from collections import defaultdict\n","from typing import Dict, Any, List, Tuple, Optional\n","import re # For cleaning up multiple newlines/spaces if needed\n","import time # For timestamp in output filename and logger\n","import sys # For checking sys.modules in FileLogger (already there)\n","# The FileLogger class you provided also implicitly uses numpy and torch if available.\n","# Ensure these are installed in your Colab environment if you expect to log objects of these types.\n","# For this specific script's core logic, numpy and torch are not directly used.\n","import numpy as np # Assuming FileLogger might encounter these\n","import torch # Assuming FileLogger might encounter these\n","\n","\n","# --- Assuming FileLogger class is defined in a previous cell or imported ---\n","class FileLogger:\n","    def __init__(self, log_file):\n","        self.log_file = log_file; log_dir = os.path.dirname(log_file)\n","        if log_dir and not os.path.exists(log_dir): os.makedirs(log_dir, exist_ok=True)\n","        try:\n","            with open(self.log_file, 'w', encoding='utf-8') as f: f.write(f\"{self._get_timestamp()}: Starting new log...\\n\")\n","        except IOError as e: print(f\"CRITICAL: Failed to initialize log file {self.log_file}: {e}\")\n","    def _get_timestamp(self): return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","    def log(self, message: str):\n","        try:\n","            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","        except Exception as e: print(f\"!!! LOGGER FAILED TO WRITE: {e} !!!\"); print(f\"{self._get_timestamp()}: {message}\")\n","    def _get_serializer(self):\n","        def default_serializer(obj):\n","            if isinstance(obj, (np.integer, np.floating)): return obj.item()\n","            if isinstance(obj, np.ndarray): return obj.tolist()\n","            if isinstance(obj, set): return sorted(list(obj))\n","            if 'torch' in sys.modules and isinstance(obj, torch.Tensor): return obj.detach().cpu().numpy().tolist()\n","            try: return json.JSONEncoder().default(obj)\n","            except TypeError: return f\"UNSERIALIZABLE:{type(obj).__name__}\"\n","        return default_serializer\n","    def log_json(self, data: Dict[str, Any], prefix=\"JSON_DATA\"):\n","        serializer = self._get_serializer(); log_entry = { \"timestamp\": self._get_timestamp(), \"type\": prefix, \"data\": data }\n","        try:\n","            json_string = json.dumps(log_entry, default=serializer, indent=2)\n","            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(json_string + \"\\n\")\n","        except Exception as e: self.log(f\"ERROR: Unexpected error during log_json (prefix={prefix}): {e}\")\n","# --- End of FileLogger assumption ---\n","\n","\n","# --- Helper Functions ---\n","def clean_text(text):\n","    \"\"\"Optional: Basic text cleaning.\"\"\"\n","    if not text or not isinstance(text, str):\n","        return \"\"\n","    text = re.sub(r'\\s*\\n\\s*', '\\n', text) # Consolidate newlines\n","    text = re.sub(r'[ \\t]+', ' ', text)    # Consolidate spaces/tabs\n","    return text.strip()\n","\n","\n","def extract_and_aggregate_content_to_json(input_path, output_json_file, logger_instance):\n","    \"\"\"\n","    Extracts summaries (prioritized) or passages from Stage 4 output files\n","    and aggregates them per QID into a single JSON output file.\n","\n","    Args:\n","        input_path (str): Path to a single Stage 4 JSON file or a directory\n","                          containing multiple Stage 4 JSON files.\n","        output_json_file (str): Path to the output JSON file where aggregated\n","                                texts per QID will be saved.\n","        logger_instance (FileLogger): An instance of the FileLogger class.\n","    \"\"\"\n","    global logger # Make logger available globally within this function if not passed\n","    logger = logger_instance\n","\n","    if not os.path.exists(input_path):\n","        logger.log(f\"ERROR: Input path '{input_path}' does not exist.\")\n","        return\n","\n","    output_dir = os.path.dirname(output_json_file)\n","    if output_dir and not os.path.exists(output_dir):\n","        os.makedirs(output_dir, exist_ok=True)\n","        logger.log(f\"INFO: Created output directory: '{output_dir}'\")\n","\n","    if os.path.isfile(input_path):\n","        files_to_process = [input_path]\n","    elif os.path.isdir(input_path):\n","        files_to_process = glob.glob(os.path.join(input_path, 'pdf_passage_analysis*.json'))\n","        if not files_to_process:\n","            files_to_process = glob.glob(os.path.join(input_path, '*.json'))\n","    else:\n","        logger.log(f\"ERROR: Input path '{input_path}' is neither a file nor a directory.\")\n","        return\n","\n","    if not files_to_process:\n","        logger.log(f\"INFO: No JSON files found to process in '{input_path}'.\")\n","        return\n","\n","    logger.log(f\"INFO: Found {len(files_to_process)} file(s) to process: {files_to_process}\")\n","\n","    qid_collected_content = defaultdict(list)\n","    total_responses_processed_count = 0 # This will count unique response_content dicts encountered\n","    processed_response_ids_from_all_files = set() # To track unique response IDs across files for counting\n","    total_content_items_collected = 0\n","    summaries_collected_count = 0\n","    passages_collected_count = 0\n","\n","    for file_idx, file_path in enumerate(files_to_process):\n","        logger.log(f\"INFO: Processing file {file_idx + 1}/{len(files_to_process)}: '{os.path.basename(file_path)}'...\")\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                stage4_data_full = json.load(f)\n","\n","            stage4_data = stage4_data_full.get(\"pdf_analysis_by_response\")\n","            if stage4_data is None:\n","                logger.log(f\"  WARN: 'pdf_analysis_by_response' key not found in '{file_path}'. Skipping this file.\")\n","                continue\n","            if not isinstance(stage4_data, dict):\n","                logger.log(f\"  WARN: 'pdf_analysis_by_response' is not a dictionary in '{file_path}'. Skipping this file.\")\n","                continue\n","\n","            file_responses_count = 0\n","            for response_id, response_content in stage4_data.items():\n","                if not isinstance(response_content, dict):\n","                    continue # Skip status entries\n","\n","                file_responses_count +=1\n","                processed_response_ids_from_all_files.add(response_id)\n","\n","\n","                for qid, q_data in response_content.items():\n","                    if not isinstance(q_data, dict):\n","                        continue\n","\n","                    content_item = {\n","                        \"source_file\": os.path.basename(file_path),\n","                        \"response_id\": response_id,\n","                        \"question_id\": qid,\n","                        \"type\": None,\n","                        \"text\": None\n","                    }\n","                    text_to_add = None\n","\n","                    summary_text = q_data.get(\"summary\")\n","                    is_summary_valid = (\n","                        summary_text is not None and\n","                        isinstance(summary_text, str) and\n","                        summary_text.strip()\n","                    )\n","\n","                    if is_summary_valid:\n","                        text_to_add = clean_text(summary_text)\n","                        content_item[\"type\"] = \"summary\"\n","                        summaries_collected_count +=1\n","                    else:\n","                        passages = q_data.get(\"extracted_passages\")\n","                        if passages and isinstance(passages, list):\n","                            valid_passages = [p for p in passages if isinstance(p, str) and p.strip()]\n","                            if valid_passages:\n","                                passage_block = \"\\n\\n\".join(valid_passages)\n","                                text_to_add = clean_text(passage_block)\n","                                content_item[\"type\"] = \"passages\"\n","                                passages_collected_count +=1\n","\n","                    if text_to_add:\n","                        content_item[\"text\"] = text_to_add\n","                        qid_collected_content[qid].append(content_item)\n","                        total_content_items_collected += 1\n","\n","            logger.log(f\"  INFO: Finished processing {file_responses_count} response entries from '{os.path.basename(file_path)}'.\")\n","\n","        except FileNotFoundError:\n","            logger.log(f\"  ERROR: File not found: '{file_path}'. Skipping.\")\n","        except json.JSONDecodeError as e:\n","            logger.log(f\"  ERROR: Invalid JSON in file '{file_path}': {e}. Skipping.\")\n","        except Exception as e:\n","            logger.log(f\"  ERROR: Unexpected error processing file '{file_path}': {e}\")\n","            import traceback\n","            logger.log(f\"TRACEBACK:\\n{traceback.format_exc()}\")\n","\n","    total_responses_processed_count = len(processed_response_ids_from_all_files)\n","\n","\n","    final_output_data = {\n","        \"metadata\": {\n","            \"creation_timestamp\": time.strftime('%Y-%m-%d %H:%M:%S %Z', time.gmtime()),\n","            \"source_input_path\": input_path,\n","            \"total_files_processed\": len(files_to_process),\n","            \"total_unique_qids_found\": len(qid_collected_content),\n","            \"total_content_items_collected\": total_content_items_collected,\n","            \"summaries_collected_count\": summaries_collected_count,\n","            \"passages_collected_count\": passages_collected_count,\n","            \"total_unique_responses_with_content_processed\": total_responses_processed_count,\n","        },\n","        \"aggregated_content_by_qid\": dict(qid_collected_content)\n","    }\n","\n","    logger.log(f\"\\n--- Aggregation Summary ---\")\n","    for key, value in final_output_data[\"metadata\"].items():\n","        logger.log(f\"  {key}: {value}\")\n","\n","    logger.log(f\"\\nINFO: Saving aggregated content to '{output_json_file}'...\")\n","    try:\n","        with open(output_json_file, 'w', encoding='-utf-8') as outfile:\n","            # Use the logger's serializer if it's more robust for your data types\n","            # otherwise, standard json.dump is fine for dicts of strings.\n","            json.dump(final_output_data, outfile, indent=2, default=logger._get_serializer() if hasattr(logger, '_get_serializer') else None)\n","        logger.log(f\"INFO: Successfully saved aggregated content to '{output_json_file}'.\")\n","    except Exception as e:\n","        logger.log(f\"ERROR: Could not write output JSON file '{output_json_file}': {e}\")\n","        logger.log(f\"TRACEBACK:\\n{traceback.format_exc()}\")\n","\n","\n","    logger.log(\"\\nINFO: Extraction process complete.\")\n","\n","\n","# --- How to use the script (when running as a Colab cell) ---\n","# Make sure FileLogger class is defined in a previous cell or imported.\n","\n","# Define where the log file for this script will be saved\n","SCRIPT_LOG_FILE = 'stage4a_text_collation.log' # UPDATE THIS PATH\n","logger = FileLogger(SCRIPT_LOG_FILE) # Initialize the logger\n","\n","# OPTION 1: Process a single Stage 4 output file\n","# INPUT_STAGE4_PATH = '/content/drive/MyDrive/Colab Notebooks/Legal/pdf_passage_analysis_by_response_minilm_gemma.json' # UPDATE\n","\n","# OPTION 2: Process all relevant .json files in a directory\n","INPUT_STAGE4_PATH = 'Stage4_Outputs' # UPDATE / CREATE THIS FOLDER\n","\n","timestamp_str = time.strftime(\"%Y%m%d_%H%M%S\")\n","OUTPUT_AGGREGATED_JSON = f'stage4a_collated_texts_for_thematic_analysis_{timestamp_str}.json' # UPDATE\n","\n","\n","# --- Safety Check ---\n","if '/Stage4_Outputs/' in INPUT_STAGE4_PATH and not os.path.exists(INPUT_STAGE4_PATH) and os.path.isdir(INPUT_STAGE4_PATH):\n","    logger.log(\"=\"*60)\n","    logger.log(\"!!! WARNING: The example INPUT_STAGE4_PATH directory for multiple files:\")\n","    logger.log(f\"!!! '{INPUT_STAGE4_PATH}'\")\n","    logger.log(\"!!! does not exist. Please create it and place your Stage 4 JSON output\")\n","    logger.log(\"!!! files there, or update the path to a single file if preferred.\")\n","    logger.log(\"!!! Script will not run effectively until paths are correct.\")\n","    logger.log(\"=\"*60)\n","elif 'path/to/your' in INPUT_STAGE4_PATH:\n","    logger.log(\"=\"*60)\n","    logger.log(\"!!! IMPORTANT: Please update 'INPUT_STAGE4_PATH' with the actual path\")\n","    logger.log(\"!!! to your Stage 4 output file or directory.\")\n","    logger.log(\"!!! Also, update 'OUTPUT_AGGREGATED_JSON' and 'SCRIPT_LOG_FILE' if needed.\")\n","    logger.log(\"=\"*60)\n","else:\n","    try:\n","        extract_and_aggregate_content_to_json(INPUT_STAGE4_PATH, OUTPUT_AGGREGATED_JSON, logger)\n","    except NameError as ne:\n","        if 'FileLogger' in str(ne):\n","            print(\"=\"*70)\n","            print(\"CRITICAL ERROR: The 'FileLogger' class is not defined in the current scope.\")\n","            print(\"Please ensure the FileLogger class definition is executed in a cell before this one.\")\n","            print(\"=\"*70)\n","        else:\n","            raise # Re-raise other NameErrors\n","    except Exception as e_main:\n","        # Use logger if available, otherwise print\n","        if 'logger' in globals() and logger:\n","            logger.log(f\"FATAL ERROR in script execution: {e_main}\")\n","            logger.log(f\"TRACEBACK:\\n{traceback.format_exc()}\")\n","        else:\n","            print(f\"FATAL ERROR in script execution: {e_main}\")\n","            import traceback\n","            traceback.print_exc()"]},{"cell_type":"markdown","metadata":{"id":"KcZm9DwmFAbx"},"source":["# 14th May\n","\n","The plan is to automate merging of existing partial Stage 4 work, then Complete Stage 4, then Thematic Reporting as per initial problem statement given on 28th Feb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tiDJjZPRFCTo"},"outputs":[],"source":["# @title --- Cell: Stage 4 Merge Script ---\n","import json\n","import os\n","import traceback\n","from collections import defaultdict # Not strictly needed if just overwriting, but good for other merge strategies\n","\n","# Ensure FileLogger class is defined in a cell above or imported\n","# For example:\n","# class FileLogger:\n","#     def __init__(self, log_file): self.log_file = log_file; # ... (rest of FileLogger) ...\n","#     def _get_serializer(self): # Ensure this handles numpy if it might appear\n","#         import numpy as np # Import if numpy types might be in your data\n","#         def default_serializer(obj):\n","#             if isinstance(obj, (np.integer, np.floating)): return obj.item()\n","#             if isinstance(obj, np.ndarray): return obj.tolist()\n","#             # Add other type handling if necessary (e.g., torch.Tensor)\n","#             try: return json.JSONEncoder().default(obj)\n","#             except TypeError: return f\"UNSERIALIZABLE:{type(obj).__name__}\"\n","#         return default_serializer\n","\n","\n","# --- Configuration for Merging ---\n","PARTIAL_STATES_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/Legal/partial_stage4_states_for_merge/' # ADJUST THIS PATH\n","CONSOLIDATED_STATE_FILE = '/content/drive/MyDrive/Colab Notebooks/Legal/consolidated_stage4_processing_state.json' # ADJUST THIS PATH\n","CONSOLIDATED_OUTPUT_JSON = '/content/drive/MyDrive/Colab Notebooks/Legal/pdf_passage_analysis_by_response_minilm_gemma.json' # ADJUST THIS PATH\n","MERGE_LOG_FILE = '/content/drive/MyDrive/Colab Notebooks/Legal/stage4_merge_process.log'\n","\n","# --- Initialize Logger for this script ---\n","merge_logger = FileLogger(MERGE_LOG_FILE)\n","merge_logger.log(\"INFO: Starting Stage 4 partial state and output merge process.\")\n","\n","def merge_partial_stage4_data(partial_states_dir, cons_state_file, cons_output_json):\n","    master_processed_ids = set()\n","    # master_results_dict holds: {response_id: {qid_data_for_that_response}}\n","    # This comes from the \"results\" key in the state files.\n","    master_results_payload = {}\n","\n","    if not os.path.isdir(partial_states_dir):\n","        merge_logger.log(f\"ERROR: Partial states directory not found: {partial_states_dir}\")\n","        return False\n","\n","    # Ensure files are processed in a somewhat consistent order (e.g., by name)\n","    # This helps if there are true overlaps and you want the \"latest\" version based on filename.\n","    partial_state_files = sorted([\n","        os.path.join(partial_states_dir, f) for f in os.listdir(partial_states_dir)\n","        if f.startswith(\"stage4_processing_state\") and f.endswith(\".json\")\n","    ])\n","\n","    if not partial_state_files:\n","        merge_logger.log(f\"WARN: No partial state files matching pattern found in {partial_states_dir}\")\n","        # If no partials, you might want to check if a single, older consolidated file exists\n","        # or if this is truly a fresh start for the consolidation.\n","        # For now, assume if no files, nothing to merge, but this might be an error condition.\n","        return True # No files to merge, so technically \"successful\" in doing nothing.\n","\n","    merge_logger.log(f\"INFO: Found {len(partial_state_files)} partial state files to merge.\")\n","\n","    for state_file_path in partial_state_files:\n","        merge_logger.log(f\"INFO: Processing partial state file: {state_file_path}\")\n","        try:\n","            with open(state_file_path, 'r', encoding='utf-8') as f_partial_state:\n","                partial_state_data = json.load(f_partial_state)\n","\n","            batch_ids = set(partial_state_data.get(\"processed_responses_ids\", []))\n","            # 'results' in the state file is the {response_id: {qid_data}} structure\n","            batch_results_payload = partial_state_data.get(\"results\", {})\n","\n","            master_processed_ids.update(batch_ids)\n","\n","            for resp_id, resp_data in batch_results_payload.items():\n","                if resp_id in master_results_payload and master_results_payload[resp_id] != resp_data:\n","                    # This indicates a response ID was processed differently in different batches,\n","                    # or the same response was processed in multiple overlapping batches.\n","                    # The simple overwrite here takes the version from the \"later\" file (due to sorting).\n","                    merge_logger.log(f\"WARN: Response ID {resp_id} found in multiple partial states with potentially different data. Overwriting with data from {state_file_path}.\")\n","                master_results_payload[resp_id] = resp_data\n","            merge_logger.log(f\"INFO: Merged {len(batch_ids)} IDs and data for {len(batch_results_payload)} responses from {state_file_path}\")\n","\n","        except FileNotFoundError:\n","            merge_logger.log(f\"ERROR: File not found during merge loop: {state_file_path}. Skipping.\")\n","        except json.JSONDecodeError:\n","            merge_logger.log(f\"ERROR: JSON decode error in {state_file_path}. Skipping.\")\n","        except Exception as e_merge:\n","            merge_logger.log(f\"ERROR: Failed to process partial state file {state_file_path}: {e_merge}\")\n","            merge_logger.log(traceback.format_exc())\n","            # Decide: continue or stop on error? For now, continue to try and salvage as much as possible.\n","\n","    merge_logger.log(f\"INFO: Merge complete. Total unique processed IDs collected: {len(master_processed_ids)}. Total unique response result entries: {len(master_results_payload)}.\")\n","\n","    # Save Consolidated State File\n","    consolidated_state_data_to_save = {\n","        \"processed_responses_ids\": sorted(list(master_processed_ids)),\n","        \"results\": master_results_payload # This is the merged content\n","    }\n","    try:\n","        with open(cons_state_file, 'w', encoding='utf-8') as f_state_out:\n","            # Use FileLogger's serializer if available and handles complex types like numpy\n","            # If merge_logger._get_serializer exists and handles types from Stage 4 output\n","            json.dump(consolidated_state_data_to_save, f_state_out, indent=2, default=getattr(merge_logger, '_get_serializer', None))\n","        merge_logger.log(f\"INFO: Created consolidated Stage 4 state file: {cons_state_file}\")\n","    except Exception as e_save_state:\n","        merge_logger.log(f\"ERROR: Failed to save consolidated state file {cons_state_file}: {e_save_state}\")\n","        merge_logger.log(traceback.format_exc())\n","        return False\n","\n","    # Save Consolidated Stage 4 Output JSON (which is used by Stage 5)\n","    # The structure is {\"pdf_analysis_by_response\": master_results_payload}\n","    final_output_structure_s4 = {\"pdf_analysis_by_response\": master_results_payload}\n","    try:\n","        with open(cons_output_json, 'w', encoding='utf-8') as f_json_out:\n","            json.dump(final_output_structure_s4, f_json_out, indent=2, default=getattr(merge_logger, '_get_serializer', None))\n","        merge_logger.log(f\"INFO: Created consolidated Stage 4 output JSON: {cons_output_json}\")\n","    except Exception as e_save_json:\n","        merge_logger.log(f\"ERROR: Failed to save consolidated output JSON {cons_output_json}: {e_save_json}\")\n","        merge_logger.log(traceback.format_exc())\n","        return False\n","\n","    return True\n","\n","# --- Main call to the merge function ---\n","if __name__ == \"__main__\": # This guard prevents execution if just importing\n","    merge_success = merge_partial_stage4_data(\n","        PARTIAL_STATES_DIRECTORY,\n","        CONSOLIDATED_STATE_FILE,\n","        CONSOLIDATED_OUTPUT_JSON\n","    )\n","    if merge_success:\n","        merge_logger.log(\"SUCCESS: Stage 4 partial files merged successfully.\")\n","        merge_logger.log(f\"Next Step: Configure your main Stage 4 script to use:\")\n","        merge_logger.log(f\"  STATE_FILE = '{CONSOLIDATED_STATE_FILE}'\")\n","        merge_logger.log(f\"  PDF_ANALYSIS_OUTPUT_FILE = '{CONSOLIDATED_OUTPUT_JSON}'\")\n","        merge_logger.log(f\"  And then run it to process any remaining responses.\")\n","    else:\n","        merge_logger.log(\"ERROR: Merging Stage 4 partial files failed. Please check logs at {MERGE_LOG_FILE}.\")"]},{"cell_type":"markdown","metadata":{"id":"s9p93C4YE5sg"},"source":["# SEPARATOR"]},{"cell_type":"markdown","metadata":{"id":"yKM3SAHFE8qg"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"mJKuAT8Vw_yX"},"source":["# 18th Apr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDGxa7ogxC7K"},"outputs":[],"source":["# =====================================================\n","# @title Stage 5: Comprehensive Analysis Report Generator\n","# =====================================================\n","# Purpose: Loads the final output from Stage 4 (Retrieval + Summary attempts),\n","#          aggregates relevant content per question (prioritizing non-blank summaries,\n","#          falling back to passages), performs sentiment analysis and keyword extraction,\n","#          and reports overall statistics.\n","\n","# =====================================================\n","#                      Imports\n","# =====================================================\n","import json\n","import os\n","import sys\n","import traceback\n","import time\n","from collections import defaultdict, Counter\n","from typing import Dict, Any, List, Tuple, Optional\n","import re\n","import string\n","import numpy as np\n","\n","# --- NLP Imports ---\n","try:\n","    import nltk\n","    from nltk.corpus import stopwords\n","    from nltk.tokenize import word_tokenize, sent_tokenize\n","    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","except ImportError as e:\n","    print(f\"CRITICAL ERROR: Missing essential NLP libraries. Install them. Details: {e}\")\n","    print(\"\\nTry: pip install nltk scikit-learn numpy\")\n","    # If using VADER: nltk.download('vader_lexicon') needs to be run once\n","    sys.exit(1)\n","\n","# Import logger (assuming FileLogger class is available from previous stages)\n","# If not available, use the SimpleLogger placeholder\n","try:\n","    # Assuming FileLogger class is defined in a file named 'file_logger_module.py'\n","    # from file_logger_module import FileLogger\n","    # Placeholder if FileLogger isn't easily importable:\n","    class SimpleLogger:\n","        def __init__(self, log_file=None): self.log_file=log_file # Optional log file\n","        def log(self, msg): print(msg) # Simple print for now\n","    logger = SimpleLogger()\n","except ImportError:\n","     class SimpleLogger:\n","        def __init__(self, log_file=None): self.log_file=log_file\n","        def log(self, msg): print(msg)\n","     logger = SimpleLogger()\n","\n","class FileLogger:\n","    def __init__(self, log_file):\n","        self.log_file = log_file; log_dir = os.path.dirname(log_file)\n","        if log_dir and not os.path.exists(log_dir): os.makedirs(log_dir, exist_ok=True)\n","        try:\n","            # Use 'a' to append if resuming is common, 'w' to start fresh each time\n","            # Let's use 'w' for now, assuming state file handles resume logic primarily\n","            with open(self.log_file, 'w', encoding='utf-8') as f: f.write(f\"{self._get_timestamp()}: Starting new log...\\n\")\n","        except IOError as e: print(f\"CRITICAL: Failed to initialize log file {self.log_file}: {e}\")\n","    def _get_timestamp(self): return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","    def log(self, message: str):\n","        try:\n","            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","        except Exception as e: print(f\"!!! LOGGER FAILED TO WRITE: {e} !!!\"); print(f\"{self._get_timestamp()}: {message}\") # Fallback print\n","    def _get_serializer(self):\n","        # Define a nested function to handle custom serialization types\n","        def default_serializer(obj):\n","            if isinstance(obj, (np.integer, np.floating)): return obj.item() # Convert to standard Python int/float\n","            if isinstance(obj, np.ndarray): return obj.tolist() # Convert to Python list\n","            if isinstance(obj, set): return sorted(list(obj)) # Convert to sorted list for consistent output\n","            # Handle torch tensors if torch is imported\n","            if 'torch' in sys.modules and isinstance(obj, torch.Tensor): return obj.detach().cpu().numpy().tolist()\n","            # If none of the above, try the default JSON encoder\n","            try: return json.JSONEncoder().default(obj) # Will raise TypeError if not serializable\n","            except TypeError: return f\"UNSERIALIZABLE:{type(obj).__name__}\" # Return string representation\n","        return default_serializer # Return the nested function\n","    def log_json(self, data: Dict[str, Any], prefix=\"JSON_DATA\"):\n","        serializer = self._get_serializer(); log_entry = { \"timestamp\": self._get_timestamp(), \"type\": prefix, \"data\": data }\n","        try:\n","            json_string = json.dumps(log_entry, default=serializer, indent=2)\n","            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(json_string + \"\\n\")\n","        except Exception as e: self.log(f\"ERROR: Unexpected error during log_json (prefix={prefix}): {e}\") # Note: Uses self.log here\n","\n","\n","# =====================================================\n","#              Pipeline Configuration Constants (Stage 5)\n","# =====================================================\n","# Input: Final JSON output from the *full* Stage 4 run (with summary attempts)\n","FULL_STAGE4_OUTPUT_FILE = 'pdf_passage_analysis_by_response_minilm_gemma.json'\n","# Output: JSON file containing the structured analysis results per question\n","ANALYSIS_OUTPUT_FILE = 'comprehensive_analysis_report.json'\n","# Log file for this stage\n","STAGE5_LOG_FILE = \"stage5_comprehensive_analysis.log\"\n","# Parameters for Analysis\n","NUM_TOP_KEYWORDS = 20 # Extract more keywords for better insight\n","USE_VADER_SENTIMENT = True # Keep using VADER for speed\n","\n","# =====================================================\n","#                 Logger Initialization\n","# =====================================================\n","# logger = FileLogger(STAGE5_LOG_FILE) # Use actual FileLogger if imported\n","logger = FileLogger(STAGE5_LOG_FILE)\n","\n","# =====================================================\n","#         NLTK Setup (Ensure necessary resources)\n","# =====================================================\n","def download_nltk_resources_stage5():\n","    resources = {\n","        'corpora/stopwords': 'stopwords',\n","        'tokenizers/punkt': 'punkt',\n","        'sentiment/vader_lexicon': 'vader_lexicon'\n","    }\n","    for resource_id, download_name in resources.items():\n","        try: nltk.data.find(resource_id)\n","        except LookupError:\n","            logger.log(f\"INFO: Downloading NLTK resource: {download_name}\")\n","            try: nltk.download(download_name, quiet=True)\n","            except Exception as e: logger.log(f\"ERROR: Failed NLTK download {download_name}: {e}\")\n","\n","logger.log(\"INFO: Checking NLTK resources for Stage 5...\")\n","download_nltk_resources_stage5()\n","logger.log(\"INFO: NLTK check complete.\")\n","\n","# =====================================================\n","#       Helper Functions for Analysis\n","# =====================================================\n","def preprocess_text(text):\n","    \"\"\"Basic text cleaning: lowercasing, remove punctuation/numbers, remove stopwords.\"\"\"\n","    if not text or not isinstance(text, str): return \"\"\n","    text = text.lower()\n","    # Remove punctuation more thoroughly\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    # Remove numbers\n","    text = re.sub(r'\\d+', '', text)\n","    try:\n","        stop_words = set(stopwords.words('english'))\n","        tokens = word_tokenize(text)\n","        # Keep only alphabetic tokens (removes single letters often left over)\n","        filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n","        return \" \".join(filtered_tokens)\n","    except Exception as e:\n","        logger.log(f\"WARN: Preprocessing failed: {e}. Text starts: '{text[:50]}...'\")\n","        return \"\" # Return empty on failure to avoid polluting analysis\n","\n","def get_sentiment_vader(text):\n","    \"\"\"Calculates sentiment using VADER.\"\"\"\n","    if not text or not isinstance(text, str): return 'neutral', 0.0\n","    try:\n","        # Lazy load analyzer if needed\n","        if not hasattr(get_sentiment_vader, 'analyzer'):\n","             get_sentiment_vader.analyzer = SentimentIntensityAnalyzer()\n","        vs = get_sentiment_vader.analyzer.polarity_scores(text)\n","        # Classify based on compound score\n","        if vs['compound'] >= 0.05: sentiment = 'positive'\n","        elif vs['compound'] <= -0.05: sentiment = 'negative'\n","        else: sentiment = 'neutral'\n","        return sentiment, vs['compound']\n","    except Exception as e:\n","        logger.log(f\"ERROR: VADER sentiment failed: {e}. Text starts: '{text[:50]}...'\")\n","        return 'error', 0.0\n","\n","def get_top_keywords_tfidf(texts: List[str], num_keywords: int) -> List[Tuple[str, float]]:\n","    \"\"\"Extracts top keywords using TF-IDF.\"\"\"\n","    if not texts: return []\n","    valid_texts = [t for t in texts if isinstance(t, str) and t.strip()]\n","    if not valid_texts: return []\n","    try:\n","        processed_texts = [preprocess_text(text) for text in valid_texts]\n","        processed_texts = [t for t in processed_texts if t]\n","        if not processed_texts:\n","             logger.log(\"WARN: No processable text left after preprocessing for keyword extraction.\")\n","             return []\n","\n","        vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), stop_words='english') # Use built-in stopwords too\n","        tfidf_matrix = vectorizer.fit_transform(processed_texts)\n","        feature_names = vectorizer.get_feature_names_out()\n","        summed_tfidf = tfidf_matrix.sum(axis=0)\n","        summed_tfidf_array = np.asarray(summed_tfidf).flatten()\n","        actual_num_keywords = min(num_keywords, len(feature_names))\n","        if actual_num_keywords <= 0: return []\n","        top_indices = np.argsort(summed_tfidf_array)[-actual_num_keywords:][::-1]\n","        # Ensure scores are standard floats for JSON\n","        keywords = [(feature_names[i], float(summed_tfidf_array[i])) for i in top_indices]\n","        return keywords\n","    except ValueError as ve: logger.log(f\"WARN: TF-IDF ValueError: {ve}\"); return []\n","    except Exception as e: logger.log(f\"ERROR: TF-IDF extraction failed: {e}\"); return []\n","\n","# =====================================================\n","#                   Main Execution Block (Stage 5)\n","# =====================================================\n","if __name__ == \"__main__\":\n","    logger.log(\"=\" * 50); logger.log(\" INFO: Starting Stage 5: Comprehensive Analysis Report \"); logger.log(\"=\" * 50)\n","    logger.log(f\"INFO: Input Stage 4 Data: {FULL_STAGE4_OUTPUT_FILE}\")\n","    logger.log(f\"INFO: Output Analysis File: {ANALYSIS_OUTPUT_FILE}\")\n","\n","    # --- Overall Counters ---\n","    overall_stats = {\n","        \"total_responses_in_input\": 0,\n","        \"responses_with_pdf_content_analyzed\": 0,\n","        \"questions_analyzed_count\": 0,\n","        \"total_content_items_analyzed\": 0, # Sum of summaries + passages used\n","        \"total_non_blank_summaries_used\": 0,\n","        \"total_passages_used_as_fallback\": 0,\n","        \"overall_sentiment_distribution\": Counter(),\n","        \"average_sentiment_compound_overall\": [], # Collect all compound scores\n","    }\n","\n","    analysis_results_per_qid = {} # Main output structure\n","    main_success = False\n","    start_time = time.time()\n","\n","    try:\n","        # 1. Load Full Data from Stage 4 Output\n","        logger.log(f\"INFO: Loading full Stage 4 data from {FULL_STAGE4_OUTPUT_FILE}...\")\n","        try:\n","            with open(FULL_STAGE4_OUTPUT_FILE, 'r', encoding='utf-8') as f: full_stage4_data = json.load(f)\n","            retrieval_data = full_stage4_data.get(\"pdf_analysis_by_response\") # Use the correct top-level key\n","            if retrieval_data is None: raise ValueError(\"'pdf_analysis_by_response' key not found.\")\n","            overall_stats[\"total_responses_in_input\"] = len(retrieval_data)\n","            logger.log(f\"INFO: Loaded Stage 4 data for {overall_stats['total_responses_in_input']} responses.\")\n","        except FileNotFoundError: logger.log(f\"ERROR: Stage 4 input file not found: {FULL_STAGE4_OUTPUT_FILE}\"); sys.exit(1)\n","        except (json.JSONDecodeError, ValueError) as e: logger.log(f\"ERROR: Failed parsing JSON/invalid format in {FULL_STAGE4_OUTPUT_FILE}: {e}\"); sys.exit(1)\n","        except Exception as e: logger.log(f\"ERROR: Unexpected error loading {FULL_STAGE4_OUTPUT_FILE}: {e}\"); tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\"); sys.exit(1)\n","\n","        # 2. Aggregate Content (Summaries or Passages) per Question\n","        logger.log(\"INFO: Aggregating content (prioritizing summaries) per question...\")\n","        # Structure: qid -> {\"question_text\": str, \"content_items\": list, \"source_types\": Counter}\n","        aggregated_content_by_qid = defaultdict(lambda: {\"question_text\": \"\", \"content_items\": [], \"source_types\": Counter()})\n","        question_texts = {}\n","\n","        for response_id, response_data in retrieval_data.items():\n","            if not isinstance(response_data, dict): continue # Skip simple status entries like {\"status\": \"pdf_missing_or_short\"}\n","            if \"status\" in response_data and response_data[\"status\"] in [\"pdf_missing_or_short\", \"error_processing_response\", \"error_pdf_embedding\", \"pdf_no_sentences\", \"error_json_decode\"]:\n","                continue # Skip responses where high-level PDF processing failed\n","\n","            found_content_flag = False\n","            for qid, q_data in response_data.items():\n","                if isinstance(q_data, dict):\n","                    # Store question text once\n","                    if qid not in question_texts and \"question_text\" in q_data:\n","                         question_texts[qid] = q_data[\"question_text\"]\n","\n","                    content_to_add = None; source_type = \"none\"\n","\n","                    # --- Prioritize Non-Blank Summary ---\n","                    summary_text = q_data.get(\"summary\")\n","                    if q_data.get(\"status\") == \"success_summarized\" and summary_text is not None and isinstance(summary_text, str) and summary_text.strip():\n","                        content_to_add = summary_text.strip()\n","                        source_type = \"summary\"\n","                        overall_stats[\"total_non_blank_summaries_used\"] += 1\n","                    # --- Fallback to Extracted Passages ---\n","                    elif q_data.get(\"extracted_passages\"):\n","                        passages = q_data[\"extracted_passages\"]\n","                        if isinstance(passages, list) and passages:\n","                            # Join passages into a single block for analysis\n","                            passage_block = \"\\n\\n\".join(p for p in passages if isinstance(p, str) and p.strip())\n","                            if passage_block.strip():\n","                                content_to_add = passage_block\n","                                source_type = \"passages\"\n","                                overall_stats[\"total_passages_used_as_fallback\"] += 1\n","\n","                    # If content was found, aggregate it\n","                    if content_to_add:\n","                        aggregated_content_by_qid[qid][\"content_items\"].append(content_to_add)\n","                        aggregated_content_by_qid[qid][\"source_types\"][source_type] += 1\n","                        found_content_flag = True\n","\n","            if found_content_flag:\n","                overall_stats[\"responses_with_pdf_content_analyzed\"] += 1\n","        # --- End Aggregation Loop ---\n","\n","        overall_stats[\"questions_analyzed_count\"] = len(aggregated_content_by_qid)\n","        logger.log(f\"INFO: Aggregated content from {overall_stats['responses_with_pdf_content_analyzed']} responses across {overall_stats['questions_analyzed_count']} questions.\")\n","        logger.log(f\"INFO: Total non-blank summaries used: {overall_stats['total_non_blank_summaries_used']}\")\n","        logger.log(f\"INFO: Total passage blocks used as fallback: {overall_stats['total_passages_used_as_fallback']}\")\n","\n","        # 3. Perform Analysis on Aggregated Content per Question\n","        logger.log(\"INFO: Performing sentiment and keyword analysis per question...\")\n","        for qid, data in aggregated_content_by_qid.items():\n","            analysis_results_per_qid[qid] = {}\n","            analysis_results_per_qid[qid][\"question_text\"] = question_texts.get(qid, f\"Unknown Text for {qid}\")\n","            all_content_items = data[\"content_items\"]\n","            analysis_results_per_qid[qid][\"content_analyzed_count\"] = len(all_content_items)\n","            analysis_results_per_qid[qid][\"source_distribution\"] = dict(data[\"source_types\"])\n","\n","            if not all_content_items:\n","                logger.log(f\"WARN: No content items to analyze for QID {qid}.\")\n","                analysis_results_per_qid[qid][\"sentiment\"] = {\"distribution\": {}, \"average_compound\": 0.0}\n","                analysis_results_per_qid[qid][\"top_keywords_tfidf\"] = []\n","                continue\n","\n","            overall_stats[\"total_content_items_analyzed\"] += len(all_content_items)\n","\n","            # Sentiment Analysis\n","            sentiments = []; compound_scores = []\n","            for item in all_content_items:\n","                sentiment_label, compound_score = get_sentiment_vader(item)\n","                sentiments.append(sentiment_label)\n","                compound_scores.append(compound_score)\n","                # Add to overall counters/lists\n","                overall_stats[\"overall_sentiment_distribution\"][sentiment_label] += 1\n","                overall_stats[\"average_sentiment_compound_overall\"].append(compound_score)\n","\n","            sentiment_distribution = Counter(sentiments)\n","            average_compound = np.mean(compound_scores) if compound_scores else 0.0\n","            analysis_results_per_qid[qid][\"sentiment\"] = {\n","                \"distribution\": dict(sentiment_distribution),\n","                \"average_compound\": float(average_compound)\n","            }\n","\n","            # Keyword Extraction\n","            top_keywords = get_top_keywords_tfidf(all_content_items, NUM_TOP_KEYWORDS)\n","            analysis_results_per_qid[qid][\"top_keywords_tfidf\"] = top_keywords\n","\n","            if (len(analysis_results_per_qid) % 5 == 0): logger.log(f\"INFO: ...analyzed {len(analysis_results_per_qid)}/{len(aggregated_content_by_qid)} questions.\")\n","\n","        logger.log(\"INFO: Analysis complete.\")\n","\n","        # Calculate final overall stats\n","        if overall_stats[\"average_sentiment_compound_overall\"]:\n","            overall_stats[\"average_sentiment_compound_overall\"] = float(np.mean(overall_stats[\"average_sentiment_compound_overall\"]))\n","        else:\n","             overall_stats[\"average_sentiment_compound_overall\"] = 0.0\n","        overall_stats[\"overall_sentiment_distribution\"] = dict(overall_stats[\"overall_sentiment_distribution\"])\n","\n","\n","        # 4. Prepare and Save Final Report Data\n","        final_report_data = {\n","            \"overall_analysis_statistics\": overall_stats,\n","            \"analysis_by_question\": analysis_results_per_qid\n","        }\n","\n","        with open(ANALYSIS_OUTPUT_FILE, 'w', encoding='utf-8') as f:\n","             # Use a serializer that handles numpy floats potentially left from np.mean\n","             json.dump(final_report_data, f, indent=2, default=logger._get_serializer())\n","        logger.log(f\"INFO: Comprehensive analysis report saved to {ANALYSIS_OUTPUT_FILE}\")\n","        main_success = True\n","\n","    except Exception as main_e:\n","        logger.log(f\"CRITICAL: FATAL ERROR during analysis: {main_e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"CRITICAL TRACEBACK:\\n{tb_str}\")\n","\n","    finally:\n","        end_time = time.time()\n","        # Log final overall stats\n","        logger.log(\"=\" * 20 + \" Overall Statistics Summary \" + \"=\" * 20)\n","        for key, value in overall_stats.items():\n","            if key == \"average_sentiment_compound_overall\":\n","                logger.log(f\"  {key}: {value:.4f}\")\n","            else:\n","                logger.log(f\"  {key}: {value}\")\n","        logger.log(\"=\" * 66)\n","        logger.log(f\"INFO: Stage 5 finished in {end_time - start_time:.2f} seconds.\")\n","        logger.log(\"=\" * 50)\n","        if main_success: logger.log(\" INFO: Stage 5 Analysis Finished Successfully \")\n","        else: logger.log(\" ERROR: Stage 5 Analysis Finished With Errors \")\n","        logger.log(\"=\" * 50)"]},{"cell_type":"markdown","metadata":{"id":"jgGvaonsgh-5"},"source":["# 17th Apr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RwpewUJugk6O"},"outputs":[],"source":["# =====================================================\n","# @title Stage 4: PDF Content Analysis (Retrieval & Selective Summarization)\n","# =====================================================\n","# Purpose: Processes original responses, extracts PDF text, retrieves\n","#          relevant passages using local embeddings (MiniLM), selectively\n","#          generates LLM summaries (Gemma 2B) for retrieved passages,\n","#          and includes runtime limits with resume capability.\n","\n","# =====================================================\n","#                      Imports\n","# =====================================================\n","import json\n","import os\n","import sys\n","import traceback\n","import time\n","import gc\n","import re\n","from collections import defaultdict\n","from typing import Dict, Any, List, Tuple, Optional\n","import warnings\n","import argparse # Kept for potential future use\n","import tempfile # Needed for test mode\n","\n","# Suppress specific warnings if needed\n","warnings.filterwarnings(\"ignore\", message=\".*Using the latest cached version.*\")\n","warnings.filterwarnings(\"ignore\", message=\".*torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.*\")\n","\n","\n","# --- ML/NLP Imports ---\n","try:\n","    import torch\n","    import numpy as np\n","    from sentence_transformers import SentenceTransformer, util\n","    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","    import nltk\n","    from nltk.tokenize import sent_tokenize\n","except ImportError as e:\n","    # Cannot use logger before it's initialized\n","    print(f\"CRITICAL ERROR: Missing essential ML/NLP libraries. Install them. Details: {e}\")\n","    print(\"\\nTry: pip install torch numpy sentence-transformers transformers accelerate bitsandbytes nltk scikit-learn\")\n","    sys.exit(1)\n","\n","# --- Optional Imports ---\n","try:\n","    # Only required if USE_QUANTIZATION is True\n","    from bitsandbytes.cextension import ColabNotSupportedError\n","except ImportError:\n","    ColabNotSupportedError = None # Define dummy class if not importable\n","    pass # Silently ignore if bitsandbytes is not installed but quantization is False\n","\n","\n","# =====================================================\n","#                Custom FileLogger Class\n","# =====================================================\n","# (Using simplified version compatible with logger.log(\"LEVEL: ...\"))\n","class FileLogger:\n","    def __init__(self, log_file):\n","        self.log_file = log_file; log_dir = os.path.dirname(log_file)\n","        if log_dir and not os.path.exists(log_dir): os.makedirs(log_dir, exist_ok=True)\n","        try:\n","            # Use 'a' to append if resuming is common, 'w' to start fresh each time\n","            # Let's use 'w' for now, assuming state file handles resume logic primarily\n","            with open(self.log_file, 'w', encoding='utf-8') as f: f.write(f\"{self._get_timestamp()}: Starting new log...\\n\")\n","        except IOError as e: print(f\"CRITICAL: Failed to initialize log file {self.log_file}: {e}\")\n","    def _get_timestamp(self): return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","    def log(self, message: str):\n","        try:\n","            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","        except Exception as e: print(f\"!!! LOGGER FAILED TO WRITE: {e} !!!\"); print(f\"{self._get_timestamp()}: {message}\") # Fallback print\n","    def _get_serializer(self):\n","        # Define a nested function to handle custom serialization types\n","        def default_serializer(obj):\n","            if isinstance(obj, (np.integer, np.floating)): return obj.item() # Convert to standard Python int/float\n","            if isinstance(obj, np.ndarray): return obj.tolist() # Convert to Python list\n","            if isinstance(obj, set): return sorted(list(obj)) # Convert to sorted list for consistent output\n","            # Handle torch tensors if torch is imported\n","            if 'torch' in sys.modules and isinstance(obj, torch.Tensor): return obj.detach().cpu().numpy().tolist()\n","            # If none of the above, try the default JSON encoder\n","            try: return json.JSONEncoder().default(obj) # Will raise TypeError if not serializable\n","            except TypeError: return f\"UNSERIALIZABLE:{type(obj).__name__}\" # Return string representation\n","        return default_serializer # Return the nested function\n","    def log_json(self, data: Dict[str, Any], prefix=\"JSON_DATA\"):\n","        serializer = self._get_serializer(); log_entry = { \"timestamp\": self._get_timestamp(), \"type\": prefix, \"data\": data }\n","        try:\n","            json_string = json.dumps(log_entry, default=serializer, indent=2)\n","            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(json_string + \"\\n\")\n","        except Exception as e: self.log(f\"ERROR: Unexpected error during log_json (prefix={prefix}): {e}\") # Note: Uses self.log here\n","\n","# =====================================================\n","#              Pipeline Configuration Constants (Stage 4)\n","# =====================================================\n","\n","# --- Input/Output Files ---\n","ORIGINAL_RESPONSES_FILE = 'processed_responses.jsonl' # Source for raw data + PDF text\n","STAGE2_AGGREGATION_FILE = 'question_centric_aggregation_output.json' # Source for QID->Text map\n","PDF_ANALYSIS_OUTPUT_FILE = 'pdf_passage_analysis_by_response_minilm_gemma.json' # Final output\n","STATE_FILE = 'stage4_processing_state.json' # For resuming partial runs\n","\n","# --- Model Configuration ---\n","EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2' # Sticking with MiniLM based on tests\n","LLM_MODEL_ID = 'google/gemma-3-4b-it' # Corrected to Gemma 2B Instruct\n","USE_QUANTIZATION = True # Enable 4-bit quantization for LLM\n","\n","# --- Retrieval & Summarization Parameters ---\n","MIN_SIMILARITY_SCORE = 0.5    # Threshold for passage retrieval\n","CONTEXT_WINDOW_SIZE = 1       # Sentences before/after matched sentence for passage context\n","TOP_K_PASSAGES = 3            # Max relevant passages to retrieve/store per Q\n","NUM_SUMMARY_SENTENCES = 3     # Target sentences for LLM summary\n","MIN_ANSWERS_FOR_SUMMARY = 1   # Min number of text answers needed before attempting summary (can increase)\n","MIN_CHARS_FOR_SUMMARY = 150   # Min total chars in combined passages to attempt summary\n","LLM_MAX_NEW_TOKENS = 350      # Max tokens for the generated summary (adjust based on NUM_SUMMARY_SENTENCES)\n","\n","# --- PDF Processing ---\n","MIN_PDF_TEXT_LENGTH = 100 # Ignore PDFs shorter than this\n","\n","# --- Runtime & Logging ---\n","MAX_RUNTIME_MINUTES = 60 # 60 * 10 # e.g., 10 hours (adjust based on Colab limits/needs)\n","SAVE_STATE_INTERVAL_SECONDS = 60 * 15 # Save progress every 15 minutes\n","STAGE4_LOG_FILE = \"stage4_retrieval_summary_minilm_gemma.log\"\n","ENABLE_DEBUG_LOGGING = False # True # Enable detailed logs, including summary debugging\n","TEST_SINGLE_RESPONSE_ID = None # Set to integer ID for testing, None for full run\n","\n","# --- Manual Question Selection for Summarization ---\n","# Dictionary mapping Question ID -> True/False (whether to summarize)\n","QUESTIONS_TO_SUMMARIZE = {\n","    \"Q1\": True, \"Q2\": True, \"Q3\": True, \"Q4\": True, \"Q5\": True, \"Q6\": True, \"Q7\": True,\n","    \"Q8\": True, \"Q9\": True, \"Q10\": True,\"Q11\": True, \"Q12\": True, \"Q13\": True, \"Q14\": True,\n","    \"Q15\": True, \"Q16\": False,\"Q17\": True, \"Q18\": True, \"Q19\": True,\"Q20\": False, \"Q21\": True,\n","    \"Q22\": True, \"Q23\": True, \"Q24\": True, \"Q25\": True, \"Q26\": True, \"Q27\": True, \"Q28\": True,\n","    \"Q29\": True, \"Q30\": True, \"Q31\": True,\"Q32\": False,\"Q33\": False,\"Q34\": False,\"Q35\": False,\n","    \"Q36\": False,\n","    # Add any other QIDs if applicable, defaulting to False if not present\n","}\n","# =====================================================\n","#                 Logger Initialization\n","# =====================================================\n","logger = FileLogger(STAGE4_LOG_FILE)\n","# =====================================================\n","#         NLTK Setup\n","# =====================================================\n","def download_nltk_resource(resource_id, download_name):\n","    \"\"\"Checks for an NLTK resource and downloads it if missing.\"\"\"\n","    try: nltk.data.find(resource_id); # logger.log(f\"INFO: NLTK resource '{download_name}' already present.\") # Less verbose\n","    except LookupError:\n","        logger.log(f\"INFO: NLTK resource '{download_name}' not found. Downloading...\");\n","        try: nltk.download(download_name, quiet=True); logger.log(f\"INFO: Downloaded NLTK resource: {download_name}\")\n","        except Exception as dl_e: logger.log(f\"ERROR: Failed NLTK download {download_name}: {dl_e}\")\n","    except Exception as e: logger.log(f\"ERROR: NLTK check error {download_name}: {e}\")\n","logger.log(\"INFO: Checking NLTK resources for sentence tokenization...\")\n","download_nltk_resource('tokenizers/punkt', 'punkt')\n","download_nltk_resource('tokenizers/punkt_tab', 'punkt_tab')\n","logger.log(\"INFO: NLTK check complete.\")\n","# =====================================================\n","#       Helper Functions\n","# =====================================================\n","def save_stage4_output(output_file: str, output_data: Dict[str, Any]):\n","    \"\"\"Saves the final Stage 4 output JSON.\"\"\"\n","    logger.log(f\"INFO: Saving Stage 4 output to {output_file}...\")\n","    serializer = logger._get_serializer()\n","    try:\n","        # Wrap results in a top-level key for valid JSON\n","        final_output = {\"pdf_analysis_by_response\": output_data}\n","        with open(output_file, 'w', encoding='utf-8') as f: json.dump(final_output, f, indent=2, default=serializer)\n","        logger.log(f\"INFO: Successfully saved Stage 4 output.\")\n","    except Exception as e: logger.log(f\"ERROR: Saving Stage 4 output failed: {e}\"); tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","\n","def save_state(state_file: str, processed_ids: set, current_results: Dict[str, Any]):\n","    \"\"\"Saves the current processing state to allow resumption.\"\"\"\n","    logger.log(f\"INFO: Saving processing state to {state_file}...\")\n","    try:\n","        # Ensure results dict doesn't contain problematic references if possible\n","        # Convert set to list for JSON compatibility\n","        state_data = {\"processed_responses_ids\": sorted(list(processed_ids)), \"results\": current_results}\n","        temp_state_file = state_file + \".tmp\" # Save to temp first\n","        with open(temp_state_file, 'w', encoding='utf-8') as f: json.dump(state_data, f, indent=2, default=logger._get_serializer())\n","        os.replace(temp_state_file, state_file) # Atomic rename/replace\n","        logger.log(f\"INFO: State saved successfully ({len(processed_ids)} responses processed).\")\n","    except Exception as e: logger.log(f\"ERROR: Failed to save state to {state_file}: {e}\"); tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","\n","def load_state(state_file: str) -> Tuple[set, Dict[str, Any]]:\n","    \"\"\"Loads processing state from a file.\"\"\"\n","    if os.path.exists(state_file):\n","        logger.log(f\"INFO: Found state file {state_file}. Attempting to load state...\")\n","        try:\n","            with open(state_file, 'r', encoding='utf-8') as f: state_data = json.load(f)\n","            # Convert list back to set, ensure results is a dict\n","            processed_ids = set(state_data.get(\"processed_responses_ids\", []));\n","            results = state_data.get(\"results\", {})\n","            if not isinstance(results, dict): raise TypeError(\"Loaded 'results' is not a dictionary.\")\n","            logger.log(f\"INFO: Successfully loaded state. Resuming after {len(processed_ids)} processed responses.\")\n","            return processed_ids, results\n","        except Exception as e: logger.log(f\"ERROR: Failed to load or parse state from {state_file}: {e}. Corrupted file? Starting fresh.\"); return set(), {}\n","    else: logger.log(\"INFO: No state file found. Starting fresh.\"); return set(), {}\n","\n","def get_local_embeddings(texts: List[str], model: SentenceTransformer, device: str, batch_size: int = 64) -> Optional[np.ndarray]:\n","    \"\"\"Generates embeddings using a local SentenceTransformer model.\"\"\"\n","    if not texts: return None\n","    try:\n","        # Corrected Debug Log\n","        if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG: Generating local embeddings for {len(texts)} texts on {device}...\")\n","        start_emb = time.time(); model.to(device)\n","        embeddings = model.encode(texts, convert_to_tensor=True, device=device, show_progress_bar=False, batch_size=batch_size)\n","        emb_time = time.time() - start_emb\n","        if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG: Local embedding generation took {emb_time:.2f}s\")\n","        return embeddings.detach().cpu().numpy() # Detach before moving to CPU\n","    except Exception as e: logger.log(f\"ERROR: Failed to generate local embeddings: {e}\"); tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\"); return None\n","\n","def retrieve_relevant_passages(question_embedding: np.ndarray, sentences: List[str], sentence_embeddings: np.ndarray, threshold: float, top_k: int, context_window: int) -> Tuple[List[str], float | None]:\n","    \"\"\"Finds top K relevant passages above threshold using PyTorch cosine similarity.\"\"\"\n","    # (Using corrected version with explicit torch.float32 conversion)\n","    relevant_passages = []; highest_score_found = None\n","    if question_embedding is None or sentence_embeddings is None or not sentences: return relevant_passages, highest_score_found\n","    if question_embedding.ndim == 1: question_embedding = question_embedding.reshape(1, -1)\n","    if sentence_embeddings.ndim == 1: sentence_embeddings = sentence_embeddings.reshape(1, -1)\n","    if sentence_embeddings.shape[0] != len(sentences): logger.log(f\"ERROR: Sent/Emb count mismatch!\"); return [], None\n","    if question_embedding.shape[1] != sentence_embeddings.shape[1]: logger.log(f\"ERROR: Emb dim mismatch!\"); return [], None\n","    try:\n","        target_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        q_tensor = torch.from_numpy(question_embedding).to(dtype=torch.float32, device=target_device)\n","        s_tensor = torch.from_numpy(sentence_embeddings).to(dtype=torch.float32, device=target_device)\n","        cosine_scores_tensor = util.cos_sim(q_tensor, s_tensor)[0]\n","        cosine_scores = cosine_scores_tensor.cpu().numpy()\n","        sorted_indices = np.argsort(cosine_scores)[::-1]; added_passage_hashes = set()\n","        for idx in sorted_indices:\n","            score = cosine_scores[idx]\n","            if score < threshold: break\n","            if highest_score_found is None: highest_score_found = float(score)\n","            start_idx = max(0, idx - context_window); end_idx = min(len(sentences), idx + context_window + 1)\n","            if start_idx >= end_idx or start_idx >= len(sentences): continue\n","            context_sentences = sentences[start_idx:end_idx]; passage_text = \" \".join(context_sentences).strip(); passage_hash = hash(passage_text)\n","            if passage_text and passage_hash not in added_passage_hashes:\n","                relevant_passages.append(passage_text); added_passage_hashes.add(passage_hash)\n","                if len(relevant_passages) >= top_k: break\n","        return relevant_passages, highest_score_found\n","    except Exception as e: logger.log(f\"ERROR: Failed during passage retrieval: {e}\"); tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\"); return [], None\n","\n","def summarize_context_block(text_chunk: str, question_text: str, qid: str, summarization_pipe: pipeline, num_sentences: int) -> Optional[str]:\n","    \"\"\"Generates a summary for a given text chunk using the LLM pipeline.\"\"\"\n","    # Added qid for specific debugging if needed\n","    if not text_chunk or not summarization_pipe: return None\n","    # Using the detailed prompt structure\n","    messages = [{\"role\": \"user\", \"content\": f\"Carefully review all the following responses to the question: \\\"{question_text}\\\". Then, concisely summarize the main themes or points expressed across these responses in exactly {num_sentences} distinct sentences. Focus on capturing the essence of the input.\\n\\nResponses:\\n\\\"\\\"\\\"\\n{text_chunk}\\n\\\"\\\"\\\"\"}]\n","    try:\n","        prompt_formatted = summarization_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        # Explicitly set generation parameters known to work better for summarization/instruction following\n","        generation_args = {\n","            \"max_new_tokens\": LLM_MAX_NEW_TOKENS,\n","            \"do_sample\": False, # More deterministic summaries\n","            # \"temperature\": 0.7, # Typically not needed with do_sample=False\n","            # \"top_k\": 50,\n","            # \"top_p\": 0.95,\n","            # Add pad_token_id if tokenizer has one and pipeline doesn't handle it\n","            \"pad_token_id\": summarization_pipe.tokenizer.pad_token_id if summarization_pipe.tokenizer.pad_token_id else summarization_pipe.tokenizer.eos_token_id\n","        }\n","        if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG: Calling LLM for summary QID {qid} ({len(text_chunk)} chars input)...\")\n","        start_llm = time.time(); outputs = summarization_pipe(prompt_formatted, **generation_args); llm_time = time.time() - start_llm\n","        if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG: LLM summary generation QID {qid} took {llm_time:.2f}s\")\n","\n","        # --- Debugging Area ---\n","        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n","            logger.log(f\"ERROR [Summarize QID {qid}]: LLM pipeline returned unexpected/empty output: {outputs}\")\n","            return None\n","        generated_text_full = outputs[0]['generated_text']\n","        if ENABLE_DEBUG_LOGGING: # Log raw output only in debug mode to avoid huge logs\n","             logger.log(f\"DEBUG [Summarize QID {qid}]: Raw generated_text_full (first 500 chars):\\n'''\\n{generated_text_full[:500]}\\n'''\")\n","        # --- End Debugging Area ---\n","\n","        # Attempt to parse using the expected marker for Gemma\n","        response_marker = \"<start_of_turn>model\\n\"; summary_start_index = generated_text_full.rfind(response_marker)\n","        if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG [Summarize QID {qid}]: response_marker found at index: {summary_start_index}\")\n","\n","        summary_text = \"\"\n","        if summary_start_index != -1:\n","            summary_text = generated_text_full[summary_start_index + len(response_marker):].strip()\n","            if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG [Summarize QID {qid}]: Extracted summary via marker (first 300 chars):\\n'''\\n{summary_text[:300]}\\n'''\")\n","        else:\n","            logger.log(f\"WARN [Summarize QID {qid}]: Could not find standard model response marker. Attempting prompt removal fallback.\")\n","            # Fallback: try removing the prompt\n","            if generated_text_full.startswith(prompt_formatted):\n","                 summary_text = generated_text_full[len(prompt_formatted):].strip()\n","                 if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG [Summarize QID {qid}]: Extracted summary via fallback (first 300 chars):\\n'''\\n{summary_text[:300]}\\n'''\")\n","            else:\n","                 summary_text = f\"Error: Could not reliably extract summary (marker missing & output mismatch).\"\n","                 logger.log(f\"ERROR [Summarize QID {qid}]: Fallback failed. Output mismatch. Starts: {generated_text_full[:100]}\")\n","\n","        # Final check and return\n","        if summary_text.startswith(\"Error:\") or not summary_text.strip():\n","             logger.log(f\"WARN [Summarize QID {qid}]: Final extracted summary is empty or indicates error.\")\n","             return \"\" # Return empty string to signify failure to summarize meaningfully\n","        return summary_text\n","\n","    except Exception as pipe_err:\n","        logger.log(f\"ERROR: Exception during LLM pipeline execution/parsing QID {qid}: {pipe_err}\")\n","        logger.log(traceback.format_exc())\n","        return None # Return None on exception\n","\n","# =====================================================\n","#                   Main Execution Block (Stage 4)\n","# =====================================================\n","if __name__ == \"__main__\":\n","\n","    logger.log(\"=\" * 50); logger.log(\" INFO: Starting Stage 4: PDF Retrieval & Summarization (MiniLM + Gemma) \"); logger.log(\"=\" * 50)\n","    logger.log(f\"INFO: Input Original Responses (JSONL): {ORIGINAL_RESPONSES_FILE}\")\n","    logger.log(f\"INFO: Input Stage 2 Aggregation (Q Map): {STAGE2_AGGREGATION_FILE}\")\n","    logger.log(f\"INFO: Output PDF Analysis File (JSON): {PDF_ANALYSIS_OUTPUT_FILE}\")\n","    logger.log(f\"INFO: State File for Resumption: {STATE_FILE}\")\n","    logger.log(f\"INFO: Embedding Model: {EMBEDDING_MODEL_NAME}\")\n","    logger.log(f\"INFO: LLM Model: {LLM_MODEL_ID}\")\n","    logger.log(f\"INFO: Use Quantization: {USE_QUANTIZATION}\")\n","    logger.log(f\"INFO: Min Similarity Score: {MIN_SIMILARITY_SCORE}\")\n","    logger.log(f\"INFO: Runtime Limit (Minutes): {MAX_RUNTIME_MINUTES}\")\n","    logger.log(f\"INFO: Test Mode Single Response ID: {TEST_SINGLE_RESPONSE_ID}\")\n","\n","    # --- Load LLM and Embedding Models ---\n","    embedding_model = None; loaded_llm_tokenizer = None; model_instance_llm = None; summarization_pipe = None\n","    models_loaded_success = False; device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    try:\n","        logger.log(f\"INFO: Loading embedding model: {EMBEDDING_MODEL_NAME} onto {device}...\")\n","        embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)\n","        logger.log(\"INFO: Embedding model loaded successfully.\")\n","        logger.log(f\"INFO: Loading LLM tokenizer for: {LLM_MODEL_ID}...\")\n","        loaded_llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)\n","        if getattr(loaded_llm_tokenizer, 'pad_token', None) is None and getattr(loaded_llm_tokenizer, 'eos_token', None):\n","            logger.log(f\"WARN: LLM Tokenizer missing pad token; setting to eos_token.\")\n","            loaded_llm_tokenizer.pad_token = loaded_llm_tokenizer.eos_token\n","            loaded_llm_tokenizer.padding_side = \"left\" # Usually needed for generation tasks\n","        elif getattr(loaded_llm_tokenizer, 'pad_token', None) is None:\n","             logger.log(f\"WARN: LLM Tokenizer missing pad token and eos_token. Padding may fail.\")\n","        logger.log(\"INFO: LLM Tokenizer loaded.\")\n","        bnb_config = None; effective_quantization = USE_QUANTIZATION\n","        if USE_QUANTIZATION:\n","            logger.log(\"INFO: Setting up 4-bit quantization...\")\n","            try:\n","                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n","                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n","                logger.log(f\"INFO: 4-bit quantization configured (Compute dtype: {compute_dtype}).\")\n","            except ImportError: logger.log(\"ERROR: bitsandbytes not found. Install it for quantization. Disabling quantization.\"); effective_quantization = False\n","            except ColabNotSupportedError: logger.log(\"ERROR: Bits and Bytes Colab issue detected. Disabling quantization.\"); effective_quantization = False\n","            except Exception as q_err: logger.log(f\"ERROR: Failed BitsAndBytesConfig: {q_err}. Disabling quantization.\"); effective_quantization = False\n","        logger.log(f\"INFO: Loading LLM model: {LLM_MODEL_ID} (Quantization: {effective_quantization})...\")\n","        if not torch.cuda.is_available() and effective_quantization: logger.log(\"WARN: Quantization enabled but CUDA not available! Model will load on CPU, likely very slow.\")\n","        # Load LLM\n","        model_instance_llm = AutoModelForCausalLM.from_pretrained(\n","            LLM_MODEL_ID,\n","            quantization_config=bnb_config if effective_quantization else None,\n","            torch_dtype=compute_dtype if not effective_quantization else None, # Set dtype if not quantizing\n","            device_map=\"auto\", # Let accelerate handle device placement\n","            trust_remote_code=True # Needed for some models like Gemma 2\n","        )\n","        logger.log(\"INFO: LLM model loaded.\")\n","        logger.log(\"INFO: Creating text-generation pipeline...\")\n","        summarization_pipe = pipeline(\n","            \"text-generation\",\n","            model=model_instance_llm,\n","            tokenizer=loaded_llm_tokenizer,\n","            # device arg is usually not needed here when device_map=\"auto\" is used in from_pretrained\n","        )\n","        logger.log(\"INFO: LLM Pipeline created successfully.\")\n","        models_loaded_success = True\n","    except Exception as load_err:\n","        logger.log(f\"CRITICAL: Failed to load models/pipeline: {load_err}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","        sys.exit(1) # Exit if models fail to load\n","\n","    # --- Main Processing Steps ---\n","    main_success = False; results = {}; processed_responses_ids = set(); qid_to_embedding = {}\n","    try:\n","        # 1. Load State if not in test mode\n","        if TEST_SINGLE_RESPONSE_ID is None:\n","            processed_responses_ids, results = load_state(STATE_FILE)\n","        else:\n","            logger.log(\"INFO: Test mode enabled. Ignoring state file.\")\n","            processed_responses_ids, results = set(), {} # Ensure clean start for test\n","\n","        # 2. Load Question Texts (always needed)\n","        logger.log(f\"INFO: Loading question texts from {STAGE2_AGGREGATION_FILE}...\")\n","        qid_to_text = {}\n","        try:\n","            with open(STAGE2_AGGREGATION_FILE, 'r', encoding='utf-8') as f: stage2_data = json.load(f)\n","            question_data_from_stage2 = stage2_data.get(\"question_data\", {})\n","            for qid, data in question_data_from_stage2.items():\n","                if \"question_text\" in data: qid_to_text[qid] = data[\"question_text\"]\n","            if not qid_to_text: raise ValueError(\"No question texts found.\")\n","            logger.log(f\"INFO: Loaded text for {len(qid_to_text)} questions.\")\n","        except Exception as e: logger.log(f\"ERROR: Failed loading question texts: {e}\"); sys.exit(1)\n","\n","        # 3. Pre-compute Question Embeddings (always needed)\n","        logger.log(\"INFO: Pre-computing embeddings for all target questions...\")\n","        target_qids = list(qid_to_text.keys()); target_qtexts = list(qid_to_text.values())\n","        question_embeddings_all = get_local_embeddings(target_qtexts, embedding_model, device)\n","        if question_embeddings_all is None: logger.log(\"CRITICAL: Failed to generate question embeddings.\"); sys.exit(1)\n","        qid_to_embedding = {qid: emb for qid, emb in zip(target_qids, question_embeddings_all)}\n","        logger.log(f\"INFO: Generated embeddings for {len(qid_to_embedding)} questions.\")\n","\n","        # 4. Determine Input File (Original or Temp for Test)\n","        input_file_path = ORIGINAL_RESPONSES_FILE; temp_file_for_testing = None\n","        if TEST_SINGLE_RESPONSE_ID is not None:\n","            logger.log(f\"WARN: --- RUNNING IN SINGLE RESPONSE TEST MODE (ID: {TEST_SINGLE_RESPONSE_ID}) ---\")\n","            try:\n","                with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.jsonl', encoding='utf-8') as temp_file:\n","                    temp_file_for_testing = temp_file.name; logger.log(f\"INFO: Created temporary test file: {temp_file_for_testing}\"); found = False\n","                    try:\n","                        with open(ORIGINAL_RESPONSES_FILE, 'r', encoding='utf-8') as original_file:\n","                            for line in original_file:\n","                                try:\n","                                    response = json.loads(line)\n","                                    if str(response.get(\"id\")) == str(TEST_SINGLE_RESPONSE_ID):\n","                                        temp_file.write(line); found = True; logger.log(f\"INFO: Copied response ID {TEST_SINGLE_RESPONSE_ID}.\"); break\n","                                except json.JSONDecodeError: continue # Skip bad lines in source\n","                    except FileNotFoundError: logger.log(f\"ERROR: Original file not found: {ORIGINAL_RESPONSES_FILE}\"); sys.exit(1)\n","                    if not found: logger.log(f\"ERROR: Test response ID {TEST_SINGLE_RESPONSE_ID} not found.\"); sys.exit(1)\n","                input_file_path = temp_file_for_testing # Override input path\n","            except Exception as temp_err: logger.log(f\"ERROR: Error setting up test mode: {temp_err}\"); tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\"); sys.exit(1)\n","        else: logger.log(f\"INFO: --- RUNNING IN FULL/RESUME MODE (Input: {input_file_path}) ---\")\n","\n","        # 5. Main Processing Loop\n","        processed_in_this_run = 0; pdfs_processed_this_run = 0; pdfs_skipped_this_run = 0; summarization_tasks_run = 0; summarization_tasks_skipped_time = 0\n","        start_processing_time = time.time(); last_save_time = start_processing_time\n","        MAX_RUNTIME_SECONDS = MAX_RUNTIME_MINUTES * 60; hit_time_limit = False\n","\n","        logger.log(f\"INFO: Starting processing loop for {input_file_path}...\")\n","        try:\n","            with open(input_file_path, 'r', encoding='utf-8') as infile:\n","                line_num_processed = 0 # Counter for lines actually processed (not skipped by state)\n","                for line_num, line in enumerate(infile, 1):\n","                    # --- Runtime Limit Check (Only in full run mode) ---\n","                    if TEST_SINGLE_RESPONSE_ID is None:\n","                         elapsed_seconds = time.time() - start_processing_time\n","                         if elapsed_seconds >= MAX_RUNTIME_SECONDS:\n","                             logger.log(f\"WARN: Reached runtime limit ({elapsed_seconds:.0f}s >= {MAX_RUNTIME_SECONDS}s). Stopping processing loop.\")\n","                             hit_time_limit = True; break # Exit the loop\n","                         # --- Periodic State Saving (Only in full run mode) ---\n","                         current_time = time.time()\n","                         if current_time - last_save_time >= SAVE_STATE_INTERVAL_SECONDS:\n","                             save_state(STATE_FILE, processed_responses_ids, results)\n","                             last_save_time = current_time\n","\n","                    # --- Initialize variables for this response ---\n","                    response_id = f\"Unknown_Line_{line_num}\" # Default ID\n","                    try:\n","                        line_strip = line.strip();\n","                        if not line_strip: continue # Skip empty lines\n","                        response_object = json.loads(line_strip);\n","                        response_id = str(response_object.get(\"id\", response_id)) # Get actual ID, ensure string\n","\n","                        # --- Skip if already processed (Resume Logic) ---\n","                        if response_id in processed_responses_ids:\n","                             # Log skipping less frequently in full runs\n","                             if ENABLE_DEBUG_LOGGING and (line_num_processed % 1000 == 0) and TEST_SINGLE_RESPONSE_ID is None:\n","                                 logger.log(f\"DEBUG: Skipping already processed response ID {response_id}\")\n","                             continue # Move to next line\n","\n","                        # If not skipped, process it\n","                        processed_in_this_run += 1\n","                        line_num_processed +=1 # Increment counter for lines attempted in this run\n","\n","                        # Initialize entry in results dict (overwrites if loaded from state)\n","                        results[response_id] = {}\n","\n","                        if ENABLE_DEBUG_LOGGING and (processed_in_this_run % 100 == 0 or processed_in_this_run <= 5 or TEST_SINGLE_RESPONSE_ID is not None) : logger.log(f\"DEBUG: Processing response {processed_in_this_run} (ID: {response_id}, Line: {line_num})...\")\n","\n","                        # --- Check for PDF Text ---\n","                        pdf_text = response_object.get(\"pdf_text\")\n","                        if not pdf_text or len(pdf_text) < MIN_PDF_TEXT_LENGTH:\n","                            pdfs_skipped_this_run += 1\n","                            results[response_id] = {\"status\": \"pdf_missing_or_short\"} # Set top-level status\n","                            processed_responses_ids.add(response_id) # Mark as processed\n","                            continue # Skip to the next response line\n","\n","                        # --- Process PDF ---\n","                        pdf_processed_flag = True\n","                        cleaned_pdf_text = re.sub(r'\\s+', ' ', pdf_text).strip()\n","                        sentences = nltk.sent_tokenize(cleaned_pdf_text); sentences = [s.strip() for s in sentences if s.strip()]\n","\n","                        if not sentences: status_msg = \"pdf_no_sentences\"; logger.log(f\"WARN: {status_msg} for Resp ID {response_id}.\")\n","                        else: sentence_embeddings = get_local_embeddings(sentences, embedding_model, device)\n","\n","                        if not sentences or sentence_embeddings is None:\n","                            status_msg = \"error_pdf_processing_or_embedding\" if sentences else \"pdf_no_sentences\"\n","                            results[response_id] = {\"status\": status_msg} # Set top-level status\n","                            processed_responses_ids.add(response_id) # Mark as processed\n","                            continue # Skip to next response line\n","\n","                        # --- Loop through Questions for this PDF ---\n","                        for qid in target_qids:\n","                            question_text = qid_to_text[qid]; q_embedding = qid_to_embedding.get(qid)\n","                            result_for_qid = {\"question_text\": question_text} # Initialize result for this QID\n","                            results[response_id][qid] = result_for_qid # Store immediately\n","\n","                            if q_embedding is None:\n","                                result_for_qid[\"status\"] = \"error_missing_q_embedding\"\n","                                continue # Go to next question\n","\n","                            relevant_passages, top_score = retrieve_relevant_passages(q_embedding, sentences, sentence_embeddings, MIN_SIMILARITY_SCORE, TOP_K_PASSAGES, CONTEXT_WINDOW_SIZE)\n","\n","                            if relevant_passages:\n","                                result_for_qid[\"status\"] = \"passages_extracted\"\n","                                result_for_qid[\"extracted_passages\"] = relevant_passages\n","                                result_for_qid[\"passages_found\"] = len(relevant_passages)\n","                                result_for_qid[\"top_passage_score\"] = float(top_score) if top_score is not None else None\n","\n","                                # --- Conditional Summarization ---\n","                                should_summarize_config = QUESTIONS_TO_SUMMARIZE.get(qid, False)\n","                                # Check time limit again *before* calling LLM (only needed in full run)\n","                                can_summarize_time = True\n","                                if TEST_SINGLE_RESPONSE_ID is None:\n","                                     elapsed_seconds = time.time() - start_processing_time\n","                                     if elapsed_seconds >= MAX_RUNTIME_SECONDS:\n","                                         if not hit_time_limit: logger.log(f\"INFO: Time limit reached during Q loop ({elapsed_seconds:.0f}s). Skipping summary for QID {qid}.\"); hit_time_limit = True\n","                                         can_summarize_time = False\n","                                     else:\n","                                         can_summarize_time = True\n","\n","                                combined_passage_text = \"\\n\\n\".join(relevant_passages)\n","                                passage_quality_ok = len(combined_passage_text) >= MIN_CHARS_FOR_SUMMARY\n","\n","                                if should_summarize_config and passage_quality_ok and can_summarize_time:\n","                                    summarization_tasks_run += 1\n","                                    summary = summarize_context_block(combined_passage_text, question_text, qid, summarization_pipe, NUM_SUMMARY_SENTENCES) # Pass qid for logging\n","                                    if summary is not None: # Check for None explicitly (indicates error)\n","                                        result_for_qid[\"status\"] = \"success_summarized\"\n","                                        result_for_qid[\"summary\"] = summary\n","                                    else: # Summarization function returned None (error)\n","                                        result_for_qid[\"status\"] = \"error_llm_pipe_post_retrieval\"\n","                                elif should_summarize_config and not passage_quality_ok:\n","                                    result_for_qid[\"status\"] = \"skipped_passage_quality\"\n","                                    if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG: Skipped summary QID {qid}, Resp {response_id} due to passage quality.\")\n","                                elif should_summarize_config and not can_summarize_time:\n","                                    summarization_tasks_skipped_time += 1\n","                                    result_for_qid[\"status\"] = \"skipped_runtime_limit\"\n","                                # --- End Summarization Logic ---\n","\n","                            else: # No relevant passages found\n","                                result_for_qid[\"status\"] = \"no_relevant_passages_found\"; result_for_qid[\"passages_found\"] = 0; result_for_qid[\"top_passage_score\"] = None\n","\n","                        # --- End of question loop ---\n","                        if pdf_processed_flag: pdfs_processed_this_run += 1\n","                        processed_responses_ids.add(response_id) # Mark response as fully processed\n","\n","                    except json.JSONDecodeError as json_err:\n","                        logger.log(f\"WARN: Skipping line {line_num} due to JSON parse error: {json_err}\")\n","                        results[response_id] = {\"status\": \"error_json_decode\"} # Store error status\n","                        processed_responses_ids.add(response_id) # Mark as processed to avoid retry\n","                    except Exception as resp_err:\n","                        logger.log(f\"ERROR: Processing response line {line_num} (ID: {response_id}) failed: {resp_err}\")\n","                        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","                        results[response_id] = {\"status\": \"error_processing_response\", \"error_message\": str(resp_err)}\n","                        processed_responses_ids.add(response_id) # Mark as processed\n","                    finally:\n","                        # Cleanup per-response temporary variables conditionally\n","                        if 'sentence_embeddings' in locals() and sentence_embeddings is not None:\n","                            del sentence_embeddings\n","                        if 'sentences' in locals():\n","                            del sentences\n","                        # We know gc exists, so call it directly\n","                        gc.collect()                # --- End of file processing loop ---\n","\n","            logger.log(f\"INFO: Finished processing file loop. Processed in this run: {processed_in_this_run}, PDFs analyzed: {pdfs_processed_this_run}, PDFs skipped: {pdfs_skipped_this_run}, Summaries attempted: {summarization_tasks_run}.\")\n","            if hit_time_limit: logger.log(\"WARN: Processing stopped due to runtime limit.\")\n","\n","        except FileNotFoundError: logger.log(f\"ERROR: Input file not found: {input_file_path}\"); sys.exit(1)\n","        except Exception as file_err: logger.log(f\"ERROR: Unhandled error during file processing: {file_err}\"); tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","\n","        # 6. Final Save (State and Output)\n","        # Save state one last time if in full run mode and processed something\n","        if TEST_SINGLE_RESPONSE_ID is None and processed_in_this_run > 0:\n","             save_state(STATE_FILE, processed_responses_ids, results)\n","        # Save the final accumulated results\n","        save_stage4_output(PDF_ANALYSIS_OUTPUT_FILE, results)\n","        main_success = True\n","\n","    except Exception as main_e:\n","        logger.log(f\"CRITICAL: FATAL ERROR in main execution block: {main_e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"CRITICAL TRACEBACK:\\n{tb_str}\")\n","        # Attempt to save state even on fatal error if possible\n","        if TEST_SINGLE_RESPONSE_ID is None and 'processed_responses_ids' in locals() and 'results' in locals():\n","            logger.log(\"CRITICAL: Attempting to save state before exiting due to fatal error...\")\n","            save_state(STATE_FILE, processed_responses_ids, results)\n","\n","    finally:\n","        # --- Cleanup ---\n","        # Clean up the temporary file if it was created for test mode\n","        if temp_file_for_testing and os.path.exists(temp_file_for_testing):\n","            try: os.unlink(temp_file_for_testing); logger.log(f\"INFO: Deleted temporary test file: {temp_file_for_testing}\")\n","            except OSError as e: logger.log(f\"WARN: Could not delete temporary test file {temp_file_for_testing}: {e}\")\n","\n","        logger.log(\"INFO: Cleaning up models and resources...\")\n","        del embedding_model; del loaded_llm_tokenizer; del model_instance_llm; del summarization_pipe\n","        del qid_to_embedding; del question_embeddings_all; del results; del processed_responses_ids; gc.collect()\n","        if torch.cuda.is_available():\n","             logger.log(\"INFO: Emptying CUDA cache...\")\n","             try: torch.cuda.empty_cache()\n","             except Exception as cuda_err: logger.log(f\"WARN: Error emptying CUDA cache: {cuda_err}\")\n","        logger.log(\"INFO: Cleanup complete.\")\n","        # --- Final Status Log ---\n","        logger.log(\"=\" * 50)\n","        if main_success: logger.log(\" INFO: Stage 4 Processing Finished Successfully \")\n","        else: logger.log(\" ERROR: Stage 4 Processing Finished With Errors or Did Not Complete \")\n","        logger.log(\"=\" * 50)"]},{"cell_type":"markdown","metadata":{"id":"TxSp1OSA8EWH"},"source":["# 8th Apr (Form Data Only)\n","\n","There are typically two main stages involving these UDS structures in the simplified flow (focusing only on form data analysis):\n","\n","Stage 1: Generate Simplified Response-Centric UDS\n","\n","- Input: The original raw responses data (e.g., processed_responses.jsonl), which might contain structured form data and (now ignored) PDF information.\n","\n","- Process: Run the Simplified Response-Centric UDS Script. This script:\n","\n"," - Reads the original raw data to discover all unique questions and create the template_questions map (create_template_questions).\n","\n"," - Iterates through each response in the raw data.\n","\n"," - For each response, it processes only the structured form answers using process_form_answers.\n","\n"," - It skips all PDF processing (process_pdf_answers is commented out or removed).\n","\n"," - It outputs a data structure for each response, containing the processed form answers, metadata, and statistics reflecting only the form data (PDF counts/details will be zero or absent).\n","\n","- Output: A file containing the Simplified Response-Centric UDS (e.g., response_centric_uds_analysis_simplified.json). This file lists individual responses with their processed form data.\n","\n","Stage 2: Generate Question-Centric Aggregation (from Form Data)\n","\n","- Input:\n","\n"," - The Simplified Response-Centric UDS file generated in Stage 1.\n","\n"," - The original raw responses data again (needed to recreate the exact same template_questions map using create_template_questions for consistency).\n","\n","- Process: Run the Simplified Question-Centric Aggregation Script. This script:\n","\n"," - Recreates the template_questions map from the original raw data.\n","\n"," - Loads the entire list of responses from the Simplified Response-Centric UDS file.\n","\n"," - Iterates through this list and aggregates the results by question ID, using the simplified aggregate_responses_by_question function which only considers the form_answers field from its input.\n","\n"," - Calculates counts (like answered_form_count, unanswered_by_form_count) and collects examples for each question based only on the form data.\n","\n","- Output: A file containing the Question-Centric Aggregation (e.g., question_centric_aggregation_output.json). This file structures the data by question, summarizing the form answer statistics across all responses.\n","\n","In Summary:\n","\n","the order is:\n","\n","1. Process raw data -> Simplified Response-Centric UDS (capturing only form data per response).\n","\n","1. Process Simplified Response-Centric UDS -> Question-Centric Aggregation (summarizing form data across responses, grouped by question).\n","\n","This two-stage approach allows you to first normalize and structure the data per response (handling only the desired input source - forms) and then perform the cross-response analysis by pivoting to a question-centric view."]},{"cell_type":"markdown","metadata":{"id":"mgk3o0WBIlqm"},"source":[" ## Step 1: Response-Centric UDS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4yE4qZ5jxi3_"},"outputs":[],"source":["# @title Pipeline Configuration Constants\n","# =====================================================\n","#              Pipeline Configuration Constants\n","# =====================================================\n","\n","# --- Core Input/Output File Definitions ---\n","\n","# The absolute starting point: Original raw responses data file (JSON Lines format assumed)\n","# Used by Stage 1 for processing, and Stage 2 for consistent template creation.\n","ORIGINAL_RESPONSES_FILE = 'processed_responses.jsonl'\n","\n","# The intermediate file: Output of Stage 1 (simplified response-centric UDS)\n","# and Input for Stage 2. (JSON Lines format recommended).\n","SIMPLIFIED_RESPONSE_UDS_FILE = 'response_centric_uds_simplified.jsonl'\n","\n","# The final output file: Result of Stage 2 (question-centric aggregation)\n","# (Single JSON object format).\n","QUESTION_CENTRIC_UDS_FILE = 'question_centric_aggregation_output.json'\n","\n","\n","# --- Logging Configuration ---\n","\n","# Log file specific to Stage 1 processing.\n","STAGE1_LOG_FILE = \"stage1_response_centric_processing.log\"\n","# Log file specific to Stage 2 aggregation.\n","STAGE2_LOG_FILE = \"stage2_question_centric_aggregation.log\"\n","# Global toggle for detailed logging in both stages.\n","ENABLE_DEBUG_LOGGING = True\n","\n","\n","# --- Stage 1 Specific Configuration ---\n","\n","# Set to an integer ID to process only that single response in Stage 1,\n","# helpful for debugging. Set to None to process the full input file.\n","TEST_SINGLE_RESPONSE_ID = None # Example: 838011019\n","\n","\n","# --- Stage 2 Specific Configuration ---\n","\n","# Limits the number of example form answers stored per question\n","# in the final question-centric output file.\n","MAX_EXAMPLE_ANSWERS = 10\n","\n","# =====================================================\n","#             End of Configuration Constants\n","# ====================================================="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFISoo09TrIk"},"outputs":[],"source":["# @title Helper Functions\n","# =====================================================\n","#                      Imports\n","# =====================================================\n","# REMOVED: re, nltk, sklearn, transformers, torch\n","import json\n","import numpy as np\n","import time\n","import os\n","import traceback # For manual traceback logging\n","import sys # For potential fallback print\n","from typing import Dict, Any, Tuple\n","from collections import defaultdict # Keep if used elsewhere\n","\n","# =====================================================\n","#                Custom FileLogger Class\n","# =====================================================\n","class FileLogger:\n","    def __init__(self, log_file):\n","        self.log_file = log_file\n","        log_dir = os.path.dirname(log_file)\n","        if log_dir and not os.path.exists(log_dir):\n","             os.makedirs(log_dir, exist_ok=True)\n","        try:\n","            # Use 'a' for append mode to avoid overwriting logs on restart if desired,\n","            # or keep 'w' to start fresh each run. Let's keep 'w' for now.\n","            with open(self.log_file, 'w', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: Starting new log...\\n\")\n","        except IOError as e:\n","            print(f\"CRITICAL: Failed to initialize log file {self.log_file}: {e}\")\n","            # Optionally exit if logging is critical\n","            # sys.exit(1)\n","\n","    def _get_timestamp(self):\n","        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","\n","    def log(self, message: str):\n","        try:\n","            with open(self.log_file, 'a', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","        except Exception as e:\n","            # Fallback print if logging fails\n","            print(f\"!!! LOGGER FAILED TO WRITE: {e} !!!\")\n","            print(f\"{self._get_timestamp()}: {message}\") # Log to console as fallback\n","\n","    def log_json(self, data: Dict[str, Any], prefix=\"JSON_DATA\"):\n","        serializer = self._get_serializer()\n","        try:\n","            log_entry = {\n","                 \"timestamp\": self._get_timestamp(),\n","                 \"type\": prefix,\n","                 \"data\": data\n","            }\n","            # Use the custom serializer for dumping\n","            json_string = json.dumps(log_entry, default=serializer, indent=2)\n","            with open(self.log_file, 'a', encoding='utf-8') as f:\n","                 f.write(json_string + \"\\n\")\n","        except TypeError as e:\n","            # Log the serialization error using the standard log method\n","            self.log(f\"ERROR: Failed to serialize data for JSON logging: {e} - Data Keys: {list(data.keys())}\")\n","            # Optionally log traceback for serialization errors\n","            # tb_str = traceback.format_exc(); self.log(f\"TRACEBACK:\\n{tb_str}\")\n","        except Exception as e:\n","             # Log any other unexpected errors during JSON logging\n","             self.log(f\"ERROR: Unexpected error during log_json: {e}\")\n","             tb_str = traceback.format_exc(); self.log(f\"TRACEBACK:\\n{tb_str}\")\n","\n","    def _get_serializer(self):\n","        # Define a nested function to handle custom serialization types\n","        def default_serializer(obj):\n","            # Handle NumPy numeric types\n","            if isinstance(obj, (np.integer, np.floating)):\n","                 return obj.item() # Convert to standard Python int/float\n","            # Handle NumPy arrays\n","            if isinstance(obj, np.ndarray):\n","                return obj.tolist() # Convert to Python list\n","            # REMOVED TORCH TENSOR HANDLING BLOCK:\n","            # if isinstance(obj, (torch.Tensor)):\n","            #      return obj.detach().cpu().numpy().tolist()\n","            # Handle sets\n","            if isinstance(obj, set):\n","                return sorted(list(obj)) # Convert to sorted list for consistent output\n","            # If none of the above, try the default JSON encoder\n","            try:\n","                # This will raise TypeError if the object is not serializable by default\n","                return json.JSONEncoder().default(obj)\n","            except TypeError:\n","                # If default encoder fails, return a string representation\n","                # You could log the unserializable type here if needed for debugging\n","                # self.log(f\"DEBUG: Unserializable type encountered: {type(obj).__name__}\")\n","                return f\"UNSERIALIZABLE:{type(obj).__name__}\"\n","        return default_serializer # Return the nested function\n","\n","# =====================================================\n","#                 Global Configuration (Simplified)\n","# =====================================================\n","# REMOVED: PDF_ANSWER_CONFIDENCE_THRESHOLD\n","# REMOVED: CHUNK_MERGE_SIMILARITY_THRESHOLD\n","\n","\n","def create_template_questions(original_responses_file: str) -> Tuple[Dict[str, str], Dict[str, str]]:\n","    \"\"\"\n","    Creates template questions map (ID -> Text) and reverse map (Text -> ID)\n","    by reading the *original* source responses file (JSON Lines format assumed).\n","    \"\"\"\n","    template_questions = {} # id -> text\n","    text_to_id = {}       # text -> id\n","    unique_questions = set()\n","    logger.log(f\"INFO: Creating template questions map from original source: {original_responses_file}\")\n","    try:\n","        with open(original_responses_file, 'r', encoding='utf-8') as f:\n","            processed_lines = 0\n","            for line_num, line in enumerate(f, 1):\n","                try:\n","                    response = json.loads(line)\n","                    # Adjust 'responses' key if your original source structure is different\n","                    form_responses = response.get('responses', {})\n","                    for question_text in form_responses.keys():\n","                        if question_text not in unique_questions:\n","                             unique_questions.add(question_text)\n","                             question_id = f\"Q{len(template_questions) + 1}\"\n","                             template_questions[question_id] = question_text\n","                             text_to_id[question_text] = question_id\n","                    processed_lines += 1\n","                except json.JSONDecodeError:\n","                    logger.log(f\"WARN: Skipping invalid JSON line {line_num} in {original_responses_file}\")\n","            logger.log(f\"INFO: Processed {processed_lines} lines from original file. Found {len(template_questions)} unique questions.\")\n","    except FileNotFoundError:\n","        logger.log(f\"ERROR: Original source responses file not found: {original_responses_file}\")\n","        return {}, {}\n","    except Exception as e:\n","        logger.log(f\"ERROR: Failed template creation from original file: {e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","        return {}, {}\n","    if not template_questions:\n","        logger.log(\"WARN: No template questions generated. Check original source file format/content.\")\n","    return template_questions, text_to_id\n","\n","\n","\n","# Example Usage (Optional - for testing)\n","# if __name__ == '__main__':\n","#     logger.log(\"This is a standard log message.\")\n","#     my_data = {\"a\": 1, \"b\": np.array([1, 2, 3]), \"c\": {4, 5, 2}, \"d\": np.int64(10)}\n","#     logger.log_json(my_data, prefix=\"TEST_DATA\")\n","#     logger.log(\"Test complete.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4TJ0mJhHmoR"},"outputs":[],"source":["# @title Process Form Answers\n","def process_form_answers(\n","    response: Dict[str, Any],\n","    text_to_id: Dict[str, str],\n","    template_questions: Dict[str, str] # <-- Add template_questions\n","    ) -> Tuple[Dict[str, Any], Dict[str, Dict[str, Any]]]:\n","    \"\"\"Processes form answers and includes the canonical question text.\"\"\" # Updated docstring\n","    form_meta = {'text_answers': 0, 'selection_answers': 0}; form_answers = {}\n","    for question_text_from_form, data in response.get(\"responses\", {}).items():\n","        # Find the canonical QID using the text_to_id map\n","        if question_text_from_form in text_to_id:\n","             qid = text_to_id[question_text_from_form]\n","             # Get the canonical question text using the QID\n","             question_text_canonical = template_questions.get(qid, f\"QUESTION TEXT NOT FOUND FOR QID {qid}\") # Safeguard\n","\n","             if \"text\" in data and data[\"text\"] is not None:\n","                 form_answers[qid] = {\n","                     \"question_text\": question_text_canonical, # Add question text\n","                     \"type\": \"text\",\n","                     \"content\": data[\"text\"]\n","                     }\n","                 form_meta['text_answers'] += 1\n","             elif \"selected\" in data and data[\"selected\"] is not None:\n","                 form_answers[qid] = {\n","                      \"question_text\": question_text_canonical, # Add question text\n","                      \"type\": \"selection\",\n","                      \"content\": data[\"selected\"]\n","                      }\n","                 form_meta['selection_answers'] += 1\n","        # Optional: Log if a question text from the form doesn't map to a known QID\n","        # else:\n","        #    logger.log(f\"WARN: Question text from form '{question_text_from_form[:50]}...' not found in text_to_id map for response {response.get('id', 'N/A')}.\")\n","\n","    return form_meta, form_answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdsXN7my8Ixi"},"outputs":[],"source":["# @title Disable Process-PDF-Answers\n","def process_response(\n","    response: Dict[str, Any],\n","    template_questions: Dict[str, str],\n","    text_to_id: Dict[str, str]\n","    # PDF_CONFIDENCE_THRESHOLD is global\n","    ) -> Dict[str, Any]:\n","    \"\"\"\n","    Processes a single response, including form and PDF answers (now with question text embedded),\n","    and calculates comparison statistics.\n","    \"\"\"\n","    response_id = response.get('id', 'UNKNOWN_ID')\n","    logger.log(f\"INFO: Processing response ID: {response_id}\")\n","\n","    # Pass template_questions to process_form_answers so it can embed the text\n","    form_meta, form_answers = process_form_answers(response, text_to_id, template_questions)\n","\n","    # process_pdf_answers now returns pdf_answers with question text embedded\n","    # pdf_meta, pdf_answers, pdf_confidence_scores = process_pdf_answers(response, template_questions)\n","\n","    # Provide default empty values since PDF processing is skipped\n","    logger.log(f\"INFO: PDF processing skipped for response ID: {response_id}\")\n","    pdf_meta = {\"processing_skipped_reason\": \"PDF processing disabled by configuration\"} # Or simply {}\n","    pdf_answers = {}\n","    pdf_confidence_scores = {}\n","\n","    # --- Calculate Comparison Statistics using Sets ---\n","    # (No changes needed here, still based on QIDs)\n","    all_template_qids = set(template_questions.keys())\n","    form_answered_qids = set(form_answers.keys())\n","    pdf_answered_qids = set(pdf_answers.keys()) # pdf_answers still only contains keys >= threshold\n","\n","    answered_by_both_set = form_answered_qids.intersection(pdf_answered_qids)\n","    answered_by_form_only_set = form_answered_qids.difference(pdf_answered_qids)\n","    answered_by_pdf_only_set = pdf_answered_qids.difference(form_answered_qids)\n","    total_answered_set = form_answered_qids.union(pdf_answered_qids)\n","    unanswered_set = all_template_qids.difference(total_answered_set)\n","\n","    # Populate the response_stats dictionary\n","    # (No changes needed here)\n","    response_stats = {\n","        'total_template_questions': len(template_questions),\n","        'form_text_answered_count': form_meta.get('text_answers', 0),\n","        'form_selection_answered_count': form_meta.get('selection_answers', 0),\n","        'pdf_answered_count': len(pdf_answered_qids),\n","        'answered_by_form_only_count': len(answered_by_form_only_set),\n","        'answered_by_pdf_only_count': len(answered_by_pdf_only_set),\n","        'answered_by_both_count': len(answered_by_both_set),\n","        'total_unique_answered_count': len(total_answered_set),\n","        'unanswered_count': len(unanswered_set),\n","        'pdf_confidence_scores': pdf_confidence_scores\n","    }\n","    # --- End Statistics Calculation ---\n","\n","    # Compile the final UDS entry for this response\n","    # No extra enrichment step needed here now\n","    response_data = {\n","        \"response_id\": response_id,\n","        \"form_meta\": form_meta,\n","        \"form_answers\": form_answers, # Already contains question_text\n","        \"pdf_meta\": pdf_meta,\n","        \"pdf_answers\": pdf_answers,   # Already contains question_text\n","        \"response_stats\": response_stats\n","    }\n","\n","    # Log the key outcomes (counts remain useful for a quick check)\n","    logger.log(f\"INFO: Finished response ID: {response_id}. \"\n","               f\"Form Ans: {len(form_answered_qids)}, \"\n","               f\"PDF Ans (>=thr): {response_stats['pdf_answered_count']}, \"\n","               f\"Both: {response_stats['answered_by_both_count']}, \"\n","               f\"FormOnly: {response_stats['answered_by_form_only_count']}, \"\n","               f\"PDFOnly: {response_stats['answered_by_pdf_only_count']}, \"\n","               f\"Unanswered: {response_stats['unanswered_count']}\")\n","\n","    return response_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1owPdPxr0zB"},"outputs":[],"source":["# @title Main Execution Block\n","import json\n","import os\n","import sys\n","import tempfile\n","import traceback\n","# Assume FileLogger, create_template_questions, process_form_answers are defined above\n","# Assume a simplified process_response function is also defined (as shown below for context)\n","\n","# =====================================================\n","#  Main Execution Block (Simplified for Stage 1)\n","# =====================================================\n","if __name__ == \"__main__\":\n","    # --- Configuration --- (Referencing the constants above)\n","    INPUT_RESPONSES_FILE = ORIGINAL_RESPONSES_FILE # Use the consistent name\n","    OUTPUT_UDS_FILE = SIMPLIFIED_RESPONSE_UDS_FILE # Use the consistent name\n","    TEST_ID = TEST_SINGLE_RESPONSE_ID # Use the consistent name\n","    LOG_FILE = STAGE1_LOG_FILE # Use the stage-specific log file\n","\n","    # --- Logger Initialization --- (Assuming logger class is defined)\n","    logger = FileLogger(LOG_FILE)\n","    # (Enable debug logging can be checked within logger calls if needed)\n","\n","    logger.log(\"=\" * 50); logger.log(f\" INFO: Starting Stage 1: Simplified Response-Centric UDS Creation (Log: {LOG_FILE})\"); logger.log(\"=\" * 50)\n","    logger.log(f\"INFO: Debug logging enabled: {ENABLE_DEBUG_LOGGING}\")\n","    logger.log(f\"INFO: Input Raw Responses: {INPUT_RESPONSES_FILE}\")\n","    logger.log(f\"INFO: Output Simplified UDS (JSONL): {OUTPUT_UDS_FILE}\")\n","\n","    input_file_to_process = INPUT_RESPONSES_FILE\n","    temp_filename = None\n","\n","    # --- Test Mode Setup (Optional) ---\n","    if TEST_SINGLE_RESPONSE_ID is not None:\n","        logger.log(f\"WARN: --- RUNNING IN SINGLE RESPONSE TEST MODE (ID: {TEST_SINGLE_RESPONSE_ID}) ---\")\n","        try:\n","            # Create a temporary file to hold only the test response\n","            with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.jsonl', encoding='utf-8') as temp_file:\n","                temp_filename = temp_file.name\n","                found = False\n","                try:\n","                    with open(INPUT_RESPONSES_FILE, 'r', encoding='utf-8') as original_file:\n","                        for line_num, line in enumerate(original_file, 1):\n","                            try:\n","                                response = json.loads(line)\n","                                if response.get(\"id\") == TEST_SINGLE_RESPONSE_ID:\n","                                    json.dump(response, temp_file); temp_file.write('\\n')\n","                                    logger.log(f\"INFO: Copied response ID {TEST_SINGLE_RESPONSE_ID} to temp file: {temp_filename}\")\n","                                    found = True; break\n","                            except json.JSONDecodeError: logger.log(f\"WARN: Skipping invalid JSON line {line_num} in original file during test setup.\")\n","                except FileNotFoundError: logger.log(f\"ERROR: Original file not found for test setup: {INPUT_RESPONSES_FILE}\"); sys.exit(1)\n","                if not found: logger.log(f\"ERROR: Test response ID {TEST_SINGLE_RESPONSE_ID} not found in {INPUT_RESPONSES_FILE}.\"); sys.exit(1)\n","\n","            input_file_to_process = temp_filename # Process only the temp file\n","            logger.log(f\"INFO: Using temporary file as input: {input_file_to_process}\")\n","        except Exception as temp_err:\n","            logger.log(f\"ERROR: Error setting up test mode: {temp_err}\")\n","            tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","            if temp_filename and os.path.exists(temp_filename): os.unlink(temp_filename) # Cleanup attempt\n","            sys.exit(1)\n","    else:\n","         logger.log(f\"INFO: --- RUNNING IN FULL BATCH MODE (Input: {input_file_to_process}) ---\")\n","\n","    # --- Main Processing Steps ---\n","    main_success = False\n","    try:\n","        # 1. Create Template Questions Map from FULL ORIGINAL dataset\n","        logger.log(f\"INFO: Reading full dataset ({INPUT_RESPONSES_FILE}) to create template questions...\")\n","        # Assumes create_template_questions returns dicts: id->text, text->id\n","        template_questions, text_to_id = create_template_questions(INPUT_RESPONSES_FILE)\n","        if not template_questions:\n","            logger.log(\"ERROR: No template questions created from original file. Cannot proceed.\")\n","            sys.exit(1) # Essential for mapping answers\n","        logger.log(f\"INFO: Discovered {len(template_questions)} unique questions in template.\")\n","\n","        # 2. Process Input File Line-by-Line\n","        processed_count = 0\n","        logger.log(f\"INFO: Processing responses from '{input_file_to_process}' and writing to '{OUTPUT_UDS_FILE}'...\")\n","        # Open output file for writing JSON Lines\n","        with open(OUTPUT_UDS_FILE, 'w', encoding='utf-8') as outfile:\n","            # Open input file (original or temp) for reading\n","            with open(input_file_to_process, 'r', encoding='utf-8') as infile:\n","                for line_num, line in enumerate(infile, 1):\n","                    try:\n","                        raw_response = json.loads(line)\n","                        # Process using the simplified function\n","                        processed_data = process_response(raw_response, template_questions, text_to_id)\n","                        # Write the result as one line in the output JSONL file\n","                        json.dump(processed_data, outfile) # Uses standard JSON encoder\n","                        outfile.write('\\n')\n","                        processed_count += 1\n","                        # Log progress occasionally for large files\n","                        if processed_count % 200 == 0:\n","                             logger.log(f\"INFO: ... processed {processed_count} responses\")\n","                    except json.JSONDecodeError:\n","                        logger.log(f\"WARN: Skipping invalid JSON on line {line_num} of {input_file_to_process}\")\n","                    except Exception as proc_err:\n","                        # Try to get ID for better error logging\n","                        response_id_for_error = 'UNKNOWN_ID_ON_ERROR'\n","                        try: response_id_for_error = json.loads(line).get('id', response_id_for_error)\n","                        except: pass\n","                        logger.log(f\"ERROR: Failed to process response ID {response_id_for_error} on line {line_num}: {proc_err}\")\n","                        # Optionally log full traceback for debugging specific response errors\n","                        # tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","\n","            logger.log(f\"INFO: Finished processing. Processed {processed_count} responses.\")\n","            main_success = True # Mark success if the loop finished\n","\n","    except FileNotFoundError as fnf_err:\n","        logger.log(f\"CRITICAL: Input file not found: {fnf_err}\")\n","    except Exception as main_e: # Catch other fatal errors (e.g., template creation failure, file open errors)\n","        logger.log(\"CRITICAL:\" + \"=\"*10 + \" FATAL ERROR in main execution block \" + \"=\"*10)\n","        logger.log(f\"CRITICAL: Error: {main_e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","        logger.log(\"CRITICAL:\" + \"=\"*50)\n","\n","    finally:\n","        # Clean up the temporary file if it was created\n","        if temp_filename and os.path.exists(temp_filename):\n","            try:\n","                os.unlink(temp_filename)\n","                logger.log(f\"INFO: Deleted temporary file: {temp_filename}\")\n","            except OSError as e:\n","                logger.log(f\"WARN: Could not delete temporary file {temp_filename}: {e}\")\n","\n","        logger.log(\"=\" * 50)\n","        if main_success: logger.log(\" INFO: Simplified Response-Centric Processing Finished Successfully \")\n","        else: logger.log(\" ERROR: Simplified Response-Centric Processing Finished With Errors \")\n","        logger.log(\"=\" * 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMIJM-ToL45Z"},"outputs":[],"source":["For Reference only\n","# @title Main Execution Block (For Reference only)\n","# --- Imports --- (Keep only necessary ones: json, os, sys, traceback, time, potentially numpy, your logger)\n","import json\n","import sys\n","import traceback\n","import time\n","# ... other necessary standard library imports ...\n","# from your_helper_module import logger, create_template_questions, aggregate_responses_by_question, save_output # Assuming these are defined elsewhere\n","\n","if __name__ == \"__main__\":\n","\n","    logger.log(\"=\" * 50); logger.log(\" INFO: Starting Question-Centric UDS Aggregation (No Summarization)\"); logger.log(\"=\" * 50)\n","    # Log key config parameters (Only relevant ones like input/output files)\n","    # logger.log(f\"INFO: Original Responses File: {ORIGINAL_RESPONSES_FILE}\") # Assuming these vars are defined\n","    # logger.log(f\"INFO: Response-Centric UDS Input: {RESPONSE_CENTRIC_UDS_FILE}\")\n","    # logger.log(f\"INFO: Question-Centric UDS Output: {QUESTION_CENTRIC_UDS_FILE}\")\n","\n","    # --- Main Processing Steps ---\n","    main_success = False\n","    try:\n","        # 1. Create Template Questions Map (Needs original responses file)\n","        # Make sure ORIGINAL_RESPONSES_FILE variable is defined somewhere\n","        template_questions, text_to_id = create_template_questions(ORIGINAL_RESPONSES_FILE)\n","        if not template_questions:\n","            logger.log(\"ERROR: Failed to create template questions map. Cannot proceed.\")\n","            sys.exit(1)\n","        logger.log(f\"INFO: Template created with {len(template_questions)} unique questions.\")\n","\n","        # 2. Load Response-Centric UDS Data (Contains processed form answers)\n","        response_centric_input_data = None\n","        try:\n","            # Make sure RESPONSE_CENTRIC_UDS_FILE variable is defined somewhere\n","            with open(RESPONSE_CENTRIC_UDS_FILE, 'r', encoding='utf-8') as f:\n","                response_centric_input_data = json.load(f)\n","            logger.log(f\"INFO: Successfully loaded response-centric data from {RESPONSE_CENTRIC_UDS_FILE}.\")\n","            if \"response_data\" not in response_centric_input_data or not isinstance(response_centric_input_data.get(\"response_data\"), list):\n","                 logger.log(\"ERROR: Input file format error: 'response_data' key missing or not a list.\")\n","                 sys.exit(1)\n","        except FileNotFoundError:\n","            logger.log(f\"ERROR: Response-centric input file not found: {RESPONSE_CENTRIC_UDS_FILE}\")\n","            sys.exit(1)\n","        # ... other loading error handling ...\n","\n","        # 3. Perform Aggregation (NO Summarization)\n","        # **IMPORTANT**: aggregate_responses_by_question MUST be modified\n","        #               to *only* group data and not expect/use models/tokenizer.\n","        response_data_list = response_centric_input_data.get(\"response_data\", [])\n","        logger.log(f\"INFO: Starting aggregation for {len(response_data_list)} responses...\")\n","        question_centric_data = aggregate_responses_by_question(\n","            response_data_list,\n","            template_questions\n","            # NO model/tokenizer/device arguments passed here\n","        )\n","        logger.log(\"INFO: Aggregation complete.\")\n","\n","        # 4. Save the Question-Centric Output\n","        if question_centric_data:\n","            # Make sure QUESTION_CENTRIC_UDS_FILE variable is defined somewhere\n","            save_output(QUESTION_CENTRIC_UDS_FILE, question_centric_data)\n","            main_success = True\n","        else:\n","            logger.log(\"WARN: Aggregation resulted in empty data. No output file saved.\")\n","\n","    except Exception as main_e:\n","        logger.log(\"CRITICAL:\" + \"=\"*10 + \" FATAL ERROR in main execution block \" + \"=\"*10)\n","        logger.log(f\"CRITICAL: Error: {main_e}\")\n","        logger.log(traceback.format_exc())\n","        logger.log(\"CRITICAL:\" + \"=\"*50)\n","\n","    finally:\n","        # Minimal cleanup if any (e.g., close files if not using 'with')\n","        logger.log(\"INFO: Cleanup complete (minimal cleanup needed).\")\n","\n","        logger.log(\"=\" * 50)\n","        if main_success: logger.log(\" INFO: Question-Centric Aggregation Finished Successfully \")\n","        else: logger.log(\" ERROR: Question-Centric Aggregation Finished With Errors \")\n","        logger.log(\"=\" * 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ByDaK6WJm_0d"},"outputs":[],"source":["%tb"]},{"cell_type":"markdown","metadata":{"id":"AKbIfBYDT1BY"},"source":["## Step 2: Question-Centric UDS\n","\n","1.  **Single JSON Object:** The `save_output` function in the Stage 2 script takes the final `question_centric_data` dictionary (which itself contains question IDs as keys) and wraps it under a top-level key (e.g., `\"question_data\"`). It then uses `json.dump` to write this *entire wrapped structure* as **one single, valid JSON object** into the file specified by `QUESTION_CENTRIC_UDS_FILE`. This is why the extension is typically `.json`.\n","\n","2.  **Questions as Keys/Sub-tags:** Inside that single JSON object (specifically within the value associated with `\"question_data\"`), each unique question identified in the survey (`Q1`, `Q2`, `Q3`, etc.) becomes a **key**. The value associated with each question key is another dictionary holding all the aggregated statistics (`answered_form_count`, `unanswered_by_form_count`, `form_answer_examples`, etc.) *for that specific question* across the entire set of responses processed.\n","\n","So, the structure looks conceptually like this:\n","\n","```json\n","// Contents of question_centric_aggregation_output.json\n","{\n","  \"question_data\": { // Top-level wrapper\n","    \"Q1\": { // Key = Question ID\n","      \"question_text\": \"What is your name?\",\n","      \"total_responses_processed_for_question\": 500,\n","      \"answered_form_count\": 498,\n","      \"unanswered_by_form_count\": 2,\n","      \"form_answer_examples\": [\n","        {\"response_id\": 123, \"content\": \"Alice\", \"type\": \"text\"},\n","        {\"response_id\": 456, \"content\": \"Bob\", \"type\": \"text\"},\n","        // ... up to MAX_EXAMPLE_ANSWERS\n","      ]\n","    },\n","    \"Q2\": { // Key = Question ID\n","      \"question_text\": \"What is your favorite color?\",\n","      \"total_responses_processed_for_question\": 500,\n","      \"answered_form_count\": 450,\n","      \"unanswered_by_form_count\": 50,\n","      \"form_answer_examples\": [\n","         {\"response_id\": 123, \"content\": \"Blue\", \"type\": \"selection\"},\n","         {\"response_id\": 789, \"content\": \"Green\", \"type\": \"selection\"},\n","         // ...\n","      ]\n","    },\n","    // ... more entries for Q3, Q4, etc. ...\n","  }\n","}\n","```\n","\n","This structure is ideal for the purpose of Stage 2: providing a **holistic, aggregated view of the answers to each specific question** across the entire survey dataset, making cross-response analysis easy for individual questions. It contrasts with the Stage 1 output (JSON Lines), where each line/object represented a single response's data across multiple questions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L9GANm6OT7Kz"},"outputs":[],"source":["# @title Aggregate Responses by Question\n","# =====================================================\n","#   Question-Centric UDS Aggregation (Form Data Only)\n","# =====================================================\n","# Purpose: Aggregates pre-processed response-centric data\n","#          (containing only form answers) into a\n","#          question-centric structure for analysis.\n","\n","# =====================================================\n","#                      Imports\n","# =====================================================\n","import json\n","import os\n","import numpy as np # Keep for potential use in helpers/future stats\n","import time\n","import traceback\n","import sys\n","from collections import defaultdict\n","from typing import Dict, Any, List, Tuple\n","\n","\n","# =====================================================\n","#       Helper Functions\n","# =====================================================\n","\n","\n","def get_json_serializer():\n","    \"\"\"Returns a robust JSON serializer function (handles numpy, sets).\"\"\"\n","    def default_serializer(obj):\n","        if isinstance(obj, (np.integer, np.floating)): return obj.item()\n","        if isinstance(obj, np.ndarray): return obj.tolist()\n","        if isinstance(obj, set): return sorted(list(obj))\n","        return json.JSONEncoder().default(obj)\n","    return default_serializer\n","\n","def save_output(output_file: str, question_centric_data: Dict[str, Any]):\n","    \"\"\"Saves the final question-centric output JSON.\"\"\"\n","    logger.log(f\"INFO: Saving question-centric aggregation output to {output_file}...\")\n","    serializer = get_json_serializer()\n","    try:\n","        # Structure the final output with a top-level key\n","        final_output = {\"question_data\": question_centric_data}\n","        with open(output_file, 'w', encoding='utf-8') as f:\n","             json.dump(final_output, f, default=serializer, indent=2)\n","        logger.log(f\"INFO: Successfully saved question-centric aggregation output.\")\n","    except IOError as e:\n","        logger.log(f\"ERROR: Could not write to output file {output_file}: {e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","    except TypeError as e:\n","        logger.log(f\"ERROR: Failed to serialize data for output file {output_file}: {e}\")\n","        problematic_keys = []\n","        # Attempt to identify problematic keys during serialization failure\n","        if isinstance(question_centric_data, dict):\n","            for k, v in question_centric_data.items():\n","                try: json.dumps({k: v}, default=serializer)\n","                except TypeError: problematic_keys.append(k)\n","        if problematic_keys: logger.log(f\"ERROR: Problematic top-level keys during serialization: {problematic_keys}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","    except Exception as e:\n","        logger.log(f\"ERROR: An unexpected error occurred during saving: {e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","\n","# =====================================================\n","#      Core Question-Centric Aggregation Logic (SIMPLIFIED)\n","# =====================================================\n","\n","def aggregate_responses_by_question(\n","    response_centric_data: List[Dict[str, Any]],\n","    template_questions: Dict[str, str]\n","    ) -> Dict[str, Any]:\n","    \"\"\"\n","    Aggregates data from the response-centric list into a question-centric structure,\n","    focusing ONLY on structured form answers.\n","    \"\"\"\n","    if not response_centric_data:\n","        logger.log(\"WARN: No response data provided for aggregation.\")\n","        return {}\n","    if not template_questions:\n","        logger.log(\"ERROR: Template questions map is empty. Cannot perform aggregation.\")\n","        return {}\n","\n","    # Initialize the main aggregation dictionary (Simplified structure)\n","    # Defines the default structure for each new question ID encountered\n","    question_aggregation = defaultdict(lambda: {\n","        \"question_text\": \"\",\n","        \"total_responses_processed_for_question\": 0, # Responses where this Q was potentially applicable\n","        \"answered_form_count\": 0,\n","        \"unanswered_by_form_count\": 0, # Count where no form answer was present\n","        \"form_answer_examples\": []      # Store {response_id, content, type}\n","    })\n","\n","    logger.log(f\"INFO: Starting aggregation for {len(template_questions)} questions across {len(response_centric_data)} responses (Form data only).\")\n","\n","    # --- Pre-populate basic info for all template questions ---\n","    total_responses_in_batch = len(response_centric_data)\n","    for qid, qtext in template_questions.items():\n","        question_aggregation[qid][\"question_text\"] = qtext\n","        # Set the total number of responses this question could have appeared in\n","        question_aggregation[qid][\"total_responses_processed_for_question\"] = total_responses_in_batch\n","\n","    # --- Iterate through each response from the input list ---\n","    for i, response in enumerate(response_centric_data):\n","        response_id = response.get(\"response_id\", f\"UNKNOWN_{i}\")\n","        # Extract form answers (assuming structure QID -> {question_text, type, content})\n","        form_answers = response.get(\"form_answers\", {})\n","\n","        # PDF related data from the input file is ignored in this simplified logic\n","\n","        # --- Iterate through each *template* question ID to update its stats ---\n","        for qid in template_questions.keys():\n","            # Check if *this* response contains an answer for *this* template question\n","            has_form_answer = qid in form_answers\n","\n","            # Increment counts based solely on form answer presence\n","            if has_form_answer:\n","                question_aggregation[qid][\"answered_form_count\"] += 1\n","                # Store form examples if limit not reached\n","                if len(question_aggregation[qid][\"form_answer_examples\"]) < MAX_EXAMPLE_ANSWERS:\n","                    answer_detail = form_answers.get(qid, {}) # Get the details for this QID\n","                    # Store relevant parts (adjust keys if needed based on input structure)\n","                    question_aggregation[qid][\"form_answer_examples\"].append({\n","                        \"response_id\": response_id,\n","                        \"content\": answer_detail.get(\"content\"),\n","                        \"type\": answer_detail.get(\"type\")\n","                    })\n","            else:\n","                # No form answer was found for this QID in this response\n","                question_aggregation[qid][\"unanswered_by_form_count\"] += 1\n","\n","    logger.log(\"INFO: Aggregation loop finished. No further per-question stats calculated.\")\n","\n","    # --- No final calculations needed (fractions removed) ---\n","\n","    return dict(question_aggregation) # Convert defaultdict back to dict for output\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYSQtJg0vUWt"},"outputs":[],"source":["# @title Main Execution Block\n","# =====================================================\n","#                   Main Execution Block (Stage 2 - Question-Centric Aggregation)\n","# =====================================================\n","if __name__ == \"__main__\":\n","    # --- Configuration --- (Referencing the constants above)\n","    RESPONSE_CENTRIC_UDS_FILE = SIMPLIFIED_RESPONSE_UDS_FILE # Use the consistent name (Input for this stage)\n","    ORIGINAL_RESPONSES_SRC = ORIGINAL_RESPONSES_FILE # Use the consistent name (Needed for template)\n","    QUESTION_CENTRIC_OUTPUT_FILE = QUESTION_CENTRIC_UDS_FILE # Use the consistent name\n","    LOG_FILE = STAGE2_LOG_FILE # Use the stage-specific log file\n","    # MAX_EXAMPLE_ANSWERS is used within the aggregate_responses_by_question function\n","\n","    # --- Logger Initialization --- (Assuming logger class is defined)\n","    logger = FileLogger(LOG_FILE)\n","    # (Enable debug logging can be checked within logger calls if needed)\n","\n","    logger.log(\"=\" * 50); logger.log(f\" INFO: Starting Stage 2: Question-Centric Aggregation (Log: {LOG_FILE})\"); logger.log(\"=\" * 50)\n","    logger.log(f\"INFO: Input Simplified Response UDS: {RESPONSE_CENTRIC_UDS_FILE}\")\n","    logger.log(f\"INFO: Original Responses Source (for Template): {ORIGINAL_RESPONSES_SRC}\")\n","    logger.log(f\"INFO: Output Question-Centric Aggregation: {QUESTION_CENTRIC_OUTPUT_FILE}\")\n","\n","    main_success = False\n","    try:\n","        # 1. Create the Question ID <-> Text Mapping from original source file\n","        logger.log(f\"INFO: Reading {ORIGINAL_RESPONSES_FILE} to create template questions...\")\n","        template_questions, text_to_id = create_template_questions(ORIGINAL_RESPONSES_FILE)\n","\n","        if not template_questions:\n","            logger.log(\"ERROR: Failed to create template questions map from original file. Cannot proceed.\")\n","            sys.exit(1)\n","        logger.log(f\"INFO: Template map created with {len(template_questions)} unique questions.\")\n","\n","        # 2. Load the (Simplified) Response-Centric UDS Data (from JSON Lines)\n","        response_data_list = []\n","        try:\n","            logger.log(f\"INFO: Loading response-centric data from JSON Lines file: {RESPONSE_CENTRIC_UDS_FILE}\")\n","            with open(RESPONSE_CENTRIC_UDS_FILE, 'r', encoding='utf-8') as f:\n","                for line_num, line in enumerate(f, 1):\n","                    try:\n","                        line_strip = line.strip()\n","                        if not line_strip: continue # Skip empty lines\n","                        response_object = json.loads(line_strip)\n","                        # Optional validation: check for expected keys\n","                        if not all(k in response_object for k in [\"response_id\", \"form_answers\", \"response_stats\"]):\n","                            logger.log(f\"WARN: Skipping line {line_num} in {RESPONSE_CENTRIC_UDS_FILE} due to missing expected keys.\")\n","                            continue\n","                        response_data_list.append(response_object)\n","                    except json.JSONDecodeError as e:\n","                        logger.log(f\"ERROR: Failed to parse JSON on line {line_num} of {RESPONSE_CENTRIC_UDS_FILE}: {e}. Skipping line.\")\n","\n","            logger.log(f\"INFO: Successfully loaded {len(response_data_list)} responses from {RESPONSE_CENTRIC_UDS_FILE}.\")\n","            if not response_data_list:\n","                 logger.log(f\"ERROR: No valid response data loaded from {RESPONSE_CENTRIC_UDS_FILE}. Cannot aggregate.\")\n","                 sys.exit(1)\n","\n","        except FileNotFoundError:\n","            logger.log(f\"ERROR: Response-centric input file not found: {RESPONSE_CENTRIC_UDS_FILE}\")\n","            sys.exit(1)\n","        except Exception as e:\n","            logger.log(f\"ERROR: Unexpected error loading {RESPONSE_CENTRIC_UDS_FILE}: {e}\")\n","            tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","            sys.exit(1)\n","\n","        # 3. Perform Aggregation (using the simplified function)\n","        logger.log(f\"INFO: Starting aggregation on {len(response_data_list)} responses...\")\n","        question_centric_data = aggregate_responses_by_question( # Calls the simplified version\n","            response_data_list,\n","            template_questions\n","        )\n","        logger.log(f\"INFO: Aggregation complete. Aggregated data for {len(question_centric_data)} questions.\")\n","\n","        # 4. Save the Question-Centric Output\n","        if question_centric_data:\n","            save_output(QUESTION_CENTRIC_UDS_FILE, question_centric_data) # Uses standard json.dump\n","            main_success = True\n","        else:\n","            logger.log(\"WARN: Aggregation resulted in empty data. No output file saved.\")\n","\n","    except Exception as main_e: # Catch unexpected errors in main script flow\n","        logger.log(\"CRITICAL:\" + \"=\"*10 + \" FATAL ERROR in main execution block \" + \"=\"*10)\n","        logger.log(f\"CRITICAL: Error: {main_e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","        logger.log(\"CRITICAL:\" + \"=\"*50)\n","\n","    finally:\n","        logger.log(\"=\" * 50)\n","        if main_success: logger.log(\" INFO: Question-Centric Aggregation Finished Successfully \")\n","        else: logger.log(\" ERROR: Question-Centric Aggregation Finished With Errors \")\n","        logger.log(\"=\" * 50)"]},{"cell_type":"markdown","metadata":{"id":"hPxmtVfdIqce"},"source":["## Step 3:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8uQHuRULf1h"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y4Jq8aK95ohO"},"outputs":[],"source":["# @title Question-Centric Summarization using LLM\n","# =====================================================\n","#   Stage 3: Question-Centric Summarization using LLM\n","# =====================================================\n","# Purpose: Loads aggregated counts from Stage 2, reads full\n","#          answers from Stage 1 for selected questions,\n","#          generates LLM summaries, and saves them.\n","\n","# =====================================================\n","#                      Imports\n","# =====================================================\n","import json\n","import os\n","import sys\n","import traceback # For traceback logging\n","import time\n","import gc # Garbage Collection\n","from collections import defaultdict\n","from typing import Dict, Any, List\n","\n","# Potentially heavy imports - keep them together\n","try:\n","    import torch\n","    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","    # accelerate is usually needed for device_map='auto'\n","    import accelerate\n","except ImportError as e:\n","    print(f\"CRITICAL ERROR: Missing essential libraries (torch, transformers, accelerate). Install them. Details: {e}\")\n","    sys.exit(1)\n","\n","try:\n","    # Only required if USE_QUANTIZATION is True\n","    from bitsandbytes.cextension import ColabNotSupportedError\n","except ImportError:\n","    # Define dummy class if bitsandbytes is not installed but quantization is False\n","    ColabNotSupportedError = type('ColabNotSupportedError', (Exception,), {})\n","    print(\"WARN: bitsandbytes not found. Quantization will be disabled if requested.\")\n","    pass\n","\n","# =====================================================\n","#                Custom FileLogger Class\n","# =====================================================\n","# Reusing the simplified FileLogger (no torch dependency)\n","class FileLogger:\n","    def __init__(self, log_file):\n","        self.log_file = log_file\n","        log_dir = os.path.dirname(log_file)\n","        if log_dir and not os.path.exists(log_dir):\n","             os.makedirs(log_dir, exist_ok=True)\n","        try:\n","            with open(self.log_file, 'w', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: Starting new log...\\n\")\n","        except IOError as e:\n","            print(f\"CRITICAL: Failed to initialize log file {self.log_file}: {e}\")\n","\n","    def _get_timestamp(self):\n","        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","\n","    def log(self, message: str):\n","        \"\"\"Logs a message to the file.\"\"\"\n","        try:\n","            with open(self.log_file, 'a', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","        except Exception as e:\n","            print(f\"!!! LOGGER FAILED TO WRITE: {e} !!!\")\n","            print(f\"{self._get_timestamp()}: {message}\") # Fallback print\n","\n","    def log_json(self, data: Dict[str, Any], prefix=\"JSON_DATA\"):\n","        \"\"\"Logs a dictionary as formatted JSON.\"\"\"\n","        serializer = self._get_serializer()\n","        try:\n","            log_entry = {\n","                 \"timestamp\": self._get_timestamp(),\n","                 \"type\": prefix,\n","                 \"data\": data\n","            }\n","            json_string = json.dumps(log_entry, default=serializer, indent=2)\n","            with open(self.log_file, 'a', encoding='utf-8') as f:\n","                 f.write(json_string + \"\\n\")\n","        except TypeError as e:\n","            self.log(f\"ERROR: Failed to serialize data for JSON logging: {e} - Data Keys: {list(data.keys())}\")\n","        except Exception as e:\n","             self.log(f\"ERROR: Unexpected error during log_json: {e}\")\n","             tb_str = traceback.format_exc(); self.log(f\"TRACEBACK:\\n{tb_str}\")\n","\n","    def _get_serializer(self):\n","        \"\"\"Returns a serializer function for json.dump.\"\"\"\n","        def default_serializer(obj):\n","            # NumPy types can sometimes appear via libraries, keep if needed.\n","            # import numpy as np # Import only if needed\n","            # if isinstance(obj, (np.integer, np.floating)): return obj.item()\n","            # if isinstance(obj, np.ndarray): return obj.tolist()\n","            if isinstance(obj, set): return sorted(list(obj))\n","            try: return json.JSONEncoder().default(obj)\n","            except TypeError: return f\"UNSERIALIZABLE:{type(obj).__name__}\"\n","        return default_serializer\n","\n","# =====================================================\n","#              Pipeline Configuration Constants\n","# =====================================================\n","\n","# --- Input/Output Files ---\n","# Input 1: Aggregated data from Stage 2 (Single JSON object, contains counts, examples, texts)\n","STAGE2_AGGREGATION_FILE = 'question_centric_aggregation_output.json'\n","# Input 2: Simplified response data from Stage 1 (JSON Lines, used to get ALL answers)\n","STAGE1_SIMPLIFIED_UDS_FILE = 'response_centric_uds_simplified.jsonl'\n","# Output: Summaries generated by this script (Single JSON object)\n","SUMMARIES_OUTPUT_FILE = 'question_centric_summaries_output.json'\n","\n","# --- LLM Configuration ---\n","MODEL_ID = \"google/gemma-2-2b-it\" # Recommend 2B for Colab Free Tier\n","# MODEL_ID = \"google/gemma-2-9b-it\" # Use if you have sufficient GPU RAM (>18GB ideal)\n","USE_QUANTIZATION = True # Set to False if bitsandbytes is unavailable or causing issues\n","# os.environ['HF_TOKEN'] = \"hf_...\" # Set if needed\n","\n","# --- Summarization Parameters ---\n","NUM_SUMMARY_SENTENCES = 3 # Target sentences for the summary\n","MIN_ANSWERS_FOR_SUMMARY = 3 # Min *total* answers reported by Stage 2 to even *attempt* reading Stage 1 file\n","\n","# --- Logging ---\n","STAGE3_LOG_FILE = \"stage3_question_summarization.log\"\n","ENABLE_DEBUG_LOGGING = True # Controls some optional logging calls\n","\n","# --- Manual Question Selection (REVISED as per user) ---\n","QUESTIONS_TO_SUMMARIZE = {\n","    \"Q1\": True, \"Q2\": True, \"Q3\": True, \"Q4\": True, \"Q5\": True,\n","    \"Q6\": True, \"Q7\": True, \"Q8\": True, \"Q9\": True, \"Q10\": True,\n","    \"Q11\": True, \"Q12\": True, \"Q13\": True, \"Q14\": True, \"Q15\": True,\n","    \"Q16\": False, \"Q17\": True, \"Q18\": True, \"Q19\": True, \"Q20\": True,\n","    \"Q21\": True, \"Q22\": True, \"Q23\": True, \"Q24\": True, \"Q25\": True,\n","    \"Q26\": True, \"Q27\": True, \"Q28\": True, \"Q29\": True, \"Q30\": True,\n","    \"Q31\": True, \"Q32\": False, \"Q33\": False, \"Q34\": False, \"Q35\": False,\n","    \"Q36\": False,\n","}\n","\n","# =====================================================\n","#                 Logger Initialization\n","# =====================================================\n","logger = FileLogger(STAGE3_LOG_FILE)\n","\n","# =====================================================\n","#       Helper Function (for saving final output)\n","# =====================================================\n","def save_summaries_output(output_file: str, summaries_data: Dict[str, Any]):\n","    \"\"\"Saves the final summaries output JSON.\"\"\"\n","    logger.log(f\"INFO: Saving summaries output to {output_file}...\")\n","    try:\n","        final_output = {\"question_summaries\": summaries_data}\n","        with open(output_file, 'w', encoding='utf-8') as f:\n","             json.dump(final_output, f, indent=2)\n","        logger.log(f\"INFO: Successfully saved summaries output.\")\n","    except IOError as e:\n","        logger.log(f\"ERROR: Could not write to output file {output_file}: {e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","    except TypeError as e:\n","        logger.log(f\"ERROR: Failed to serialize summary data for output file {output_file}: {e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","    except Exception as e:\n","        logger.log(f\"ERROR: An unexpected error occurred during saving summaries: {e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","\n","\n","# =====================================================\n","#                   Main Execution Block (Stage 3)\n","# =====================================================\n","if __name__ == \"__main__\":\n","\n","    # --- Log Initial Configuration ---\n","    logger.log(\"=\" * 50); logger.log(\" INFO: Starting Stage 3: Question Summarization \"); logger.log(\"=\" * 50)\n","    logger.log(f\"INFO: Input Aggregated UDS (Stage 2): {STAGE2_AGGREGATION_FILE}\")\n","    logger.log(f\"INFO: Input Simplified UDS (Stage 1): {STAGE1_SIMPLIFIED_UDS_FILE}\")\n","    logger.log(f\"INFO: Output Summaries File: {SUMMARIES_OUTPUT_FILE}\")\n","    logger.log(f\"INFO: LLM Model ID: {MODEL_ID}\")\n","    logger.log(f\"INFO: Use Quantization: {USE_QUANTIZATION}\")\n","    logger.log(f\"INFO: Min Answers for Summary Attempt: {MIN_ANSWERS_FOR_SUMMARY}\")\n","    logger.log(f\"INFO: Target Summary Sentences: {NUM_SUMMARY_SENTENCES}\")\n","    logger.log(f\"INFO: Debug Logging Enabled: {ENABLE_DEBUG_LOGGING}\")\n","\n","    # --- Load LLM Resources ---\n","    loaded_tokenizer = None\n","    model_instance = None\n","    summarization_pipe = None\n","    llm_load_success = False\n","    compute_dtype = torch.float16 # Default compute type\n","\n","    try:\n","        # --- Tokenizer ---\n","        logger.log(f\"INFO: Loading tokenizer for: {MODEL_ID}...\")\n","        loaded_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n","\n","        if getattr(loaded_tokenizer, 'pad_token', None) is None:\n","             logger.log(\"WARN: Tokenizer missing pad token; setting to eos_token.\")\n","             if getattr(loaded_tokenizer, 'eos_token', None) is not None:\n","                 loaded_tokenizer.pad_token = loaded_tokenizer.eos_token\n","                 loaded_tokenizer.padding_side = \"left\"\n","                 logger.log(\"INFO: Set pad_token to eos_token and padding_side to left.\")\n","             else:\n","                 logger.log(\"ERROR: Tokenizer missing both pad_token and eos_token! Cannot set padding.\")\n","                 sys.exit(\"CRITICAL: Tokenizer setup failed.\")\n","\n","        logger.log(\"INFO: Tokenizer loaded.\")\n","\n","        # --- Quantization Config (Optional) ---\n","        bnb_config = None\n","        effective_quantization = USE_QUANTIZATION\n","        if USE_QUANTIZATION:\n","            logger.log(\"INFO: Setting up 4-bit quantization...\")\n","            try:\n","                compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n","                logger.log(f\"INFO: Using compute dtype: {compute_dtype}\")\n","\n","                bnb_config = BitsAndBytesConfig(\n","                    load_in_4bit=True,\n","                    bnb_4bit_quant_type=\"nf4\",\n","                    bnb_4bit_compute_dtype=compute_dtype,\n","                    bnb_4bit_use_double_quant=True,\n","                )\n","                logger.log(\"INFO: 4-bit quantization configured.\")\n","            except ImportError:\n","                logger.log(\"ERROR: bitsandbytes not found. Install it for quantization. Disabling quantization.\")\n","                effective_quantization = False\n","            except ColabNotSupportedError:\n","                 logger.log(\"ERROR: Bits and Bytes Colab issue detected. Disabling quantization.\")\n","                 effective_quantization = False\n","            except Exception as q_err:\n","                 logger.log(f\"ERROR: Failed to configure BitsAndBytes: {q_err}. Disabling quantization.\")\n","                 tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","                 effective_quantization = False\n","\n","        # --- Main Model Loading ---\n","        logger.log(f\"INFO: Loading main model: {MODEL_ID} (Quantization: {effective_quantization})...\")\n","        if not torch.cuda.is_available():\n","            logger.log(\"WARN: CUDA not available, device_map='auto' will use CPU. Model loading/inference will be very slow.\")\n","\n","        model_instance = AutoModelForCausalLM.from_pretrained(\n","            MODEL_ID,\n","            quantization_config=bnb_config if effective_quantization else None,\n","            device_map=\"auto\",\n","            torch_dtype=compute_dtype if not effective_quantization else None,\n","            trust_remote_code=True # Recommended for Gemma 2\n","        )\n","        logger.log(\"INFO: Main model loaded.\")\n","\n","        # --- Pipeline Creation ---\n","        logger.log(\"INFO: Creating text-generation pipeline...\")\n","        summarization_pipe = pipeline(\n","            \"text-generation\",\n","            model=model_instance,\n","            tokenizer=loaded_tokenizer,\n","        )\n","        logger.log(\"INFO: Pipeline created successfully.\")\n","        llm_load_success = True\n","\n","    except ImportError as ie:\n","         logger.log(f\"ERROR: Missing libraries needed for LLM. Install requirements (torch, transformers, accelerate, bitsandbytes?). Details: {ie}\")\n","         sys.exit(1)\n","    except Exception as e:\n","        logger.log(f\"CRITICAL: Failed to load LLM models/pipeline: {e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","        logger.log(\"CRITICAL: Cannot proceed with summarization.\")\n","        sys.exit(1)\n","\n","\n","    # --- Main Processing Steps ---\n","    main_success = False\n","    summaries_data = {} # Dictionary to store final results: QID -> {question_text, status, summary, ...}\n","\n","    try:\n","        # 1. Load Aggregated Data (Counts & Texts) from Stage 2\n","        logger.log(f\"INFO: Loading aggregated data from {STAGE2_AGGREGATION_FILE}...\")\n","        try:\n","            with open(STAGE2_AGGREGATION_FILE, 'r', encoding='utf-8') as f:\n","                aggregated_data = json.load(f)\n","\n","            if \"question_data\" not in aggregated_data or not isinstance(aggregated_data.get(\"question_data\"), dict):\n","                logger.log(f\"ERROR: Input file {STAGE2_AGGREGATION_FILE} missing 'question_data' key or it's not a dictionary.\")\n","                sys.exit(1)\n","            question_data_from_stage2 = aggregated_data[\"question_data\"]\n","            logger.log(f\"INFO: Successfully loaded aggregated data for {len(question_data_from_stage2)} questions.\")\n","        except FileNotFoundError:\n","            logger.log(f\"ERROR: Aggregated input file not found: {STAGE2_AGGREGATION_FILE}\")\n","            sys.exit(1)\n","        except json.JSONDecodeError as e:\n","            logger.log(f\"ERROR: Failed to parse JSON from {STAGE2_AGGREGATION_FILE}: {e}\")\n","            sys.exit(1)\n","        except Exception as e:\n","            logger.log(f\"ERROR: Unexpected error loading {STAGE2_AGGREGATION_FILE}: {e}\")\n","            tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","            sys.exit(1)\n","\n","\n","        # 2. Iterate through Questions, Filter, Read Full Answers, and Generate Summaries\n","        if not llm_load_success:\n","             logger.log(\"WARN: LLM failed to load earlier. Skipping summarization loop.\")\n","        else:\n","            logger.log(f\"INFO: Starting summarization processing...\")\n","            processed_q_count = 0\n","            summarized_q_count = 0\n","            skipped_q_count = 0\n","            error_q_count = 0\n","\n","            # Iterate through the questions defined in the Stage 2 aggregated data\n","            for qid, q_data in question_data_from_stage2.items():\n","                processed_q_count += 1\n","                question_text = q_data.get(\"question_text\", f\"Unknown Text for {qid}\")\n","\n","                # --- Initial Filtering (Before Reading Stage 1 File) ---\n","                should_summarize_flag = QUESTIONS_TO_SUMMARIZE.get(qid, False)\n","                if not should_summarize_flag:\n","                    if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG: Skipping QID {qid} (Marked False in config).\")\n","                    skipped_q_count += 1\n","                    summaries_data[qid] = {\"question_text\": question_text, \"status\": \"skipped_config\", \"summary\": None}\n","                    continue\n","\n","                # Check count reported by Stage 2\n","                total_form_answers_count_s2 = q_data.get(\"answered_form_count\", 0)\n","                if total_form_answers_count_s2 < MIN_ANSWERS_FOR_SUMMARY:\n","                    logger.log(f\"INFO: Skipping QID {qid} - Insufficient total answers reported by Stage 2 ({total_form_answers_count_s2} < {MIN_ANSWERS_FOR_SUMMARY}).\")\n","                    skipped_q_count += 1\n","                    summaries_data[qid] = {\n","                        \"question_text\": question_text,\n","                        \"status\": \"skipped_low_answers_s2_count\",\n","                        \"summary\": None,\n","                        \"total_answer_count_reported_s2\": total_form_answers_count_s2\n","                    }\n","                    continue\n","                # --- End Initial Filtering ---\n","\n","                # --- Collect ALL Answers for this QID from Stage 1 File ---\n","                logger.log(f\"INFO: QID {qid} selected. Reading {STAGE1_SIMPLIFIED_UDS_FILE} to collect all answers...\")\n","                all_text_answers_for_qid = []\n","                read_error_occurred = False\n","                try:\n","                    with open(STAGE1_SIMPLIFIED_UDS_FILE, 'r', encoding='utf-8') as infile:\n","                        for line_num, line in enumerate(infile, 1):\n","                            try:\n","                                line_strip = line.strip()\n","                                if not line_strip: continue\n","                                response_object = json.loads(line_strip)\n","                                form_answers_in_resp = response_object.get(\"form_answers\", {})\n","                                if qid in form_answers_in_resp:\n","                                    answer_detail = form_answers_in_resp[qid]\n","                                    # Check type and content validity\n","                                    if answer_detail.get('type') == 'text' and answer_detail.get('content') is not None:\n","                                        content_str = str(answer_detail['content']).strip()\n","                                        if content_str: # Ensure it's not just whitespace\n","                                            all_text_answers_for_qid.append(content_str)\n","                            except json.JSONDecodeError as json_err:\n","                                logger.log(f\"WARN: Skipping line {line_num} in {STAGE1_SIMPLIFIED_UDS_FILE} due to JSON parse error: {json_err}\")\n","                            except Exception as inner_read_err:\n","                                logger.log(f\"WARN: Error processing line {line_num} in {STAGE1_SIMPLIFIED_UDS_FILE} for QID {qid}: {inner_read_err}\")\n","                                # Continue reading other lines if possible\n","\n","                except FileNotFoundError:\n","                    logger.log(f\"ERROR: Stage 1 file not found ({STAGE1_SIMPLIFIED_UDS_FILE}) while collecting answers for QID {qid}.\")\n","                    read_error_occurred = True\n","                except Exception as read_err:\n","                     logger.log(f\"ERROR: Failed reading Stage 1 file ({STAGE1_SIMPLIFIED_UDS_FILE}) for QID {qid}: {read_err}\")\n","                     tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","                     read_error_occurred = True\n","\n","                if read_error_occurred:\n","                     summaries_data[qid] = {\n","                          \"question_text\": question_text,\n","                          \"status\": \"error_reading_stage1_answers\",\n","                          \"summary\": None\n","                     }\n","                     error_q_count += 1\n","                     continue # Skip to next QID if we couldn't read the answers\n","\n","                actual_answers_collected_count = len(all_text_answers_for_qid)\n","                logger.log(f\"INFO: Collected {actual_answers_collected_count} valid text answers for QID {qid} from Stage 1 file.\")\n","                # --- End Collecting ALL answers ---\n","\n","                # --- Final Check on Collected Answer Count ---\n","                if actual_answers_collected_count < MIN_ANSWERS_FOR_SUMMARY:\n","                     logger.log(f\"INFO: Skipping QID {qid} - Insufficient valid text answers found after reading Stage 1 file ({actual_answers_collected_count} < {MIN_ANSWERS_FOR_SUMMARY}).\")\n","                     skipped_q_count += 1\n","                     summaries_data[qid] = {\n","                          \"question_text\": question_text,\n","                          \"status\": \"skipped_low_answers_post_read\",\n","                          \"summary\": None,\n","                          \"actual_answers_collected\": actual_answers_collected_count\n","                     }\n","                     continue\n","                # --- End Final Check ---\n","\n","                # --- Proceed with Summarization ---\n","                logger.log(f\"INFO: Summarizing {actual_answers_collected_count} answers for QID {qid}...\")\n","                start_q_time = time.time()\n","                summary_text = None # Initialize summary variable\n","\n","                try:\n","                    # Prepare text chunk using ALL collected answers\n","                    text_chunk = \"\\n\\n\".join(all_text_answers_for_qid)\n","                    # Optional: Add truncation for extremely long combined text\n","                    # MAX_INPUT_LEN = 30000 # Example character limit\n","                    # if len(text_chunk) > MAX_INPUT_LEN:\n","                    #    logger.log(f\"WARN: Truncating input text for QID {qid} to {MAX_INPUT_LEN} chars.\")\n","                    #    text_chunk = text_chunk[:MAX_INPUT_LEN]\n","\n","                    # Format prompt for Gemma instruct model\n","                    messages = [\n","                        {\"role\": \"user\",\n","                         \"content\": f\"Carefully review all the following responses to the question: \\\"{question_text}\\\". Then, concisely summarize the main themes or points expressed across these responses in exactly {NUM_SUMMARY_SENTENCES} distinct sentences.\\n\\nResponses:\\n\\\"\\\"\\\"\\n{text_chunk}\\n\\\"\\\"\\\"\"}\n","                    ]\n","\n","                    prompt_formatted = summarization_pipe.tokenizer.apply_chat_template(\n","                        messages, tokenize=False, add_generation_prompt=True\n","                    )\n","\n","                    generation_args = {\n","                        \"max_new_tokens\": 200 + NUM_SUMMARY_SENTENCES * 50, # Generous token allowance\n","                        \"do_sample\": False,\n","                        \"temperature\": None, \"top_p\": None,\n","                        }\n","                    if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG: Calling LLM for QID {qid}...\")\n","                    outputs = summarization_pipe(prompt_formatted, **generation_args)\n","\n","                    # --- Extract the generated summary ---\n","                    generated_text_full = outputs[0]['generated_text']\n","                    if generated_text_full.startswith(prompt_formatted):\n","                        summary_text = generated_text_full[len(prompt_formatted):].strip()\n","                        status = \"success\"\n","                    else:\n","                        # Fallback extraction (less reliable)\n","                        response_marker = \"<start_of_turn>model\\n\"\n","                        summary_start_index = generated_text_full.rfind(response_marker)\n","                        if summary_start_index != -1:\n","                             summary_text = generated_text_full[summary_start_index + len(response_marker):].strip()\n","                             status = \"success\"\n","                             logger.log(f\"INFO: Used fallback marker extraction for QID {qid}.\")\n","                        else:\n","                             logger.log(f\"WARN: Could not reliably extract summary for QID {qid}. Output may contain prompt.\")\n","                             summary_text = generated_text_full # Return full output\n","                             status = \"success_extraction_uncertain\"\n","                    # --- End Extraction ---\n","\n","                    end_q_time = time.time()\n","                    logger.log(f\"INFO: Successfully processed summary for QID {qid} in {end_q_time - start_q_time:.2f} seconds.\")\n","                    summaries_data[qid] = {\n","                         \"question_text\": question_text,\n","                         \"status\": status,\n","                         \"summary\": summary_text,\n","                         \"total_answers_summarized\": actual_answers_collected_count\n","                     }\n","                    summarized_q_count += 1\n","\n","                except Exception as pipe_err:\n","                    end_q_time = time.time()\n","                    logger.log(f\"ERROR: LLM pipeline failed for QID {qid} after {end_q_time - start_q_time:.2f} seconds: {pipe_err}\")\n","                    tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","                    summaries_data[qid] = {\n","                        \"question_text\": question_text,\n","                        \"status\": \"error_llm_pipe\",\n","                        \"summary\": None,\n","                        \"error_message\": str(pipe_err),\n","                        \"answers_attempted\": actual_answers_collected_count\n","                        }\n","                    error_q_count += 1\n","\n","                # --- Progress Logging ---\n","                if (processed_q_count % 5 == 0) or (processed_q_count == len(question_data_from_stage2)):\n","                     logger.log(f\"INFO: ...progress: processed {processed_q_count}/{len(question_data_from_stage2)} questions (Summarized: {summarized_q_count}, Skipped: {skipped_q_count}, Errors: {error_q_count}).\")\n","\n","\n","            logger.log(f\"INFO: Summarization loop finished. Processed: {processed_q_count}, Summarized: {summarized_q_count}, Skipped: {skipped_q_count}, Errors: {error_q_count}.\")\n","\n","        # 3. Save the Summaries\n","        save_summaries_output(SUMMARIES_OUTPUT_FILE, summaries_data)\n","        main_success = True # Mark success if loop completed or was skipped gracefully\n","\n","\n","    except Exception as main_e: # Catch unexpected errors in main flow\n","        logger.log(\"CRITICAL:\" + \"=\"*10 + \" FATAL ERROR in main execution block \" + \"=\"*10)\n","        logger.log(f\"CRITICAL: Error: {main_e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","        logger.log(\"CRITICAL:\" + \"=\"*50)\n","\n","    finally:\n","        # --- Cleanup ---\n","        logger.log(\"INFO: Cleaning up LLM resources...\")\n","        # Place deletion in try-except blocks in case variables were never assigned\n","        try: del summarization_pipe\n","        except NameError: pass\n","        try: del model_instance\n","        except NameError: pass\n","        try: del loaded_tokenizer\n","        except NameError: pass\n","\n","        gc.collect() # Explicitly call garbage collector\n","        if torch.cuda.is_available():\n","             logger.log(\"INFO: Emptying CUDA cache...\")\n","             try:\n","                 torch.cuda.empty_cache()\n","             except Exception as cuda_err:\n","                  logger.log(f\"WARN: Error emptying CUDA cache: {cuda_err}\")\n","        logger.log(\"INFO: Cleanup complete.\")\n","\n","        logger.log(\"=\" * 50)\n","        if main_success: logger.log(\" INFO: Stage 3 Summarization Finished Successfully \")\n","        else: logger.log(\" ERROR: Stage 3 Summarization Finished With Errors or Did Not Complete \")\n","        logger.log(\"=\" * 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8bohr53M9J4"},"outputs":[],"source":["SUPERCEDED\n","# @title Question-Centric Summarization using LLM (superceded)\n","# =====================================================\n","#   Stage 3: Question-Centric Summarization using LLM\n","# =====================================================\n","# Purpose: Loads aggregated data from Stage 2, and for a\n","#          manually selected subset of questions, generates\n","#          LLM-based summaries of the collected form answers.\n","\n","# =====================================================\n","#                      Imports\n","# =====================================================\n","import json\n","import os\n","import sys\n","import traceback # For traceback logging\n","import time\n","import gc # Garbage Collection\n","from collections import defaultdict\n","from typing import Dict, Any, List\n","\n","# Potentially heavy imports - keep them together\n","try:\n","    import torch\n","    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","    # You might need 'accelerate' implicitly for device_map='auto'\n","    # import accelerate\n","except ImportError as e:\n","    # Cannot log yet as logger isn't initialized, print critical error and exit\n","    print(f\"CRITICAL ERROR: Missing essential libraries (torch, transformers). Install them. Details: {e}\")\n","    sys.exit(1)\n","\n","try:\n","    # Only required if USE_QUANTIZATION is True\n","    from bitsandbytes.cextension import ColabNotSupportedError\n","except ImportError:\n","    # Define dummy class if bitsandbytes is not installed but quantization is False\n","    ColabNotSupportedError = type('ColabNotSupportedError', (Exception,), {})\n","    pass # Silently ignore if not needed\n","\n","# =====================================================\n","#                Custom FileLogger Class\n","# =====================================================\n","# Reusing the simplified FileLogger (no torch dependency)\n","class FileLogger:\n","    def __init__(self, log_file):\n","        self.log_file = log_file\n","        log_dir = os.path.dirname(log_file)\n","        if log_dir and not os.path.exists(log_dir):\n","             os.makedirs(log_dir, exist_ok=True)\n","        try:\n","            # Open in append mode 'a' to preserve logs across restarts if needed,\n","            # or 'w' to start fresh each run. Using 'w' for now.\n","            with open(self.log_file, 'w', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: Starting new log...\\n\")\n","        except IOError as e:\n","            print(f\"CRITICAL: Failed to initialize log file {self.log_file}: {e}\")\n","            # Optionally exit if logging is absolutely critical\n","            # sys.exit(1)\n","\n","    def _get_timestamp(self):\n","        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n","\n","    def log(self, message: str):\n","        \"\"\"Logs a message to the file.\"\"\"\n","        try:\n","            with open(self.log_file, 'a', encoding='utf-8') as f:\n","                f.write(f\"{self._get_timestamp()}: {message}\\n\")\n","        except Exception as e:\n","            # Fallback print if logging to file fails\n","            print(f\"!!! LOGGER FAILED TO WRITE: {e} !!!\")\n","            print(f\"{self._get_timestamp()}: {message}\") # Log to console as fallback\n","\n","    def log_json(self, data: Dict[str, Any], prefix=\"JSON_DATA\"):\n","        \"\"\"Logs a dictionary as formatted JSON.\"\"\"\n","        serializer = self._get_serializer()\n","        try:\n","            log_entry = {\n","                 \"timestamp\": self._get_timestamp(),\n","                 \"type\": prefix,\n","                 \"data\": data\n","            }\n","            # Use the custom serializer for dumping\n","            json_string = json.dumps(log_entry, default=serializer, indent=2)\n","            with open(self.log_file, 'a', encoding='utf-8') as f:\n","                 f.write(json_string + \"\\n\")\n","        except TypeError as e:\n","            # Log the serialization error using the standard log method\n","            self.log(f\"ERROR: Failed to serialize data for JSON logging: {e} - Data Keys: {list(data.keys())}\")\n","        except Exception as e:\n","             # Log any other unexpected errors during JSON logging\n","             self.log(f\"ERROR: Unexpected error during log_json: {e}\")\n","             # Log traceback for unexpected errors\n","             tb_str = traceback.format_exc()\n","             self.log(f\"TRACEBACK:\\n{tb_str}\")\n","\n","    def _get_serializer(self):\n","        \"\"\"Returns a serializer function for json.dump.\"\"\"\n","        # Define a nested function to handle custom serialization types\n","        def default_serializer(obj):\n","            # Handle NumPy numeric types if numpy is imported and used\n","            # import numpy as np # Import locally if needed just here\n","            # if isinstance(obj, (np.integer, np.floating)): return obj.item()\n","            # if isinstance(obj, np.ndarray): return obj.tolist()\n","\n","            # Handle sets\n","            if isinstance(obj, set):\n","                return sorted(list(obj)) # Convert to sorted list\n","            # If none of the above, try the default JSON encoder\n","            try:\n","                # This will raise TypeError if the object is not serializable by default\n","                return json.JSONEncoder().default(obj)\n","            except TypeError:\n","                # If default encoder fails, return a string representation\n","                return f\"UNSERIALIZABLE:{type(obj).__name__}\"\n","        return default_serializer # Return the nested function\n","\n","# =====================================================\n","#              Pipeline Configuration Constants\n","# =====================================================\n","\n","# --- Input/Output Files ---\n","# Input: Aggregated data from Stage 2 (Single JSON object)\n","QUESTION_CENTRIC_UDS_FILE = 'question_centric_aggregation_output.json'\n","# Output: Summaries generated by this script (Single JSON object)\n","SUMMARIES_OUTPUT_FILE = 'question_centric_summaries_output.json'\n","\n","# --- LLM Configuration ---\n","# Choose model based on available resources (Colab free tier might struggle with 9B)\n","MODEL_ID = \"google/gemma-2-2b-it\" # Smaller Gemma 2 model, better for Colab free\n","# MODEL_ID = \"google/gemma-2-9b-it\" # Larger model, needs more GPU RAM\n","USE_QUANTIZATION = True # Set to False if bitsandbytes is unavailable or causing issues\n","# Consider setting HF token if needed for gated models: os.environ['HF_TOKEN'] = \"hf_...\"\n","\n","# --- Summarization Parameters ---\n","NUM_SUMMARY_SENTENCES = 3 # How many sentences to ask for in the summary\n","MIN_ANSWERS_FOR_SUMMARY = 3 # Minimum number of text answers needed to attempt summary\n","\n","# --- Logging ---\n","STAGE3_LOG_FILE = \"stage3_question_summarization.log\"\n","ENABLE_DEBUG_LOGGING = True # Controls some optional logging calls\n","\n","# --- Manual Question Selection (REVISED as per user) ---\n","# Dictionary mapping Question ID -> True/False (whether to summarize)\n","QUESTIONS_TO_SUMMARIZE = {\n","    \"Q1\": True,   # \"Should there be a criminal offence...\"\n","    \"Q2\": True,   # \"Should consent be required for other tracking data...\"\n","    \"Q3\": True,   # \"How should employers provide enhanced transparency...\"\n","    \"Q4\": True,   # \"Noting the current individual rights...\"\n","    \"Q5\": True,   # \"If privacy protections for employees were introduced...\"\n","    \"Q6\": True,   # \"If you are a small business operator...\"\n","    \"Q7\": True,   # \"What additional requirements should apply to mitigate privacy risks (biometric)...\"\n","    \"Q8\": True,   # \"What would the impact of the proposed individual rights be...\"\n","    \"Q9\": True,   # \"Are further exceptions required for any of the proposed individual rights?\"\n","    \"Q10\": True,  # \"What baseline privacy outcomes should be included in APP 11?\"\n","    \"Q11\": True,  # \"What are the barriers APP entities face to minimise collection (identity credential)...\"\n","    \"Q12\": True,  # \"Should APP entities be required to take reasonable steps (Notifiable Data Breach)...\"\n","    \"Q13\": True,  # \"What additional support... needed to assist smaller media organisations...\"\n","    \"Q14\": True,  # \"Should the scope of research permitted without consent be broadened?\"\n","    \"Q15\": True,  # \"Should there be a single exception for research without consent...\"\n","    \"Q16\": False, # \"Which entity is the most appropriate body to develop guidelines...\"\n","    \"Q17\": True,  # \"What privacy-related issues do APP entities face (financial abuse)...\"\n","    \"Q18\": True,  # \"How can financial institutions act in the interests of customers (financial abuse/capacity)...\"\n","    \"Q19\": True,  # \"Should the permitted general situations... be amended (safeguarding)...\"\n","    \"Q20\": True,  # \"What types of decisions are likely to have a legal or similarly significant effect...\"\n","    \"Q21\": True,  # \"Should there be exceptions to a right for individuals to request meaningful information (automated decisions)...\"\n","    \"Q22\": True,  # \"What would be the impact... in relation to direct marketing...\"\n","    \"Q23\": True,  # \"What would be the impact... in relation to targeting...\"\n","    \"Q24\": True,  # \"What would be the impact... in relation to sale of personal information...\"\n","    \"Q25\": True,  # \"Are there any technical or other challenges... providing information about algorithms...\"\n","    \"Q26\": True,  # \"Please share any examples of situations where greater transparency... is not necessary...\"\n","    \"Q27\": True,  # \"If small business non-APP entities... brought into the scope...\"\n","    \"Q28\": True,  # \"Should the extraterritorial scope... be amended...\"\n","    \"Q29\": True,  # \"Should disclosures... to overseas recipients via publication... be subject to an exception...\"\n","    \"Q30\": True,  # \"How can reporting processes for Notifiable Data Breaches be streamlined...\"\n","    \"Q31\": True,  # \"If you would like to provide general feedback...\"\n","    \"Q32\": False, # \"What is your name?\"\n","    \"Q33\": False, # \"Who are you making this submission for?\"\n","    \"Q34\": False, # \"What is your organisation?\"\n","    \"Q35\": False, # \"What sector is your organisation a part of?\"\n","    \"Q36\": False, # \"What is your position in the organisation?\"\n","}\n","\n","# =====================================================\n","#                 Logger Initialization\n","# =====================================================\n","logger = FileLogger(STAGE3_LOG_FILE)\n","\n","# =====================================================\n","#       Helper Function (for saving final output)\n","# =====================================================\n","def save_summaries_output(output_file: str, summaries_data: Dict[str, Any]):\n","    \"\"\"Saves the final summaries output JSON.\"\"\"\n","    logger.log(f\"INFO: Saving summaries output to {output_file}...\")\n","    # No complex types expected in final summary dict, standard encoder should work\n","    try:\n","        # Structure the final output with a top-level key\n","        final_output = {\"question_summaries\": summaries_data}\n","        with open(output_file, 'w', encoding='utf-8') as f:\n","             json.dump(final_output, f, indent=2)\n","        logger.log(f\"INFO: Successfully saved summaries output.\")\n","    except IOError as e:\n","        # Corrected logger usage\n","        logger.log(f\"ERROR: Could not write to output file {output_file}: {e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","    except TypeError as e:\n","        # Corrected logger usage\n","        logger.log(f\"ERROR: Failed to serialize summary data for output file {output_file}: {e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","    except Exception as e:\n","        # Corrected logger usage\n","        logger.log(f\"ERROR: An unexpected error occurred during saving summaries: {e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","\n","\n","# =====================================================\n","#                   Main Execution Block (Stage 3)\n","# =====================================================\n","if __name__ == \"__main__\":\n","\n","    # --- Log Initial Configuration ---\n","    logger.log(\"=\" * 50); logger.log(\" INFO: Starting Stage 3: Question Summarization \"); logger.log(\"=\" * 50)\n","    logger.log(f\"INFO: Input Aggregated UDS: {QUESTION_CENTRIC_UDS_FILE}\")\n","    logger.log(f\"INFO: Output Summaries File: {SUMMARIES_OUTPUT_FILE}\")\n","    logger.log(f\"INFO: LLM Model ID: {MODEL_ID}\")\n","    logger.log(f\"INFO: Use Quantization: {USE_QUANTIZATION}\")\n","    logger.log(f\"INFO: Min Answers for Summary: {MIN_ANSWERS_FOR_SUMMARY}\")\n","    logger.log(f\"INFO: Target Summary Sentences: {NUM_SUMMARY_SENTENCES}\")\n","    logger.log(f\"INFO: Debug Logging Enabled: {ENABLE_DEBUG_LOGGING}\")\n","\n","    # --- Load LLM Resources ---\n","    loaded_tokenizer = None\n","    model_instance = None\n","    summarization_pipe = None\n","    llm_load_success = False\n","\n","    try:\n","        # --- Tokenizer ---\n","        logger.log(f\"INFO: Loading tokenizer for: {MODEL_ID}...\")\n","        loaded_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n","\n","        if getattr(loaded_tokenizer, 'pad_token', None) is None:\n","             # Corrected logger usage\n","             logger.log(\"WARN: Tokenizer missing pad token; setting to eos_token.\")\n","             if getattr(loaded_tokenizer, 'eos_token', None) is not None:\n","                 loaded_tokenizer.pad_token = loaded_tokenizer.eos_token\n","                 loaded_tokenizer.padding_side = \"left\" # Important for batch generation if used later\n","                 logger.log(\"INFO: Set pad_token to eos_token and padding_side to left.\")\n","             else:\n","                 # Corrected logger usage\n","                 logger.log(\"ERROR: Tokenizer missing both pad_token and eos_token! Cannot set padding.\")\n","                 sys.exit(\"CRITICAL: Tokenizer setup failed.\") # Exit cleanly\n","\n","        logger.log(\"INFO: Tokenizer loaded.\")\n","\n","        # --- Quantization Config (Optional) ---\n","        bnb_config = None\n","        effective_quantization = USE_QUANTIZATION # Track if quantization is actually used\n","        if USE_QUANTIZATION:\n","            logger.log(\"INFO: Setting up 4-bit quantization...\")\n","            try:\n","                # Check torch version and CUDA availability for bfloat16 support\n","                compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n","                logger.log(f\"INFO: Using compute dtype: {compute_dtype}\")\n","\n","                bnb_config = BitsAndBytesConfig(\n","                    load_in_4bit=True,\n","                    bnb_4bit_quant_type=\"nf4\",\n","                    bnb_4bit_compute_dtype=compute_dtype,\n","                    bnb_4bit_use_double_quant=True,\n","                )\n","                logger.log(\"INFO: 4-bit quantization configured.\")\n","            except ImportError:\n","                # Corrected logger usage\n","                logger.log(\"ERROR: bitsandbytes not found. Install it for quantization. Disabling quantization.\")\n","                effective_quantization = False\n","            except ColabNotSupportedError:\n","                 # Corrected logger usage\n","                 logger.log(\"ERROR: Bits and Bytes Colab issue detected (often related to older versions or specific hardware). Disabling quantization.\")\n","                 effective_quantization = False\n","            except Exception as q_err:\n","                 # Corrected logger usage\n","                 logger.log(f\"ERROR: Failed to configure BitsAndBytes: {q_err}. Disabling quantization.\")\n","                 tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","                 effective_quantization = False\n","\n","        # --- Main Model Loading ---\n","        logger.log(f\"INFO: Loading main model: {MODEL_ID} (Quantization: {effective_quantization})...\")\n","        if not torch.cuda.is_available():\n","            # Corrected logger usage\n","            logger.log(\"WARN: CUDA not available, device_map='auto' will use CPU. Model loading/inference will be very slow.\")\n","\n","        model_instance = AutoModelForCausalLM.from_pretrained(\n","            MODEL_ID,\n","            quantization_config=bnb_config if effective_quantization else None,\n","            device_map=\"auto\",\n","            torch_dtype=compute_dtype if not effective_quantization else None, # Set dtype if not quantizing\n","            trust_remote_code=True # Often needed for newer models like Gemma 2\n","        )\n","        logger.log(\"INFO: Main model loaded.\")\n","\n","        # --- Pipeline Creation ---\n","        logger.log(\"INFO: Creating text-generation pipeline...\")\n","        summarization_pipe = pipeline(\n","            \"text-generation\",\n","            model=model_instance,\n","            tokenizer=loaded_tokenizer,\n","            # device is handled by device_map\n","            # Consider adding default max_length here if desired\n","            # max_length=512 # Example\n","        )\n","        logger.log(\"INFO: Pipeline created successfully.\")\n","        llm_load_success = True\n","\n","    except ImportError as ie:\n","         # Corrected logger usage\n","         logger.log(f\"ERROR: Missing libraries needed for LLM. Install requirements (torch, transformers, accelerate, bitsandbytes?). Details: {ie}\")\n","         sys.exit(1)\n","    except Exception as e:\n","        # Corrected logger usage\n","        logger.log(f\"CRITICAL: Failed to load LLM models/pipeline: {e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","        logger.log(\"CRITICAL: Cannot proceed with summarization.\")\n","        sys.exit(1)\n","\n","\n","    # --- Main Processing Steps ---\n","    main_success = False\n","    summaries_data = {} # Dictionary to store QID -> {status, summary, ...}\n","\n","    try:\n","        # 1. Load Aggregated Data from Stage 2\n","        logger.log(f\"INFO: Loading aggregated data from {QUESTION_CENTRIC_UDS_FILE}...\")\n","        try:\n","            with open(QUESTION_CENTRIC_UDS_FILE, 'r', encoding='utf-8') as f:\n","                aggregated_data = json.load(f)\n","\n","            if \"question_data\" not in aggregated_data or not isinstance(aggregated_data.get(\"question_data\"), dict):\n","                # Corrected logger usage\n","                logger.log(f\"ERROR: Input file {QUESTION_CENTRIC_UDS_FILE} missing 'question_data' key or it's not a dictionary.\")\n","                sys.exit(1)\n","            question_data = aggregated_data[\"question_data\"]\n","            logger.log(f\"INFO: Successfully loaded aggregated data for {len(question_data)} questions.\")\n","        except FileNotFoundError:\n","            # Corrected logger usage\n","            logger.log(f\"ERROR: Aggregated input file not found: {QUESTION_CENTRIC_UDS_FILE}\")\n","            sys.exit(1)\n","        except json.JSONDecodeError as e:\n","            # Corrected logger usage\n","            logger.log(f\"ERROR: Failed to parse JSON from {QUESTION_CENTRIC_UDS_FILE}: {e}\")\n","            sys.exit(1)\n","        except Exception as e:\n","            # Corrected logger usage\n","            logger.log(f\"ERROR: Unexpected error loading {QUESTION_CENTRIC_UDS_FILE}: {e}\")\n","            tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","            sys.exit(1)\n","\n","\n","        # 2. Iterate through Questions and Generate Summaries (only if LLM loaded)\n","        if not llm_load_success:\n","             # Corrected logger usage\n","             logger.log(\"WARN: LLM failed to load. Skipping summarization loop.\")\n","        else:\n","            logger.log(f\"INFO: Starting summarization loop for selected questions...\")\n","            processed_q_count = 0\n","            summarized_q_count = 0\n","            skipped_q_count = 0\n","            error_q_count = 0\n","\n","            # Iterate through the questions found in the Stage 2 data\n","            for qid, q_data in question_data.items():\n","                processed_q_count += 1\n","                question_text = q_data.get(\"question_text\", f\"Unknown Text for {qid}\")\n","\n","                # --- Filtering Logic ---\n","                should_summarize_flag = QUESTIONS_TO_SUMMARIZE.get(qid, False) # Check manual config\n","                if not should_summarize_flag:\n","                    if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG: Skipping QID {qid} (Marked False in config).\")\n","                    skipped_q_count += 1\n","                    summaries_data[qid] = {\"status\": \"skipped_config\", \"summary\": None}\n","                    continue\n","\n","                answers = q_data.get(\"form_answer_examples\", [])\n","                # Filter for non-empty text answers\n","                text_answers = [str(ans['content']) for ans in answers\n","                                if ans.get('type') == 'text' and ans.get('content') is not None and str(ans['content']).strip()]\n","\n","                if len(text_answers) < MIN_ANSWERS_FOR_SUMMARY:\n","                    logger.log(f\"INFO: Skipping QID {qid} - Insufficient valid text answers ({len(text_answers)} found < {MIN_ANSWERS_FOR_SUMMARY} required).\")\n","                    skipped_q_count += 1\n","                    summaries_data[qid] = {\"status\": \"skipped_low_answers\", \"summary\": None, \"answer_count\": len(text_answers)}\n","                    continue\n","                # --- End Filtering Logic ---\n","\n","                logger.log(f\"INFO: Processing QID {qid} for summarization ({len(text_answers)} answers)...\")\n","                start_q_time = time.time()\n","\n","                # Prepare text chunk for LLM\n","                text_chunk = \"\\n\\n\".join(text_answers)\n","\n","                # Format prompt for Gemma instruct model\n","                # Note: Gemma-2 templates might differ slightly, check model card if issues arise\n","                messages = [\n","                    # Removed role for Gemma 2 - it might prefer just user/model turns or specific syntax\n","                    {\"role\": \"user\",\n","                     \"content\": f\"Carefully review all the following responses to the question: \\\"{question_text}\\\". Then, concisely summarize the main themes or points expressed across these responses in exactly {NUM_SUMMARY_SENTENCES} distinct sentences.\\n\\nResponses:\\n\\\"\\\"\\\"\\n{text_chunk}\\n\\\"\\\"\\\"\"}\n","                ]\n","\n","                try:\n","                    # Generate prompt string using tokenizer\n","                    prompt_formatted = summarization_pipe.tokenizer.apply_chat_template(\n","                        messages, tokenize=False, add_generation_prompt=True # Add prompt for generation turn\n","                    )\n","\n","                    # Call the LLM pipeline\n","                    # Adjust max_new_tokens based on expected summary length + buffer\n","                    # Setting temperature low for more factual summary\n","                    generation_args = {\n","                        \"max_new_tokens\": 150 + NUM_SUMMARY_SENTENCES * 40, # Estimate tokens needed\n","                        \"do_sample\": False, # Turn off sampling for deterministic output\n","                        \"temperature\": None, # Not needed if do_sample=False\n","                        \"top_p\": None, # Not needed if do_sample=False\n","                        }\n","                    if ENABLE_DEBUG_LOGGING: logger.log(f\"DEBUG: Calling LLM for QID {qid}...\")\n","                    outputs = summarization_pipe(prompt_formatted, **generation_args)\n","\n","                    # Extract the generated summary\n","                    generated_text_full = outputs[0]['generated_text']\n","\n","                    # Robust extraction: Remove the input prompt part\n","                    # Check if the generated text starts with the formatted prompt\n","                    if generated_text_full.startswith(prompt_formatted):\n","                        summary_text = generated_text_full[len(prompt_formatted):].strip()\n","                    else:\n","                        # Fallback if template format caused unexpected output start\n","                        # Try finding common model response markers (may vary)\n","                        response_marker = \"<start_of_turn>model\\n\" # Common marker\n","                        summary_start_index = generated_text_full.rfind(response_marker)\n","                        if summary_start_index != -1:\n","                             summary_text = generated_text_full[summary_start_index + len(response_marker):].strip()\n","                        else:\n","                             # If still not found, log warning and maybe return full output for inspection\n","                             # Corrected logger usage\n","                             logger.log(f\"WARN: Could not reliably extract summary for QID {qid} using standard markers/prompt removal. Output may contain prompt residue.\")\n","                             summary_text = generated_text_full # Return full output as best guess\n","\n","                    end_q_time = time.time()\n","                    logger.log(f\"INFO: Successfully generated summary for QID {qid} in {end_q_time - start_q_time:.2f} seconds.\")\n","                    summaries_data[qid] = {\"status\": \"success\", \"summary\": summary_text, \"answer_count\": len(text_answers)}\n","                    summarized_q_count += 1\n","\n","                except Exception as pipe_err:\n","                    end_q_time = time.time()\n","                    # Corrected logger usage\n","                    logger.log(f\"ERROR: LLM pipeline failed for QID {qid} after {end_q_time - start_q_time:.2f} seconds: {pipe_err}\")\n","                    tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","                    summaries_data[qid] = {\"status\": \"error_llm_pipe\", \"summary\": None, \"error_message\": str(pipe_err)}\n","                    error_q_count += 1 # Count errors separately from skipped\n","\n","                # Log progress within loop\n","                if (processed_q_count % 5 == 0) or (processed_q_count == len(question_data)):\n","                     logger.log(f\"INFO: ...progress: processed {processed_q_count}/{len(question_data)} questions (Summarized: {summarized_q_count}, Skipped: {skipped_q_count}, Errors: {error_q_count}).\")\n","\n","\n","            logger.log(f\"INFO: Summarization loop finished. Processed: {processed_q_count}, Summarized: {summarized_q_count}, Skipped: {skipped_q_count}, Errors: {error_q_count}.\")\n","\n","        # 3. Save the Summaries\n","        save_summaries_output(SUMMARIES_OUTPUT_FILE, summaries_data)\n","        main_success = True # Mark success if loop completed or was skipped gracefully\n","\n","\n","    except Exception as main_e: # Catch unexpected errors in main flow\n","        # Corrected logger usage\n","        logger.log(\"CRITICAL:\" + \"=\"*10 + \" FATAL ERROR in main execution block \" + \"=\"*10)\n","        logger.log(f\"CRITICAL: Error: {main_e}\")\n","        tb_str = traceback.format_exc(); logger.log(f\"TRACEBACK:\\n{tb_str}\")\n","        logger.log(\"CRITICAL:\" + \"=\"*50)\n","\n","    finally:\n","        # --- Cleanup ---\n","        logger.log(\"INFO: Cleaning up LLM resources...\")\n","        del summarization_pipe\n","        del model_instance\n","        del loaded_tokenizer\n","        gc.collect() # Explicitly call garbage collector\n","        if torch.cuda.is_available():\n","             logger.log(\"INFO: Emptying CUDA cache...\")\n","             try:\n","                 torch.cuda.empty_cache()\n","             except Exception as cuda_err:\n","                  # Corrected logger usage\n","                  logger.log(f\"WARN: Error emptying CUDA cache: {cuda_err}\")\n","        logger.log(\"INFO: Cleanup complete.\")\n","\n","        logger.log(\"=\" * 50)\n","        if main_success: logger.log(\" INFO: Stage 3 Summarization Finished Successfully \")\n","        # Corrected logger usage\n","        else: logger.log(\" ERROR: Stage 3 Summarization Finished With Errors or Did Not Complete \")\n","        logger.log(\"=\" * 50)"]},{"cell_type":"markdown","metadata":{"id":"fFMn-ChK8GjL"},"source":["# END"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1JP7oz7_5WZ2WqbTIoUrG1szmGj8TlL70","timestamp":1746754765711}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1355e01a44f24c25a8dd270ad0dd302d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91a375b095844345937fe4f364545ab3","IPY_MODEL_69547a5938364762a056a8b190b107ba","IPY_MODEL_06c6242a2b3843fd8403dd299e7a5fe6"],"layout":"IPY_MODEL_447406aba33d4eaaa9e221d6a6d045c3"}},"91a375b095844345937fe4f364545ab3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e44ce7d2609542e2bfc000870e913fd5","placeholder":"â€‹","style":"IPY_MODEL_04db27acf6f04a779b72a821fccbe921","value":""}},"69547a5938364762a056a8b190b107ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d676651d47e543159652391d386c2c02","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7cec0a2fbf5b4811be6e8d40049e0dd5","value":0}},"06c6242a2b3843fd8403dd299e7a5fe6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84bf459b020449dc87dc46751e046dfe","placeholder":"â€‹","style":"IPY_MODEL_10e76bb57df044b5a8b91cbd9053f54a","value":"â€‡0/0â€‡[00:00&lt;?,â€‡?it/s]"}},"447406aba33d4eaaa9e221d6a6d045c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e44ce7d2609542e2bfc000870e913fd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04db27acf6f04a779b72a821fccbe921":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d676651d47e543159652391d386c2c02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7cec0a2fbf5b4811be6e8d40049e0dd5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84bf459b020449dc87dc46751e046dfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10e76bb57df044b5a8b91cbd9053f54a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}