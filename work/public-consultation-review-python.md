# -*- coding: utf-8 -*-
"""Public Consultation Responses Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q7GDPYhEQTfVbKR8kVBxmWlhfxUwK3Gw

Refer to requirements on wiki

# Pre-requisites
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/Colab Notebooks/Legal

pwd

pip install -U transformers bitsandbytes accelerate

# @title Authenticate Environment
from huggingface_hub import notebook_login

notebook_login()

"""# 19th May Afternoon"""

# @title TEST CELL (JSON Output): Q4 (Scenario A - Summaries) & Q16 (Scenario B - Passages) Thematic Analysis

# --- Test Cell Imports ---
import json
import os
import sys
import traceback
import time
import gc
import re
from collections import defaultdict
from typing import Dict, Any, List

import torch
import numpy as np
# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline # In Cell 0
# try:
#     from bitsandbytes.cextension import ColabNotSupportedError # In Cell 0
# except ImportError:
#     ColabNotSupportedError = None

# --- Configuration for this Test Cell ---
TEST_JSON_BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # <<<--- ADJUST IF NEEDED
TEST_JSON_LOG_FILE = os.path.join(TEST_JSON_BASE_PROJECT_DIR, f'test_q4a_q16b_json_themes_{time.strftime("%Y%m%d_%H%M%S")}.log')

# LLM Config
TEST_JSON_LLM_MODEL_ID = 'google/gemma-3-4b-it'  # <<<--- Using 2b-it. Adjust if needed.
TEST_JSON_USE_QUANTIZATION = False             # <<<--- Adjust if needed.

# Thematic Analysis Parameters
TEST_JSON_NUM_THEMES_TO_REQUEST = 3
TEST_JSON_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2800
TEST_JSON_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 500 # Increased slightly for JSON structure
TEST_JSON_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 750 # Increased slightly for JSON structure
TEST_JSON_MAPREDUCE_CHUNK_OVERLAP_WORDS = 30

# Input file from Stage 4a (Phase 2 output)
TEST_JSON_INPUT_COLLATED_PDF_TEXTS_JSON = os.path.join(TEST_JSON_BASE_PROJECT_DIR, 'phase2_stage4a_collated_pdf_texts_20250518_233834.json') # <<<--- UPDATE THIS FILENAME

# Source for actual question texts
TEST_JSON_QUESTION_TEXT_SOURCE_FILE = os.path.join(TEST_JSON_BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # <<<--- ENSURE THIS IS CORRECT

# --- FileLogger (Assuming defined in Cell 1 or globally) ---
try:
    logger_test_json = FileLogger(TEST_JSON_LOG_FILE)
    logger_test_json.log(f"INFO: Test Cell (JSON Themes) - Logger Initialized.")
except NameError:
    print("CRITICAL: FileLogger class not defined. Please ensure Cell 1 has been executed.")
    class PrintLogger: # Fallback
        def log(self, msg): print(f"{time.strftime('%Y%m%d_%H%M%S')}: {msg}")
    logger_test_json = PrintLogger()
    logger_test_json.log("WARN: Using fallback PrintLogger.")

# --- Helper: Text Chunker (for Scenario A) ---
def create_text_chunks_json_test(full_text: str, tokenizer_for_chunking, max_tokens: int, overlap_words: int = 30):
    # ... (Same as create_text_chunks_for_scenario_a, using logger_test_json) ...
    words = full_text.split()
    if not words: return []
    chunks = []
    current_chunk_words = []
    logger_test_json.log(f"DEBUG CHUNKER_JSON: Starting to chunk text of {len(words)} words into max {max_tokens} token chunks.")
    for word_idx, word in enumerate(words):
        potential_chunk_text = " ".join(current_chunk_words + [word])
        num_tokens = len(tokenizer_for_chunking.encode(potential_chunk_text, add_special_tokens=False))
        if num_tokens <= max_tokens: current_chunk_words.append(word)
        else:
            if current_chunk_words: chunks.append(" ".join(current_chunk_words))
            if chunks and overlap_words > 0:
                prev_chunk_words = chunks[-1].split()
                overlap_start_idx = max(0, len(prev_chunk_words) - overlap_words)
                current_chunk_words = prev_chunk_words[overlap_start_idx:] + [word]
            else: current_chunk_words = [word]
            new_chunk_text_check = " ".join(current_chunk_words)
            new_chunk_tokens_check = len(tokenizer_for_chunking.encode(new_chunk_text_check, add_special_tokens=False))
            if new_chunk_tokens_check > max_tokens:
                if len(current_chunk_words) > 1 and chunks: current_chunk_words = [word]
                elif len(current_chunk_words) == 1: logger_test_json.log(f"    WARN CHUNKER_JSON: Single word '{word}' oversized ({new_chunk_tokens_check} tokens).")
    if current_chunk_words: chunks.append(" ".join(current_chunk_words))
    logger_test_json.log(f"INFO CHUNKER_JSON: Split text into {len(chunks)} chunks.")
    return chunks

# --- Helper: Batch Individual Items (for Scenario B map step) ---
def batch_text_items_json_test(text_items_list: List[str], tokenizer_for_batching, max_tokens_per_batch: int):
    # ... (Same as batch_text_items_for_scenario_b, using logger_test_json) ...
    batches = []
    current_batch_item_texts = []
    current_batch_concatenated_text = ""
    logger_test_json.log(f"DEBUG BATCHER_JSON: Starting to batch {len(text_items_list)} items into max {max_tokens_per_batch} token batches.")
    for item_idx, item_text_original in enumerate(text_items_list):
        if not item_text_original or not item_text_original.strip(): continue
        text_to_try_adding = item_text_original
        if current_batch_item_texts: text_to_try_adding = "\n\n--- Next Excerpt ---\n" + item_text_original
        potential_full_batch_text = current_batch_concatenated_text + text_to_try_adding
        num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))
        if num_tokens <= max_tokens_per_batch:
            current_batch_item_texts.append(item_text_original)
            current_batch_concatenated_text = potential_full_batch_text
        else:
            if current_batch_item_texts: batches.append(current_batch_item_texts)
            single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))
            if single_item_tokens > max_tokens_per_batch:
                logger_test_json.log(f"  WARN BATCHER_JSON: Item {item_idx} ({single_item_tokens} tokens) oversized. Own batch.")
                batches.append([item_text_original])
                current_batch_item_texts = []
                current_batch_concatenated_text = ""
            else:
                current_batch_item_texts = [item_text_original]
                current_batch_concatenated_text = item_text_original
    if current_batch_item_texts: batches.append(current_batch_item_texts)
    logger_test_json.log(f"INFO BATCHER_JSON: Created {len(batches)} batches of items.")
    return batches


# --- Helper: LLM Call for Thematic Points (Requests JSON Output) ---
def get_llm_json_themes_test(text_input_for_llm: str, question_text_for_prompt: str,
                             num_themes_requested: int, llm_pipe: pipeline, max_new_tokens: int,
                             log_prefix_qid_scenario: str, map_reduce_step_type: str):
    json_format_example = (
        "{\n"
        "  \"themes\": [\n"
        "    {\n"
        "      \"theme_label\": \"Concise Label for Theme 1\",\n"
        "      \"explanation\": \"A 1-2 sentence explanation of Theme 1 based on the provided text.\"\n"
        "    },\n"
        "    {\n"
        "      \"theme_label\": \"Concise Label for Theme 2\",\n"
        "      \"explanation\": \"A 1-2 sentence explanation of Theme 2 based on the provided text.\"\n"
        "    }\n"
        "    // ... up to " + str(num_themes_requested) + " themes ...\n"
        "  ]\n"
        "}"
    )
    num_map_themes_to_ask = 3 # Ask for up to 3 themes from each map chunk/batch

    prompt_instruction = ""
    if map_reduce_step_type == "SCENARIO_A_MAP": # Chunk of concatenated summaries
        prompt_instruction = (
            f"You are an expert policy analyst. The following text is a CHUNK of concatenated PREVIOUSLY-GENERATED SUMMARIES from multiple PDF submissions responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on this provided CHUNK OF SUMMARIES, identify up to {num_map_themes_to_ask} most prominent and distinct points, arguments, or themes.\n"
            f"Respond *only* with a single, valid JSON object formatted as follows, containing a list called 'themes'. Each object in the 'themes' list must have two string keys: 'theme_label' and 'explanation'.\n"
            f"Example JSON format:\n{json_format_example}\n\n"
            f"Chunk of Summaries:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"JSON Output:"
        )
    elif map_reduce_step_type == "SCENARIO_B_MAP": # Batch of original passages
        prompt_instruction = (
            f"You are an expert policy analyst. Carefully review the following BATCH of original excerpts/passages from PDF submissions, all responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on this provided BATCH of original excerpts/passages, identify up to {num_map_themes_to_ask} prominent and distinct points, arguments, or themes.\n"
            f"Respond *only* with a single, valid JSON object formatted as follows, containing a list called 'themes'. Each object in the 'themes' list must have two string keys: 'theme_label' and 'explanation'.\n"
            f"Example JSON format:\n{json_format_example}\n\n"
            f"Batch of Excerpts/Passages:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"JSON Output:"
        )
    elif map_reduce_step_type == "REDUCE_INTERMEDIATE_THEMES":
        # Input `text_input_for_llm` for reduce is a string of preliminary JSON outputs or parsed themes
        prompt_instruction = (
            f"You are an expert policy analyst. The following are sets of preliminary themes/points (potentially in JSON format or as text summaries of themes) that were individually extracted in previous steps from responses to the question: '{question_text_for_prompt}'.\n"
            f"Your task is to synthesize these preliminary themes into the overall top {num_themes_requested} overarching and distinct common points or themes.\n"
            f"Respond *only* with a single, valid JSON object formatted as follows, containing a list called 'themes'. Each object in the 'themes' list must have two string keys: 'theme_label' (a concise title) and 'explanation' (a 1-2 sentence summary).\n"
            f"Example JSON format:\n{json_format_example}\n\n"
            f"Preliminary Themes/Points Provided:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"JSON Output (Synthesized Top {num_themes_requested} Themes):"
        )
    else: # DIRECT_PROCESSING
        prompt_instruction = (
            f"You are an expert policy analyst. Carefully review all the following text (which consists of combined PDF summaries OR a single batch of excerpts/passages) responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on the provided text, identify the top {num_themes_requested} most common and distinct points, arguments, or themes.\n"
            f"Respond *only* with a single, valid JSON object formatted as follows, containing a list called 'themes'. Each object in the 'themes' list must have two string keys: 'theme_label' and 'explanation'.\n"
            f"Example JSON format:\n{json_format_example}\n\n"
            f"Provided Text:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"JSON Output (Top {num_themes_requested} Common Points):"
        )

    messages = [{"role": "user", "content": prompt_instruction}]
    try:
        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        generation_args = {"max_new_tokens": max_new_tokens, "do_sample": False,
                           "pad_token_id": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id}

        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True)
        input_text_tokens = llm_pipe.tokenizer.encode(text_input_for_llm, add_special_tokens=False)
        model_max_len = 8192
        # ... (model_max_len detection logic from previous get_llm_thematic_points_ab_test, using logger_test_json)
        if hasattr(llm_pipe.model.config, 'max_position_embeddings'): model_max_len = llm_pipe.model.config.max_position_embeddings
        elif hasattr(llm_pipe.model.config, 'max_sequence_length'): model_max_len = llm_pipe.model.config.max_sequence_length
        elif hasattr(llm_pipe.tokenizer, 'model_max_length'): model_max_len = llm_pipe.tokenizer.model_max_length
        if "gemma" in llm_pipe.model.config.model_type.lower() and model_max_len > 8192 : model_max_len = 8192


        logger_test_json.log(f"DEBUG LLM_JSON_CALL: {map_reduce_step_type} for {log_prefix_qid_scenario}. Prompt toks: {len(prompt_tokens)}. Content toks: {len(input_text_tokens)}. Max new: {max_new_tokens}. Model max_len: {model_max_len}.")
        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95: # Check against determined model_max_len
             logger_test_json.log(f"WARN LLM_JSON_CALL: {log_prefix_qid_scenario} - Step {map_reduce_step_type} - Prompt+Output tokens might exceed context ({model_max_len}).")

        outputs = llm_pipe(prompt_formatted, **generation_args)

        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):
            logger_test_json.log(f"ERROR LLM_JSON_CALL: {map_reduce_step_type} LLM unexpected output for {log_prefix_qid_scenario}.")
            return "LLM_ERROR: No output generated."
        generated_text_full = outputs[0]['generated_text']

        llm_response_text = generated_text_full
        # Try to extract only the JSON part, be more aggressive
        json_match = re.search(r"```json\s*(\{.*?\})\s*```", generated_text_full, re.DOTALL)
        if json_match:
            llm_response_text = json_match.group(1).strip()
        else:
            json_match_alt = re.search(r"(\{.*?\})", generated_text_full, re.DOTALL) # Look for any {...}
            if json_match_alt:
                llm_response_text = json_match_alt.group(1).strip()
                logger_test_json.log(f"WARN LLM_JSON_CALL: {map_reduce_step_type} for {log_prefix_qid_scenario} - Used generic JSON block extraction. Review output.")
            else: # Fallback: try stripping prompt if it's an exact match (less likely with JSON instructions)
                if generated_text_full.startswith(prompt_formatted):
                    llm_response_text = generated_text_full[len(prompt_formatted):].strip()
                else: # If no JSON found and prompt doesn't match, might be just model's text response
                    model_response_marker = "<start_of_turn>model\n"
                    if model_response_marker in generated_text_full:
                         llm_response_text = generated_text_full.split(model_response_marker, 1)[-1].strip()
                    # else llm_response_text remains generated_text_full
                    logger_test_json.log(f"WARN LLM_JSON_CALL: {map_reduce_step_type} for {log_prefix_qid_scenario} - Could not isolate JSON. Output may be non-JSON or include prompt.")

        return llm_response_text
    except Exception as e:
        logger_test_json.log(f"ERROR LLM_JSON_CALL: {map_reduce_step_type} LLM exception for {log_prefix_qid_scenario}: {e}")
        logger_test_json.log(traceback.format_exc())
        return f"LLM_ERROR: Exception - {str(e)}"

# --- Helper: Parse LLM JSON Thematic Output ---
def parse_llm_json_themes_test(llm_json_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:
    parsed_themes = []
    if not llm_json_output_text or llm_json_output_text.startswith("LLM_ERROR:"):
        return [{"theme_label": "LLM Error in Generation", "explanation": llm_json_output_text}]

    try:
        # Basic cleanup that was in get_llm_json_themes_test is already done before this parser is called.
        # This parser now expects a string that *should* be JSON.
        data = json.loads(llm_json_output_text) # Expects cleaned JSON string

        if isinstance(data, dict) and "themes" in data and isinstance(data["themes"], list):
            for theme_obj in data["themes"]:
                if isinstance(theme_obj, dict) and "theme_label" in theme_obj and "explanation" in theme_obj:
                    parsed_themes.append({
                        "theme_label": str(theme_obj["theme_label"]),
                        "explanation": str(theme_obj["explanation"])
                    })
                    if len(parsed_themes) >= num_expected_themes:
                        break
            if parsed_themes:
                return parsed_themes
            else:
                 logger_test_json.log("WARN JSON_PARSER: Valid JSON but 'themes' list malformed or empty.")
                 return [{"theme_label": "Valid JSON, themes list malformed/empty", "explanation": llm_json_output_text[:200]+"..."}]
        else:
            logger_test_json.log("WARN JSON_PARSER: LLM output was valid JSON but not expected structure {'themes': [...]}." )
            return [{"theme_label": "Valid JSON, unexpected structure", "explanation": llm_json_output_text[:200]+"..."}]
    except json.JSONDecodeError as e:
        logger_test_json.log(f"ERROR JSON_PARSER: LLM output not valid JSON. Error: {e}")
        logger_test_json.log(f"LLM Raw Output for JSON Parse Error:\n{llm_json_output_text}")
        return [{"theme_label": "Invalid JSON Output", "explanation": f"JSONDecodeError: {e}. Raw: {llm_json_output_text[:200]}..."}]
    except Exception as e_gen:
        logger_test_json.log(f"ERROR JSON_PARSER: Unexpected error parsing: {e_gen}")
        return [{"theme_label": "Unexpected Parsing Error", "explanation": f"Error: {e_gen}. Raw: {llm_json_output_text[:200]}..."}]


# --- Main Test Execution Function ---
def run_thematic_analysis_tests_with_json():
    logger_test_json.log("INFO: Test Cell (JSON) - Starting Thematic Analysis Scenarios Test (Q4-A, Q16-B).")

    # --- 0. Load LLM ---
    llm_thematic_pipe_test = None
    tokenizer_for_llm_test = None
    try:
        logger_test_json.log(f"INFO TEST JSON: Loading LLM: {TEST_JSON_LLM_MODEL_ID} (Quant: {TEST_JSON_USE_QUANTIZATION})")
        tokenizer_for_llm_test = AutoTokenizer.from_pretrained(TEST_JSON_LLM_MODEL_ID)
        if getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is not None:
            tokenizer_for_llm_test.pad_token = tokenizer_for_llm_test.eos_token
        elif getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is None:
             logger_test_json.log(f"CRITICAL TEST JSON: Tokenizer for {TEST_JSON_LLM_MODEL_ID} has neither pad_token nor eos_token.")

        bnb_config_test = None; effective_quant_test = TEST_JSON_USE_QUANTIZATION
        if TEST_JSON_USE_QUANTIZATION and torch.cuda.is_available():
            try:
                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
                bnb_config_test = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=compute_dtype)
            except ImportError: effective_quant_test = False; logger_test_json.log("WARN TEST JSON: bitsandbytes not imported.")
            except ColabNotSupportedError: effective_quant_test = False; logger_test_json.log("WARN TEST JSON: ColabNotSupportedError for bnb.")
            except Exception as e_bnb: effective_quant_test = False; logger_test_json.log(f"WARN TEST JSON: BNB config failed: {e_bnb}.")
        elif TEST_JSON_USE_QUANTIZATION: effective_quant_test = False; logger_test_json.log("WARN TEST JSON: Quant requested, No CUDA.")

        model_for_llm_test = AutoModelForCausalLM.from_pretrained(
            TEST_JSON_LLM_MODEL_ID,
            quantization_config=bnb_config_test if effective_quant_test else None,
            torch_dtype=torch.bfloat16 if not effective_quant_test and torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (torch.float32 if not effective_quant_test else None),
            device_map="auto", trust_remote_code=True
        )
        llm_thematic_pipe_test = pipeline("text-generation", model=model_for_llm_test, tokenizer=tokenizer_for_llm_test)
        logger_test_json.log("INFO TEST JSON: LLM Pipeline Loaded.")
    except Exception as e:
        logger_test_json.log(f"CRITICAL TEST JSON: Failed to load LLM: {e}\n{traceback.format_exc()}")
        return

    # --- 1. Load Full Stage 4a Data ---
    all_qids_collated_data = {}
    # ... (same loading logic as before, using TEST_JSON_INPUT_COLLATED_PDF_TEXTS_JSON and logger_test_json) ...
    try:
        with open(TEST_JSON_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:
            stage4a_data = json.load(f)
        all_qids_collated_data = stage4a_data.get("aggregated_pdf_content_by_qid", {})
        if not all_qids_collated_data:
            logger_test_json.log(f"ERROR TEST JSON: No 'aggregated_pdf_content_by_qid' in {TEST_JSON_INPUT_COLLATED_PDF_TEXTS_JSON}.")
            return
        logger_test_json.log(f"INFO TEST JSON: Loaded data for {len(all_qids_collated_data)} QIDs from {TEST_JSON_INPUT_COLLATED_PDF_TEXTS_JSON}.")
    except Exception as e:
        logger_test_json.log(f"CRITICAL TEST JSON: Failed to load Stage 4a data: {e}\n{traceback.format_exc()}")
        return

    # --- 2. Load All Question Texts ---
    qid_to_actual_question_text_map = {}
    # ... (same loading logic as before, using TEST_JSON_QUESTION_TEXT_SOURCE_FILE and logger_test_json) ...
    try:
        logger_test_json.log(f"INFO TEST JSON: Loading Q texts from: {TEST_JSON_QUESTION_TEXT_SOURCE_FILE}")
        if not os.path.exists(TEST_JSON_QUESTION_TEXT_SOURCE_FILE):
             raise FileNotFoundError(f"Q text source file not found: {TEST_JSON_QUESTION_TEXT_SOURCE_FILE}")
        with open(TEST_JSON_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:
            qtext_source_data = json.load(f_qtext)
        if "question_data" in qtext_source_data:
            for qid_key, data_val in qtext_source_data.get("question_data", {}).items():
                qid_to_actual_question_text_map[qid_key] = data_val.get("question_text", f"Text for {qid_key} NF")
        logger_test_json.log(f"INFO TEST JSON: Loaded {len(qid_to_actual_question_text_map)} Q texts.")
    except Exception as e:
        logger_test_json.log(f"ERROR TEST JSON: Failed to load Q texts: {e}\n{traceback.format_exc()}")


    # ==============================================================================
    # --- Scenario A: Map-Reduce on Stage 4 SUMMARIES for Q4 ---
    # ==============================================================================
    qid_for_scenario_a = "Q4"
    logger_test_json.log("\n" + "="*30 + f" SCENARIO A (JSON): Map-Reduce on PDF SUMMARIES for {qid_for_scenario_a} " + "="*30)

    q_a_all_items = all_qids_collated_data.get(qid_for_scenario_a, [])
    q_a_summaries = [item['text'] for item in q_a_all_items if item.get('type') == 'pdf_summary' and item.get('text')]
    question_text_for_q_a = qid_to_actual_question_text_map.get(qid_for_scenario_a, f"Question {qid_for_scenario_a} text not loaded")

    scenario_a_final_themes = []
    scenario_a_map_outputs_raw = []
    scenario_a_reduce_output_raw = "N/A"

    if not q_a_summaries:
        logger_test_json.log(f"WARN TEST JSON SCENARIO A: No 'pdf_summary' items for QID {qid_for_scenario_a}.")
    else:
        logger_test_json.log(f"INFO TEST JSON SCENARIO A: Found {len(q_a_summaries)} PDF summaries for {qid_for_scenario_a}.")
        concatenated_q_a_summaries = "\n\n--- Next Summary Entry ---\n".join(q_a_summaries)

        summary_chunks_A = create_text_chunks_json_test(
            concatenated_q_a_summaries, llm_thematic_pipe_test.tokenizer,
            TEST_JSON_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK, TEST_JSON_MAPREDUCE_CHUNK_OVERLAP_WORDS
        )

        if len(summary_chunks_A) > 1:
            logger_test_json.log(f"INFO TEST JSON SCENARIO A: {qid_for_scenario_a} summaries split into {len(summary_chunks_A)} chunks.")
            intermediate_themes_A_list = []
            for i, chunk in enumerate(summary_chunks_A):
                logger_test_json.log(f"  SCENARIO A - MAP CHUNK {i+1}/{len(summary_chunks_A)} (Chars: {len(chunk)})")
                map_out = get_llm_json_themes_test(
                    chunk, question_text_for_q_a, 3, llm_thematic_pipe_test,
                    TEST_JSON_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,
                    f"{qid_for_scenario_a}_A_map_chunk{i+1}", "SCENARIO_A_MAP"
                )
                scenario_a_map_outputs_raw.append(map_out)
                if not map_out.startswith("LLM_ERROR:"): intermediate_themes_A_list.append(map_out) # Store raw JSON output
                time.sleep(0.2)

            if intermediate_themes_A_list:
                # For reduce step, we pass the raw JSON strings from map step, or parsed themes joined as string
                # Passing raw JSON strings might be too verbose. Let's try parsing them first.
                parsed_intermediate_themes_for_reduce_A = []
                for raw_json_map_out in intermediate_themes_A_list:
                    parsed_map_themes = parse_llm_json_themes_test(raw_json_map_out, 3)
                    for pt in parsed_map_themes: # Convert parsed themes to a string format for reduce prompt
                        if not pt.get("theme_label","").startswith("LLM Error") and not pt.get("theme_label","").startswith("Unparsed"):
                             parsed_intermediate_themes_for_reduce_A.append(f"Theme Label: {pt.get('theme_label')}\nExplanation: {pt.get('explanation')}")

                if parsed_intermediate_themes_for_reduce_A:
                    combined_intermediate_A = "\n\n---\n\n".join(parsed_intermediate_themes_for_reduce_A)
                    logger_test_json.log(f"  SCENARIO A - REDUCE Step for {qid_for_scenario_a} (Input Chars for reduce: {len(combined_intermediate_A)})")
                    scenario_a_reduce_output_raw = get_llm_json_themes_test(
                        combined_intermediate_A, question_text_for_q_a, TEST_JSON_NUM_THEMES_TO_REQUEST,
                        llm_thematic_pipe_test, TEST_JSON_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,
                        f"{qid_for_scenario_a}_A_reduce", "REDUCE_INTERMEDIATE_THEMES"
                    )
                    scenario_a_final_themes = parse_llm_json_themes_test(scenario_a_reduce_output_raw, TEST_JSON_NUM_THEMES_TO_REQUEST)
                else:
                    logger_test_json.log(f"ERROR TEST_JSON SCENARIO A: No successfully parsed intermediate themes from MAP step for {qid_for_scenario_a}.")
                    scenario_a_reduce_output_raw = f"ERROR: No successfully parsed intermediate themes from MAP step for {qid_for_scenario_a} (Summaries)."
            else:
                logger_test_json.log(f"ERROR TEST_JSON SCENARIO A: No valid (non-error) intermediate themes from MAP step for {qid_for_scenario_a}.")
                scenario_a_reduce_output_raw = f"ERROR: No valid intermediate themes from MAP step for {qid_for_scenario_a} (Summaries)."

        elif summary_chunks_A:
             logger_test_json.log(f"INFO TEST_JSON SCENARIO A: {qid_for_scenario_a} summaries fit in one chunk. Direct LLM call.")
             scenario_a_reduce_output_raw = get_llm_json_themes_test(
                 summary_chunks_A[0], question_text_for_q_a, TEST_JSON_NUM_THEMES_TO_REQUEST,
                 llm_thematic_pipe_test, TEST_JSON_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,
                 f"{qid_for_scenario_a}_A_direct", "DIRECT_PROCESSING"
             )
             scenario_a_final_themes = parse_llm_json_themes_test(scenario_a_reduce_output_raw, TEST_JSON_NUM_THEMES_TO_REQUEST)
             scenario_a_map_outputs_raw.append(scenario_a_reduce_output_raw) # Store direct output
        else:
            logger_test_json.log(f"WARN TEST_JSON SCENARIO A: No summary chunks created for {qid_for_scenario_a}.")

    logger_test_json.log("\n--- SCENARIO A (Map-Reduce on SUMMARIES, JSON Output) - Q4 Final Themes ---")
    # ... (logging of Scenario A results) ...
    if scenario_a_final_themes:
        for i, theme in enumerate(scenario_a_final_themes): logger_test_json.log(f"  A Theme {i+1} for {qid_for_scenario_a}: {theme.get('theme_label')}\n     Explanation: {theme.get('explanation')}")
    else: logger_test_json.log(f"  No final themes parsed for Scenario A on {qid_for_scenario_a}.")
    logger_test_json.log(f"  Scenario A - Raw Map Outputs for {qid_for_scenario_a} (first one, max 500 chars):")
    logger_test_json.log(f"  {(scenario_a_map_outputs_raw[0][:500] + '...') if scenario_a_map_outputs_raw and scenario_a_map_outputs_raw[0] else 'N/A'}")
    logger_test_json.log(f"  Scenario A - Raw Reduce Output for {qid_for_scenario_a} (max 500 chars):")
    logger_test_json.log(f"  {(scenario_a_reduce_output_raw[:500] + '...') if scenario_a_reduce_output_raw else 'N/A'}")


    # ==============================================================================
    # --- Scenario B: Map-Reduce on Stage 4 PASSAGES for Q16 ---
    # ==============================================================================
    qid_for_scenario_b = "Q16"
    logger_test_json.log("\n" + "="*30 + f" SCENARIO B (JSON): Map-Reduce on PDF PASSAGES for {qid_for_scenario_b} " + "="*30)

    q_b_all_items = all_qids_collated_data.get(qid_for_scenario_b, [])
    q_b_passages = [item['text'] for item in q_b_all_items if item.get('type') == 'pdf_passages' and item.get('text')]
    question_text_for_q_b = qid_to_actual_question_text_map.get(qid_for_scenario_b, f"Question {qid_for_scenario_b} text not loaded")

    scenario_b_final_themes = []
    scenario_b_map_outputs_raw = []
    scenario_b_reduce_output_raw = "N/A"

    if not q_b_passages:
        logger_test_json.log(f"WARN TEST_JSON SCENARIO B: No 'pdf_passages' items found for QID {qid_for_scenario_b}.")
    else:
        logger_test_json.log(f"INFO TEST_JSON SCENARIO B: Found {len(q_b_passages)} PDF passage blocks for {qid_for_scenario_b}.")

        passage_batches_B = batch_text_items_json_test(
            q_b_passages, llm_thematic_pipe_test.tokenizer,
            TEST_JSON_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK
        )
        logger_test_json.log(f"INFO TEST_JSON SCENARIO B: Grouped {qid_for_scenario_b} passages into {len(passage_batches_B)} batches.")

        if len(passage_batches_B) > 0:
            intermediate_themes_B_list = []
            for i, batch_of_passage_strings in enumerate(passage_batches_B):
                current_batch_concatenated_text = "\n\n--- Next Excerpt ---\n".join(batch_of_passage_strings)
                logger_test_json.log(f"  SCENARIO B - MAP BATCH {i+1}/{len(passage_batches_B)} for {qid_for_scenario_b} (Passage items: {len(batch_of_passage_strings)}, Chars: {len(current_batch_concatenated_text)})")
                map_out = get_llm_json_themes_test(
                    current_batch_concatenated_text, question_text_for_q_b, 3,
                    llm_thematic_pipe_test, TEST_JSON_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,
                    f"{qid_for_scenario_b}_B_map_batch{i+1}", "SCENARIO_B_MAP"
                )
                scenario_b_map_outputs_raw.append(map_out)
                if not map_out.startswith("LLM_ERROR:"): intermediate_themes_B_list.append(map_out) # Store raw JSON
                time.sleep(0.2)

            if intermediate_themes_B_list:
                # Similar to Scenario A, parse intermediate themes before combining for reduce
                parsed_intermediate_themes_for_reduce_B = []
                for raw_json_map_out_B in intermediate_themes_B_list:
                    parsed_map_themes_B = parse_llm_json_themes_test(raw_json_map_out_B, 3)
                    for pt_B in parsed_map_themes_B:
                        if not pt_B.get("theme_label","").startswith("LLM Error") and not pt_B.get("theme_label","").startswith("Unparsed") and not pt_B.get("theme_label","").startswith("Valid JSON"):
                             parsed_intermediate_themes_for_reduce_B.append(f"Theme Label: {pt_B.get('theme_label')}\nExplanation: {pt_B.get('explanation')}")

                if parsed_intermediate_themes_for_reduce_B:
                    combined_intermediate_B = "\n\n---\n\n".join(parsed_intermediate_themes_for_reduce_B)
                    logger_test_json.log(f"  SCENARIO B - REDUCE Step for {qid_for_scenario_b} (Input Chars for reduce: {len(combined_intermediate_B)})")
                    scenario_b_reduce_output_raw = get_llm_json_themes_test(
                        combined_intermediate_B, question_text_for_q_b, TEST_JSON_NUM_THEMES_TO_REQUEST,
                        llm_thematic_pipe_test, TEST_JSON_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,
                        f"{qid_for_scenario_b}_B_reduce", "REDUCE_INTERMEDIATE_THEMES"
                    )
                    scenario_b_final_themes = parse_llm_json_themes_test(scenario_b_reduce_output_raw, TEST_JSON_NUM_THEMES_TO_REQUEST)
                else:
                    logger_test_json.log(f"ERROR TEST_JSON SCENARIO B: No successfully parsed intermediate themes from MAP step for {qid_for_scenario_b} (Passages).")
                    scenario_b_reduce_output_raw = f"ERROR: No successfully parsed intermediate themes from MAP step for {qid_for_scenario_b} (Passages)."
            else:
                logger_test_json.log(f"ERROR TEST_JSON SCENARIO B: No valid intermediate themes from MAP step for {qid_for_scenario_b} (Passages).")
                scenario_b_reduce_output_raw = f"ERROR: No valid intermediate themes from MAP step for {qid_for_scenario_b} (Passages)."
        else:
            logger_test_json.log(f"WARN TEST_JSON SCENARIO B: No passage batches created for {qid_for_scenario_b}.")

    logger_test_json.log("\n--- SCENARIO B (Map-Reduce on PDF PASSAGES, JSON Output) - Q16 Final Themes ---")
    # ... (logging of Scenario B results) ...
    if scenario_b_final_themes:
        for i, theme in enumerate(scenario_b_final_themes): logger_test_json.log(f"  B Theme {i+1} for {qid_for_scenario_b}: {theme.get('theme_label')}\n     Explanation: {theme.get('explanation')}")
    else: logger_test_json.log(f"  No final themes parsed for Scenario B on {qid_for_scenario_b}.")
    logger_test_json.log(f"  Scenario B - Raw Map Outputs for {qid_for_scenario_b} (first one, max 500 chars):")
    logger_test_json.log(f"  {(scenario_b_map_outputs_raw[0][:500] + '...') if scenario_b_map_outputs_raw and scenario_b_map_outputs_raw[0] else 'N/A'}")
    logger_test_json.log(f"  Scenario B - Raw Reduce Output for {qid_for_scenario_b} (max 500 chars):")
    logger_test_json.log(f"  {(scenario_b_reduce_output_raw[:500] + '...') if scenario_b_reduce_output_raw else 'N/A'}")


    logger_test_json.log("\nINFO: Test Cell (JSON) - All Thematic Analysis Scenarios Test Finished.")

    logger_test_json.log("INFO TEST JSON: Cleaning up test LLM resources...")
    # ... (cleanup code as before) ...
    if 'llm_thematic_pipe_test' in locals() and llm_thematic_pipe_test is not None: del llm_thematic_pipe_test
    if 'model_for_llm_test' in locals() and model_for_llm_test is not None: del model_for_llm_test
    if 'tokenizer_for_llm_test' in locals() and tokenizer_for_llm_test is not None: del tokenizer_for_llm_test
    gc.collect()
    if torch.cuda.is_available(): torch.cuda.empty_cache()
    logger_test_json.log("INFO TEST JSON: Test LLM cleanup complete.")

# --- Execute the Test ---
# print("REMINDER: Update TEST_JSON_... config variables, especially file paths and LLM ID.")
# print("REMINDER: Ensure FileLogger class is defined and prerequisite cells (0, 1) are run.")
# print("REMINDER: To run the test, uncomment the line below.")
run_thematic_analysis_tests_with_json()

# @title TEST CELL: Q4 (Scenario A - Summaries) & Q16 (Scenario B - Passages) Thematic Analysis

# --- Test Cell Imports (ensure these are run if not already globally available) ---
import json
import os
import sys
import traceback
import time
import gc
import re
from collections import defaultdict
from typing import Dict, Any, List

import torch
import numpy as np
# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline # In Cell 0
# try:
#     from bitsandbytes.cextension import ColabNotSupportedError # In Cell 0
# except ImportError:
#     ColabNotSupportedError = None

# --- Configuration for this Test Cell ---
TEST_BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # <<<--- ADJUST IF NEEDED
TEST_LOG_FILE = os.path.join(TEST_BASE_PROJECT_DIR, f'test_q4a_q16b_thematic_analysis_{time.strftime("%Y%m%d_%H%M%S")}.log')

# LLM Config
TEST_LLM_MODEL_ID = 'google/gemma-3-4b-it'  # <<<--- Using 2b-it. Adjust to your PHASE3_LLM_MODEL_ID if desired.
TEST_USE_QUANTIZATION = False             # <<<--- Set to your PHASE3_USE_QUANTIZATION.

# Thematic Analysis Parameters
TEST_NUM_THEMES_TO_REQUEST = 3 # Final number of themes for both scenarios
TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2800
TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 400
TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 600
TEST_MAPREDUCE_CHUNK_OVERLAP_WORDS = 30

# Input file from Stage 4a (Phase 2 output)
TEST_INPUT_COLLATED_PDF_TEXTS_JSON = os.path.join(TEST_BASE_PROJECT_DIR, 'phase2_stage4a_collated_pdf_texts_20250518_233834.json') # <<<--- UPDATE THIS FILENAME

# Source for actual question texts
TEST_QUESTION_TEXT_SOURCE_FILE = os.path.join(TEST_BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # <<<--- ENSURE THIS IS CORRECT

# --- FileLogger (Assuming defined in Cell 1 or globally) ---
try:
    logger_test_ab = FileLogger(TEST_LOG_FILE) # Changed logger name for this combined test
    logger_test_ab.log("INFO: Test Cell (Q4-A, Q16-B) - Logger Initialized.")
except NameError:
    print("CRITICAL: FileLogger class not defined. Please ensure Cell 1 has been executed.")
    class PrintLogger: # Fallback
        def log(self, msg): print(f"{time.strftime('%Y%m%d_%H%M%S')}: {msg}")
    logger_test_ab = PrintLogger()
    logger_test_ab.log("WARN: Using fallback PrintLogger.")

# --- Helper: Text Chunker (for Scenario A - chunks a single long string of concatenated summaries) ---
def create_text_chunks_for_scenario_a(full_text: str, tokenizer_for_chunking, max_tokens: int, overlap_words: int = 30):
    # ... (Same robust create_text_chunks_for_test / create_text_chunks_scenario_a from previous response)
    # ... (Ensure it uses logger_test_ab.log())
    words = full_text.split()
    if not words: return []
    chunks = []
    current_chunk_words = []
    logger_test_ab.log(f"DEBUG CHUNKER_A: Starting to chunk text of {len(words)} words into max {max_tokens} token chunks.")
    for word_idx, word in enumerate(words):
        potential_chunk_text = " ".join(current_chunk_words + [word])
        num_tokens = len(tokenizer_for_chunking.encode(potential_chunk_text, add_special_tokens=False))
        if num_tokens <= max_tokens:
            current_chunk_words.append(word)
        else:
            if current_chunk_words:
                chunks.append(" ".join(current_chunk_words))
            if chunks and overlap_words > 0:
                prev_chunk_words = chunks[-1].split()
                overlap_start_idx = max(0, len(prev_chunk_words) - overlap_words)
                current_chunk_words = prev_chunk_words[overlap_start_idx:] + [word]
            else:
                current_chunk_words = [word]
            new_chunk_text_check = " ".join(current_chunk_words)
            new_chunk_tokens_check = len(tokenizer_for_chunking.encode(new_chunk_text_check, add_special_tokens=False))
            if new_chunk_tokens_check > max_tokens:
                if len(current_chunk_words) > 1 and chunks: current_chunk_words = [word]
                elif len(current_chunk_words) == 1: logger_test_ab.log(f"    WARN CHUNKER_A: Single word '{word}' oversized ({new_chunk_tokens_check} tokens).")
    if current_chunk_words: chunks.append(" ".join(current_chunk_words))
    logger_test_ab.log(f"INFO CHUNKER_A: Split text into {len(chunks)} chunks.")
    return chunks

# --- Helper: Batch Individual Items (for Scenario B map step - batches a list of text items) ---
def batch_text_items_for_scenario_b(text_items_list: List[str], tokenizer_for_batching, max_tokens_per_batch: int):
    # ... (Same robust batch_text_items_for_test from previous response)
    # ... (Ensure it uses logger_test_ab.log())
    batches = []
    current_batch_item_texts = []
    current_batch_concatenated_text = ""
    logger_test_ab.log(f"DEBUG BATCHER_B: Starting to batch {len(text_items_list)} items into max {max_tokens_per_batch} token batches.")
    for item_idx, item_text_original in enumerate(text_items_list):
        if not item_text_original or not item_text_original.strip(): continue
        text_to_try_adding = item_text_original
        if current_batch_item_texts: text_to_try_adding = "\n\n--- Next Excerpt ---\n" + item_text_original
        potential_full_batch_text = current_batch_concatenated_text + text_to_try_adding
        num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))
        if num_tokens <= max_tokens_per_batch:
            current_batch_item_texts.append(item_text_original)
            current_batch_concatenated_text = potential_full_batch_text
        else:
            if current_batch_item_texts: batches.append(current_batch_item_texts)
            single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))
            if single_item_tokens > max_tokens_per_batch:
                logger_test_ab.log(f"  WARN BATCHER_B: Item {item_idx} ({single_item_tokens} tokens) oversized. Own batch.")
                batches.append([item_text_original])
                current_batch_item_texts = []
                current_batch_concatenated_text = ""
            else:
                current_batch_item_texts = [item_text_original]
                current_batch_concatenated_text = item_text_original
    if current_batch_item_texts: batches.append(current_batch_item_texts)
    logger_test_ab.log(f"INFO BATCHER_B: Created {len(batches)} batches of items.")
    return batches

# --- Helper: LLM Call for Thematic Points ---
def get_llm_thematic_points_ab_test(text_input_for_llm: str, question_text_for_prompt: str,
                                 num_themes_requested: int, llm_pipe: pipeline, max_new_tokens: int,
                                 log_prefix_qid_scenario: str, map_reduce_step_type: str):
    # ... (Same get_llm_thematic_points_test function from previous test cell, including robust context window check)
    # ... (Ensure it uses logger_test_ab.log())
    prompt_instruction = "" # Create this based on map_reduce_step_type like before
    if map_reduce_step_type == "SCENARIO_A_MAP": # Chunk of concatenated summaries
        prompt_instruction = (
            f"You are an expert policy analyst. The following text is a CHUNK of concatenated PREVIOUSLY-GENERATED SUMMARIES from multiple PDF submissions responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on this provided CHUNK OF SUMMARIES, identify up to 3 most prominent and distinct points, arguments, or themes they collectively raise.\n"
            f"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 points are evident, list only those that are clear and distinct.\n\n"
            f"Chunk of Summaries:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Prominent Points from this Chunk of Summaries:"
        )
    elif map_reduce_step_type == "SCENARIO_B_MAP": # Batch of original passages
        prompt_instruction = (
            f"You are an expert policy analyst. Carefully review the following BATCH of original excerpts/passages from PDF submissions, all responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on this provided BATCH of original excerpts/passages, identify up to 3 prominent and distinct points, arguments, or themes they collectively raise.\n"
            f"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 points are evident, list only those that are clear and distinct.\n\n"
            f"Batch of Excerpts/Passages:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Prominent Points from this Batch:"
        )
    elif map_reduce_step_type == "REDUCE_INTERMEDIATE_THEMES":
        prompt_instruction = (
             f"You are an expert policy analyst. The following are sets of preliminary themes/points that were individually extracted in previous steps from responses to the question: '{question_text_for_prompt}'.\n"
            f"Your task is to synthesize these preliminary themes into the overall top {num_themes_requested} overarching and distinct common points or themes.\n"
            f"For each final synthesized point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). Consolidate similar preliminary themes. Ensure the final themes are distinct and well-supported by the range of preliminary themes provided.\n\n"
            f"Preliminary Themes/Points Provided:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Top {num_themes_requested} Synthesized Common Points:"
        )
    else: # DIRECT_PROCESSING
        prompt_instruction = (
            f"You are an expert policy analyst. Carefully review all the following text (which consists of combined PDF summaries OR a single batch of excerpts/passages) responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on the provided text, identify the top {num_themes_requested} most common and distinct points, arguments, or themes raised by the respondents.\n"
            f"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes_requested} points are evident, list only those that are clear.\n\n"
            f"Provided Text:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Top {num_themes_requested} Common Points:"
        )
    messages = [{"role": "user", "content": prompt_instruction}]
    try:
        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        generation_args = {"max_new_tokens": max_new_tokens, "do_sample": False,
                           "pad_token_id": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id}
        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True)
        input_text_tokens = llm_pipe.tokenizer.encode(text_input_for_llm, add_special_tokens=False)
        model_max_len = 8192
        if hasattr(llm_pipe.model.config, 'max_position_embeddings'): model_max_len = llm_pipe.model.config.max_position_embeddings
        elif hasattr(llm_pipe.model.config, 'max_sequence_length'): model_max_len = llm_pipe.model.config.max_sequence_length
        elif hasattr(llm_pipe.tokenizer, 'model_max_length'): model_max_len = llm_pipe.tokenizer.model_max_length
        logger_test_ab.log(f"DEBUG LLM_CALL: {map_reduce_step_type} for {log_prefix_qid_scenario}. Prompt toks: {len(prompt_tokens)}. Content toks: {len(input_text_tokens)}. Max new: {max_new_tokens}. Model max_len: {model_max_len}.")
        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95:
             logger_test_ab.log(f"WARN LLM_CALL: {log_prefix_qid_scenario} - Step {map_reduce_step_type} - Prompt+Output tokens might exceed context ({model_max_len}).")
        outputs = llm_pipe(prompt_formatted, **generation_args)
        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):
            logger_test_ab.log(f"ERROR LLM_CALL: {map_reduce_step_type} LLM unexpected output for {log_prefix_qid_scenario}.")
            return "LLM_ERROR: No output generated."
        generated_text_full = outputs[0]['generated_text']
        llm_response_text = generated_text_full[len(prompt_formatted):].strip() if generated_text_full.startswith(prompt_formatted) else generated_text_full.split("<start_of_turn>model\n", 1)[-1].strip() if "<start_of_turn>model\n" in generated_text_full else generated_text_full
        return llm_response_text
    except Exception as e:
        logger_test_ab.log(f"ERROR LLM_CALL: {map_reduce_step_type} LLM exception for {log_prefix_qid_scenario}: {e}")
        logger_test_ab.log(traceback.format_exc())
        return f"LLM_ERROR: Exception - {str(e)}"

# --- Helper: Parse LLM Thematic Output ---
def parse_llm_themes_ab_test(llm_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:
    # ... (Same robust parse_llm_themes_test function from previous test cell, ensure it uses logger_test_ab) ...
    parsed_themes = []
    if not llm_output_text or llm_output_text.startswith("LLM_ERROR:"):
        return [{"theme_label": "LLM Error or Parsing Error", "explanation": llm_output_text}]
    theme_pattern = re.compile(r"Theme Label \d+[:\-\s]*(.*?)\s*Explanation \d+[:\-\s]*(.*?)(?=(Theme Label \d+|$|\Z))", re.DOTALL | re.IGNORECASE) # Added \Z for end of string
    matches = theme_pattern.findall(llm_output_text)
    for match_idx, match in enumerate(matches):
        if len(parsed_themes) < num_expected_themes:
            parsed_themes.append({"theme_label": match[0].strip(), "explanation": match[1].strip()})
        else: break
    if not parsed_themes and llm_output_text.strip() and not llm_output_text.startswith("LLM_ERROR:"):
        potential_blocks = re.split(r'\n\s*(?=\d+[\.\)\-])', "\n" + llm_output_text.strip())
        for block in potential_blocks:
            block = block.strip()
            if not block: continue
            if len(parsed_themes) >= num_expected_themes: break
            label_part = block.split('\n')[0]
            explanation_part = block
            parsed_themes.append({"theme_label": re.sub(r"^\d+[\.\)\-]\s*", "", label_part).strip()[:150], "explanation": explanation_part})
    if not parsed_themes and llm_output_text.strip() and not llm_output_text.startswith("LLM_ERROR:"):
        parsed_themes.append({"theme_label": "Unparsed LLM Output Block", "explanation": llm_output_text.strip()})
    return parsed_themes[:num_expected_themes]


# --- Main Test Execution Function ---
def run_thematic_analysis_tests():
    logger_test_ab.log("INFO: Test Cell - Starting Thematic Analysis Scenarios Test (Q4-A, Q16-B).")

    # --- 0. Load LLM (once for all tests in this cell) ---
    llm_thematic_pipe_test = None
    tokenizer_for_llm_test = None # For chunkers/batchers
    try:
        logger_test_ab.log(f"INFO TEST: Loading LLM: {TEST_LLM_MODEL_ID} (Quant: {TEST_USE_QUANTIZATION})")
        tokenizer_for_llm_test = AutoTokenizer.from_pretrained(TEST_LLM_MODEL_ID)
        if getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is not None:
            tokenizer_for_llm_test.pad_token = tokenizer_for_llm_test.eos_token
        elif getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is None:
             logger_test_ab.log(f"CRITICAL TEST: Tokenizer for {TEST_LLM_MODEL_ID} has neither pad_token nor eos_token.")

        bnb_config_test = None
        effective_quant_test = TEST_USE_QUANTIZATION
        if TEST_USE_QUANTIZATION and torch.cuda.is_available():
            try:
                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
                bnb_config_test = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=compute_dtype)
                logger_test_ab.log(f"INFO TEST: BNB config with compute_dtype: {compute_dtype}")
            except ImportError: # Specific for bitsandbytes
                 logger_test_ab.log(f"WARN TEST: bitsandbytes not imported. Disabling quantization.")
                 effective_quant_test = False
            except ColabNotSupportedError: # Specific error from bnb
                 logger_test_ab.log(f"WARN TEST: Bitsandbytes ColabNotSupportedError. Disabling quantization.")
                 effective_quant_test = False
            except Exception as e_bnb:
                logger_test_ab.log(f"WARN TEST: BNB config failed: {e_bnb}. Disabling quantization.")
                effective_quant_test = False
        elif TEST_USE_QUANTIZATION and not torch.cuda.is_available():
            logger_test_ab.log("WARN TEST: Quantization requested, No CUDA. Disabling quantization.")
            effective_quant_test = False

        model_for_llm_test = AutoModelForCausalLM.from_pretrained(
            TEST_LLM_MODEL_ID,
            quantization_config=bnb_config_test if effective_quant_test else None,
            torch_dtype=torch.bfloat16 if not effective_quant_test and torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (torch.float32 if not effective_quant_test else None),
            device_map="auto", trust_remote_code=True
        )
        llm_thematic_pipe_test = pipeline("text-generation", model=model_for_llm_test, tokenizer=tokenizer_for_llm_test)
        logger_test_ab.log("INFO TEST: LLM Pipeline Loaded.")
    except Exception as e:
        logger_test_ab.log(f"CRITICAL TEST: Failed to load LLM: {e}")
        logger_test_ab.log(traceback.format_exc())
        return # Cannot proceed without LLM

    # --- 1. Load Full Stage 4a Data (once) ---
    all_qids_collated_data = {}
    try:
        with open(TEST_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:
            stage4a_data = json.load(f)
        all_qids_collated_data = stage4a_data.get("aggregated_pdf_content_by_qid", {})
        if not all_qids_collated_data:
            logger_test_ab.log("ERROR TEST: No 'aggregated_pdf_content_by_qid' found in Stage 4a output. Aborting.")
            return
        logger_test_ab.log(f"INFO TEST: Loaded collated data for {len(all_qids_collated_data)} QIDs from {TEST_INPUT_COLLATED_PDF_TEXTS_JSON}.")
    except Exception as e:
        logger_test_ab.log(f"CRITICAL TEST: Failed to load Stage 4a data from {TEST_INPUT_COLLATED_PDF_TEXTS_JSON}: {e}")
        logger_test_ab.log(traceback.format_exc())
        return

    # --- 2. Load All Question Texts (once) ---
    qid_to_actual_question_text_map = {}
    try:
        logger_test_ab.log(f"INFO TEST: Attempting to load question texts from: {TEST_QUESTION_TEXT_SOURCE_FILE}")
        if not os.path.exists(TEST_QUESTION_TEXT_SOURCE_FILE):
             raise FileNotFoundError(f"Question text source file not found: {TEST_QUESTION_TEXT_SOURCE_FILE}")
        with open(TEST_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:
            qtext_source_data = json.load(f_qtext)
        if "question_data" in qtext_source_data:
            for qid_key, data_val in qtext_source_data.get("question_data", {}).items():
                qid_to_actual_question_text_map[qid_key] = data_val.get("question_text", f"Text for {qid_key} not found")
            logger_test_ab.log(f"INFO TEST: Loaded {len(qid_to_actual_question_text_map)} question texts (from 'question_data' structure).")
        elif "analysis_by_question" in qtext_source_data: # Fallback for CAR_35 like structure
             for qid_key, data_val in qtext_source_data.get("analysis_by_question", {}).items():
                qid_to_actual_question_text_map[qid_key] = data_val.get("question_text", f"Text for {qid_key} not found")
             logger_test_ab.log(f"INFO TEST: Loaded {len(qid_to_actual_question_text_map)} question texts (from 'analysis_by_question' structure).")
        else:
            logger_test_ab.log(f"WARN TEST: Neither 'question_data' nor 'analysis_by_question' key found in {TEST_QUESTION_TEXT_SOURCE_FILE}. Question texts may be missing.")
    except Exception as e:
        logger_test_ab.log(f"ERROR TEST: Failed to load question texts: {e}")
        logger_test_ab.log(traceback.format_exc())


    # ==============================================================================
    # --- Scenario A: Map-Reduce on Stage 4 SUMMARIES for Q4 ---
    # ==============================================================================
    qid_for_scenario_a = "Q4"
    logger_test_ab.log("\n" + "="*30 + f" SCENARIO A: Map-Reduce on PDF SUMMARIES for {qid_for_scenario_a} " + "="*30)

    q_a_all_items = all_qids_collated_data.get(qid_for_scenario_a, [])
    q_a_summaries = [item['text'] for item in q_a_all_items if item.get('type') == 'pdf_summary' and item.get('text')]
    question_text_for_q_a = qid_to_actual_question_text_map.get(qid_for_scenario_a, f"Question {qid_for_scenario_a} text not loaded")

    scenario_a_final_themes = []
    scenario_a_map_outputs_raw = []
    scenario_a_reduce_output_raw = "N/A"

    if not q_a_summaries:
        logger_test_ab.log(f"WARN TEST SCENARIO A: No 'pdf_summary' items found for QID {qid_for_scenario_a}. Scenario A cannot proceed for this QID.")
    else:
        logger_test_ab.log(f"INFO TEST SCENARIO A: Found {len(q_a_summaries)} PDF summaries for {qid_for_scenario_a}.")
        concatenated_q_a_summaries = "\n\n--- Next Summary Entry ---\n".join(q_a_summaries)
        logger_test_ab.log(f"INFO TEST SCENARIO A: Concatenated summaries char length for {qid_for_scenario_a}: {len(concatenated_q_a_summaries)}")

        summary_chunks_A = create_text_chunks_for_scenario_a(
            concatenated_q_a_summaries,
            llm_thematic_pipe_test.tokenizer,
            TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK,
            TEST_MAPREDUCE_CHUNK_OVERLAP_WORDS
        )

        if len(summary_chunks_A) > 1:
            logger_test_ab.log(f"INFO TEST SCENARIO A: {qid_for_scenario_a} summaries split into {len(summary_chunks_A)} chunks for Map step.")
            intermediate_themes_A_list = []
            for i, chunk in enumerate(summary_chunks_A):
                logger_test_ab.log(f"  SCENARIO A - MAP CHUNK {i+1}/{len(summary_chunks_A)} for {qid_for_scenario_a} (Chars: {len(chunk)})")
                map_out = get_llm_thematic_points_ab_test(
                    chunk, question_text_for_q_a, 3,
                    llm_thematic_pipe_test, TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,
                    f"{qid_for_scenario_a}_A_map_chunk{i+1}", "SCENARIO_A_MAP"
                )
                scenario_a_map_outputs_raw.append(map_out)
                if not map_out.startswith("LLM_ERROR:"): intermediate_themes_A_list.append(f"Themes from Chunk {i+1}:\n{map_out}")
                time.sleep(0.2)

            if intermediate_themes_A_list:
                combined_intermediate_A = "\n\n".join(intermediate_themes_A_list)
                logger_test_ab.log(f"  SCENARIO A - REDUCE Step for {qid_for_scenario_a} (Input Chars for reduce: {len(combined_intermediate_A)})")
                scenario_a_reduce_output_raw = get_llm_thematic_points_ab_test(
                    combined_intermediate_A, question_text_for_q_a, TEST_NUM_THEMES_TO_REQUEST,
                    llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,
                    f"{qid_for_scenario_a}_A_reduce", "REDUCE_INTERMEDIATE_THEMES"
                )
                scenario_a_final_themes = parse_llm_themes_ab_test(scenario_a_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)
            else:
                logger_test_ab.log(f"ERROR TEST SCENARIO A: No intermediate themes from MAP step for {qid_for_scenario_a}.")
                scenario_a_reduce_output_raw = f"ERROR: No intermediate themes from MAP step for {qid_for_scenario_a} (Summaries)."
        elif summary_chunks_A:
             logger_test_ab.log(f"INFO TEST SCENARIO A: {qid_for_scenario_a} summaries fit in one chunk. Direct LLM call.")
             scenario_a_reduce_output_raw = get_llm_thematic_points_ab_test(
                 summary_chunks_A[0], question_text_for_q_a, TEST_NUM_THEMES_TO_REQUEST,
                 llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,
                 f"{qid_for_scenario_a}_A_direct", "DIRECT_PROCESSING"
             )
             scenario_a_final_themes = parse_llm_themes_ab_test(scenario_a_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)
             scenario_a_map_outputs_raw.append(scenario_a_reduce_output_raw)
        else:
            logger_test_ab.log(f"WARN TEST SCENARIO A: No summary chunks created for {qid_for_scenario_a}.")

    logger_test_ab.log("\n--- SCENARIO A (Map-Reduce on SUMMARIES) - Q4 Final Themes ---")
    if scenario_a_final_themes:
        for i, theme in enumerate(scenario_a_final_themes): logger_test_ab.log(f"  A Theme {i+1} for {qid_for_scenario_a}: {theme.get('theme_label')}\n     Explanation: {theme.get('explanation')}")
    else: logger_test_ab.log(f"  No final themes parsed for Scenario A on {qid_for_scenario_a}.")
    logger_test_ab.log(f"  Scenario A - Raw Map Outputs for {qid_for_scenario_a} (first one, max 500 chars):")
    logger_test_ab.log(f"  {(scenario_a_map_outputs_raw[0][:500] + '...') if scenario_a_map_outputs_raw and scenario_a_map_outputs_raw[0] else 'N/A'}")
    logger_test_ab.log(f"  Scenario A - Raw Reduce Output for {qid_for_scenario_a} (max 500 chars):")
    logger_test_ab.log(f"  {(scenario_a_reduce_output_raw[:500] + '...') if scenario_a_reduce_output_raw else 'N/A'}")


    # ==============================================================================
    # --- Scenario B: Map-Reduce on Stage 4 PASSAGES for Q16 ---
    # ==============================================================================
    qid_for_scenario_b = "Q16"
    logger_test_ab.log("\n" + "="*30 + f" SCENARIO B: Map-Reduce on PDF PASSAGES for {qid_for_scenario_b} " + "="*30)

    q_b_all_items = all_qids_collated_data.get(qid_for_scenario_b, [])
    q_b_passages = [item['text'] for item in q_b_all_items if item.get('type') == 'pdf_passages' and item.get('text')]
    question_text_for_q_b = qid_to_actual_question_text_map.get(qid_for_scenario_b, f"Question {qid_for_scenario_b} text not loaded")

    scenario_b_final_themes = []
    scenario_b_map_outputs_raw = []
    scenario_b_reduce_output_raw = "N/A"

    if not q_b_passages:
        logger_test_ab.log(f"WARN TEST SCENARIO B: No 'pdf_passages' items found for QID {qid_for_scenario_b}. Scenario B cannot proceed for this QID.")
    else:
        logger_test_ab.log(f"INFO TEST SCENARIO B: Found {len(q_b_passages)} PDF passage blocks for {qid_for_scenario_b}.")

        passage_batches_B = batch_text_items_for_scenario_b( # Renamed to avoid conflict
            q_b_passages,
            llm_thematic_pipe_test.tokenizer,
            TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK
        )
        logger_test_ab.log(f"INFO TEST SCENARIO B: Grouped {qid_for_scenario_b} passages into {len(passage_batches_B)} batches for Map step.")

        if len(passage_batches_B) > 0:
            intermediate_themes_B_list = []
            for i, batch_of_passage_strings in enumerate(passage_batches_B): # Each item is a list of passage strings
                current_batch_concatenated_text = "\n\n--- Next Excerpt ---\n".join(batch_of_passage_strings)
                logger_test_ab.log(f"  SCENARIO B - MAP BATCH {i+1}/{len(passage_batches_B)} for {qid_for_scenario_b} (Passage items: {len(batch_of_passage_strings)}, Chars: {len(current_batch_concatenated_text)})")
                map_out = get_llm_thematic_points_ab_test(
                    current_batch_concatenated_text, question_text_for_q_b, 3,
                    llm_thematic_pipe_test, TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,
                    f"{qid_for_scenario_b}_B_map_batch{i+1}", "SCENARIO_B_MAP"
                )
                scenario_b_map_outputs_raw.append(map_out)
                if not map_out.startswith("LLM_ERROR:"): intermediate_themes_B_list.append(f"Themes from Batch {i+1}:\n{map_out}")
                time.sleep(0.2)

            if intermediate_themes_B_list:
                combined_intermediate_B = "\n\n".join(intermediate_themes_B_list)
                logger_test_ab.log(f"  SCENARIO B - REDUCE Step for {qid_for_scenario_b} (Input Chars: {len(combined_intermediate_B)})")
                scenario_b_reduce_output_raw = get_llm_thematic_points_ab_test(
                    combined_intermediate_B, question_text_for_q_b, TEST_NUM_THEMES_TO_REQUEST,
                    llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,
                    f"{qid_for_scenario_b}_B_reduce", "REDUCE_INTERMEDIATE_THEMES"
                )
                scenario_b_final_themes = parse_llm_themes_ab_test(scenario_b_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)
            else:
                logger_test_ab.log(f"ERROR TEST SCENARIO B: No intermediate themes from MAP step for {qid_for_scenario_b} (Passages).")
                scenario_b_reduce_output_raw = f"ERROR: No intermediate themes from MAP step for {qid_for_scenario_b} (Passages)."
        else:
            logger_test_ab.log(f"WARN TEST SCENARIO B: No passage batches created for {qid_for_scenario_b}.")

    logger_test_ab.log("\n--- SCENARIO B (Map-Reduce on PDF PASSAGES) - Q16 Final Themes ---")
    if scenario_b_final_themes:
        for i, theme in enumerate(scenario_b_final_themes): logger_test_ab.log(f"  B Theme {i+1} for {qid_for_scenario_b}: {theme.get('theme_label')}\n     Explanation: {theme.get('explanation')}")
    else: logger_test_ab.log(f"  No final themes parsed for Scenario B on {qid_for_scenario_b}.")
    logger_test_ab.log(f"  Scenario B - Raw Map Outputs for {qid_for_scenario_b} (first one, max 500 chars):")
    logger_test_ab.log(f"  {(scenario_b_map_outputs_raw[0][:500] + '...') if scenario_b_map_outputs_raw and scenario_b_map_outputs_raw[0] else 'N/A'}")
    logger_test_ab.log(f"  Scenario B - Raw Reduce Output for {qid_for_scenario_b} (max 500 chars):")
    logger_test_ab.log(f"  {(scenario_b_reduce_output_raw[:500] + '...') if scenario_b_reduce_output_raw else 'N/A'}")


    logger_test_ab.log("\nINFO: Test Cell - All Thematic Analysis Scenarios Test Finished.")

    logger_test_ab.log("INFO TEST: Cleaning up test LLM resources...")
    if 'llm_thematic_pipe_test' in locals() and llm_thematic_pipe_test is not None: del llm_thematic_pipe_test
    if 'model_for_llm_test' in locals() and model_for_llm_test is not None: del model_for_llm_test
    if 'tokenizer_for_llm_test' in locals() and tokenizer_for_llm_test is not None: del tokenizer_for_llm_test
    gc.collect()
    if torch.cuda.is_available(): torch.cuda.empty_cache()
    logger_test_ab.log("INFO TEST: Test LLM cleanup complete.")

# --- Execute the Test ---
# print("REMINDER: Update TEST_... config variables, especially file paths and LLM ID.")
# print("REMINDER: Ensure FileLogger class is defined and prerequisite cells (0, 1) are run.")
# print("REMINDER: To run the test, uncomment the line below.")
# run_thematic_analysis_tests()

# @title Quick script to find QIDs with passage counts
import json
stage4a_file = '/content/drive/MyDrive/Colab Notebooks/Legal/phase2_stage4a_collated_pdf_texts_20250518_233834.json' # Your actual file
passage_counts_per_qid = {}
with open(stage4a_file, 'r') as f:
    data = json.load(f)
for qid, items in data.get("aggregated_pdf_content_by_qid", {}).items():
    passage_counts_per_qid[qid] = sum(1 for item in items if item.get('type') == 'pdf_passages')

sorted_passage_counts = sorted(passage_counts_per_qid.items(), key=lambda x: x[1], reverse=True)
print("QIDs with most 'pdf_passages' items:")
for qid, count in sorted_passage_counts[:10]:
    print(f"{qid}: {count} passages")

# @title TEST CELL: Q4 Thematic Analysis - SCENARIO A (Map-Reduce on PDF Summaries) ONLY

# --- Test Cell Imports (ensure these are run if not already globally available) ---
import json
import os
import sys
import traceback
import time
import gc
import re
from collections import defaultdict
from typing import Dict, Any, List

import torch
import numpy as np
# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline # Already in Cell 0
# try:
#     from bitsandbytes.cextension import ColabNotSupportedError # Already in Cell 0
# except ImportError:
#     ColabNotSupportedError = None

# --- Configuration for this Test Cell ---
TEST_A_BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # <<<--- ADJUST IF NEEDED
TEST_A_LOG_FILE = os.path.join(TEST_A_BASE_PROJECT_DIR, f'test_q4_scenario_A_{time.strftime("%Y%m%d_%H%M%S")}.log')

# LLM Config
TEST_A_LLM_MODEL_ID = 'google/gemma-3-4b-it'  # <<<--- Using 2b-it for test. Adjust to your PHASE3_LLM_MODEL_ID (e.g., 'google/gemma-7b-it') if desired.
TEST_A_USE_QUANTIZATION = False             # <<<--- Set to your PHASE3_USE_QUANTIZATION

# Thematic Analysis Parameters
TEST_A_NUM_THEMES_TO_REQUEST = 3
TEST_A_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2800 # Max *content* tokens for map step input (for chunks of concatenated summaries)
TEST_A_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 400
TEST_A_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 600
TEST_A_MAPREDUCE_CHUNK_OVERLAP_WORDS = 30

# Input file from Stage 4a (Phase 2 output)
TEST_A_INPUT_COLLATED_PDF_TEXTS_JSON = os.path.join(TEST_A_BASE_PROJECT_DIR, 'phase2_stage4a_collated_pdf_texts_20250518_233834.json') # <<<--- UPDATE THIS FILENAME

# Source for actual question texts
TEST_A_QUESTION_TEXT_SOURCE_FILE = os.path.join(TEST_A_BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # <<<--- ENSURE THIS IS CORRECT

# Specific QID for this test
TEST_A_QID_TO_ANALYZE = "Q4"

# --- FileLogger (Assuming defined in Cell 1 or globally) ---
try:
    logger_test_A = FileLogger(TEST_A_LOG_FILE)
    logger_test_A.log(f"INFO: Test Scenario A - Logger Initialized for QID: {TEST_A_QID_TO_ANALYZE}.")
except NameError:
    print("CRITICAL: FileLogger class not defined. Please ensure Cell 1 has been executed.")
    class PrintLogger: # Fallback
        def log(self, msg): print(f"{time.strftime('%Y%m%d_%H%M%S')}: {msg}")
    logger_test_A = PrintLogger()
    logger_test_A.log("WARN: Using fallback PrintLogger.")

# --- Helper: Text Chunker (for Scenario A - chunks a single long string) ---
def create_text_chunks_scenario_a(full_text: str, tokenizer_for_chunking, max_tokens: int, overlap_words: int = 30):
    # ... (Using the create_text_chunks_for_test from previous response, ensure it uses logger_test_A.log())
    words = full_text.split()
    if not words: return []
    chunks = []
    current_chunk_words = []
    logger_test_A.log(f"DEBUG CHUNKER_A: Starting to chunk text of {len(words)} words into max {max_tokens} token chunks.")
    for word_idx, word in enumerate(words):
        potential_chunk_text = " ".join(current_chunk_words + [word])
        num_tokens = len(tokenizer_for_chunking.encode(potential_chunk_text, add_special_tokens=False))
        if num_tokens <= max_tokens:
            current_chunk_words.append(word)
        else:
            if current_chunk_words:
                chunks.append(" ".join(current_chunk_words))
                # logger_test_A.log(f"  DEBUG CHUNKER_A: Added chunk {len(chunks)} with {len(current_chunk_words)} words, {len(tokenizer_for_chunking.encode(chunks[-1], add_special_tokens=False))} tokens.")
            if chunks and overlap_words > 0:
                prev_chunk_words = chunks[-1].split()
                overlap_start_idx = max(0, len(prev_chunk_words) - overlap_words)
                current_chunk_words = prev_chunk_words[overlap_start_idx:] + [word]
            else:
                current_chunk_words = [word]
            new_chunk_text_check = " ".join(current_chunk_words)
            new_chunk_tokens_check = len(tokenizer_for_chunking.encode(new_chunk_text_check, add_special_tokens=False))
            if new_chunk_tokens_check > max_tokens:
                if len(current_chunk_words) > 1 and chunks:
                    current_chunk_words = [word]
                elif len(current_chunk_words) == 1:
                     logger_test_A.log(f"    WARN CHUNKER_A: Single word '{word}' oversized ({new_chunk_tokens_check} tokens).")
    if current_chunk_words:
        chunks.append(" ".join(current_chunk_words))
        # logger_test_A.log(f"  DEBUG CHUNKER_A: Added final chunk {len(chunks)} with {len(current_chunk_words)} words, {len(tokenizer_for_chunking.encode(chunks[-1], add_special_tokens=False))} tokens.")
    logger_test_A.log(f"INFO CHUNKER_A: Split text into {len(chunks)} chunks.")
    return chunks

# --- Helper: LLM Call for Thematic Points ---
def get_llm_thematic_points_scenario_a(text_input_for_llm: str, question_text_for_prompt: str,
                                       num_themes_requested: int, llm_pipe: pipeline, max_new_tokens: int,
                                       log_prefix_qid: str, map_reduce_step_type: str): # "SCENARIO_A_MAP", "REDUCE_INTERMEDIATE_THEMES", "DIRECT_PROCESSING"
    prompt_instruction = ""
    if map_reduce_step_type == "SCENARIO_A_MAP":
        prompt_instruction = (
            f"You are an expert policy analyst. The following text is a CHUNK of concatenated PREVIOUSLY-GENERATED SUMMARIES from multiple PDF submissions responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on this provided CHUNK OF SUMMARIES, identify up to 3 most prominent and distinct points, arguments, or themes they collectively raise.\n"
            f"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 points are evident, list only those that are clear and distinct.\n\n"
            f"Chunk of Summaries:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Prominent Points from this Chunk of Summaries:"
        )
    elif map_reduce_step_type == "REDUCE_INTERMEDIATE_THEMES":
        prompt_instruction = ( # Same reduce prompt as before
            f"You are an expert policy analyst. The following are sets of preliminary themes/points that were individually extracted in previous steps from responses to the question: '{question_text_for_prompt}'.\n"
            f"Your task is to synthesize these preliminary themes into the overall top {num_themes_requested} overarching and distinct common points or themes.\n"
            # ... (rest of reduce prompt as in your full Cell 8)
            f"For each final synthesized point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). Consolidate similar preliminary themes. Ensure the final themes are distinct and well-supported by the range of preliminary themes provided.\n\n"
            f"Preliminary Themes/Points Provided:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Top {num_themes_requested} Synthesized Common Points:"
        )
    else: # DIRECT_PROCESSING (if concatenated summaries fit in one go)
        prompt_instruction = (
            f"You are an expert policy analyst. Carefully review all the following text (which consists of combined PDF summaries) responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on the provided text, identify the top {num_themes_requested} most common and distinct points, arguments, or themes raised by the respondents.\n"
            # ... (rest of direct prompt as in your full Cell 8)
            f"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes_requested} distinct common points are evident, list only those that are clear.\n\n"
            f"Provided Text (Combined PDF Summaries):\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Top {num_themes_requested} Common Points:"
        )
    messages = [{"role": "user", "content": prompt_instruction}]
    try:
        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        generation_args = {"max_new_tokens": max_new_tokens, "do_sample": False,
                           "pad_token_id": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id}

        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True)
        input_text_tokens = llm_pipe.tokenizer.encode(text_input_for_llm, add_special_tokens=False)

        model_max_len = 8192 # Default
        if hasattr(llm_pipe.model.config, 'max_position_embeddings'): model_max_len = llm_pipe.model.config.max_position_embeddings
        elif hasattr(llm_pipe.model.config, 'max_sequence_length'): model_max_len = llm_pipe.model.config.max_sequence_length
        elif hasattr(llm_pipe.tokenizer, 'model_max_length'): model_max_len = llm_pipe.tokenizer.model_max_length

        logger_test_A.log(f"DEBUG SCENARIO_A: {map_reduce_step_type} LLM call for {log_prefix_qid}. Prompt toks: {len(prompt_tokens)}. Content toks: {len(input_text_tokens)}. Max new: {max_new_tokens}. Model max_len: {model_max_len}.")

        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95:
             logger_test_A.log(f"WARN SCENARIO_A: {log_prefix_qid} - Step {map_reduce_step_type} - Prompt+Output tokens might exceed model context ({model_max_len}).")

        outputs = llm_pipe(prompt_formatted, **generation_args)

        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):
            logger_test_A.log(f"ERROR SCENARIO_A: {map_reduce_step_type} LLM returned unexpected for {log_prefix_qid}.")
            return "LLM_ERROR: No output generated."
        generated_text_full = outputs[0]['generated_text']
        llm_response_text = generated_text_full[len(prompt_formatted):].strip() if generated_text_full.startswith(prompt_formatted) else generated_text_full.split("<start_of_turn>model\n", 1)[-1].strip() if "<start_of_turn>model\n" in generated_text_full else generated_text_full
        return llm_response_text
    except Exception as e:
        logger_test_A.log(f"ERROR SCENARIO_A: {map_reduce_step_type} LLM exception for {log_prefix_qid}: {e}")
        logger_test_A.log(traceback.format_exc())
        return f"LLM_ERROR: Exception - {str(e)}"

# --- Helper: Parse LLM Thematic Output ---
def parse_llm_themes_scenario_a(llm_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:
    # ... (Use the same parse_llm_themes_test function from the previous test cell, ensure it uses logger_test_A) ...
    parsed_themes = []
    if not llm_output_text or llm_output_text.startswith("LLM_ERROR:"):
        return [{"theme_label": "LLM Error or Parsing Error", "explanation": llm_output_text}]
    theme_pattern = re.compile(r"Theme Label \d+[:\-\s]*(.*?)\s*Explanation \d+[:\-\s]*(.*?)(?=(Theme Label \d+|$))", re.DOTALL | re.IGNORECASE)
    matches = theme_pattern.findall(llm_output_text)
    for match_idx, match in enumerate(matches):
        if len(parsed_themes) < num_expected_themes:
            parsed_themes.append({"theme_label": match[0].strip(), "explanation": match[1].strip()})
        else: break
    if not parsed_themes and llm_output_text.strip() and not llm_output_text.startswith("LLM_ERROR:"):
        potential_blocks = re.split(r'\n\s*(?=\d+[\.\)\-])', "\n" + llm_output_text.strip())
        for block in potential_blocks:
            block = block.strip()
            if not block: continue
            if len(parsed_themes) >= num_expected_themes: break
            label_part = block.split('\n')[0] # Simplistic grab
            explanation_part = block
            parsed_themes.append({"theme_label": re.sub(r"^\d+[\.\)\-]\s*", "", label_part).strip()[:150], "explanation": explanation_part})
    if not parsed_themes and llm_output_text.strip() and not llm_output_text.startswith("LLM_ERROR:"):
        parsed_themes.append({"theme_label": "Unparsed LLM Output Block", "explanation": llm_output_text.strip()})
    return parsed_themes[:num_expected_themes]


# --- Main Test Execution Function for SCENARIO A ---
def run_q4_scenario_a_test():
    logger_test_A.log("INFO: Test Scenario A - Starting Q4 Thematic Analysis on SUMMARIES.")

    # --- 0. Load Actual Question Text for Q4 ---
    question_actual_text_q4 = f"Question text for {TEST_A_QID_TO_ANALYZE} not loaded (Default)"
    try:
        # ... (Same question text loading logic as in your previous test cell, using TEST_A_QUESTION_TEXT_SOURCE_FILE)
        logger_test_A.log(f"INFO TEST_A: Attempting to load question texts from: {TEST_A_QUESTION_TEXT_SOURCE_FILE}")
        if not os.path.exists(TEST_A_QUESTION_TEXT_SOURCE_FILE):
            raise FileNotFoundError(f"File not found: {TEST_A_QUESTION_TEXT_SOURCE_FILE}")
        with open(TEST_A_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:
            qtext_source_data = json.load(f_qtext)
        if "question_data" in qtext_source_data:
            q4_data_content = qtext_source_data.get("question_data", {}).get(TEST_A_QID_TO_ANALYZE)
            if q4_data_content and "question_text" in q4_data_content:
                question_actual_text_q4 = q4_data_content["question_text"]
                logger_test_A.log(f"INFO TEST_A: Loaded Q4 text: '{question_actual_text_q4[:100]}...'")
            else: logger_test_A.log(f"WARN TEST_A: Q4 text not in 'question_data'.")
        # Add elif for "analysis_by_question" if needed
        else: logger_test_A.log(f"WARN TEST_A: Expected keys for Q texts not in {TEST_A_QUESTION_TEXT_SOURCE_FILE}")
    except Exception as e:
        logger_test_A.log(f"ERROR TEST_A: Failed to load Q4 text: {e}")
        logger_test_A.log(traceback.format_exc())

    # --- 1. Load LLM ---
    llm_pipe_scenario_a = None
    tokenizer_scenario_a = None
    try:
        # ... (LLM loading logic as in your previous test cell, using TEST_A_LLM_MODEL_ID, TEST_A_USE_QUANTIZATION)
        logger_test_A.log(f"INFO TEST_A: Loading LLM: {TEST_A_LLM_MODEL_ID} (Quant: {TEST_A_USE_QUANTIZATION})")
        tokenizer_scenario_a = AutoTokenizer.from_pretrained(TEST_A_LLM_MODEL_ID)
        if getattr(tokenizer_scenario_a, 'pad_token', None) is None : tokenizer_scenario_a.pad_token = tokenizer_scenario_a.eos_token

        bnb_config_A = None
        effective_quant_A = TEST_A_USE_QUANTIZATION
        if TEST_A_USE_QUANTIZATION and torch.cuda.is_available():
            try:
                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
                bnb_config_A = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=compute_dtype)
            except Exception as e_bnb: effective_quant_A = False; logger_test_A.log(f"WARN TEST_A: BNB failed: {e_bnb}")
        elif TEST_A_USE_QUANTIZATION: effective_quant_A = False; logger_test_A.log("WARN TEST_A: Quant requested, No CUDA.")

        model_A = AutoModelForCausalLM.from_pretrained(
            TEST_A_LLM_MODEL_ID,
            quantization_config=bnb_config_A if effective_quant_A else None,
            torch_dtype=torch.bfloat16 if not effective_quant_A and torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (torch.float32 if not effective_quant_A else None),
            device_map="auto", trust_remote_code=True
        )
        llm_pipe_scenario_a = pipeline("text-generation", model=model_A, tokenizer=tokenizer_scenario_a)
        logger_test_A.log("INFO TEST_A: LLM Pipeline Loaded.")
    except Exception as e:
        logger_test_A.log(f"CRITICAL TEST_A: Failed to load LLM: {e}")
        logger_test_A.log(traceback.format_exc())
        return

    # --- 2. Load and Filter Q4 Summaries from Stage 4a Data ---
    q4_summaries_from_stage4a = []
    try:
        with open(TEST_A_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:
            stage4a_data = json.load(f)
        q4_all_items = stage4a_data.get("aggregated_pdf_content_by_qid", {}).get(TEST_A_QID_TO_ANALYZE, [])
        q4_summaries_from_stage4a = [item['text'] for item in q4_all_items if item.get('type') == 'pdf_summary' and item.get('text')]
        if not q4_summaries_from_stage4a:
            logger_test_A.log(f"ERROR TEST_A: No 'pdf_summary' items found for QID {TEST_A_QID_TO_ANALYZE}. Aborting Scenario A test.")
            return
        logger_test_A.log(f"INFO TEST_A: Loaded {len(q4_summaries_from_stage4a)} PDF summaries for {TEST_A_QID_TO_ANALYZE}.")
    except Exception as e:
        logger_test_A.log(f"CRITICAL TEST_A: Failed to load Stage 4a data: {e}")
        logger_test_A.log(traceback.format_exc())
        return

    # --- Scenario A: Map-Reduce on Concatenated Stage 4 Summaries ---
    concatenated_q4_summaries = "\n\n--- Next Summary Entry ---\n".join(q4_summaries_from_stage4a) # Use a clear separator
    logger_test_A.log(f"INFO TEST_A: Concatenated summaries char length: {len(concatenated_q4_summaries)}")

    final_themes_A = []
    map_outputs_raw_A = []
    reduce_output_raw_A = "N/A"

    # Chunk the concatenated string of summaries
    summary_chunks = create_text_chunks_scenario_a(
        concatenated_q4_summaries,
        llm_pipe_scenario_a.tokenizer, # Pass the loaded tokenizer
        TEST_A_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK,
        TEST_A_MAPREDUCE_CHUNK_OVERLAP_WORDS
    )

    if len(summary_chunks) > 1: # Map-Reduce needed
        logger_test_A.log(f"INFO TEST_A: Concatenated summaries for Q4 split into {len(summary_chunks)} chunks for Map step.")
        intermediate_themes_text_A = []
        for i, chunk in enumerate(summary_chunks):
            logger_test_A.log(f"  SCENARIO A - MAP CHUNK {i+1}/{len(summary_chunks)} (Chars: {len(chunk)})")
            map_out = get_llm_thematic_points_scenario_a(
                chunk, question_actual_text_q4, 3, # Request e.g. 3 themes per chunk
                llm_pipe_scenario_a, TEST_A_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,
                f"{TEST_A_QID_TO_ANALYZE}_A_map_chunk{i+1}", "SCENARIO_A_MAP"
            )
            map_outputs_raw_A.append(map_out)
            if not map_out.startswith("LLM_ERROR:"):
                intermediate_themes_text_A.append(f"Themes from Chunk {i+1}:\n{map_out}")
            time.sleep(0.5) # Small delay

        if intermediate_themes_text_A:
            combined_intermediate_A = "\n\n".join(intermediate_themes_text_A)
            logger_test_A.log(f"  SCENARIO A - REDUCE Step on intermediate themes (Input Chars: {len(combined_intermediate_A)})")
            reduce_output_raw_A = get_llm_thematic_points_scenario_a(
                combined_intermediate_A, question_actual_text_q4, TEST_A_NUM_THEMES_TO_REQUEST,
                llm_pipe_scenario_a, TEST_A_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,
                f"{TEST_A_QID_TO_ANALYZE}_A_reduce", "REDUCE_INTERMEDIATE_THEMES"
            )
            final_themes_A = parse_llm_themes_scenario_a(reduce_output_raw_A, TEST_A_NUM_THEMES_TO_REQUEST)
        else:
            logger_test_A.log("ERROR TEST_A: No valid intermediate themes from MAP step.")
            reduce_output_raw_A = "ERROR: No intermediate themes from MAP step (Summaries)."

    elif summary_chunks: # Only one chunk, direct processing
         logger_test_A.log(f"INFO TEST_A: Concatenated summaries fit in one chunk. Direct LLM call on summaries.")
         reduce_output_raw_A = get_llm_thematic_points_scenario_a(
             summary_chunks[0], question_actual_text_q4, TEST_A_NUM_THEMES_TO_REQUEST,
             llm_pipe_scenario_a, TEST_A_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS, # Use reduce tokens for final output
             f"{TEST_A_QID_TO_ANALYZE}_A_direct", "DIRECT_PROCESSING"
         )
         final_themes_A = parse_llm_themes_scenario_a(reduce_output_raw_A, TEST_A_NUM_THEMES_TO_REQUEST)
         map_outputs_raw_A.append(reduce_output_raw_A) # Store direct output as if it was a map output
    else:
        logger_test_A.log("WARN TEST_A: No summary chunks created (input likely empty after join/filter).")

    logger_test_A.log("\n" + "-"*15 + f" SCENARIO A (Map-Reduce on SUMMARIES) - {TEST_A_QID_TO_ANALYZE} Final Themes " + "-"*15)
    if final_themes_A:
        for i, theme in enumerate(final_themes_A):
            logger_test_A.log(f"  A Theme {i+1}: {theme.get('theme_label')}\n     Explanation: {theme.get('explanation')}")
    else:
        logger_test_A.log("  No final themes parsed for Scenario A.")
    logger_test_A.log("  Scenario A - Raw Map Outputs (first one if many, max 500 chars):")
    logger_test_A.log(f"  {(map_outputs_raw_A[0][:500] + '...') if map_outputs_raw_A and map_outputs_raw_A[0] else 'N/A'}")
    logger_test_A.log("  Scenario A - Raw Reduce Output (max 500 chars):")
    logger_test_A.log(f"  {(reduce_output_raw_A[:500] + '...') if reduce_output_raw_A else 'N/A'}")

    logger_test_A.log("\nINFO: Test Cell - Scenario A for Q4 Finished.")

    # Cleanup
    logger_test_A.log("INFO TEST_A: Cleaning up LLM resources for Scenario A...")
    if 'llm_pipe_scenario_a' in locals() and llm_pipe_scenario_a is not None: del llm_pipe_scenario_a
    if 'model_A' in locals() and model_A is not None: del model_A
    if 'tokenizer_scenario_a' in locals() and tokenizer_scenario_a is not None: del tokenizer_scenario_a
    gc.collect()
    if torch.cuda.is_available(): torch.cuda.empty_cache()
    logger_test_A.log("INFO TEST_A: LLM cleanup complete.")

# --- Execute the Test for Scenario A ---
# run_q4_scenario_a_test()
# print("REMINDER: Uncomment 'run_q4_scenario_a_test()' to execute SCENARIO A test for Q4.")

"""# 19th May"""

# @title TEST CELL: Q4 Thematic Analysis - Scenario A (Summaries) vs. Scenario B (Passages)

# --- Test Cell Imports (ensure these are run if not already globally available) ---
import json
import os
import sys # Used by FileLogger if not already imported
import traceback # Used by FileLogger if not already imported
import time
import gc
import re
from collections import defaultdict
from typing import Dict, Any, List # Used by FileLogger if not already imported

# Assuming ML/NLP Core Imports, Hugging Face, BitsAndBytes are done in Cell 0
# Ensure torch, numpy, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline are available
# For FileLogger, ensure numpy and torch are available if you expect to log such objects,
# though this specific test cell doesn't directly create them for logging.
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
try:
    from bitsandbytes.cextension import ColabNotSupportedError
except ImportError:
    ColabNotSupportedError = None


# --- Configuration for this Test Cell ---
TEST_BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # <<<--- ADJUST IF NEEDED
TEST_LOG_FILE = os.path.join(TEST_BASE_PROJECT_DIR, f'test_q4_thematic_analysis_{time.strftime("%Y%m%d_%H%M%S")}.log')

# LLM Config
TEST_LLM_MODEL_ID = 'google/gemma-3-4b-it'  # <<<--- IMPORTANT: Using 2b-it. Adjust to your PHASE3_LLM_MODEL_ID (e.g., 'google/gemma-7b-it') if different and you have resources.
TEST_USE_QUANTIZATION = False             # <<<--- IMPORTANT: Set to your PHASE3_USE_QUANTIZATION (True/False)

# Thematic Analysis Parameters
TEST_NUM_THEMES_TO_REQUEST = 3 # Final number of themes
TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2800 # Max *content* tokens for map step input items (text part only)
                                                 # LLM tokenizer.encode(text) to check. Needs room for prompt.
TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 400     # Max tokens LLM generates for themes *from one sub-chunk*.
TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 600  # Max tokens LLM generates for *final* themes.
TEST_MAPREDUCE_CHUNK_OVERLAP_WORDS = 30          # For create_text_chunks_for_test if used.

# Input file from Stage 4a (Phase 2 output)
# Replace with the *actual filename* generated by your Stage 4a / Phase 2 script
TEST_INPUT_COLLATED_PDF_TEXTS_JSON = os.path.join(TEST_BASE_PROJECT_DIR, 'phase2_stage4a_collated_pdf_texts_20250518_233834.json') # <<<--- UPDATE THIS FILENAME (Example from your log)

# Source for actual question texts
TEST_QUESTION_TEXT_SOURCE_FILE = os.path.join(TEST_BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # <<<--- ENSURE THIS IS CORRECT

# --- FileLogger Class (Assuming defined in Cell 1 or globally) ---
# If not, you would paste the FileLogger class definition here.
# For this example, I'll assume it's available and works as you provided.
try:
    logger_phase3_test = FileLogger(TEST_LOG_FILE)
    logger_phase3_test.log("INFO: Test Cell - Logger Initialized.")
except NameError:
    print("CRITICAL: FileLogger class not defined. Please ensure Cell 1 has been executed.")
    # Fallback to print if logger fails, to allow some output
    class PrintLogger:
        def log(self, msg): print(f"{time.strftime('%Y%m%d_%H%M%S')}: {msg}")
        def log_json(self, data, prefix="JSON"): print(f"{time.strftime('%Y%m%d_%H%M%S')}: {prefix} - {json.dumps(data, indent=2)}")
    logger_phase3_test = PrintLogger()
    logger_phase3_test.log("WARN: Using fallback PrintLogger as FileLogger was not found.")


# --- Helper: Text Chunker (for Scenario A if concatenated summaries are too long) ---
def create_text_chunks_for_test(full_text: str, tokenizer_for_chunking, max_tokens: int, overlap_words: int = 30):
    words = full_text.split() # Simple split, consider nltk.word_tokenize for more complex text
    if not words: return []

    chunks = []
    current_chunk_words = []

    logger_phase3_test.log(f"DEBUG CHUNKER: Starting to chunk text of {len(words)} words into max {max_tokens} token chunks.")

    for word_idx, word in enumerate(words):
        # Test adding the new word
        potential_chunk_text = " ".join(current_chunk_words + [word])
        num_tokens = len(tokenizer_for_chunking.encode(potential_chunk_text, add_special_tokens=False))

        if num_tokens <= max_tokens:
            current_chunk_words.append(word)
        else:
            # Current chunk plus new word is too long. Finalize current_chunk_words.
            if current_chunk_words: # Add current chunk if it has content
                chunks.append(" ".join(current_chunk_words))
                logger_phase3_test.log(f"  DEBUG CHUNKER: Added chunk {len(chunks)} with {len(current_chunk_words)} words, {len(tokenizer_for_chunking.encode(chunks[-1], add_special_tokens=False))} tokens.")

            # Start new chunk. Apply overlap from the *finalized* previous chunk.
            if chunks and overlap_words > 0:
                 # Get words from the *actual last chunk added* for overlap
                prev_chunk_words = chunks[-1].split()
                overlap_start_idx = max(0, len(prev_chunk_words) - overlap_words)
                current_chunk_words = prev_chunk_words[overlap_start_idx:] + [word]
            else: # No previous chunk or no overlap
                current_chunk_words = [word]

            # If the new chunk (overlap + new word, or just new word) is ALREADY too long
            new_chunk_text_check = " ".join(current_chunk_words)
            new_chunk_tokens_check = len(tokenizer_for_chunking.encode(new_chunk_text_check, add_special_tokens=False))
            if new_chunk_tokens_check > max_tokens:
                logger_phase3_test.log(f"  WARN CHUNKER: Word '{word}' or (overlap + word) resulted in an oversized new chunk ({new_chunk_tokens_check} tokens).")
                if len(current_chunk_words) > 1 and chunks: # If it was overlap + word that was too long
                    # Discard overlap for this problematic word, just start with the word
                    logger_phase3_test.log(f"    DEBUG CHUNKER: Discarding overlap, starting new chunk with just '{word}'.")
                    current_chunk_words = [word]
                    # Check again if the single word is too long
                    if len(tokenizer_for_chunking.encode(word, add_special_tokens=False)) > max_tokens:
                         logger_phase3_test.log(f"    WARN CHUNKER: Single word '{word}' itself is too long ({len(tokenizer_for_chunking.encode(word, add_special_tokens=False))} tokens). Adding as oversized chunk.")
                         # It will be added as its own chunk in the next 'if current_chunk_words'
                elif len(current_chunk_words) == 1: # Single word is too long
                     logger_phase3_test.log(f"    WARN CHUNKER: Single word '{word}' itself is too long ({new_chunk_tokens_check} tokens). Adding as oversized chunk.")
                     # Will be added as its own chunk below

    if current_chunk_words: # Add the last remaining chunk
        chunks.append(" ".join(current_chunk_words))
        logger_phase3_test.log(f"  DEBUG CHUNKER: Added final chunk {len(chunks)} with {len(current_chunk_words)} words, {len(tokenizer_for_chunking.encode(chunks[-1], add_special_tokens=False))} tokens.")

    logger_phase3_test.log(f"INFO CHUNKER: Split text into {len(chunks)} chunks.")
    return chunks


# --- Helper: Batch Individual Items (for Scenario B map step) ---
def batch_text_items_for_test(text_items_list: List[str], tokenizer_for_batching, max_tokens_per_batch: int):
    batches = [] # List of lists of strings (each inner list is a batch of item texts)
    current_batch_item_texts = [] # List of strings for the current batch
    current_batch_concatenated_text = "" # For token checking

    logger_phase3_test.log(f"DEBUG BATCHER: Starting to batch {len(text_items_list)} items into max {max_tokens_per_batch} token batches.")

    for item_idx, item_text_original in enumerate(text_items_list):
        if not item_text_original or not item_text_original.strip():
            logger_phase3_test.log(f"  DEBUG BATCHER: Item {item_idx} is empty, skipping.")
            continue

        # Text for next item to potentially add, with separator if needed
        text_to_try_adding = item_text_original
        if current_batch_item_texts: # If batch is not empty, add separator before next item
            text_to_try_adding = "\n\n--- Next Excerpt ---\n" + item_text_original

        potential_full_batch_text = current_batch_concatenated_text + text_to_try_adding
        num_tokens = len(tokenizer_for_batching.encode(potential_full_batch_text, add_special_tokens=False))

        if num_tokens <= max_tokens_per_batch:
            current_batch_item_texts.append(item_text_original) # Store original item text
            current_batch_concatenated_text = potential_full_batch_text # Update concatenated text for next check
        else:
            # Current batch + new item is too long. Finalize current_batch_item_texts.
            if current_batch_item_texts:
                batches.append(current_batch_item_texts)
                logger_phase3_test.log(f"  DEBUG BATCHER: Added batch {len(batches)} with {len(current_batch_item_texts)} items, {len(tokenizer_for_batching.encode(current_batch_concatenated_text, add_special_tokens=False))} tokens.")

            # Start new batch with the current item_text_original
            # Check if the item_text_original itself is too long
            single_item_tokens = len(tokenizer_for_batching.encode(item_text_original, add_special_tokens=False))
            if single_item_tokens > max_tokens_per_batch:
                logger_phase3_test.log(f"  WARN BATCHER: Item {item_idx} (len {len(item_text_original)} chars, {single_item_tokens} tokens) is too long for batch limit {max_tokens_per_batch}. Will be its own oversized batch.")
                batches.append([item_text_original]) # Add as its own batch
                logger_phase3_test.log(f"  DEBUG BATCHER: Added oversized item as batch {len(batches)}.")
                current_batch_item_texts = [] # Reset for next iteration
                current_batch_concatenated_text = ""
            else:
                current_batch_item_texts = [item_text_original]
                current_batch_concatenated_text = item_text_original

    if current_batch_item_texts: # Add any remaining batch
        batches.append(current_batch_item_texts)
        logger_phase3_test.log(f"  DEBUG BATCHER: Added final batch {len(batches)} with {len(current_batch_item_texts)} items, {len(tokenizer_for_batching.encode(current_batch_concatenated_text, add_special_tokens=False))} tokens.")

    logger_phase3_test.log(f"INFO BATCHER: Created {len(batches)} batches of items.")
    return batches


# --- Helper: LLM Call for Thematic Points ---
def get_llm_thematic_points_test(text_input_for_llm: str,
                                 question_text_for_prompt: str,
                                 num_themes_requested: int,
                                 llm_pipe: pipeline,
                                 max_new_tokens: int,
                                 qid_for_log: str, # For logging purposes
                                 map_reduce_step_type: str): # "SCENARIO_A_MAP", "SCENARIO_B_MAP", "REDUCE_INTERMEDIATE_THEMES", "DIRECT_PROCESSING"
    prompt_instruction = ""
    if map_reduce_step_type == "SCENARIO_A_MAP":
        prompt_instruction = (
            f"You are an expert policy analyst. The following text is a CHUNK of concatenated PREVIOUSLY-GENERATED SUMMARIES from multiple PDF submissions responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on this provided CHUNK OF SUMMARIES, identify up to 3 most prominent and distinct points, arguments, or themes they collectively raise.\n"
            f"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 points are evident, list only those that are clear and distinct.\n\n"
            f"Chunk of Summaries:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Prominent Points from this Chunk of Summaries:"
        )
    elif map_reduce_step_type == "SCENARIO_B_MAP":
        prompt_instruction = (
            f"You are an expert policy analyst. Carefully review the following BATCH of original excerpts/passages from PDF submissions, all responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on this provided BATCH of original excerpts/passages, identify up to 3 prominent and distinct points, arguments, or themes they collectively raise.\n"
            f"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 points are evident, list only those that are clear and distinct.\n\n"
            f"Batch of Excerpts/Passages:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Prominent Points from this Batch:"
        )
    elif map_reduce_step_type == "REDUCE_INTERMEDIATE_THEMES":
        prompt_instruction = (
            f"You are an expert policy analyst. The following are sets of preliminary themes/points that were individually extracted in previous steps from responses to the question: '{question_text_for_prompt}'.\n"
            f"Your task is to synthesize these preliminary themes into the overall top {num_themes_requested} overarching and distinct common points or themes.\n"
            f"For each final synthesized point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). Consolidate similar preliminary themes. Ensure the final themes are distinct and well-supported by the range of preliminary themes provided.\n\n"
            f"Preliminary Themes/Points Provided:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Top {num_themes_requested} Synthesized Common Points:"
        )
    else: # Direct processing (e.g. Scenario A if all summaries fit, or if only one batch in Scenario B)
        prompt_instruction = (
            f"You are an expert policy analyst. Carefully review all the following text (which consists of combined summaries OR a single batch of excerpts/passages) from PDF submissions responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on the provided text, identify the top {num_themes_requested} most common and distinct points, arguments, or themes raised by the respondents.\n"
            f"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes_requested} points are evident, list only those that are clear.\n\n"
            f"Provided Text:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Top {num_themes_requested} Common Points:"
        )
    messages = [{"role": "user", "content": prompt_instruction}]
    try:
        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        generation_args = {"max_new_tokens": max_new_tokens, "do_sample": False,
                           "pad_token_id": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id}

        # Debug logging for prompt length and input length
        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=True) # Full prompt tokens
        input_text_tokens = llm_pipe.tokenizer.encode(text_input_for_llm, add_special_tokens=False) # Just the content part
        logger_phase3_test.log(f"DEBUG TEST: {map_reduce_step_type} LLM call for {qid_for_log}. Prompt tokens: {len(prompt_tokens)}. Input content tokens: {len(input_text_tokens)}. Max new: {max_new_tokens}.")

        # if len(prompt_tokens) + max_new_tokens > (llm_pipe.model.config.max_position_embeddings or 8192) * 0.95: # Check against model's actual max context (use 95% as buffer)
        #      logger_phase3_test.log(f"WARN TEST: QID {qid_for_log} - Step {map_reduce_step_type} - Combined prompt tokens ({len(prompt_tokens)}) + max_new_tokens ({max_new_tokens}) might exceed model's context window. LLM might truncate or error.")

# Inside get_llm_thematic_points_test function

        # --- Determine model's max context window ---
        model_max_len = 8192 # Default if not found
        if hasattr(llm_pipe.model.config, 'max_position_embeddings'):
            model_max_len = llm_pipe.model.config.max_position_embeddings
        elif hasattr(llm_pipe.model.config, 'max_sequence_length'): # Common for Gemma
            model_max_len = llm_pipe.model.config.max_sequence_length
        elif hasattr(llm_pipe.tokenizer, 'model_max_length'): # Sometimes tokenizer has it
             model_max_len = llm_pipe.tokenizer.model_max_length
        logger_phase3_test.log(f"DEBUG TEST: Determined model_max_len: {model_max_len}")


        if len(prompt_tokens) + max_new_tokens > model_max_len * 0.95: # Check against determined model_max_len
             logger_phase3_test.log(f"WARN TEST: QID {qid_for_log} - Step {map_reduce_step_type} - Combined prompt tokens ({len(prompt_tokens)}) + max_new_tokens ({max_new_tokens}) might exceed model's context window ({model_max_len}). LLM might truncate or error.")
        # --- End context window check modification ---

        outputs = llm_pipe(prompt_formatted, **generation_args)

        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):
            logger_phase3_test.log(f"ERROR TEST: {map_reduce_step_type} LLM returned unexpected for QID {qid_for_log}.")
            return "LLM_ERROR: No output generated."

        generated_text_full = outputs[0]['generated_text']

        # Attempt to strip the prompt part from the generated text
        llm_response_text = generated_text_full
        if generated_text_full.startswith(prompt_formatted):
            llm_response_text = generated_text_full[len(prompt_formatted):].strip()
        else:
            # Gemma specific, or general model response start
            model_response_marker = "<start_of_turn>model\n"
            if model_response_marker in generated_text_full:
                llm_response_text = generated_text_full.split(model_response_marker, 1)[-1].strip()
            else: # If no clear marker and no prompt match, log a warning and return the full text
                logger_phase3_test.log(f"WARN TEST: {map_reduce_step_type} for QID {qid_for_log} - Could not reliably strip prompt. LLM output may contain prompt residue.")

        return llm_response_text
    except Exception as e:
        logger_phase3_test.log(f"ERROR TEST: {map_reduce_step_type} LLM call exception for QID {qid_for_log}: {e}")
        logger_phase3_test.log(traceback.format_exc())
        return f"LLM_ERROR: Exception - {str(e)}"

# --- Helper: Parse LLM Thematic Output ---
def parse_llm_themes_test(llm_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:
    parsed_themes = []
    if not llm_output_text or llm_output_text.startswith("LLM_ERROR:"):
        return [{"theme_label": "LLM Error or Parsing Error", "explanation": llm_output_text}]

    # Improved regex to capture "X. Theme Label: [Label]" and "Explanation: [Explanation]"
    # It looks for a number, then optionally "Theme Label:", then captures the label text.
    # Then it looks for "Explanation:" and captures the explanation text until the next theme number or end of string.
    # This pattern is designed to be more flexible.

    # Split the output into potential theme blocks based on numbering (e.g., "1.", "2.", etc.)
    # Add a newline at the beginning to help regex catch the first item if not starting with \n
    potential_blocks = re.split(r'\n\s*(?=\d+[\.\)\-])', "\n" + llm_output_text.strip())

    for block_idx, block in enumerate(potential_blocks):
        block = block.strip()
        if not block:
            continue

        theme_label = f"Theme {len(parsed_themes) + 1} (Label not distinctly parsed)" # Default label
        explanation = block # Default explanation is the whole block initially

        # Try to find "Theme Label:" or similar, then "Explanation:"
        # Case-insensitive search for "Theme Label" and "Explanation"
        label_match = re.search(r"^(?:\d+[\.\)\-]\s*)?(?:Theme Label\s*[:\-]?\s*)(.*?)(?:\n|$)", block, re.IGNORECASE | re.MULTILINE)
        expl_match = re.search(r"Explanation\s*[:\-]?\s*(.*)", block, re.IGNORECASE | re.DOTALL)

        if label_match:
            theme_label = label_match.group(1).strip()
            if expl_match: # If explanation keyword is also found
                explanation = expl_match.group(1).strip()
            else: # If label found, but no "Explanation:" keyword, assume rest of block after label is explanation
                # This part is tricky; LLM might not always use "Explanation:"
                # Try to take text after the found label
                explanation_start_index = block.lower().find(theme_label.lower()) + len(theme_label)
                # Look for a newline or just take the rest
                temp_expl = block[explanation_start_index:].strip()
                if temp_expl.startswith(":") or temp_expl.startswith("-"):
                    temp_expl = temp_expl[1:].strip()
                if temp_expl: # Only if there's something left after the label
                     explanation = temp_expl
                # else explanation remains the full block or needs better logic

        elif expl_match: # Found explanation but no distinct label before it
            explanation = expl_match.group(1).strip()
            # Try to infer label from text before "Explanation:"
            label_candidate_text = block[:expl_match.start()].strip()
            # Remove potential numbering like "1. " from the inferred label
            label_candidate_text = re.sub(r"^\d+[\.\)\-]\s*", "", label_candidate_text).strip()
            if label_candidate_text:
                theme_label = label_candidate_text

        # Only add if we have something meaningful
        if theme_label != f"Theme {len(parsed_themes) + 1} (Label not distinctly parsed)" or explanation != block :
             if theme_label.strip() or explanation.strip(): # Ensure at least one part has content
                parsed_themes.append({"theme_label": theme_label.strip(), "explanation": explanation.strip()})

        if len(parsed_themes) >= num_expected_themes:
            break

    if not parsed_themes and llm_output_text.strip() and not llm_output_text.startswith("LLM_ERROR:"): # Fallback for completely unparsed
        parsed_themes.append({"theme_label": "Unparsed LLM Output Block", "explanation": llm_output_text.strip()})

    return parsed_themes[:num_expected_themes]


# --- Main Test Execution Function ---
def run_q4_thematic_test_scenarios():
    logger_phase3_test.log("INFO: Test Cell - Starting Q4 Thematic Analysis Scenarios Test.")

    # --- 0. Load Actual Question Text for Q4 ---
    question_actual_text_q4_for_test = f"Question text for Q4 not loaded (Default)"
    try:
        logger_phase3_test.log(f"INFO TEST: Attempting to load question texts from: {TEST_QUESTION_TEXT_SOURCE_FILE}")
        if not os.path.exists(TEST_QUESTION_TEXT_SOURCE_FILE):
            raise FileNotFoundError(f"File not found: {TEST_QUESTION_TEXT_SOURCE_FILE}")

        with open(TEST_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:
            qtext_source_data = json.load(f_qtext)

        if "question_data" in qtext_source_data:
            q4_data = qtext_source_data.get("question_data", {}).get("Q4")
            if q4_data and "question_text" in q4_data:
                question_actual_text_q4_for_test = q4_data["question_text"]
                logger_phase3_test.log(f"INFO TEST: Successfully loaded Q4 text (Stage 2 struct): '{question_actual_text_q4_for_test[:100]}...'")
            else:
                logger_phase3_test.log(f"WARN TEST: Q4 or 'question_text' not in 'question_data' in {TEST_QUESTION_TEXT_SOURCE_FILE}")
        elif "analysis_by_question" in qtext_source_data: # Fallback to CAR_35 like structure
            q4_data = qtext_source_data.get("analysis_by_question", {}).get("Q4")
            if q4_data and "question_text" in q4_data:
                question_actual_text_q4_for_test = q4_data["question_text"]
                logger_phase3_test.log(f"INFO TEST: Successfully loaded Q4 text (CAR_35 struct): '{question_actual_text_q4_for_test[:100]}...'")
            else:
                logger_phase3_test.log(f"WARN TEST: Q4 or 'question_text' not in 'analysis_by_question' in {TEST_QUESTION_TEXT_SOURCE_FILE}")
        else:
            logger_phase3_test.log(f"WARN TEST: Expected keys for question texts not found in {TEST_QUESTION_TEXT_SOURCE_FILE}")
    except Exception as e:
        logger_phase3_test.log(f"ERROR TEST: Failed to load question text for Q4: {e}")
        logger_phase3_test.log(traceback.format_exc())

    # --- 1. Load LLM ---
    llm_thematic_pipe_test = None
    tokenizer_for_llm_test = None # To be used by chunkers
    try:
        logger_phase3_test.log(f"INFO TEST: Loading LLM: {TEST_LLM_MODEL_ID} (Quant: {TEST_USE_QUANTIZATION})")
        tokenizer_for_llm_test = AutoTokenizer.from_pretrained(TEST_LLM_MODEL_ID) # Assign to variable
        if getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is not None:
            tokenizer_for_llm_test.pad_token = tokenizer_for_llm_test.eos_token
        elif getattr(tokenizer_for_llm_test, 'pad_token', None) is None and tokenizer_for_llm_test.eos_token is None:
             logger_phase3_test.log(f"CRITICAL TEST: Tokenizer for {TEST_LLM_MODEL_ID} has neither pad_token nor eos_token.")

        bnb_config_test = None
        effective_quant_test = TEST_USE_QUANTIZATION
        if TEST_USE_QUANTIZATION and torch.cuda.is_available():
            try:
                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
                bnb_config_test = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=compute_dtype)
                logger_phase3_test.log(f"INFO TEST: BNB config with compute_dtype: {compute_dtype}")
            except Exception as e_bnb:
                logger_phase3_test.log(f"WARN TEST: BNB config failed: {e_bnb}. Disabling quantization.")
                effective_quant_test = False
        elif TEST_USE_QUANTIZATION and not torch.cuda.is_available(): # Check if CUDA is available for quantization
            logger_phase3_test.log("WARN TEST: Quantization requested but CUDA not available. Disabling quantization.")
            effective_quant_test = False # Ensure quantization is disabled

        model_for_llm_test = AutoModelForCausalLM.from_pretrained( # Assign to variable
            TEST_LLM_MODEL_ID,
            quantization_config=bnb_config_test if effective_quant_test else None,
            torch_dtype=torch.bfloat16 if not effective_quant_test and torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (torch.float32 if not effective_quant_test else None), # Conditional dtype
            device_map="auto", trust_remote_code=True
        )
        llm_thematic_pipe_test = pipeline("text-generation", model=model_for_llm_test, tokenizer=tokenizer_for_llm_test)
        logger_phase3_test.log("INFO TEST: LLM Pipeline Loaded.")
    except Exception as e:
        logger_phase3_test.log(f"CRITICAL TEST: Failed to load LLM for test: {e}")
        logger_phase3_test.log(traceback.format_exc())
        return

    # --- 2. Load Stage 4a Data for Q4 ---
    q4_all_items = []
    try:
        with open(TEST_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:
            stage4a_data = json.load(f)
        q4_all_items = stage4a_data.get("aggregated_pdf_content_by_qid", {}).get("Q4", [])
        if not q4_all_items:
            logger_phase3_test.log("ERROR TEST: No data found for Q4 in Stage 4a output. Aborting test.")
            return
        logger_phase3_test.log(f"INFO TEST: Loaded {len(q4_all_items)} text items for Q4 from Stage 4a output.")
    except Exception as e:
        logger_phase3_test.log(f"CRITICAL TEST: Failed to load Stage 4a data: {e}")
        logger_phase3_test.log(traceback.format_exc())
        return

    # --- Scenario A: Map-Reduce on Stage 4 SUMMARIES for Q4 ---
    logger_phase3_test.log("\n" + "="*30 + " SCENARIO A: Map-Reduce on Q4 PDF SUMMARIES " + "="*30)
    q4_summaries_from_stage4a = [item['text'] for item in q4_all_items if item.get('type') == 'pdf_summary' and item.get('text')]

    scenario_a_final_themes = []
    scenario_a_map_outputs_raw = []
    scenario_a_reduce_output_raw = "N/A"

    if not q4_summaries_from_stage4a:
        logger_phase3_test.log("WARN TEST SCENARIO A: No Stage 4 PDF summaries found for Q4. Scenario A cannot proceed as intended based on summaries.")
    else:
        logger_phase3_test.log(f"INFO TEST SCENARIO A: Found {len(q4_summaries_from_stage4a)} Stage 4 PDF summaries for Q4.")
        concatenated_q4_summaries = "\n\n--- Next Summary ---\n".join(q4_summaries_from_stage4a)
        logger_phase3_test.log(f"INFO TEST SCENARIO A: Concatenated summaries char length: {len(concatenated_q4_summaries)}")

        summary_chunks = create_text_chunks_for_test(
            concatenated_q4_summaries,
            llm_thematic_pipe_test.tokenizer,
            TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK,
            TEST_MAPREDUCE_CHUNK_OVERLAP_WORDS
        )

        if len(summary_chunks) > 1:
            logger_phase3_test.log(f"INFO TEST SCENARIO A: Concatenated summaries split into {len(summary_chunks)} chunks for Map step.")
            intermediate_themes_A = []
            for i, chunk in enumerate(summary_chunks):
                logger_phase3_test.log(f"  SCENARIO A - MAP CHUNK {i+1}/{len(summary_chunks)}")
                map_out = get_llm_thematic_points_test(chunk, question_actual_text_q4_for_test, 3, llm_thematic_pipe_test, TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS, "Q4_A_map_chunk", "SCENARIO_A_MAP")
                scenario_a_map_outputs_raw.append(map_out)
                if not map_out.startswith("LLM_ERROR:"): intermediate_themes_A.append(map_out)
                time.sleep(0.2) # Shorter sleep for faster test

            if intermediate_themes_A:
                combined_intermediate_A = "\n\n--- Next Set of Preliminary Themes ---\n".join(intermediate_themes_A)
                logger_phase3_test.log(f"  SCENARIO A - REDUCE Step (Input chars for reduce: {len(combined_intermediate_A)})")
                scenario_a_reduce_output_raw = get_llm_thematic_points_test(combined_intermediate_A, question_actual_text_q4_for_test, TEST_NUM_THEMES_TO_REQUEST, llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS, "Q4_A_reduce", "REDUCE_INTERMEDIATE_THEMES")
                scenario_a_final_themes = parse_llm_themes_test(scenario_a_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)
            else:
                logger_phase3_test.log("ERROR TEST SCENARIO A: No intermediate themes from MAP step for summaries.")
                scenario_a_reduce_output_raw = "ERROR: No intermediate themes from MAP step (Summaries)."
        elif summary_chunks:
             logger_phase3_test.log(f"INFO TEST SCENARIO A: Concatenated summaries fit in one chunk. Direct LLM call on summaries.")
             scenario_a_reduce_output_raw = get_llm_thematic_points_test(summary_chunks[0], question_actual_text_q4_for_test, TEST_NUM_THEMES_TO_REQUEST, llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS, "Q4_A_direct_summaries", "DIRECT_PROCESSING")
             scenario_a_final_themes = parse_llm_themes_test(scenario_a_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)
             scenario_a_map_outputs_raw.append(scenario_a_reduce_output_raw)
        else:
            logger_phase3_test.log("WARN TEST SCENARIO A: No summary chunks created (input likely empty after join/filter).")

    logger_phase3_test.log("\n--- SCENARIO A (Map-Reduce on PDF SUMMARIES) - Q4 Final Themes ---")
    if scenario_a_final_themes:
        for i, theme in enumerate(scenario_a_final_themes): logger_phase3_test.log(f"  A Theme {i+1}: {theme.get('theme_label')}\n     Explanation: {theme.get('explanation')}")
    else: logger_phase3_test.log("  No final themes parsed for Scenario A.")
    logger_phase3_test.log("  Scenario A - Raw Map Outputs (first one if many):")
    logger_phase3_test.log(f"  {scenario_a_map_outputs_raw[0][:500] if scenario_a_map_outputs_raw else 'N/A'}...")
    logger_phase3_test.log("  Scenario A - Raw Reduce Output:")
    logger_phase3_test.log(f"  {scenario_a_reduce_output_raw[:500]}...")


    # --- Scenario B: Map-Reduce on Stage 4 PASSAGES for Q4 ---
    logger_phase3_test.log("\n" + "="*30 + " SCENARIO B: Map-Reduce on Q4 PDF PASSAGES " + "="*30)
    q4_passages_from_stage4a = [item['text'] for item in q4_all_items if item.get('type') == 'pdf_passages' and item.get('text')]

    scenario_b_final_themes = []
    scenario_b_map_outputs_raw = []
    scenario_b_reduce_output_raw = "N/A"

    if not q4_passages_from_stage4a:
        logger_phase3_test.log("WARN TEST SCENARIO B: No Stage 4 PDF passages found for Q4. Scenario B cannot proceed.")
    else:
        logger_phase3_test.log(f"INFO TEST SCENARIO B: Found {len(q4_passages_from_stage4a)} Stage 4 PDF passage blocks for Q4.")

        passage_batches = batch_text_items_for_test(
            q4_passages_from_stage4a,
            llm_thematic_pipe_test.tokenizer,
            TEST_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK
        )
        logger_phase3_test.log(f"INFO TEST SCENARIO B: Grouped passages into {len(passage_batches)} batches for Map step.")

        if len(passage_batches) > 0:
            intermediate_themes_B = []
            for i, batch_of_passage_items_text in enumerate(passage_batches): # batch_of_passage_items_text is List[str]
                current_batch_concatenated_text = "\n\n--- Next Excerpt ---\n".join(batch_of_passage_items_text)
                logger_phase3_test.log(f"  SCENARIO B - MAP BATCH {i+1}/{len(passage_batches)} (Passage items in batch: {len(batch_of_passage_items_text)}, Chars: {len(current_batch_concatenated_text)})")
                map_out = get_llm_thematic_points_test(current_batch_concatenated_text, question_actual_text_q4_for_test, 3, llm_thematic_pipe_test, TEST_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS, "Q4_B_map_batch", "SCENARIO_B_MAP")
                scenario_b_map_outputs_raw.append(map_out)
                if not map_out.startswith("LLM_ERROR:"): intermediate_themes_B.append(map_out)
                time.sleep(0.2)

            if intermediate_themes_B:
                combined_intermediate_B = "\n\n--- Next Set of Preliminary Themes ---\n".join(intermediate_themes_B)
                logger_phase3_test.log(f"  SCENARIO B - REDUCE Step (Input chars for reduce: {len(combined_intermediate_B)})")
                scenario_b_reduce_output_raw = get_llm_thematic_points_test(combined_intermediate_B, question_actual_text_q4_for_test, TEST_NUM_THEMES_TO_REQUEST, llm_thematic_pipe_test, TEST_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS, "Q4_B_reduce", "REDUCE_INTERMEDIATE_THEMES")
                scenario_b_final_themes = parse_llm_themes_test(scenario_b_reduce_output_raw, TEST_NUM_THEMES_TO_REQUEST)
            else:
                logger_phase3_test.log("ERROR TEST SCENARIO B: No intermediate themes from MAP step for passages.")
                scenario_b_reduce_output_raw = "ERROR: No intermediate themes from MAP step (Passages)."
        else: # Should not happen if q4_passages_from_stage4a was not empty
            logger_phase3_test.log("WARN TEST SCENARIO B: No passage batches created (input passage list might have been empty or all items filtered).")


    logger_phase3_test.log("\n--- SCENARIO B (Map-Reduce on PDF PASSAGES) - Q4 Final Themes ---")
    if scenario_b_final_themes:
        for i, theme in enumerate(scenario_b_final_themes): logger_phase3_test.log(f"  B Theme {i+1}: {theme.get('theme_label')}\n     Explanation: {theme.get('explanation')}")
    else: logger_phase3_test.log("  No final themes parsed for Scenario B.")
    logger_phase3_test.log("  Scenario B - Raw Map Outputs (first one if many):")
    logger_phase3_test.log(f"  {scenario_b_map_outputs_raw[0][:500] if scenario_b_map_outputs_raw else 'N/A'}...")
    logger_phase3_test.log("  Scenario B - Raw Reduce Output:")
    logger_phase3_test.log(f"  {scenario_b_reduce_output_raw[:500]}...")

    logger_phase3_test.log("\nINFO: Test Cell - Q4 Thematic Analysis Scenarios Test Finished.")

    # Cleanup test LLM
    logger_phase3_test.log("INFO TEST: Cleaning up test LLM resources...")
    if 'llm_thematic_pipe_test' in locals() and llm_thematic_pipe_test is not None: del llm_thematic_pipe_test
    if 'model_for_llm_test' in locals() and model_for_llm_test is not None: del model_for_llm_test # Changed variable name
    if 'tokenizer_for_llm_test' in locals() and tokenizer_for_llm_test is not None: del tokenizer_for_llm_test # Changed variable name
    gc.collect()
    if torch.cuda.is_available(): torch.cuda.empty_cache()
    logger_phase3_test.log("INFO TEST: Test LLM cleanup complete.")

# --- Execute the Test ---
# Ensure prerequisite cells (0 for imports, 1 for FileLogger) are run.
# Update paths and model IDs in the configuration section at the top of THIS cell.
# Then, uncomment the line below to run:
run_q4_thematic_test_scenarios()

# @title Cell 7: Configuration for Phase 3 (LLM Thematic Analysis)

# Input for Phase 3 is the output from Phase 2
PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON = PHASE2_OUTPUT_COLLATED_JSON

# Output file for Phase 3
PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON = os.path.join(BASE_PROJECT_DIR, f'phase3_pdf_thematic_points_{time.strftime("%Y%m%d_%H%M%S")}.json')
PHASE3_LOG_FILE = os.path.join(BASE_PROJECT_DIR, f"phase3_llm_thematic_analysis_{time.strftime('%Y%m%d_%H%M%S')}.log")

# LLM and Tokenizer for Thematic Analysis
PHASE3_LLM_MODEL_ID = 'google/gemma-3-4b-it' # Your chosen model
PHASE3_USE_QUANTIZATION = False # Set to False as per your Phase 1 config, or True if desired for Phase 3

# Thematic Analysis Parameters
PHASE3_NUM_THEMES_TO_REQUEST = 3 # Final number of themes desired

# --- Parameters for Map-Reduce Chunking ---
PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK = 2500 # Max *content* tokens per sub-chunk for the MAP step.
                                                  # LLM (e.g. Gemma 8k context) tokenizer.encode(text) to check.
                                                  # Needs to leave room for prompt and LLM's map-step output.
PHASE3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS = 500    # Max tokens for LLM to generate themes *from one sub-chunk*.
PHASE3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS = 700 # Max tokens for LLM to generate *final* themes from intermediate themes.
PHASE3_MAPREDUCE_CHUNK_OVERLAP_WORDS = 50         # Optional: Overlap words between chunks to maintain context.

# Optional: QID selection for thematic analysis (if not processing all)
PHASE3_QIDS_TO_PROCESS_THEMATICALLY = None # ["Q4"] or None for all

# Source for actual question texts
PHASE3_QUESTION_TEXT_SOURCE_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # From original Stage 2 for question text
# OR: os.path.join(BASE_PROJECT_DIR, 'comprehensive_analysis_report.json') if CAR_35 is preferred source for Q texts

print("INFO: Cell 7 - Configuration for Phase 3 (LLM Thematic Analysis) loaded with Map-Reduce params.")

# @title Cell 8: Helper Functions & Main Logic for Phase 3 (LLM Thematic Analysis)

logger_phase3 = FileLogger(PHASE3_LOG_FILE)

# --- Helper: Text Chunker ---
def create_text_chunks(full_text: str, tokenizer, max_tokens_per_chunk: int, overlap_words: int = 50):
    """
    Splits a long text into smaller chunks based on token count, with optional word overlap.
    """
    # Simple word-based chunking as a proxy for token-based, then refine with tokenizer.
    # A more precise method would tokenize the whole text and then split token IDs.
    # This is a practical approximation for now.

    words = full_text.split() # Split by space, not ideal for all languages but okay for English
    chunks = []
    current_chunk_words = []
    current_token_count = 0

    # Estimate tokens per word (very rough, depends on tokenizer and text)
    # For a more accurate approach, tokenize word by word or sentence by sentence.
    # avg_tokens_per_word = 1.5 # Highly approximate, adjust based on observation

    for word_idx, word in enumerate(words):
        # Estimate token length of current word + space
        # word_token_len = len(tokenizer.encode(word + " ", add_special_tokens=False))

        # Simpler: check combined current_chunk_words with the new word
        potential_chunk_text = " ".join(current_chunk_words + [word])
        potential_chunk_tokens = tokenizer.encode(potential_chunk_text, add_special_tokens=False) # Don't add BOS/EOS here

        if len(potential_chunk_tokens) <= max_tokens_per_chunk:
            current_chunk_words.append(word)
        else:
            # Current chunk is full (or adding the word makes it full)
            if current_chunk_words: # Add current chunk if it has content
                chunks.append(" ".join(current_chunk_words))

            # Start new chunk, potentially with overlap
            overlap_start_index = max(0, len(current_chunk_words) - overlap_words) if chunks else 0
            current_chunk_words = current_chunk_words[overlap_start_index:] + [word] # Start new with overlap + current word

            # If even a single word is too long (after overlap logic), this needs more advanced splitting
            # For now, assume words are not excessively long token-wise.
            new_chunk_test_tokens = tokenizer.encode(" ".join(current_chunk_words), add_special_tokens=False)
            if len(new_chunk_test_tokens) > max_tokens_per_chunk and len(current_chunk_words) > 1 : # if just one word made it too long, it will be handled by next if
                 # if the overlap made it too long, reset current_chunk_words to just the new word
                 current_chunk_words = [word]


    # Add the last remaining chunk
    if current_chunk_words:
        chunks.append(" ".join(current_chunk_words))

    # Final check on chunk token lengths (can be slightly off due to word splitting)
    # This step is optional but good for ensuring compliance
    # It might require re-chunking or truncating, which adds complexity.
    # For now, we assume the above gives reasonably sized chunks.
    # A more robust chunker would tokenize the full text and split based on token IDs.

    logger_phase3.log(f"INFO: Split text into {len(chunks)} chunks for Map-Reduce.")
    if chunks:
        for i, chunk_text in enumerate(chunks[:3]): # Log first few
            logger_phase3.log(f"  DEBUG: Chunk {i} approx char length: {len(chunk_text)}, approx token length: {len(tokenizer.encode(chunk_text, add_special_tokens=False))}")
    return chunks


# --- Helper: LLM Call for Thematic Points (get_llm_thematic_points - same as before) ---
def get_llm_thematic_points(text_chunk_for_llm: str,
                            question_text_for_prompt: str,
                            num_themes: int, # Can be num_themes for sub-chunk or final num_themes
                            llm_pipe: pipeline,
                            max_new_tokens: int,
                            qid_for_log: str,
                            is_reduce_step: bool = False): # New flag
    """
    Sends a text chunk to the LLM to get thematic points.
    Returns the raw text output from the LLM.
    """
    prompt_instruction = ""
    if is_reduce_step:
        prompt_instruction = (
            f"You are an expert policy analyst. The following are sets of preliminary themes/points extracted from different parts of a larger collection of responses to the question: '{question_text_for_prompt}'.\n"
            f"Your task is to synthesize these preliminary themes into the top {num_themes} overarching and distinct common points or themes that represent the entire collection.\n"
            f"For each final synthesized point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). Consolidate similar preliminary themes into a single, more general point.\n\n"
            f"Preliminary Themes/Points Provided:\n\"\"\"\n{text_chunk_for_llm}\n\"\"\"\n\n"
            f"Top {num_themes} Synthesized Common Points:"
        )
    else: # Map step prompt
        prompt_instruction = (
            f"You are an expert policy analyst. Carefully review the following excerpt from PDF submissions responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on this provided excerpt, identify up to {num_themes} most prominent and distinct points, arguments, or themes raised.\n" # "up to" for map step
            f"For each point, provide a concise label for the point (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes} distinct common points are evident, list only those that are clear.\n\n"
            f"Excerpt:\n\"\"\"\n{text_chunk_for_llm}\n\"\"\"\n\n"
            f"Prominent Points from this Excerpt:"
        )

    messages = [{"role": "user", "content": prompt_instruction}]

    try:
        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        # Token check for prompt length
        prompt_tokens = llm_pipe.tokenizer.encode(prompt_formatted, add_special_tokens=False)
        if len(prompt_tokens) + max_new_tokens > (llm_pipe.model.config.max_position_embeddings or 8192) : # Check against model's actual max context
            logger_phase3.log(f"WARN: Phase 3 - QID {qid_for_log} - Prompt tokens ({len(prompt_tokens)}) + max_new_tokens ({max_new_tokens}) might exceed model's context window. Truncating input text chunk if this is not reduce step.")
            # For map step, the input `text_chunk_for_llm` should have already been managed by `create_text_chunks`.
            # For reduce step, if `text_chunk_for_llm` (combined intermediate themes) is too long, this is an issue.

        generation_args = {
            "max_new_tokens": max_new_tokens,
            "do_sample": False,
            "pad_token_id": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id,
        }

        step_type = "Reduce" if is_reduce_step else "Map"
        if PHASE1_ENABLE_DEBUG_LOGGING:
             logger_phase3.log(f"DEBUG: Phase 3 - {step_type} Step LLM call for QID {qid_for_log}. Input text char length: {len(text_chunk_for_llm)}")

        outputs = llm_pipe(prompt_formatted, **generation_args)

        # ... (rest of LLM output parsing, same as before) ...
        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):
            logger_phase3.log(f"ERROR: Phase 3 - {step_type} Step LLM pipeline returned unexpected/empty output for QID {qid_for_log}.")
            return "LLM_ERROR: No output generated."
        generated_text_full = outputs[0]['generated_text']
        llm_response_text = ""
        if generated_text_full.startswith(prompt_formatted):
            llm_response_text = generated_text_full[len(prompt_formatted):].strip()
        else:
            response_marker = "<start_of_turn>model\n"
            summary_start_index = generated_text_full.rfind(response_marker)
            if summary_start_index != -1:
                llm_response_text = generated_text_full[summary_start_index + len(response_marker):].strip()
            else:
                llm_response_text = generated_text_full
                logger_phase3.log(f"WARN: Phase 3 - {step_type} Step - Could not reliably strip prompt for QID {qid_for_log}.")
        return llm_response_text

    except Exception as e:
        step_type = "Reduce" if is_reduce_step else "Map"
        logger_phase3.log(f"ERROR: Phase 3 - {step_type} Step - Exception during LLM call for QID {qid_for_log}: {e}")
        logger_phase3.log(traceback.format_exc())
        return f"LLM_ERROR: Exception during generation - {str(e)}"


# --- Helper: Parse LLM Thematic Output (parse_llm_themes - same as before) ---
def parse_llm_themes(llm_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:
    # ... (parser logic from previous script, ensure it's robust) ...
    # ... (same as in your Cell 8)
    parsed_themes = []
    if not llm_output_text or llm_output_text.startswith("LLM_ERROR:"):
        return [{"theme_label": "Parsing Error", "explanation": llm_output_text}]
    current_theme_label = None
    current_explanation = ""
    lines = llm_output_text.splitlines()
    theme_item = {}

    for line in lines:
        line_stripped = line.strip()
        if not line_stripped: continue
        match_new_theme = re.match(r"^\s*(\d+[\.\)\-]?)\s*(?:Theme Label\s*[:\-]?\s*)?(.*)", line_stripped, re.IGNORECASE)
        if match_new_theme:
            if theme_item.get("theme_label"):
                if not theme_item.get("explanation"): theme_item["explanation"] = current_explanation.strip()
                if theme_item.get("theme_label") or theme_item.get("explanation"):
                    parsed_themes.append(theme_item)
            theme_item = {}
            current_explanation = ""
            label_candidate = match_new_theme.group(2).strip()
            if "explanation:" in label_candidate.lower():
                parts = re.split(r"explanation\s*[:\-]", label_candidate, maxsplit=1, flags=re.IGNORECASE)
                theme_item["theme_label"] = parts[0].strip()
                current_explanation = parts[1].strip() if len(parts) > 1 else ""
            else:
                theme_item["theme_label"] = label_candidate
        elif "explanation:" in line_stripped.lower():
            if theme_item and not theme_item.get("theme_label") and current_explanation.strip():
                theme_item["theme_label"] = current_explanation.strip()
                current_explanation = ""
            current_explanation += re.split(r"explanation\s*[:\-]", line_stripped, maxsplit=1, flags=re.IGNORECASE)[-1].strip() + " "
        elif theme_item:
            current_explanation += line_stripped + " "
    if theme_item.get("theme_label"):
        if not theme_item.get("explanation"): theme_item["explanation"] = current_explanation.strip()
        if theme_item.get("theme_label") or theme_item.get("explanation"):
            parsed_themes.append(theme_item)
    elif current_explanation.strip() and not parsed_themes:
        parsed_themes.append({"theme_label": "General Theme", "explanation": current_explanation.strip()})
    if not parsed_themes and llm_output_text:
        parsed_themes.append({"theme_label": "Unparsed LLM Output", "explanation": llm_output_text})
    return parsed_themes[:num_expected_themes]


# --- Main Function for Phase 3 (Modified for Map-Reduce) ---
def run_phase3_llm_thematic_analysis():
    global logger_phase3
    logger_phase3.log("=" * 50); logger_phase3.log(" INFO: Starting Phase 3: LLM Thematic Analysis per QID (with Map-Reduce)"); logger_phase3.log("=" * 50)
    # ... (initial logging of configs, LLM loading, Phase 2 data loading, QID text loading - same as before) ...
    # ... ENSURE llm_thematic_pipe_p3 is loaded and available ...
    # --- Load LLM for Thematic Analysis ---
    llm_tokenizer_p3 = None
    llm_model_p3 = None
    llm_thematic_pipe_p3 = None # This is our Hugging Face pipeline
    try:
        logger_phase3.log(f"INFO: Loading LLM tokenizer for Phase 3: {PHASE3_LLM_MODEL_ID}...")
        llm_tokenizer_p3 = AutoTokenizer.from_pretrained(PHASE3_LLM_MODEL_ID)
        if getattr(llm_tokenizer_p3, 'pad_token', None) is None and getattr(llm_tokenizer_p3, 'eos_token', None):
            llm_tokenizer_p3.pad_token = llm_tokenizer_p3.eos_token
            llm_tokenizer_p3.padding_side = "left"
        logger_phase3.log("INFO: Phase 3 LLM Tokenizer loaded.")

        bnb_config_p3 = None
        current_use_quantization_p3 = PHASE3_USE_QUANTIZATION
        if current_use_quantization_p3 and torch.cuda.is_available():
            try:
                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
                bnb_config_p3 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)
                logger_phase3.log(f"INFO: Phase 3 - 4-bit quantization configured (Compute dtype: {compute_dtype}).")
            except Exception as q_err:
                logger_phase3.log(f"ERROR: Phase 3 - Failed BitsAndBytesConfig: {q_err}. Disabling quantization.")
                current_use_quantization_p3 = False

        logger_phase3.log(f"INFO: Loading Phase 3 LLM model: {PHASE3_LLM_MODEL_ID} (Quantization: {current_use_quantization_p3})...")
        llm_model_p3 = AutoModelForCausalLM.from_pretrained(
            PHASE3_LLM_MODEL_ID,
            quantization_config=bnb_config_p3 if current_use_quantization_p3 else None,
            torch_dtype=torch.bfloat16 if not current_use_quantization_p3 else None, # Ensure correct dtype
            device_map="auto", trust_remote_code=True
        )
        logger_phase3.log("INFO: Phase 3 LLM model loaded.")

        llm_thematic_pipe_p3 = pipeline("text-generation", model=llm_model_p3, tokenizer=llm_tokenizer_p3)
        logger_phase3.log("INFO: Phase 3 LLM text-generation pipeline created successfully.")
    except Exception as load_err:
        logger_phase3.log(f"CRITICAL: Failed to load LLM/pipeline for Phase 3: {load_err}")
        logger_phase3.log(traceback.format_exc())
        return False

    # --- Load Collated Texts from Phase 2 ---
    try:
        with open(PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:
            phase2_data = json.load(f)
        collated_texts_by_qid_p3 = phase2_data.get("aggregated_pdf_content_by_qid", {})
        if not collated_texts_by_qid_p3: # Check if the key itself is missing or the dict is empty
            logger_phase3.log(f"ERROR: 'aggregated_pdf_content_by_qid' key not found or no data in {PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON}")
            return False
        logger_phase3.log(f"INFO: Loaded collated PDF texts for {len(collated_texts_by_qid_p3)} QIDs from Phase 2 output.")
    except Exception as e:
        logger_phase3.log(f"FATAL: Could not load Phase 2 collated texts file: {PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON}. Error: {e}")
        return False

    # --- Load Question Texts ---
    qid_to_actual_question_text_p3 = {}
    # ... (same question text loading logic as before) ...
    try:
        with open(PHASE3_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:
            qtext_source_data = json.load(f_qtext)
        if "analysis_by_question" in qtext_source_data: # CAR_35 structure
            for qid_key, data_val in qtext_source_data.get("analysis_by_question", {}).items():
                qid_to_actual_question_text_p3[qid_key] = data_val.get("question_text", f"Unknown text for {qid_key}")
        elif "question_data" in qtext_source_data: # Stage 2 form output structure
             for qid_key, data_val in qtext_source_data.get("question_data", {}).items():
                qid_to_actual_question_text_p3[qid_key] = data_val.get("question_text", f"Unknown text for {qid_key}")
        logger_phase3.log(f"INFO: Loaded actual question texts for {len(qid_to_actual_question_text_p3)} QIDs from {PHASE3_QUESTION_TEXT_SOURCE_FILE}.")
    except Exception as e:
        logger_phase3.log(f"WARN: Could not load question texts from {PHASE3_QUESTION_TEXT_SOURCE_FILE}. Prompts will use QID only. Error: {e}")


    overall_thematic_results_p3 = {}
    # ... (load existing results logic - same as before) ...
    if os.path.exists(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON):
        try:
            with open(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON, 'r', encoding='utf-8') as f_exist:
                overall_thematic_results_p3 = json.load(f_exist)
            logger_phase3.log(f"INFO: Loaded {len(overall_thematic_results_p3)} existing thematic results from {PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}")
        except Exception as e_load_exist:
            logger_phase3.log(f"WARN: Could not load existing results file {PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}. Starting fresh. Error: {e_load_exist}")


    qids_for_processing_p3 = PHASE3_QIDS_TO_PROCESS_THEMATICALLY if PHASE3_QIDS_TO_PROCESS_THEMATICALLY else sorted(collated_texts_by_qid_p3.keys())

    for qid_idx, qid in enumerate(qids_for_processing_p3):
        logger_phase3.log(f"\n--- Processing Thematic Analysis for QID {qid_idx + 1}/{len(qids_for_processing_p3)}: {qid} ---")
        if qid in overall_thematic_results_p3 and overall_thematic_results_p3[qid].get("status", "").startswith("success"):
            logger_phase3.log(f"INFO: QID {qid} already successfully processed. Skipping.")
            continue

        question_actual_text_p3 = qid_to_actual_question_text_p3.get(qid, f"Question {qid}") # Get actual question text
        text_items_for_this_qid_p3 = collated_texts_by_qid_p3.get(qid, [])
        # ... (check for empty text_items_for_this_qid_p3 - same as before) ...
        if not text_items_for_this_qid_p3:
            logger_phase3.log(f"WARN: No collated texts found for QID {qid}. Skipping thematic analysis.")
            overall_thematic_results_p3[qid] = {"question_text": question_actual_text_p3, "status": "no_collated_text_for_qid", "themes": [], "llm_raw_output_map": [], "llm_raw_output_reduce": "N/A"}
            continue

        texts_to_combine_p3 = [item['text'] for item in text_items_for_this_qid_p3 if item.get('text')]
        if not texts_to_combine_p3:
             logger_phase3.log(f"WARN: All text items for QID {qid} were empty after extraction. Skipping.")
             overall_thematic_results_p3[qid] = {"question_text": question_actual_text_p3, "status": "all_texts_empty_for_qid", "themes": [], "llm_raw_output_map": [], "llm_raw_output_reduce": "N/A"}
             continue

        current_qid_full_text = "\n\n--- New Excerpt ---\n".join(texts_to_combine_p3)
        logger_phase3.log(f"INFO: QID {qid} - Total combined character length for LLM input: {len(current_qid_full_text)}")

        # --- MAP-REDUCE LOGIC ---
        all_intermediate_themes_text = []
        raw_map_outputs = []

        # Tokenize once for length check for the whole document, then chunk based on words as an approximation.
        # A more accurate way is to tokenize the whole document and split based on token IDs.
        # This is a simplification.
        full_text_tokens = llm_thematic_pipe_p3.tokenizer.encode(current_qid_full_text, add_special_tokens=False)

        if len(full_text_tokens) > PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK:
            logger_phase3.log(f"INFO: QID {qid} - Full text ({len(full_text_tokens)} tokens) exceeds chunk limit ({PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK}). Applying Map-Reduce.")

            text_sub_chunks = create_text_chunks(
                current_qid_full_text,
                llm_thematic_pipe_p3.tokenizer, # Pass the tokenizer
                PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK,
                PHASE3_MAPREDUCE_CHUNK_OVERLAP_WORDS
            )
            logger_phase3.log(f"INFO: QID {qid} - Split into {len(text_sub_chunks)} sub-chunks for Map step.")

            for i, sub_chunk in enumerate(text_sub_chunks):
                logger_phase3.log(f"  MAP Step for QID {qid}, Chunk {i+1}/{len(text_sub_chunks)}...")
                map_llm_output = get_llm_thematic_points(
                    sub_chunk,
                    question_actual_text_p3,
                    PHASE3_NUM_THEMES_TO_REQUEST, # Ask for same number of themes from each chunk
                    llm_thematic_pipe_p3,
                    PHASE3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,
                    f"{qid}_map_chunk_{i+1}",
                    is_reduce_step=False
                )
                raw_map_outputs.append(map_llm_output)
                if not map_llm_output.startswith("LLM_ERROR:"):
                    all_intermediate_themes_text.append(map_llm_output)
                time.sleep(1) # Small delay between LLM calls if needed

            if not all_intermediate_themes_text:
                logger_phase3.log(f"ERROR: QID {qid} - Map step produced no valid intermediate themes.")
                overall_thematic_results_p3[qid] = {
                    "question_text": question_actual_text_p3, "status": "error_map_step_no_themes",
                    "themes": [], "llm_raw_output_map": raw_map_outputs, "llm_raw_output_reduce": "N/A - Map failed"
                }
                # ... (save and continue to next qid) ...
                continue # to next QID

            # Combine intermediate themes for the Reduce step
            combined_intermediate_themes = "\n\n--- Next Set of Preliminary Themes ---\n\n".join(all_intermediate_themes_text)
            logger_phase3.log(f"INFO: QID {qid} - REDUCE Step: Processing {len(all_intermediate_themes_text)} sets of intermediate themes.")

            # Check if combined_intermediate_themes itself is too long for Reduce step context
            reduce_input_tokens = llm_thematic_pipe_p3.tokenizer.encode(combined_intermediate_themes, add_special_tokens=False)
            if len(reduce_input_tokens) > PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK * 1.5 : # Heuristic, allow reduce input to be a bit larger
                logger_phase3.log(f"WARN: QID {qid} - Combined intermediate themes ({len(reduce_input_tokens)} tokens) might be too long for Reduce step. Truncating.")
                # Simple truncation (could be smarter, e.g. summarize intermediate themes first)
                # This is a complex problem: if intermediate themes are too many, need recursive reduce or different strategy.
                # For now, simple truncation:
                # How to truncate text based on token count without splitting mid-token is tricky.
                # One way: join, tokenize, then decode a slice of tokens.
                # For simplicity here, we'll just send it and rely on LLM or see if it errors.
                # A better way: if combined_intermediate_themes is too long, run create_text_chunks on IT,
                # then run reduce on EACH chunk of intermediate_themes, then another reduce layer.
                # This makes it truly recursive map-reduce.
                # For now, we'll just note the risk.
                pass


            final_llm_themes_output = get_llm_thematic_points(
                combined_intermediate_themes,
                question_actual_text_p3,
                PHASE3_NUM_THEMES_TO_REQUEST, # Final number of themes
                llm_thematic_pipe_p3,
                PHASE3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,
                f"{qid}_reduce_step",
                is_reduce_step=True
            )
            llm_raw_output_for_storage = final_llm_themes_output # This is the output of the reduce step
        else:
            # Full text is small enough, process directly (no map-reduce needed)
            logger_phase3.log(f"INFO: QID {qid} - Full text ({len(full_text_tokens)} tokens) is within limit. Processing directly.")
            final_llm_themes_output = get_llm_thematic_points(
                current_qid_full_text,
                question_actual_text_p3,
                PHASE3_NUM_THEMES_TO_REQUEST,
                llm_thematic_pipe_p3,
                PHASE3_LLM_MAX_NEW_TOKENS_FOR_THEMES, # Use the general theme token limit
                qid,
                is_reduce_step=False # Effectively a single map step
            )
            llm_raw_output_for_storage = final_llm_themes_output
            raw_map_outputs.append(final_llm_themes_output) # Store the single output as if it was a map output

        # Parse the final themes output (either from direct call or reduce step)
        parsed_themes_list = parse_llm_themes(final_llm_themes_output, PHASE3_NUM_THEMES_TO_REQUEST)
        # ... (status setting and saving overall_thematic_results_p3 - same as before) ...
        status_p3 = "success_thematic_analysis"
        if final_llm_themes_output.startswith("LLM_ERROR:"):
            status_p3 = "error_llm_theme_generation"
            if not all_intermediate_themes_text: status_p3 = "error_direct_llm_theme_generation" # if not map-reduce
            elif len(text_sub_chunks) > 0 : status_p3 = "error_reduce_step_llm_theme_generation"

        elif not parsed_themes_list or (len(parsed_themes_list) == 1 and parsed_themes_list[0]["theme_label"] == "Unparsed LLM Output"):
            status_p3 = "llm_output_parsing_failed_or_empty"
            if len(text_sub_chunks) > 0 : status_p3 = "reduce_step_llm_output_parsing_failed"
            else: status_p3 = "direct_llm_output_parsing_failed"


        overall_thematic_results_p3[qid] = {
            "question_text": question_actual_text_p3,
            "status": status_p3,
            "total_pdf_text_items_for_qid": len(texts_to_combine_p3),
            "approx_char_length_fed_to_llm_initial": len(current_qid_full_text),
            "map_reduce_applied": len(text_sub_chunks) > 0 if 'text_sub_chunks' in locals() else False,
            "num_map_chunks": len(text_sub_chunks) if 'text_sub_chunks' in locals() else 1,
            "llm_raw_output_map_chunks": raw_map_outputs, # List of raw outputs from map steps
            "llm_raw_output_final": llm_raw_output_for_storage, # Raw output from reduce or direct step
            "parsed_top_themes": parsed_themes_list
        }
        # ... (save results after each QID logic - same as before) ...
        try:
            with open(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON, 'w', encoding='utf-8') as f_out_p3:
                json.dump(overall_thematic_results_p3, f_out_p3, indent=2, default=logger_phase3._get_serializer())
            logger_phase3.log(f"INFO: Saved thematic analysis results to '{PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}' after QID {qid}.")
        except Exception as e_save_p3:
            logger_phase3.log(f"ERROR: Could not save intermediate results for Phase 3: {e_save_p3}")

        gc.collect()
        if torch.cuda.is_available(): torch.cuda.empty_cache()


    logger_phase3.log("=" * 50); logger_phase3.log(" INFO: Phase 3 (LLM Thematic Analysis with Map-Reduce) Finished "); logger_phase3.log("=" * 50)
    return True

# print("INFO: Cell 8 - Logic for Phase 3 (LLM Thematic Analysis) defined with Map-Reduce.")
# To run:
# Make sure configurations in Cell 7 are set, especially PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK
# run_phase3_llm_thematic_analysis()

# @title Cell 8: Helper Functions & Main Logic for Phase 3 (LLM Thematic Analysis) - REFINED MAP-REDUCE

logger_phase3 = FileLogger(PHASE3_LOG_FILE) # Assuming this is initialized

# --- Helper: LLM Call for Thematic Points (get_llm_thematic_points - modified prompt logic) ---
def get_llm_thematic_points(text_input_for_llm: str, # Can be a single excerpt or combined intermediate themes
                            question_text_for_prompt: str,
                            num_themes_requested: int,
                            llm_pipe: pipeline,
                            max_new_tokens: int,
                            qid_for_log: str,
                            map_reduce_step_type: str): # "MAP_INDIVIDUAL_ITEMS", "REDUCE_INTERMEDIATE_THEMES"
    """
    Sends text to the LLM to get thematic points.
    Returns the raw text output from the LLM.
    """
    prompt_instruction = ""
    if map_reduce_step_type == "REDUCE_INTERMEDIATE_THEMES":
        prompt_instruction = (
            f"You are an expert policy analyst. The following are sets of preliminary themes/points that were individually extracted from different batches of responses to the question: '{question_text_for_prompt}'.\n"
            f"Your task is to synthesize these preliminary themes into the overall top {num_themes_requested} overarching and distinct common points or themes that represent the entire collection of original responses.\n"
            f"For each final synthesized point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). Consolidate similar preliminary themes. Ensure the final themes are distinct and well-supported by the range of preliminary themes provided.\n\n"
            f"Preliminary Themes/Points Provided:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Top {num_themes_requested} Synthesized Common Points:"
        )
    elif map_reduce_step_type == "MAP_INDIVIDUAL_ITEMS":
        prompt_instruction = (
            f"You are an expert policy analyst. Carefully review the following batch of {num_themes_requested} excerpts/summaries from PDF submissions, all responding to the question: '{question_text_for_prompt}'.\n" # Assuming num_themes_requested for map might be # of items in batch
            f"Based *only* on this provided batch of excerpts/summaries, identify up to 3 prominent and distinct points, arguments, or themes they collectively raise.\n"
            f"For each point, provide a concise label (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). If fewer than 3 distinct common points are evident, list only those that are clear.\n\n"
            f"Batch of Excerpts/Summaries:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Prominent Points from this Batch:"
        )
    else: # Direct processing if no map-reduce
        prompt_instruction = (
            f"You are an expert policy analyst. Carefully review all the following excerpts/summaries from PDF submissions responding to the question: '{question_text_for_prompt}'.\n"
            f"Based *only* on the provided excerpts/summaries, identify the top {num_themes_requested} most common and distinct points, arguments, or themes raised by the respondents.\n"
            f"For each point, provide a concise label for the point (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\n"
            f"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes_requested} distinct common points are evident, list only those that are clear.\n\n"
            f"Excerpts/Summaries:\n\"\"\"\n{text_input_for_llm}\n\"\"\"\n\n"
            f"Top {num_themes_requested} Common Points:"
        )


    messages = [{"role": "user", "content": prompt_instruction}]
    # ... (rest of the get_llm_thematic_points function: try-except, LLM call, output parsing - remains similar)
    try:
        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        generation_args = {
            "max_new_tokens": max_new_tokens, "do_sample": False,
            "pad_token_id": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id,
        }
        if PHASE1_ENABLE_DEBUG_LOGGING:
             logger_phase3.log(f"DEBUG: Phase 3 - {map_reduce_step_type} LLM call for QID {qid_for_log}. Input char len: {len(text_input_for_llm)}")
        outputs = llm_pipe(prompt_formatted, **generation_args)
        # ... (output extraction logic) ...
        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):
            logger_phase3.log(f"ERROR: Phase 3 - {map_reduce_step_type} LLM pipeline returned unexpected/empty output for QID {qid_for_log}.")
            return "LLM_ERROR: No output generated."
        generated_text_full = outputs[0]['generated_text']
        llm_response_text = ""
        if generated_text_full.startswith(prompt_formatted):
            llm_response_text = generated_text_full[len(prompt_formatted):].strip()
        else: # Fallback extraction
            response_marker = "<start_of_turn>model\n"
            summary_start_index = generated_text_full.rfind(response_marker)
            if summary_start_index != -1:
                llm_response_text = generated_text_full[summary_start_index + len(response_marker):].strip()
            else:
                llm_response_text = generated_text_full
                logger_phase3.log(f"WARN: Phase 3 - {map_reduce_step_type} - Could not reliably strip prompt for QID {qid_for_log}.")
        return llm_response_text
    except Exception as e:
        logger_phase3.log(f"ERROR: Phase 3 - {map_reduce_step_type} - Exception during LLM call for QID {qid_for_log}: {e}")
        logger_phase3.log(traceback.format_exc())
        return f"LLM_ERROR: Exception during generation - {str(e)}"


# --- Helper: Parse LLM Thematic Output (parse_llm_themes - same as before) ---
# ... (ensure this function is robust for your LLM's output format) ...

# --- Main Function for Phase 3 (Modified for BETTER Map-Reduce) ---
def run_phase3_llm_thematic_analysis():
    global logger_phase3
    logger_phase3.log("=" * 50); logger_phase3.log(" INFO: Starting Phase 3: LLM Thematic Analysis per QID (Refined Map-Reduce)"); logger_phase3.log("=" * 50)
    # ... (LLM loading, Phase 2 data loading, QID text loading - same as before) ...
    # ... Ensure llm_thematic_pipe_p3 (the pipeline object) is loaded and available ...
    # ... Ensure collated_texts_by_qid_p3 and qid_to_actual_question_text_p3 are loaded ...

    overall_thematic_results_p3 = {} # To store final results
    # ... (load existing results logic - same as before) ...

    qids_for_processing_p3 = PHASE3_QIDS_TO_PROCESS_THEMATICALLY if PHASE3_QIDS_TO_PROCESS_THEMATICALLY else sorted(collated_texts_by_qid_p3.keys())

    for qid_idx, qid in enumerate(qids_for_processing_p3):
        logger_phase3.log(f"\n--- Processing Thematic Analysis for QID {qid_idx + 1}/{len(qids_for_processing_p3)}: {qid} ---")
        # ... (skip if already processed logic - same as before) ...

        question_actual_text_p3 = qid_to_actual_question_text_p3.get(qid, f"Question {qid}")
        # text_items_for_this_qid is a list of dicts: [{'source_file': ..., 'text': ...}, ...]
        text_items_for_this_qid_p3 = collated_texts_by_qid_p3.get(qid, [])
        # ... (check for empty text_items_for_this_qid_p3 - same as before) ...

        if not text_items_for_this_qid_p3: # Handle empty list for QID
            logger_phase3.log(f"WARN: No collated texts found for QID {qid}. Skipping thematic analysis.")
            overall_thematic_results_p3[qid] = {"question_text": question_actual_text_p3, "status": "no_collated_text_for_qid", "parsed_top_themes": []}
            continue

        # --- REFINED MAP-REDUCE LOGIC ---
        all_intermediate_themes_text_list = [] # Store raw text outputs of map step
        raw_map_outputs_list = [] # Store raw LLM response for each map call for debugging

        # Group individual text items (summaries/passages from Stage 4a) into batches for the MAP step
        # The goal is for the *concatenated text of items in a batch* to be within LLM limits
        map_batches = []
        current_batch_texts = []
        current_batch_token_count = 0 # Approximation by char length initially, refine with tokenizer

        # Heuristic: average tokens per character (very rough)
        # For a more robust approach, tokenize each item's text and sum tokens.
        # avg_chars_per_token = 4
        # max_chars_per_map_batch = PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK * avg_chars_per_token

        # More robust batching based on tokenizing the combined text of the batch:
        temp_batch_texts = []
        for item_idx, item in enumerate(text_items_for_this_qid_p3):
            item_text = item.get('text')
            if not item_text: continue

            temp_batch_texts.append(item_text)
            # Check token length of the current temp_batch_texts combined
            combined_text_for_batch_check = "\n\n--- Next Excerpt ---\n".join(temp_batch_texts)
            tokens_for_batch_check = llm_thematic_pipe_p3.tokenizer.encode(combined_text_for_batch_check, add_special_tokens=False)

            if len(tokens_for_batch_check) > PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK:
                if len(temp_batch_texts) > 1: # If more than one item made it too long
                    map_batches.append(temp_batch_texts[:-1]) # Add batch without the last item
                    temp_batch_texts = [temp_batch_texts[-1]] # Start new batch with the last item
                else: # Single item itself is too long
                    logger_phase3.log(f"WARN: QID {qid}, Item {item_idx} text is too long ({len(tokens_for_batch_check)} tokens) even for a single map chunk. Truncating this item.")
                    # Simple truncation for this item (could be smarter)
                    # Re-tokenize the single item and slice tokens
                    single_item_tokens = llm_thematic_pipe_p3.tokenizer.encode(item_text, add_special_tokens=False)
                    truncated_item_tokens = single_item_tokens[:PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK]
                    truncated_item_text = llm_thematic_pipe_p3.tokenizer.decode(truncated_item_tokens, skip_special_tokens=True)
                    map_batches.append([truncated_item_text])
                    temp_batch_texts = [] # Reset

        if temp_batch_texts: # Add any remaining batch
            map_batches.append(temp_batch_texts)

        logger_phase3.log(f"INFO: QID {qid} - Created {len(map_batches)} batches of text items for MAP step.")

        if not map_batches: # Should not happen if text_items_for_this_qid_p3 was not empty
             logger_phase3.log(f"WARN: QID {qid} - No map batches created. Skipping LLM analysis.")
             overall_thematic_results_p3[qid] = {"question_text": question_actual_text_p3, "status": "no_map_batches_created", "parsed_top_themes": []}
             continue


        if len(map_batches) > 1 : # Apply Map-Reduce
            logger_phase3.log(f"INFO: QID {qid} - Applying Map-Reduce with {len(map_batches)} map steps.")
            for i, batch_of_texts in enumerate(map_batches):
                batch_combined_text = "\n\n--- Next Excerpt ---\n".join(batch_of_texts)
                logger_phase3.log(f"  MAP Step for QID {qid}, Batch {i+1}/{len(map_batches)} (Items: {len(batch_of_texts)}, Chars: {len(batch_combined_text)})...")

                map_llm_output = get_llm_thematic_points(
                    batch_combined_text,
                    question_actual_text_p3,
                    3, # Request e.g., 3 themes from each small batch
                    llm_thematic_pipe_p3,
                    PHASE3_MAPREDUCE_MAP_STEP_MAX_NEW_TOKENS,
                    f"{qid}_map_batch_{i+1}",
                    map_reduce_step_type="MAP_INDIVIDUAL_ITEMS"
                )
                raw_map_outputs_list.append({"batch_index": i+1, "raw_output": map_llm_output})
                if not map_llm_output.startswith("LLM_ERROR:"):
                    all_intermediate_themes_text_list.append(f"Preliminary Themes from Batch {i+1}:\n{map_llm_output}")
                time.sleep(1) # Optional delay

            if not all_intermediate_themes_text_list:
                logger_phase3.log(f"ERROR: QID {qid} - Map step produced no valid intermediate themes.")
                # ... (store error status and continue) ...
                overall_thematic_results_p3[qid] = {
                    "question_text": question_actual_text_p3, "status": "error_map_step_no_themes",
                    "parsed_top_themes": [], "llm_raw_output_map_chunks": raw_map_outputs_list, "llm_raw_output_final": "N/A - Map failed"
                }
                # ... (save and continue logic from before) ...
                continue


            combined_intermediate_themes_for_reduce = "\n\n".join(all_intermediate_themes_text_list)
            logger_phase3.log(f"INFO: QID {qid} - REDUCE Step: Processing {len(all_intermediate_themes_text_list)} sets of intermediate themes (Total chars: {len(combined_intermediate_themes_for_reduce)}).")

            # **CRITICAL**: Check if combined_intermediate_themes_for_reduce is too long for the reduce LLM call
            reduce_input_tokens = llm_thematic_pipe_p3.tokenizer.encode(combined_intermediate_themes_for_reduce, add_special_tokens=False)
            if len(reduce_input_tokens) > PHASE3_MAPREDUCE_MAX_TOKENS_PER_INPUT_CHUNK * 2.0 : # Allow reduce input to be larger, e.g. 2x map chunk, but still limited
                logger_phase3.log(f"WARN: QID {qid} - Combined intermediate themes ({len(reduce_input_tokens)} tokens) too long for a single Reduce step. Further recursive reduction or summarization of intermediate themes would be needed for optimal results. Proceeding with potentially truncated/problematic input for now.")
                # Implement more robust handling here if this becomes common:
                # e.g., chunk combined_intermediate_themes_for_reduce and do multiple reduce calls, then a final reduce.
                # For now, we pass it as is, LLM might truncate or error.

            final_llm_themes_output = get_llm_thematic_points(
                combined_intermediate_themes_for_reduce,
                question_actual_text_p3,
                PHASE3_NUM_THEMES_TO_REQUEST, # Final number of themes
                llm_thematic_pipe_p3,
                PHASE3_MAPREDUCE_REDUCE_STEP_MAX_NEW_TOKENS,
                f"{qid}_reduce_step",
                map_reduce_step_type="REDUCE_INTERMEDIATE_THEMES"
            )
            llm_raw_final_output_for_storage = final_llm_themes_output
        else: # Only one batch, so effectively no reduce step needed, direct processing.
            logger_phase3.log(f"INFO: QID {qid} - Only one batch ({len(map_batches[0])} items). Processing directly (no separate reduce step).")
            direct_input_text = "\n\n--- Next Excerpt ---\n".join(map_batches[0])
            final_llm_themes_output = get_llm_thematic_points(
                direct_input_text,
                question_actual_text_p3,
                PHASE3_NUM_THEMES_TO_REQUEST,
                llm_thematic_pipe_p3,
                PHASE3_LLM_MAX_NEW_TOKENS_FOR_THEMES, # Max new tokens for final output
                qid,
                map_reduce_step_type="DIRECT_SINGLE_BATCH" # Or a unique type
            )
            llm_raw_final_output_for_storage = final_llm_themes_output
            raw_map_outputs_list.append({"batch_index": 0, "raw_output": final_llm_themes_output})


        parsed_themes_list = parse_llm_themes(final_llm_themes_output, PHASE3_NUM_THEMES_TO_REQUEST)
        # ... (status setting and saving overall_thematic_results_p3, adapted for map-reduce logging)
        status_p3 = "success_thematic_analysis"
        if final_llm_themes_output.startswith("LLM_ERROR:"): status_p3 = "error_llm_theme_generation_final"
        elif not parsed_themes_list or (len(parsed_themes_list) == 1 and parsed_themes_list[0]["theme_label"] == "Unparsed LLM Output"):
            status_p3 = "llm_final_output_parsing_failed_or_empty"

        overall_thematic_results_p3[qid] = {
            "question_text": question_actual_text_p3,
            "status": status_p3,
            "total_source_text_items_for_qid": len(text_items_for_this_qid_p3), # From Stage 4a
            "map_reduce_applied": len(map_batches) > 1,
            "num_map_batches": len(map_batches),
            "llm_raw_output_map_batches": raw_map_outputs_list,
            "llm_raw_output_final_synthesis": llm_raw_final_output_for_storage,
            "parsed_top_themes": parsed_themes_list
        }
        # ... (save intermediate results to JSON logic) ...
        try:
            with open(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON, 'w', encoding='utf-8') as f_out_p3:
                json.dump(overall_thematic_results_p3, f_out_p3, indent=2, default=logger_phase3._get_serializer()) # Use logger's serializer
            logger_phase3.log(f"INFO: Saved/Updated thematic analysis results to '{PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}' after QID {qid}.")
        except Exception as e_save_p3:
            logger_phase3.log(f"ERROR: Could not save intermediate results for Phase 3 (QID {qid}): {e_save_p3}")


        gc.collect()
        if torch.cuda.is_available(): torch.cuda.empty_cache()

    logger_phase3.log("=" * 50); logger_phase3.log(" INFO: Phase 3 (LLM Thematic Analysis - Refined Map-Reduce) Finished "); logger_phase3.log("=" * 50)
    return True

# print("INFO: Cell 8 - Logic for Phase 3 (LLM Thematic Analysis) with Refined Map-Reduce defined.")
# To run Phase 3:
# run_phase3_llm_thematic_analysis()

# @title Get the config of LLM
from transformers import AutoConfig

model_id = "google/gemma-3-4b-it" # Or your actual model ID
try:
    config = AutoConfig.from_pretrained(model_id)

    # Common attribute names for max sequence length / context window:
    max_len_attrs = [
        "max_position_embeddings",
        "n_positions",
        "max_sequence_length",
        "model_max_length" # Often found in tokenizer_config.json too
    ]

    context_window = None
    for attr in max_len_attrs:
        if hasattr(config, attr):
            context_window = getattr(config, attr)
            print(f"Found context window attribute '{attr}': {context_window}")
            break

    if context_window is None:
        print(f"Could not definitively determine context window from config attributes: {max_len_attrs}")
        print("Full config:", config) # Print full config to inspect manually

except Exception as e:
    print(f"Error loading config for {model_id}: {e}")

"""# 16th May

## New Stage 4: Question-Centric PDF Thematic Summarization.
"""

# @title Cell 0: Initial Setup & Global Imports

# --- Basic Python and System ---
import json
import os
import sys
import traceback
import time
import gc
import re
from collections import defaultdict, Counter
from typing import Dict, Any, List, Tuple, Optional
import warnings
import argparse # Kept for potential future use
import tempfile # Needed for test mode

# --- ML/NLP Core Imports ---
import torch
import numpy as np
import nltk

# --- Hugging Face Transformers & Related ---
from sentence_transformers import SentenceTransformer, util as sbert_util
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

# --- Suppress Warnings (Optional) ---
warnings.filterwarnings("ignore", message=".*Using the latest cached version.*")
warnings.filterwarnings("ignore", message=".*torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.*")

# --- BitsAndBytes (Optional, for Quantization) ---
try:
    from bitsandbytes.cextension import ColabNotSupportedError
except ImportError:
    ColabNotSupportedError = None # Define dummy class if not importable
    print("WARN: bitsandbytes library not found. Quantization will be disabled if requested.")
    pass

print("INFO: Cell 0 - Initial imports complete.")

# @title Cell 1: FileLogger Class & NLTK Downloads

# =====================================================
#                Custom FileLogger Class
# =====================================================
class FileLogger:
    def __init__(self, log_file_path): # Renamed log_file to log_file_path for clarity
        self.log_file_path = log_file_path
        log_dir = os.path.dirname(log_file_path)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)
        try:
            with open(self.log_file_path, 'w', encoding='utf-8') as f:
                f.write(f"{self._get_timestamp()}: Starting new log at {self.log_file_path}\n")
            print(f"INFO: Logger initialized. Log file: {self.log_file_path}")
        except IOError as e:
            print(f"CRITICAL: Failed to initialize log file {self.log_file_path}: {e}")

    def _get_timestamp(self):
        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())

    def log(self, message: str):
        try:
            with open(self.log_file_path, 'a', encoding='utf-8') as f:
                f.write(f"{self._get_timestamp()}: {message}\n")
        except Exception as e:
            print(f"!!! LOGGER FAILED TO WRITE TO {self.log_file_path}: {e} !!!")
            print(f"{self._get_timestamp()}: {message}") # Fallback print

    def _get_serializer(self):
        def default_serializer(obj):
            if isinstance(obj, (np.integer, np.floating)): return obj.item()
            if isinstance(obj, np.ndarray): return obj.tolist()
            if isinstance(obj, set): return sorted(list(obj))
            if 'torch' in sys.modules and isinstance(obj, torch.Tensor):
                return obj.detach().cpu().numpy().tolist()
            try:
                return json.JSONEncoder().default(obj)
            except TypeError:
                return f"UNSERIALIZABLE:{type(obj).__name__}"
        return default_serializer

    def log_json(self, data: Dict[str, Any], prefix="JSON_DATA"):
        serializer = self._get_serializer()
        log_entry = {"timestamp": self._get_timestamp(), "type": prefix, "data": data}
        try:
            json_string = json.dumps(log_entry, default=serializer, indent=2)
            with open(self.log_file_path, 'a', encoding='utf-8') as f:
                f.write(json_string + "\n")
        except Exception as e:
            self.log(f"ERROR: Unexpected error during log_json (prefix={prefix}): {e}")
            self.log(f"TRACEBACK for log_json error:\n{traceback.format_exc()}")


# =====================================================
#         NLTK Setup
# =====================================================
def download_nltk_resource_with_logger(logger_instance, resource_id, download_name):
    """Checks for an NLTK resource and downloads it if missing, using provided logger."""
    try:
        nltk.data.find(resource_id)
    except LookupError:
        logger_instance.log(f"INFO: NLTK resource '{download_name}' not found. Downloading...")
        try:
            nltk.download(download_name, quiet=True)
            logger_instance.log(f"INFO: Downloaded NLTK resource: {download_name}")
        except Exception as dl_e:
            logger_instance.log(f"ERROR: Failed NLTK download {download_name}: {dl_e}")
    except Exception as e:
        logger_instance.log(f"ERROR: NLTK check error {download_name}: {e}")

# Initialize a temporary logger for NLTK downloads if main logger isn't ready
# Or, ensure this cell runs after the main logger for a stage is initialized.
# For now, using print for this self-contained NLTK setup.
print("INFO: Checking/Downloading NLTK resources (using print for this setup block)...")
def temp_nltk_download(resource_id, download_name):
    try: nltk.data.find(resource_id)
    except LookupError:
        print(f"INFO: NLTK (temp): Downloading {download_name}...")
        nltk.download(download_name, quiet=True)
temp_nltk_download('tokenizers/punkt', 'punkt')
temp_nltk_download('tokenizers/punkt_tab', 'punkt_tab')
temp_nltk_download('corpora/stopwords', 'stopwords')
print("INFO: NLTK resource check complete for Cell 1.")

# @title Verify environment
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU name: {torch.cuda.get_device_name(0)}")

try:
    import bitsandbytes
    print("Successfully imported bitsandbytes!")
    # You can now try using a feature that requires GPU bitsandbytes,
    # e.g., loading a model with load_in_8bit=True from Hugging Face transformers
except Exception as e:
    print(f"Error importing bitsandbytes or bitsandbytes GPU features not available: {e}")
    print("Make sure you have:")
    print("1. Enabled GPU in Runtime > Change runtime type.")
    print("2. Installed bitsandbytes with !pip install bitsandbytes.")
    print("3. RESTARTED THE RUNTIME after installation.")

# @title Cell 2: Configuration for Phase 1 (Original Stage 4)

# --- Global Path Configuration (UPDATE THESE!) ---
# Assuming Colab environment and Google Drive mounted at /content/drive
BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # Main project folder

# --- Input Files for Phase 1 ---
PHASE1_ORIGINAL_RESPONSES_FILE = os.path.join(BASE_PROJECT_DIR, 'processed_responses.jsonl') # Source for raw data + PDF text
PHASE1_STAGE2_AGGREGATION_FILE = os.path.join(BASE_PROJECT_DIR, 'question_centric_aggregation_output.json') # Source for QID->Text map (ensure this exists or is created by your form-only Stage 2)

# --- Output & State Files for Phase 1 ---
# Directory to store chunked outputs of Phase 1 if it processes in chunks
PHASE1_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase1_OriginalStage4_Outputs/')
os.makedirs(PHASE1_OUTPUT_DIR, exist_ok=True)
# Example: PHASE1_PDF_ANALYSIS_BASE_FILENAME = 'pdf_passage_analysis_chunk' # Will be PHASE1_OUTPUT_DIR/pdf_passage_analysis_chunk_N.json
# For simplicity, if your original Stage 4 outputs to one potentially large file that it appends to or manages state for:
PHASE1_SINGLE_OUTPUT_FILE = os.path.join(PHASE1_OUTPUT_DIR, 'pdf_passage_analysis_by_response_FULL.json') # If it produces ONE file
PHASE1_STATE_FILE = os.path.join(BASE_PROJECT_DIR, 'phase1_original_stage4_processing_state.json')
PHASE1_LOG_FILE = os.path.join(BASE_PROJECT_DIR, "phase1_original_stage4_pdf_extraction.log")

# --- Model Configuration for Phase 1 ---
PHASE1_EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'
PHASE1_LLM_MODEL_ID = 'google/gemma-3-4b-it' # Changed from 3-4b-it for wider compatibility, adjust if needed
PHASE1_USE_QUANTIZATION = False

# --- Retrieval & Summarization Parameters for Phase 1 ---
PHASE1_MIN_SIMILARITY_SCORE = 0.5
PHASE1_CONTEXT_WINDOW_SIZE_SENTENCES = 1 # sentences before/after for passage
PHASE1_TOP_K_PASSAGES = 3
PHASE1_NUM_SUMMARY_SENTENCES_PER_RESPONSE_QID = 3
PHASE1_MIN_CHARS_FOR_SUMMARY = 150
PHASE1_LLM_MAX_NEW_TOKENS = 250 # Adjusted for 3 sentences from Gemma 2B

# --- PDF Processing for Phase 1 ---
PHASE1_MIN_PDF_TEXT_LENGTH = 100

# --- Runtime & Logging for Phase 1 ---
PHASE1_MAX_RUNTIME_MINUTES = 60 * 60 * 8 # e.g., 8 hours (adjust per run)
PHASE1_SAVE_STATE_INTERVAL_SECONDS = 60 * 20 # Save progress every 20 minutes
PHASE1_ENABLE_DEBUG_LOGGING = True # Set True for very detailed logs, False for production runs
PHASE1_TEST_SINGLE_RESPONSE_ID = None # Set to an ID for testing, None for full run

# --- Question Selection for Per-Response Summarization in Phase 1 ---
# This determines which QIDs will have summaries generated *within each response's PDF analysis*
PHASE1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE = {
    "Q1": True, "Q2": True, "Q3": True, "Q4": True, "Q5": True, "Q6": True, "Q7": True,
    "Q8": True, "Q9": True, "Q10": True,"Q11": True, "Q12": True, "Q13": True, "Q14": True,
    "Q15": True, "Q16": False,"Q17": True, "Q18": True, "Q19": True,"Q20": False, "Q21": True,
    "Q22": True, "Q23": True, "Q24": True, "Q25": True, "Q26": True, "Q27": True, "Q28": True,
    "Q29": True, "Q30": True, "Q31": True,"Q32": False,"Q33": False,"Q34": False,"Q35": False,
    "Q36": False,
}

print("INFO: Cell 2 - Configuration for Phase 1 (Original Stage 4) loaded.")

# @title Cell 3: Helper Functions for Phase 1 (Original Stage 4)
# These are adapted from your unstructured_text_response_centric_uds_v2_py.txt

# (Global logger for this Phase)
logger_phase1 = FileLogger(PHASE1_LOG_FILE)

def save_phase1_output_chunk(output_dir, chunk_index, output_data):
    """Saves a chunk of Phase 1 output."""
    # This function needs to be designed based on how Original Stage 4 chunks.
    # If it produces one large resumable file, this isn't needed.
    # If it processes N responses and saves, this is where it saves.
    # For now, assuming the main loop of Original Stage 4 will call a general save function.
    # This function is more conceptual if your original Stage 4 saves its full state.
    filename = os.path.join(output_dir, f"pdf_passage_analysis_chunk_{chunk_index}.json")
    logger_phase1.log(f"INFO: Saving Phase 1 output chunk to {filename}...")
    serializer = logger_phase1._get_serializer() # Use logger's serializer
    try:
        final_output = {"pdf_analysis_by_response": output_data}
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, indent=2, default=serializer)
        logger_phase1.log(f"INFO: Successfully saved Phase 1 chunk output.")
    except Exception as e:
        logger_phase1.log(f"ERROR: Saving Phase 1 chunk output failed: {e}")
        logger_phase1.log(traceback.format_exc())

def save_phase1_state(state_file, processed_response_ids_set, current_results_dict):
    """Saves the current processing state for Phase 1."""
    logger_phase1.log(f"INFO: Saving Phase 1 processing state to {state_file}...")
    try:
        state_data = {
            "processed_responses_ids": sorted(list(processed_response_ids_set)),
            "results_so_far": current_results_dict # This could be large if not careful
        }
        # Atomic save: write to temp then rename
        temp_state_file = state_file + ".tmp"
        with open(temp_state_file, 'w', encoding='utf-8') as f:
            json.dump(state_data, f, indent=2, default=logger_phase1._get_serializer())
        os.replace(temp_state_file, state_file)
        logger_phase1.log(f"INFO: Phase 1 State saved successfully ({len(processed_response_ids_set)} responses processed).")
    except Exception as e:
        logger_phase1.log(f"ERROR: Failed to save Phase 1 state to {state_file}: {e}")
        logger_phase1.log(traceback.format_exc())

def load_phase1_state(state_file):
    """Loads Phase 1 processing state from a file."""
    if os.path.exists(state_file):
        logger_phase1.log(f"INFO: Found Phase 1 state file {state_file}. Attempting to load state...")
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                state_data = json.load(f)
            processed_ids = set(state_data.get("processed_responses_ids", []))
            results = state_data.get("results_so_far", {}) # Results loaded from previous run
            if not isinstance(results, dict):
                logger_phase1.log("ERROR: Loaded 'results_so_far' is not a dictionary. Starting fresh.")
                return set(), {}
            logger_phase1.log(f"INFO: Successfully loaded Phase 1 state. Resuming after {len(processed_ids)} processed responses.")
            return processed_ids, results
        except Exception as e:
            logger_phase1.log(f"ERROR: Failed to load/parse Phase 1 state from {state_file}: {e}. Starting fresh.")
            return set(), {}
    else:
        logger_phase1.log("INFO: No Phase 1 state file found. Starting fresh.")
        return set(), {}

def get_phase1_local_embeddings(texts: List[str], model: SentenceTransformer, device: str, batch_size: int = 32):
    if not texts: return None
    try:
        if PHASE1_ENABLE_DEBUG_LOGGING: logger_phase1.log(f"DEBUG: Phase 1 - Generating local embeddings for {len(texts)} texts on {device}...")
        model.to(device) # Ensure model is on the correct device
        # embeddings = model.encode(texts, convert_to_tensor=True, device=device, show_progress_bar=False, batch_size=batch_size)
        # Detach from graph and move to CPU before converting to numpy
        # return embeddings.detach().cpu().numpy()

        # Corrected sbert usage
        embeddings_tensor = model.encode(texts, convert_to_tensor=True, show_progress_bar=PHASE1_ENABLE_DEBUG_LOGGING, batch_size=batch_size, device=device)
        return embeddings_tensor.cpu().numpy() # No need to detach if not part of a computation graph being tracked
    except Exception as e:
        logger_phase1.log(f"ERROR: Phase 1 - Failed to generate local embeddings: {e}")
        logger_phase1.log(traceback.format_exc())
        return None

def retrieve_phase1_relevant_passages(question_embedding: np.ndarray,
                                     pdf_sentences: List[str],
                                     pdf_sentence_embeddings: np.ndarray,
                                     similarity_threshold: float,
                                     top_k: int,
                                     context_window_sentences: int):
    relevant_passages_texts = []
    highest_score_found = None
    if question_embedding is None or pdf_sentence_embeddings is None or not pdf_sentences:
        return relevant_passages_texts, highest_score_found

    # Ensure question_embedding is 2D
    if question_embedding.ndim == 1:
        question_embedding = question_embedding.reshape(1, -1)

    try:
        # Convert numpy arrays to PyTorch tensors for sbert_util.cos_sim
        q_tensor = torch.from_numpy(question_embedding).to(dtype=torch.float32)
        s_tensor = torch.from_numpy(pdf_sentence_embeddings).to(dtype=torch.float32)

        # Move tensors to the same device (e.g., CPU if one is on CPU and other on GPU, or consistent GPU)
        # Assuming q_tensor and s_tensor might be on different devices or need to be moved to where SentenceTransformer model ran
        target_device = s_tensor.device # Assume sentence embeddings are on the target device
        q_tensor = q_tensor.to(target_device)

        cosine_scores_tensor = sbert_util.cos_sim(q_tensor, s_tensor)[0] # Get the first row of scores
        cosine_scores = cosine_scores_tensor.cpu().numpy() # Move to CPU for numpy operations

        # Sort scores in descending order and get indices
        sorted_indices = np.argsort(cosine_scores)[::-1]

        added_passage_hashes = set() # To avoid duplicate passages from overlapping contexts

        for idx in sorted_indices:
            score = cosine_scores[idx]
            if score < similarity_threshold:
                break # Scores are sorted, no need to check further

            if highest_score_found is None: # Capture the top score
                highest_score_found = float(score)

            start_idx = max(0, idx - context_window_sentences)
            end_idx = min(len(pdf_sentences), idx + context_window_sentences + 1)

            context_passage_list = pdf_sentences[start_idx:end_idx]
            passage_text = " ".join(context_passage_list).strip()
            passage_hash = hash(passage_text)

            if passage_text and passage_hash not in added_passage_hashes:
                relevant_passages_texts.append(passage_text)
                added_passage_hashes.add(passage_hash)
                if len(relevant_passages_texts) >= top_k:
                    break

        return relevant_passages_texts, highest_score_found

    except Exception as e:
        logger_phase1.log(f"ERROR: Phase 1 - Failed during passage retrieval: {e}")
        logger_phase1.log(traceback.format_exc())
        return [], None


def summarize_phase1_context_block(text_chunk: str, question_text_for_prompt: str, qid_for_log: str,
                                 llm_pipe: pipeline, num_target_sentences: int, max_new_tokens_for_summary: int):
    if not text_chunk or not llm_pipe: return None

    messages = [{
        "role": "user",
        "content": f"Carefully review the following text excerpt which is in response to the question: \"{question_text_for_prompt}\". "
                   f"Concisely summarize the main points from this excerpt in approximately {num_target_sentences} distinct sentences. "
                   f"Focus on capturing the essence of the input text relevant to the question.\n\nExcerpt:\n\"\"\"\n{text_chunk}\n\"\"\""
    }]

    try:
        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        generation_args = {
            "max_new_tokens": max_new_tokens_for_summary,
            "do_sample": False, # For deterministic summaries
            "pad_token_id": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id,
        }

        if PHASE1_ENABLE_DEBUG_LOGGING:
            logger_phase1.log(f"DEBUG: Phase 1 - Calling LLM for per-response summary (QID {qid_for_log}, {len(text_chunk)} chars input)...")

        outputs = llm_pipe(prompt_formatted, **generation_args)

        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):
            logger_phase1.log(f"ERROR: Phase 1 - LLM pipeline returned unexpected/empty output for QID {qid_for_log}: {outputs}")
            return None

        generated_text_full = outputs[0]['generated_text']
###
        if PHASE1_ENABLE_DEBUG_LOGGING or not summary_text.strip(): # Log if debug or if summary ends up empty
            logger_phase1.log(f"DEBUG/WARN: QID {qid_for_log} - LLM Raw Output (generated_text_full):\nSOF<<<<<\n{generated_text_full}\n>>>>>EOF")
###
        # Extract summary (logic from your original Stage 4)
        summary_text = ""
        if generated_text_full.startswith(prompt_formatted):
            summary_text = generated_text_full[len(prompt_formatted):].strip()
        else:
            response_marker = "<start_of_turn>model\n" # Specific to Gemma instruct
            summary_start_index = generated_text_full.rfind(response_marker)
            if summary_start_index != -1:
                summary_text = generated_text_full[summary_start_index + len(response_marker):].strip()
            else:
                logger_phase1.log(f"WARN: Phase 1 - Could not find standard model response marker for QID {qid_for_log}. Using full output minus prompt attempt.")
                # Fallback: try to remove the known prompt string if it's not an exact start match (less reliable)
                # This might not be perfect if the LLM slightly alters the prompt start in its echo.
                if prompt_formatted in generated_text_full:
                    summary_text = generated_text_full.split(prompt_formatted, 1)[-1].strip()
                else:
                    summary_text = generated_text_full # Best guess

        if summary_text.startswith("Error:") or not summary_text.strip():
            logger_phase1.log(f"WARN: Phase 1 - Final extracted summary for QID {qid_for_log} is empty or indicates error.")
            return ""
        return summary_text

    except Exception as e:
        logger_phase1.log(f"ERROR: Phase 1 - Exception during LLM summarization for QID {qid_for_log}: {e}")
        logger_phase1.log(traceback.format_exc())
        return None

print("INFO: Cell 3 - Helper functions for Phase 1 (Original Stage 4) defined.")

# @title Cell 4: Main Execution Logic for Phase 1 (Original Stage 4)

def run_phase1_pdf_extraction():
    global logger_phase1 # Use the logger initialized for this phase
    logger_phase1.log("=" * 50)
    logger_phase1.log(" INFO: Starting Phase 1: PDF Content Extraction (Original Stage 4 Logic)")
    logger_phase1.log("=" * 50)
    # Log key configurations
    logger_phase1.log(f"INFO: Input Original Responses (JSONL): {PHASE1_ORIGINAL_RESPONSES_FILE}")
    logger_phase1.log(f"INFO: Input QID Map (Stage 2 Form Output): {PHASE1_STAGE2_AGGREGATION_FILE}")
    logger_phase1.log(f"INFO: Output Directory for Chunks: {PHASE1_OUTPUT_DIR} (or single file: {PHASE1_SINGLE_OUTPUT_FILE})") # Adjust log based on actual save strategy
    logger_phase1.log(f"INFO: State File: {PHASE1_STATE_FILE}")
    logger_phase1.log(f"INFO: Embedding Model: {PHASE1_EMBEDDING_MODEL_NAME}")
    logger_phase1.log(f"INFO: LLM Model for Per-Response Summaries: {PHASE1_LLM_MODEL_ID}")
    logger_phase1.log(f"INFO: Use Quantization: {PHASE1_USE_QUANTIZATION}")

    # --- Determine Device ---
    phase1_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger_phase1.log(f"INFO: Phase 1 using device: {phase1_device}")

    # --- Load Models ---
    embedding_model_p1 = None
    llm_tokenizer_p1 = None
    llm_model_p1 = None
    llm_summarization_pipe_p1 = None
    models_loaded_successfully_p1 = False

    # Create a local variable to store the *effective* quantization state for this run.
    # Initialize it with the global configuration value.
    current_run_use_quantization = PHASE1_USE_QUANTIZATION # Local variable for this function's scope

    try:
        logger_phase1.log(f"INFO: Loading embedding model: {PHASE1_EMBEDDING_MODEL_NAME}...")
        embedding_model_p1 = SentenceTransformer(PHASE1_EMBEDDING_MODEL_NAME, device=phase1_device)
        logger_phase1.log("INFO: Embedding model loaded.")

        logger_phase1.log(f"INFO: Loading LLM tokenizer for: {PHASE1_LLM_MODEL_ID}...")
        llm_tokenizer_p1 = AutoTokenizer.from_pretrained(PHASE1_LLM_MODEL_ID)
        if getattr(llm_tokenizer_p1, 'pad_token', None) is None and getattr(llm_tokenizer_p1, 'eos_token', None):
            llm_tokenizer_p1.pad_token = llm_tokenizer_p1.eos_token
            llm_tokenizer_p1.padding_side = "left" # Typical for Causal LMs
        logger_phase1.log("INFO: LLM Tokenizer loaded.")

        bnb_config_p1 = None
        if current_run_use_quantization and torch.cuda.is_available():
            logger_phase1.log("INFO: Setting up 4-bit quantization for LLM...")
            try:
                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
                bnb_config_p1 = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=compute_dtype,
                    bnb_4bit_use_double_quant=True
                )
                logger_phase1.log(f"INFO: 4-bit quantization configured (Compute dtype: {compute_dtype}).")
            except Exception as q_err:
                logger_phase1.log(f"ERROR: Failed BitsAndBytesConfig: {q_err}. Disabling quantization for LLM.")
                current_run_use_quantization = False # Modify ONLY the local variable

        logger_phase1.log(f"INFO: Loading LLM model: {PHASE1_LLM_MODEL_ID} (Quantization: {current_run_use_quantization})...")
        llm_model_p1 = AutoModelForCausalLM.from_pretrained(
            PHASE1_LLM_MODEL_ID,
            quantization_config=bnb_config_p1 if PHASE1_USE_QUANTIZATION else None,
            torch_dtype=torch.bfloat16 if not PHASE1_USE_QUANTIZATION else None, # Gemma prefers bfloat16
            device_map="auto", # Handles multi-GPU or CPU if CUDA not available for model
            trust_remote_code=True # Often needed for Gemma
        )
        logger_phase1.log("INFO: LLM model loaded.")

        llm_summarization_pipe_p1 = pipeline(
            "text-generation",
            model=llm_model_p1,
            tokenizer=llm_tokenizer_p1,
            # device handled by device_map
        )
        logger_phase1.log("INFO: LLM text-generation pipeline created successfully.")
        models_loaded_successfully_p1 = True

    except Exception as load_err:
        logger_phase1.log(f"CRITICAL: Failed to load models/pipeline for Phase 1: {load_err}")
        logger_phase1.log(traceback.format_exc())
        # Depending on requirements, you might exit or try to proceed without LLM summarization
        # For now, let's assume if LLM fails, we can't do per-response summaries but can still extract passages.
        # The main loop will need to handle llm_summarization_pipe_p1 being None.
        llm_summarization_pipe_p1 = None # Ensure it's None if loading failed

    # --- Load QID to Text map and Pre-compute Question Embeddings ---
    qid_to_text_map_p1 = {}
    qid_to_embedding_map_p1 = {}
    try:
        with open(PHASE1_STAGE2_AGGREGATION_FILE, 'r', encoding='utf-8') as f_s2:
            stage2_data = json.load(f_s2)
        for qid, data in stage2_data.get("question_data", {}).items():
            if "question_text" in data:
                qid_to_text_map_p1[qid] = data["question_text"]
        if not qid_to_text_map_p1: raise ValueError("No question texts found in Stage 2 aggregation file.")
        logger_phase1.log(f"INFO: Loaded text for {len(qid_to_text_map_p1)} questions.")

        target_qids_p1 = list(qid_to_text_map_p1.keys())
        target_qtexts_p1 = list(qid_to_text_map_p1.values())
        if embedding_model_p1: # Only if embedding model loaded
            question_embeddings_all_p1 = get_phase1_local_embeddings(target_qtexts_p1, embedding_model_p1, phase1_device)
            if question_embeddings_all_p1 is None:
                raise ValueError("Failed to generate question embeddings.")
            qid_to_embedding_map_p1 = {qid: emb for qid, emb in zip(target_qids_p1, question_embeddings_all_p1)}
            logger_phase1.log(f"INFO: Generated embeddings for {len(qid_to_embedding_map_p1)} questions.")
        else: # Critical if embedding model failed
            logger_phase1.log("CRITICAL: Embedding model not loaded. Cannot perform PDF passage retrieval. Exiting Phase 1.")
            return False # Indicate failure

    except Exception as e_qload:
        logger_phase1.log(f"ERROR: Failed loading question texts/embeddings for Phase 1: {e_qload}")
        logger_phase1.log(traceback.format_exc())
        return False # Indicate failure

    # --- Load State for Resumption ---
    # `results_p1` will store {"response_id": {"QID": {"summary": ..., "passages": ...}}}
    processed_response_ids_p1, results_p1 = load_phase1_state(PHASE1_STATE_FILE)

    # --- Determine Input File for Iteration (Original or Temp for Test) ---
    input_file_for_iteration_p1 = PHASE1_ORIGINAL_RESPONSES_FILE
    # Logic for TEST_SINGLE_RESPONSE_ID (creates temp file)
    # This part of your original Stage 4 can be reused here if needed for testing.
    # For brevity, assuming full run for now. If TEST_SINGLE_RESPONSE_ID is set,
    # input_file_for_iteration_p1 would point to the temp file with that single response.
    # Ensure this temp file creation uses PHASE1_ORIGINAL_RESPONSES_FILE as its source.

    # --- Main Processing Loop (Iterating through responses) ---
    processed_in_this_run_p1 = 0
    pdfs_analyzed_this_run_p1 = 0
    start_processing_time_p1 = time.time()
    last_save_time_p1 = start_processing_time_p1
    MAX_RUNTIME_SECONDS_P1 = PHASE1_MAX_RUNTIME_MINUTES * 60

    logger_phase1.log(f"INFO: Starting Phase 1 processing loop for {input_file_for_iteration_p1}...")
    try:
        # It's often better to read large JSONL files line by line
        # rather than loading all responses into memory at once.
        with open(input_file_for_iteration_p1, 'r', encoding='utf-8') as infile:
            for line_num, line in enumerate(infile, 1):
                # Runtime Limit Check
                if time.time() - start_processing_time_p1 >= MAX_RUNTIME_SECONDS_P1:
                    logger_phase1.log(f"WARN: Phase 1 reached runtime limit. Stopping.")
                    break
                # Periodic State Saving
                if time.time() - last_save_time_p1 >= PHASE1_SAVE_STATE_INTERVAL_SECONDS:
                    save_phase1_state(PHASE1_STATE_FILE, processed_response_ids_p1, results_p1)
                    last_save_time_p1 = time.time()

                try:
                    response_obj_p1 = json.loads(line)
                    response_id_p1 = str(response_obj_p1.get("id", f"UnknownLine{line_num}"))

                    if response_id_p1 in processed_response_ids_p1:
                        continue # Skip already processed

                    results_p1[response_id_p1] = {} # Initialize dict for this response's QID results

                    pdf_text_p1 = response_obj_p1.get("pdf_text")
                    if not pdf_text_p1 or len(pdf_text_p1) < PHASE1_MIN_PDF_TEXT_LENGTH:
                        results_p1[response_id_p1] = {"status": "pdf_missing_or_short"}
                        processed_response_ids_p1.add(response_id_p1)
                        continue

                    # Get or process PDF sentences and embeddings
                    # Reusing the caching function from your "Revised Stage 4" idea is good here
                    # but original Stage 4 processes one PDF fully then moves to next.
                    # So, no cross-PDF caching needed *within this Stage 4 response loop itself*.
                    # The caching is beneficial if Stage 4 itself is restarted for the same PDF.

                    _cleaned_pdf_text_p1 = re.sub(r'\s+', ' ', pdf_text_p1).strip()
                    _pdf_sentences_p1 = nltk.sent_tokenize(_cleaned_pdf_text_p1)
                    _pdf_sentences_p1 = [s.strip() for s in _pdf_sentences_p1 if s.strip()]

                    _pdf_sentence_embeddings_p1 = None
                    if _pdf_sentences_p1 and embedding_model_p1:
                         _pdf_sentence_embeddings_p1 = get_phase1_local_embeddings(
                             _pdf_sentences_p1, embedding_model_p1, phase1_device
                         )

                    if not _pdf_sentences_p1 or _pdf_sentence_embeddings_p1 is None:
                        results_p1[response_id_p1] = {"status": "pdf_no_sentences_or_embedding_error"}
                        processed_response_ids_p1.add(response_id_p1)
                        continue

                    pdfs_analyzed_this_run_p1 +=1

                    # Inner loop for QIDs for this response's PDF
                    for qid_p1 in target_qids_p1:
                        q_text_p1 = qid_to_text_map_p1[qid_p1]
                        q_emb_p1 = qid_to_embedding_map_p1.get(qid_p1)

                        current_q_result_p1 = {"question_text": q_text_p1}
                        if q_emb_p1 is None:
                            current_q_result_p1["status"] = "error_missing_q_embedding"
                            results_p1[response_id_p1][qid_p1] = current_q_result_p1
                            continue

                        passages_p1, top_score_p1 = retrieve_phase1_relevant_passages(
                            q_emb_p1, _pdf_sentences_p1, _pdf_sentence_embeddings_p1,
                            PHASE1_MIN_SIMILARITY_SCORE, PHASE1_TOP_K_PASSAGES, PHASE1_CONTEXT_WINDOW_SIZE_SENTENCES
                        )

                        if passages_p1:
                            current_q_result_p1["status"] = "passages_extracted"
                            current_q_result_p1["extracted_passages"] = passages_p1
                            current_q_result_p1["top_passage_score"] = float(top_score_p1) if top_score_p1 is not None else None

                            # Conditional Per-Response/Per-QID Summarization
                            if llm_summarization_pipe_p1 and PHASE1_QUESTIONS_TO_SUMMARIZE_PER_RESPONSE.get(qid_p1, False):
                                combined_passage_text_p1 = "\n\n".join(passages_p1)
                                if len(combined_passage_text_p1) >= PHASE1_MIN_CHARS_FOR_SUMMARY:
                                    summary_p1 = summarize_phase1_context_block(
                                        combined_passage_text_p1, q_text_p1, qid_p1,
                                        llm_summarization_pipe_p1,
                                        PHASE1_NUM_SUMMARY_SENTENCES_PER_RESPONSE_QID,
                                        PHASE1_LLM_MAX_NEW_TOKENS
                                    )
                                    if summary_p1 is not None: # Check for None from error
                                        current_q_result_p1["summary"] = summary_p1
                                        if summary_p1: # If not empty string from LLM error
                                             current_q_result_p1["status"] = "success_summarized"
                                        else: # Empty string often means LLM had an issue but didn't except
                                             current_q_result_p1["status"] = "summary_attempted_empty_result"
                                    else: # Summarization function returned None (explicit error)
                                        current_q_result_p1["status"] = "error_llm_summarization_response_qid"
                                else:
                                    current_q_result_p1["status"] = "passages_extracted_too_short_for_summary"
                        else:
                            current_q_result_p1["status"] = "no_relevant_passages_found"

                        results_p1[response_id_p1][qid_p1] = current_q_result_p1

                    processed_response_ids_p1.add(response_id_p1)
                    processed_in_this_run_p1 += 1
                    if processed_in_this_run_p1 % 50 == 0:
                        logger_phase1.log(f"INFO: Phase 1 - Processed {processed_in_this_run_p1} new responses in this run...")
                        gc.collect()
                        if torch.cuda.is_available(): torch.cuda.empty_cache()


                except json.JSONDecodeError:
                    logger_phase1.log(f"WARN: Phase 1 - Skipping line {line_num} due to JSON parse error.")
                    results_p1[f"ErrorLine{line_num}"] = {"status": "error_json_decode_line"}
                except Exception as e_resp:
                    logger_phase1.log(f"ERROR: Phase 1 - Processing response line {line_num} (ID attempt: {response_id_p1 if 'response_id_p1' in locals() else 'N/A'}) failed: {e_resp}")
                    logger_phase1.log(traceback.format_exc())
                    results_p1[response_id_p1 if 'response_id_p1' in locals() else f"ErrorRespLine{line_num}"] = {"status": "error_processing_response", "error_message": str(e_resp)}
                    if 'response_id_p1' in locals() : processed_response_ids_p1.add(response_id_p1) # Mark as processed to avoid retrying broken response

    except FileNotFoundError:
        logger_phase1.log(f"ERROR: Phase 1 - Input file not found: {input_file_for_iteration_p1}")
        return False
    except Exception as e_file_loop:
        logger_phase1.log(f"ERROR: Phase 1 - Unhandled error during file processing loop: {e_file_loop}")
        logger_phase1.log(traceback.format_exc())
        # Attempt to save state even on this error
        save_phase1_state(PHASE1_STATE_FILE, processed_response_ids_p1, results_p1)
        return False
    finally:
        # Final save of state and results
        logger_phase1.log(f"INFO: Phase 1 - Loop finished. Total new responses processed in this run: {processed_in_this_run_p1}. Total PDFs analyzed: {pdfs_analyzed_this_run_p1}.")
        save_phase1_state(PHASE1_STATE_FILE, processed_response_ids_p1, results_p1)

        # Decide how to save the final "results_p1".
        # If it's one large file that "load_phase1_state" can handle, save it directly.
        # If it's meant to be chunked, this part needs to align with your original Stage 4's chunking save logic.
        # For now, let's save it as a single (potentially large) file.
        # This assumes PHASE1_SINGLE_OUTPUT_FILE is the target.
        logger_phase1.log(f"INFO: Saving all Phase 1 accumulated results to {PHASE1_SINGLE_OUTPUT_FILE}...")
        try:
            with open(PHASE1_SINGLE_OUTPUT_FILE, 'w', encoding='utf-8') as f_final:
                json.dump({"pdf_analysis_by_response": results_p1}, f_final, indent=2, default=logger_phase1._get_serializer())
            logger_phase1.log("INFO: Final Phase 1 results saved.")
        except Exception as e_final_save:
            logger_phase1.log(f"ERROR: Could not save final Phase 1 results: {e_final_save}")
            logger_phase1.log(traceback.format_exc())

        # Cleanup models
        logger_phase1.log("INFO: Phase 1 - Cleaning up models and resources...")
        del embedding_model_p1, llm_tokenizer_p1, llm_model_p1, llm_summarization_pipe_p1
        del qid_to_embedding_map_p1 #, question_embeddings_all_p1 (if defined)
        gc.collect()
        if torch.cuda.is_available(): torch.cuda.empty_cache()
        logger_phase1.log("INFO: Phase 1 - Cleanup complete.")

    logger_phase1.log("=" * 50); logger_phase1.log(" INFO: Phase 1 Processing Finished "); logger_phase1.log("=" * 50)
    return True # Indicate success

# print("INFO: Cell 4 - Main Execution Logic for Phase 1 defined.")
# To run Phase 1:
run_phase1_pdf_extraction()

# @title Cell 5: Configuration for Phase 2 (Stage 4a)

# Input for Phase 2 is the output directory from Phase 1 (or the single file if Phase 1 produced that)
PHASE2_INPUT_PHASE1_OUTPUT_PATH = PHASE1_OUTPUT_DIR # If Phase 1 saved chunks in this dir
# Or if Phase 1 saved one big file:
# PHASE2_INPUT_PHASE1_OUTPUT_PATH = PHASE1_SINGLE_OUTPUT_FILE

PHASE2_OUTPUT_COLLATED_JSON = os.path.join(BASE_PROJECT_DIR, f'phase2_stage4a_collated_pdf_texts_{time.strftime("%Y%m%d_%H%M%S")}.json')
PHASE2_LOG_FILE = os.path.join(BASE_PROJECT_DIR, f"phase2_stage4a_text_collation_{time.strftime('%Y%m%d_%H%M%S')}.log")

print("INFO: Cell 5 - Configuration for Phase 2 (Stage 4a) loaded.")

# @title Cell 6: Helper Functions & Main Logic for Phase 2 (Stage 4a)
# This is the script for "extract_and_aggregate_content_to_json" we developed.
import glob # <--- ADD THIS LINE

logger_phase2 = FileLogger(PHASE2_LOG_FILE) # Logger for this phase

# Re-define clean_text or ensure it's available globally
def clean_text_phase2(text):
    if not text or not isinstance(text, str): return ""
    text = re.sub(r'\s*\n\s*', '\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    return text.strip()

def run_phase2_collate_pdf_texts(input_phase1_path, output_collated_json_file):
    global logger_phase2
    logger_phase2.log("=" * 50); logger_phase2.log(" INFO: Starting Phase 2: PDF Text Collation (Stage 4a Logic)"); logger_phase2.log("=" * 50)

    # Copied and adapted from the script "extract_and_aggregate_content_to_json"
    if not os.path.exists(input_phase1_path):
        logger_phase2.log(f"ERROR: Input path '{input_phase1_path}' for Phase 1 outputs does not exist.")
        return False

    output_dir_p2 = os.path.dirname(output_collated_json_file)
    if output_dir_p2 and not os.path.exists(output_dir_p2):
        os.makedirs(output_dir_p2, exist_ok=True)
        logger_phase2.log(f"INFO: Created output directory for Phase 2: '{output_dir_p2}'")

    files_to_process_p2 = []
    if os.path.isfile(input_phase1_path):
        files_to_process_p2 = [input_phase1_path]
    elif os.path.isdir(input_phase1_path):
        # This glob should match the output file names from your Original Stage 4 if it chunks,
        # or the single file if it produces one.
        files_to_process_p2 = glob.glob(os.path.join(input_phase1_path, 'pdf_passage_analysis*.json'))
        if not files_to_process_p2: # Fallback
             files_to_process_p2 = glob.glob(os.path.join(input_phase1_path, '*.json'))
    else:
        logger_phase2.log(f"ERROR: Input path '{input_phase1_path}' is neither a file nor a directory.")
        return False

    if not files_to_process_p2:
        logger_phase2.log(f"INFO: No JSON files from Phase 1 found to process in '{input_phase1_path}'.")
        return False
    logger_phase2.log(f"INFO: Found {len(files_to_process_p2)} Phase 1 output file(s) to process.")

    qid_collected_content_p2 = defaultdict(list)
    # ... (Rest of the `extract_and_aggregate_content_to_json` logic from previous message) ...
    # ... ensuring it uses `logger_phase2` and `clean_text_phase2` ...
    # ... and saves to `output_collated_json_file` ...
    # Key part is the loop:
    total_content_items_collected_p2 = 0
    summaries_collected_count_p2 = 0
    passages_collected_count_p2 = 0
    all_processed_response_ids_p2 = set()


    for file_idx, file_path in enumerate(files_to_process_p2):
        logger_phase2.log(f"INFO: Processing Phase 1 file {file_idx + 1}/{len(files_to_process_p2)}: '{os.path.basename(file_path)}'...")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                phase1_output_data = json.load(f)

            pdf_analysis_by_response_p2 = phase1_output_data.get("pdf_analysis_by_response")
            if not pdf_analysis_by_response_p2:
                logger_phase2.log(f"  WARN: 'pdf_analysis_by_response' key not found or empty in '{file_path}'.")
                continue

            for response_id, response_q_data in pdf_analysis_by_response_p2.items():
                all_processed_response_ids_p2.add(response_id)
                if not isinstance(response_q_data, dict): continue

                for qid, q_content in response_q_data.items():
                    if not isinstance(q_content, dict): continue # Skip question text if it's just a string here

                    content_item_p2 = {
                        "source_file": os.path.basename(file_path),
                        "response_id": response_id,
                        "question_id": qid, # Assuming qid is the actual QID key
                        "type": None, "text": None
                    }
                    text_to_add_p2 = None
                    summary_p2 = q_content.get("summary")
                    if summary_p2 and isinstance(summary_p2, str) and summary_p2.strip():
                        text_to_add_p2 = clean_text_phase2(summary_p2)
                        content_item_p2["type"] = "pdf_summary"
                        summaries_collected_count_p2 +=1
                    elif q_content.get("extracted_passages"):
                        passages_p2 = q_content.get("extracted_passages")
                        if passages_p2 and isinstance(passages_p2, list):
                            valid_passages_p2 = [p for p in passages_p2 if isinstance(p, str) and p.strip()]
                            if valid_passages_p2:
                                text_to_add_p2 = clean_text_phase2("\n\n".join(valid_passages_p2))
                                content_item_p2["type"] = "pdf_passages"
                                passages_collected_count_p2 +=1

                    if text_to_add_p2:
                        content_item_p2["text"] = text_to_add_p2
                        qid_collected_content_p2[qid].append(content_item_p2)
                        total_content_items_collected_p2 += 1
        except Exception as e:
            logger_phase2.log(f"ERROR: Processing Phase 1 file '{file_path}' for Phase 2: {e}")
            logger_phase2.log(traceback.format_exc())

    final_output_data_p2 = {
        "metadata": {
            "creation_timestamp": time.strftime('%Y-%m-%d %H:%M:%S %Z', time.gmtime()),
            "source_phase1_output_path": input_phase1_path,
            "total_phase1_files_processed": len(files_to_process_p2),
            "total_unique_qids_found": len(qid_collected_content_p2),
            "total_content_items_collated": total_content_items_collected_p2,
            "pdf_summaries_collated": summaries_collected_count_p2,
            "pdf_passages_collated": passages_collected_count_p2,
            "total_unique_responses_covered": len(all_processed_response_ids_p2)
        },
        "aggregated_pdf_content_by_qid": dict(qid_collected_content_p2)
    }
    logger_phase2.log(f"\n--- Phase 2 Aggregation Summary ---")
    for key, value in final_output_data_p2["metadata"].items(): logger_phase2.log(f"  {key}: {value}")

    try:
        with open(output_collated_json_file, 'w', encoding='utf-8') as outfile:
            json.dump(final_output_data_p2, outfile, indent=2, default=logger_phase2._get_serializer())
        logger_phase2.log(f"INFO: Successfully saved Phase 2 collated PDF content to '{output_collated_json_file}'.")
    except Exception as e:
        logger_phase2.log(f"ERROR: Could not write Phase 2 output JSON: {e}")
        logger_phase2.log(traceback.format_exc())

    logger_phase2.log("=" * 50); logger_phase2.log(" INFO: Phase 2 (PDF Text Collation) Finished "); logger_phase2.log("=" * 50)
    return True

# print("INFO: Cell 6 - Logic for Phase 2 (Stage 4a) defined.")
# To run Phase 2:
# Make sure PHASE1_OUTPUT_DIR (or PHASE1_SINGLE_OUTPUT_FILE) contains the COMPLETE output from Phase 1
run_phase2_collate_pdf_texts(PHASE2_INPUT_PHASE1_OUTPUT_PATH, PHASE2_OUTPUT_COLLATED_JSON)

# @title Cell 7: Configuration for Phase 3 (LLM Thematic Analysis)

# Input for Phase 3 is the output from Phase 2
PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON = PHASE2_OUTPUT_COLLATED_JSON # Use the output file name from Phase 2 config

# Output file for Phase 3
PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON = os.path.join(BASE_PROJECT_DIR, f'phase3_pdf_thematic_points_{time.strftime("%Y%m%d_%H%M%S")}.json')
PHASE3_LOG_FILE = os.path.join(BASE_PROJECT_DIR, f"phase3_llm_thematic_analysis_{time.strftime('%Y%m%d_%H%M%S')}.log")

# LLM and Tokenizer for Thematic Analysis (can be same as Phase 1's LLM, or different)
PHASE3_LLM_MODEL_ID = 'google/gemma-3-4b-it' # Or PHASE1_LLM_MODEL_ID
PHASE3_USE_QUANTIZATION = False                # Or PHASE1_USE_QUANTIZATION

# Thematic Analysis Parameters
PHASE3_NUM_THEMES_TO_REQUEST = 3
PHASE3_MAX_TOKENS_FOR_LLM_INPUT_CHUNK = 3000 # Max tokens to feed LLM in one go for map-reduce sub-problem
                                            # This is for the *text content*. Prompt will add more.
PHASE3_LLM_MAX_NEW_TOKENS_FOR_THEMES = 700  # Max tokens for LLM to generate for themes (e.g., 3 themes * (label + 2 sentences explanation))
                                            # Adjust based on NUM_THEMES_TO_REQUEST and desired explanation length

# Optional: QID selection for thematic analysis (if not processing all)
PHASE3_QIDS_TO_PROCESS_THEMATICALLY = ["Q4"] # None # Set to a list of QIDs like ["Q1", "Q4"] to process only those, or None for all

# Source for actual question texts (e.g., your CAR_35 or Stage 2 form output)
PHASE3_QUESTION_TEXT_SOURCE_FILE = os.path.join(BASE_PROJECT_DIR, 'comprehensive_analysis_report.json') # Example: CAR_35
# OR use PHASE1_STAGE2_AGGREGATION_FILE if that's more reliable for question texts

print("INFO: Cell 7 - Configuration for Phase 3 (LLM Thematic Analysis) loaded.")

# @title Cell 8: Helper Functions & Main Logic for Phase 3 (LLM Thematic Analysis)

logger_phase3 = FileLogger(PHASE3_LOG_FILE)

# --- Helper: LLM Call for Thematic Points ---
def get_llm_thematic_points(text_chunk_for_llm: str,
                            question_text_for_prompt: str,
                            num_themes: int,
                            llm_pipe: pipeline,
                            max_new_tokens: int,
                            qid_for_log: str):
    """
    Sends a text chunk to the LLM to get thematic points.
    Returns the raw text output from the LLM.
    """
    messages = [{
        "role": "user",
        "content": f"You are an expert policy analyst. Carefully review all the following excerpts from PDF submissions responding to the question: '{question_text_for_prompt}'.\n"
                   f"Based *only* on the provided excerpts, identify the top {num_themes} (e.g., three) most common and distinct points, arguments, or themes raised by the respondents.\n"
                   f"For each point, provide a concise label for the point (as 'Theme Label X:') and a brief 1-2 sentence explanation of that point (as 'Explanation X:').\n"
                   f"List them clearly in a numbered format (1., 2., 3.). If fewer than {num_themes} distinct common points are evident, list only those that are clear.\n\n"
                   f"Excerpts:\n\"\"\"\n{text_chunk_for_llm}\n\"\"\"\n\n"
                   f"Top {num_themes} Common Points:"
    }]

    try:
        prompt_formatted = llm_pipe.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        generation_args = {
            "max_new_tokens": max_new_tokens,
            "do_sample": False,
            "pad_token_id": llm_pipe.tokenizer.pad_token_id if llm_pipe.tokenizer.pad_token_id is not None else llm_pipe.tokenizer.eos_token_id,
        }
        if PHASE1_ENABLE_DEBUG_LOGGING: # Use a global debug flag or pass one
             logger_phase3.log(f"DEBUG: Phase 3 - LLM call for QID {qid_for_log}. Prompt length (chars): {len(prompt_formatted)}, Input text length (chars): {len(text_chunk_for_llm)}")

        outputs = llm_pipe(prompt_formatted, **generation_args)

        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):
            logger_phase3.log(f"ERROR: Phase 3 - LLM pipeline returned unexpected/empty output for QID {qid_for_log}.")
            return "LLM_ERROR: No output generated."

        generated_text_full = outputs[0]['generated_text']

        llm_response_text = ""
        if generated_text_full.startswith(prompt_formatted):
            llm_response_text = generated_text_full[len(prompt_formatted):].strip()
        else:
            response_marker = "<start_of_turn>model\n"
            summary_start_index = generated_text_full.rfind(response_marker)
            if summary_start_index != -1:
                llm_response_text = generated_text_full[summary_start_index + len(response_marker):].strip()
            else:
                llm_response_text = generated_text_full # Take raw if unsure
                logger_phase3.log(f"WARN: Phase 3 - Could not reliably strip prompt for QID {qid_for_log}.")

        return llm_response_text

    except Exception as e:
        logger_phase3.log(f"ERROR: Phase 3 - Exception during LLM call for QID {qid_for_log}: {e}")
        logger_phase3.log(traceback.format_exc())
        return f"LLM_ERROR: Exception during generation - {str(e)}"

# --- Helper: Parse LLM Thematic Output ---
def parse_llm_themes(llm_output_text: str, num_expected_themes: int) -> List[Dict[str, str]]:
    """
    Parses the LLM's text output to extract structured themes.
    This is a basic parser and might need significant refinement based on actual LLM output.
    """
    parsed_themes = []
    if not llm_output_text or llm_output_text.startswith("LLM_ERROR:"):
        return [{"theme_label": "Parsing Error", "explanation": llm_output_text}]

    # Attempt to find numbered points
    # Regex to find "X. Theme Label: [Label]" and "Explanation X: [Explanation]"
    # This regex is complex and highly dependent on consistent LLM output.
    # It tries to capture: number, label, and explanation separately.
    # Pattern assumes "Theme Label X:" and "Explanation X:" might appear.
    # Simpler: look for "1.", "2.", "3." and then try to find "Theme Label:" and "Explanation:" after each.

    # Simpler approach: Split by common theme delimiters if LLM is consistent
    # For example, if LLM outputs "1. Theme Label: ... Explanation: ..." then "2. Theme Label: ..."

    # Let's try a line-by-line approach looking for keywords
    current_theme_label = None
    current_explanation = ""

    # Normalize potential list markers (e.g., "1.", "1)", "1-")
    lines = llm_output_text.splitlines()
    theme_item = {}

    for line in lines:
        line_stripped = line.strip()
        if not line_stripped:
            continue

        # Try to find a new theme item marker (e.g., "1.", "2. Theme Label:")
        # More robustly: if line starts with a number followed by . or )
        # and (contains "Theme Label" or we just assume it's a new theme if previous one had explanation)
        match_new_theme = re.match(r"^\s*(\d+[\.\)\-]?)\s*(?:Theme Label\s*[:\-]?\s*)?(.*)", line_stripped, re.IGNORECASE)

        if match_new_theme:
            if theme_item.get("theme_label"): # Save previous theme if fully formed
                if not theme_item.get("explanation"): theme_item["explanation"] = current_explanation.strip()
                if theme_item.get("theme_label") or theme_item.get("explanation"): # Ensure it's not empty
                    parsed_themes.append(theme_item)

            theme_item = {} # Start new theme
            current_explanation = ""
            label_candidate = match_new_theme.group(2).strip()

            # If "Explanation:" is also on this line after the label
            if "explanation:" in label_candidate.lower():
                parts = re.split(r"explanation\s*[:\-]", label_candidate, maxsplit=1, flags=re.IGNORECASE)
                theme_item["theme_label"] = parts[0].strip()
                current_explanation = parts[1].strip() if len(parts) > 1 else ""
            else:
                theme_item["theme_label"] = label_candidate

        elif "explanation:" in line_stripped.lower():
            # If we haven't captured a label yet for the current theme_item,
            # then previous non-explanation lines might constitute the label.
            if theme_item and not theme_item.get("theme_label") and current_explanation.strip():
                theme_item["theme_label"] = current_explanation.strip()
                current_explanation = "" # Reset current_explanation as it was actually the label

            current_explanation += re.split(r"explanation\s*[:\-]", line_stripped, maxsplit=1, flags=re.IGNORECASE)[-1].strip() + " "

        elif theme_item: # If we are in a theme_item (label started) and it's not an explanation line start
            current_explanation += line_stripped + " " # Append to current explanation or potential label

    # Add the last processed theme
    if theme_item.get("theme_label"):
        if not theme_item.get("explanation"): theme_item["explanation"] = current_explanation.strip()
        if theme_item.get("theme_label") or theme_item.get("explanation"):
            parsed_themes.append(theme_item)
    elif current_explanation.strip() and not parsed_themes: # case where output is just a single block of text
        parsed_themes.append({"theme_label": "General Theme", "explanation": current_explanation.strip()})


    if not parsed_themes and llm_output_text: # Fallback if no structured themes found
        parsed_themes.append({"theme_label": "Unparsed LLM Output", "explanation": llm_output_text})

    return parsed_themes[:num_expected_themes] # Return up to the number of expected themes

# --- Main Function for Phase 3 ---
def run_phase3_llm_thematic_analysis():
    global logger_phase3
    logger_phase3.log("=" * 50); logger_phase3.log(" INFO: Starting Phase 3: LLM Thematic Analysis per QID"); logger_phase3.log("=" * 50)
    logger_phase3.log(f"INFO: Input Collated PDF Texts (from Phase 2/Stage 4a): {PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON}")
    logger_phase3.log(f"INFO: LLM Model for Thematic Analysis: {PHASE3_LLM_MODEL_ID}")
    logger_phase3.log(f"INFO: Output Thematic Analysis File: {PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}")

    # --- Load LLM for Thematic Analysis ---
    llm_tokenizer_p3 = None
    llm_model_p3 = None
    llm_thematic_pipe_p3 = None
    # (Reuse LLM loading logic from Phase 1, adapting variable names)
    try:
        logger_phase3.log(f"INFO: Loading LLM tokenizer for Phase 3: {PHASE3_LLM_MODEL_ID}...")
        llm_tokenizer_p3 = AutoTokenizer.from_pretrained(PHASE3_LLM_MODEL_ID)
        if getattr(llm_tokenizer_p3, 'pad_token', None) is None and getattr(llm_tokenizer_p3, 'eos_token', None):
            llm_tokenizer_p3.pad_token = llm_tokenizer_p3.eos_token
            llm_tokenizer_p3.padding_side = "left"
        logger_phase3.log("INFO: Phase 3 LLM Tokenizer loaded.")

        bnb_config_p3 = None
        current_use_quantization_p3 = PHASE3_USE_QUANTIZATION # Use phase-specific config
        if current_use_quantization_p3 and torch.cuda.is_available():
            try:
                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
                bnb_config_p3 = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)
                logger_phase3.log(f"INFO: Phase 3 - 4-bit quantization configured (Compute dtype: {compute_dtype}).")
            except Exception as q_err:
                logger_phase3.log(f"ERROR: Phase 3 - Failed BitsAndBytesConfig: {q_err}. Disabling quantization.")
                current_use_quantization_p3 = False

        logger_phase3.log(f"INFO: Loading Phase 3 LLM model: {PHASE3_LLM_MODEL_ID} (Quantization: {current_use_quantization_p3})...")
        llm_model_p3 = AutoModelForCausalLM.from_pretrained(
            PHASE3_LLM_MODEL_ID,
            quantization_config=bnb_config_p3 if current_use_quantization_p3 else None,
            torch_dtype=torch.bfloat16 if not current_use_quantization_p3 else None,
            device_map="auto", trust_remote_code=True
        )
        logger_phase3.log("INFO: Phase 3 LLM model loaded.")

        llm_thematic_pipe_p3 = pipeline("text-generation", model=llm_model_p3, tokenizer=llm_tokenizer_p3)
        logger_phase3.log("INFO: Phase 3 LLM text-generation pipeline created successfully.")
    except Exception as load_err:
        logger_phase3.log(f"CRITICAL: Failed to load LLM/pipeline for Phase 3: {load_err}")
        logger_phase3.log(traceback.format_exc())
        return False # Cannot proceed

    # --- Load Collated Texts from Phase 2 (Stage 4a) ---
    try:
        with open(PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON, 'r', encoding='utf-8') as f:
            phase2_data = json.load(f)
        collated_texts_by_qid_p3 = phase2_data.get("aggregated_pdf_content_by_qid", {}) # Ensure correct key
        if not collated_texts_by_qid_p3:
            logger_phase3.log(f"ERROR: No 'aggregated_pdf_content_by_qid' found in {PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON}")
            return False
        logger_phase3.log(f"INFO: Loaded collated PDF texts for {len(collated_texts_by_qid_p3)} QIDs from Phase 2 output.")
    except Exception as e:
        logger_phase3.log(f"FATAL: Could not load Phase 2 collated texts file: {PHASE3_INPUT_COLLATED_PDF_TEXTS_JSON}. Error: {e}")
        return False

    # --- Load Question Texts for Prompts ---
    qid_to_actual_question_text_p3 = {}
    try:
        with open(PHASE3_QUESTION_TEXT_SOURCE_FILE, 'r', encoding='utf-8') as f_qtext:
            qtext_source_data = json.load(f_qtext)
        # Adjust key based on the source file (CAR_35 or Stage 2 form output)
        if "analysis_by_question" in qtext_source_data: # From CAR_35
            for qid, data in qtext_source_data.get("analysis_by_question", {}).items():
                qid_to_actual_question_text_p3[qid] = data.get("question_text", f"Unknown text for {qid}")
        elif "question_data" in qtext_source_data: # From Stage 2 form output
             for qid, data in qtext_source_data.get("question_data", {}).items():
                qid_to_actual_question_text_p3[qid] = data.get("question_text", f"Unknown text for {qid}")
        logger_phase3.log(f"INFO: Loaded actual question texts for {len(qid_to_actual_question_text_p3)} QIDs.")
    except Exception as e:
        logger_phase3.log(f"WARN: Could not load question texts from {PHASE3_QUESTION_TEXT_SOURCE_FILE}. Prompts will use QID only. Error: {e}")


    # --- Main Loop: Iterate QID by QID for Thematic Analysis ---
    overall_thematic_results_p3 = {}
    if os.path.exists(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON):
        try:
            with open(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON, 'r', encoding='utf-8') as f_exist:
                overall_thematic_results_p3 = json.load(f_exist)
            logger_phase3.log(f"INFO: Loaded {len(overall_thematic_results_p3)} existing thematic results from {PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}")
        except Exception as e_load_exist:
            logger_phase3.log(f"WARN: Could not load existing results file {PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}. Starting fresh. Error: {e_load_exist}")

    qids_for_processing_p3 = PHASE3_QIDS_TO_PROCESS_THEMATICALLY if PHASE3_QIDS_TO_PROCESS_THEMATICALLY else sorted(collated_texts_by_qid_p3.keys())

    for qid_idx, qid in enumerate(qids_for_processing_p3):
        logger_phase3.log(f"\n--- Processing Thematic Analysis for QID {qid_idx + 1}/{len(qids_for_processing_p3)}: {qid} ---")

        if qid in overall_thematic_results_p3 and overall_thematic_results_p3[qid].get("status", "").startswith("success"):
            logger_phase3.log(f"INFO: QID {qid} already successfully processed. Skipping.")
            continue

        question_actual_text_p3 = qid_to_actual_question_text_p3.get(qid, f"Question {qid}")

        text_items_for_this_qid_p3 = collated_texts_by_qid_p3.get(qid, [])
        if not text_items_for_this_qid_p3:
            logger_phase3.log(f"WARN: No collated texts found for QID {qid}. Skipping thematic analysis for it.")
            overall_thematic_results_p3[qid] = {"question_text": question_actual_text_p3, "status": "no_collated_text_for_qid", "themes": []}
            continue

        # Combine texts for this QID
        # Each item in text_items_for_this_qid_p3 is {'source_file': ..., 'response_id': ..., 'question_id': ..., 'type': ..., 'text': ...}
        texts_to_combine_p3 = [item['text'] for item in text_items_for_this_qid_p3 if item.get('text')]
        if not texts_to_combine_p3:
            logger_phase3.log(f"WARN: All text items for QID {qid} were empty. Skipping.")
            overall_thematic_results_p3[qid] = {"question_text": question_actual_text_p3, "status": "all_texts_empty_for_qid", "themes": []}
            continue

        current_qid_full_text_chunk = "\n\n--- New Excerpt ---\n".join(texts_to_combine_p3)

        # --- Context Window Management & LLM Call ---
        # This is where the map-reduce logic would be implemented if current_qid_full_text_chunk is too large
        # For now, simplified: if too large, it will likely error or truncate in the LLM call.
        # A robust implementation needs chunking of current_qid_full_text_chunk,
        # then map (LLM themes per chunk), then reduce (LLM themes of themes).

        llm_raw_themes_output = get_llm_thematic_points(
            current_qid_full_text_chunk,
            question_actual_text_p3,
            PHASE3_NUM_THEMES_TO_REQUEST,
            llm_thematic_pipe_p3,
            PHASE3_LLM_MAX_NEW_TOKENS_FOR_THEMES,
            qid
        )

        parsed_themes_list = parse_llm_themes(llm_raw_themes_output, PHASE3_NUM_THEMES_TO_REQUEST)

        status_p3 = "success_thematic_analysis"
        if llm_raw_themes_output.startswith("LLM_ERROR:"):
            status_p3 = "error_llm_theme_generation"
        elif not parsed_themes_list or (len(parsed_themes_list) == 1 and parsed_themes_list[0]["theme_label"] == "Unparsed LLM Output"):
            status_p3 = "llm_output_parsing_failed_or_empty"

        overall_thematic_results_p3[qid] = {
            "question_text": question_actual_text_p3,
            "status": status_p3,
            "total_pdf_text_items_for_qid": len(texts_to_combine_p3),
            "approx_char_length_fed_to_llm": len(current_qid_full_text_chunk), # Add this
            "llm_raw_output": llm_raw_themes_output,
            "parsed_top_themes": parsed_themes_list
        }

        # Save after each QID
        try:
            with open(PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON, 'w', encoding='utf-8') as f_out_p3:
                json.dump(overall_thematic_results_p3, f_out_p3, indent=2, default=logger_phase3._get_serializer())
            logger_phase3.log(f"INFO: Saved thematic analysis results to '{PHASE3_OUTPUT_THEMATIC_ANALYSIS_JSON}' after QID {qid}.")
        except Exception as e_save_p3:
            logger_phase3.log(f"ERROR: Could not save intermediate results for Phase 3: {e_save_p3}")

        gc.collect()
        if torch.cuda.is_available(): torch.cuda.empty_cache()

    # --- Final Save (redundant if saving after each QID, but good for final confirmation) ---
    # ... (same save logic as above) ...
    logger_phase3.log("=" * 50); logger_phase3.log(" INFO: Phase 3 (LLM Thematic Analysis) Finished "); logger_phase3.log("=" * 50)
    return True


# print("INFO: Cell 8 - Logic for Phase 3 (LLM Thematic Analysis) defined.")
# To run Phase 3:
# Make sure PHASE2_OUTPUT_COLLATED_JSON points to the COMPLETE output from Phase 2
# run_phase3_llm_thematic_analysis()

# @title Cell 9: Main Orchestration Cell

if __name__ == "__main__": # Standard guard
    # --- Phase 1 Execution ---
    # IMPORTANT: This phase needs to run to completion for ALL ~485 responses.
    # Due to its length, you might run it separately or ensure its state file
    # (PHASE1_STATE_FILE) is robustly managed for resumption.
    # If PHASE1_SINGLE_OUTPUT_FILE already exists and is complete, you can optionally skip.

    PHASE1_COMPLETE = False # Set to True if you know Phase 1 output is complete and up-to-date
    if os.path.exists(PHASE1_SINGLE_OUTPUT_FILE): # Or check based on your chunking strategy
        # Add a more robust check if possible, e.g., number of responses in state file
        # For now, simple existence check
        # user_confirms_complete = input(f"INFO: Phase 1 output file '{PHASE1_SINGLE_OUTPUT_FILE}' exists. Assume it's complete for all responses? (yes/no): ")
        # if user_confirms_complete.lower() == 'yes':
        #     PHASE1_COMPLETE = True
        #     logger_master.log("INFO: User confirmed Phase 1 output is complete. Skipping Phase 1 execution.")
        # For automated runs, you might need a flag or a more sophisticated check.
        # For now, if the main output file exists, let's assume we might not need to run it,
        # BUT IT'S SAFER TO RUN IT if you're unsure, as it uses a state file to resume.
        # For this example, we'll assume it needs to be run or resumed.
        print(f"INFO: Phase 1 output file '{PHASE1_SINGLE_OUTPUT_FILE}' target. Script will use state to resume/run.")


    print("\n>>> EXECUTING PHASE 1: PDF Content Extraction (Original Stage 4 Logic) <<<")
    # Ensure logger_phase1 is re-initialized if you run this cell multiple times without kernel restart
    logger_phase1 = FileLogger(PHASE1_LOG_FILE)
    phase1_success = run_phase1_pdf_extraction()
    if not phase1_success:
        logger_phase1.log("CRITICAL: Phase 1 failed or did not complete. Subsequent phases may use incomplete data or fail.")
        # Decide whether to stop the entire pipeline here
        # sys.exit("Phase 1 failed.")
        print("CRITICAL: Phase 1 failed. Please check logs. Stopping further execution in this orchestration.")
    else:
        print(">>> PHASE 1 COMPLETED SUCCESSFULLY <<<\n")

        # --- Phase 2 Execution ---
        # This runs only if Phase 1 was successful (or deemed complete)
        print("\n>>> EXECUTING PHASE 2: PDF Text Collation (Stage 4a Logic) <<<")
        logger_phase2 = FileLogger(PHASE2_LOG_FILE)
        # Determine the correct input for Phase 2
        # If Phase 1 produces one file, use PHASE1_SINGLE_OUTPUT_FILE
        # If Phase 1 produces chunks into PHASE1_OUTPUT_DIR, use that directory
        input_for_phase2 = PHASE1_SINGLE_OUTPUT_FILE if os.path.exists(PHASE1_SINGLE_OUTPUT_FILE) else PHASE1_OUTPUT_DIR

        phase2_success = run_phase2_collate_pdf_texts(input_for_phase2, PHASE2_OUTPUT_COLLATED_JSON)
        if not phase2_success:
            logger_phase2.log("CRITICAL: Phase 2 failed. Cannot proceed to Phase 3.")
            print("CRITICAL: Phase 2 failed. Please check logs. Stopping further execution.")
        else:
            print(">>> PHASE 2 COMPLETED SUCCESSFULLY <<<\n")

            # --- Phase 3 Execution ---
            # This runs only if Phase 2 was successful
            print("\n>>> EXECUTING PHASE 3: LLM Thematic Analysis per QID <<<")
            logger_phase3 = FileLogger(PHASE3_LOG_FILE)
            phase3_success = run_phase3_llm_thematic_analysis()
            if not phase3_success:
                logger_phase3.log("ERROR: Phase 3 thematic analysis encountered issues.")
                print("ERROR: Phase 3 thematic analysis encountered issues. Please check logs.")
            else:
                print(">>> PHASE 3 COMPLETED SUCCESSFULLY <<<\n")
                print("PIPELINE EXECUTION FINISHED.")

    # Final message
    master_log_file_for_orchestration = os.path.join(BASE_PROJECT_DIR, "pipeline_orchestration_main.log")
    master_logger = FileLogger(master_log_file_for_orchestration) # For overall pipeline status
    master_logger.log("="*50)
    master_logger.log("Overall Pipeline Orchestration Attempt Concluded.")
    master_logger.log(f"Phase 1 Success: {phase1_success if 'phase1_success' in locals() else 'Not Run or Failed Early'}")
    master_logger.log(f"Phase 2 Success: {phase2_success if 'phase2_success' in locals() else 'Not Run'}")
    master_logger.log(f"Phase 3 Success: {phase3_success if 'phase3_success' in locals() else 'Not Run'}")
    master_logger.log("Please check individual phase log files for details.")
    master_logger.log("="*50)

print("INFO: Cell 9 - Main Orchestration Cell defined. Execute this cell to run the pipeline.")

"""# 15th May

## Stage 4a-Helper - Draft Keyphrase Generation
"""

import json
import os
import time
from collections import Counter
import re
# For more advanced NLP if you want to try it:
# import spacy
# from nltk.corpus import stopwords
# from sklearn.feature_extraction.text import CountVectorizer

# (Assume FileLogger class is defined and initialized as 'logger')
# Example:
# SCRIPT_LOG_FILE_KP_HELPER = 'stage4a_keyphrase_helper.log'
# logger = FileLogger(SCRIPT_LOG_FILE_KP_HELPER)


def preprocess_for_candidates(text, remove_stopwords=True):
    text = text.lower()
    # Remove typical summarization intro phrases more aggressively
    text = re.sub(r"here.s a \d-sentence summary of the responses?,? capturing the core themes:", "", text)
    text = re.sub(r"here.s a \d-sentence summary of the response, capturing the core themes:", "", text)
    text = re.sub(r"here.s a \d-sentence summary, capturing the core points:", "", text)
    text = re.sub(r"this response doesn't address the specific question.*", "", text, flags=re.IGNORECASE)
    text = re.sub(r"please provide the responses.*", "", text, flags=re.IGNORECASE)

    text = re.sub(r'[^\w\s-]', '', text) # Keep hyphens for compound words, remove other punctuation
    text = re.sub(r'\d+', '', text)     # Remove numbers

    # More aggressive stopword removal for keyphrase candidates
    # if remove_stopwords:
    #     # Consider a domain-specific stopword list + standard English
    #     stop_words = set(stopwords.words('english'))
    #     # Add more common but uninformative words from your domain if needed
    #     # e.g., stop_words.update(["proposal", "act", "privacy", "individual", "rights"])
    #     # - be careful not to remove actual keywords
    #     words = text.split()
    #     words = [word for word in words if word not in stop_words and len(word) > 2]
    #     text = " ".join(words)

    text = re.sub(r'\s+', ' ', text).strip() # Normalize whitespace
    return text

def extract_candidate_phrases_ngrams(texts, n_values=(2, 3, 4), top_k=15):
    """Extracts top n-grams as candidate phrases."""
    candidate_phrases = defaultdict(list)
    for n in n_values:
        all_n_grams = []
        for text in texts:
            processed_text = preprocess_for_candidates(text, remove_stopwords=False) # Keep stopwords for natural phrases initially
            words = processed_text.split()
            if len(words) < n:
                continue
            for i in range(len(words) - n + 1):
                ngram = " ".join(words[i:i+n])
                # Filter out n-grams that are likely just common function words or too short
                if len(ngram.replace(" ", "")) > 5 : # Heuristic: at least 5 non-space characters
                    all_n_grams.append(ngram)

        # Filter n-grams that are substrings of longer, more frequent n-grams (simple version)
        counts = Counter(all_n_grams)
        top_phrases_for_n = []
        # Sort by count, then by length (longer preferred if count is same)
        sorted_phrases = sorted(counts.items(), key=lambda item: (-item[1], -len(item[0])))

        added_phrases_set = set() # To avoid adding shorter versions if a longer one covers it
        for phrase, count in sorted_phrases:
            is_substring = False
            for added_p in added_phrases_set:
                if phrase in added_p:
                    is_substring = True
                    break
            if not is_substring:
                top_phrases_for_n.append((phrase, count))
                added_phrases_set.add(phrase)
            if len(top_phrases_for_n) >= top_k:
                break
        candidate_phrases[f'{n}-grams'] = top_phrases_for_n
    return candidate_phrases

# If using spaCy for Noun Phrase Chunks (more advanced, requires spaCy installation and model)
# def extract_candidate_phrases_spacy_np(texts, nlp_spacy, top_k=20):
#     all_noun_phrases = []
#     for text in texts:
#         # Limit text length for spaCy if it's very long
#         doc = nlp_spacy(text[:5000]) # Process first 5000 chars
#         for chunk in doc.noun_chunks:
#             # Filter noun chunks - e.g., not too short, not just pronouns/determiners
#             if len(chunk.text.split()) > 1 and len(chunk.text) > 3:
#                 all_noun_phrases.append(chunk.text.lower().strip())
#     return Counter(all_noun_phrases).most_common(top_k)


def generate_draft_keyphrases_for_qid(qid, texts_for_qid, sample_size=20, nlp_spacy_model=None):
    """
    Generates draft keyphrases for a single QID.
    - Provides a sample of texts for human/LLM review.
    - Extracts candidate n-grams.
    - Optionally, extracts noun phrases if spaCy is used.
    """
    output = {
        "qid": qid,
        "sample_texts_for_review": [],
        "candidate_ngrams": {},
        # "candidate_noun_phrases_spacy": [] # If using spaCy
    }

    if not texts_for_qid:
        return output

    # Take a sample of texts (e.g., first N, or random N)
    # Prioritize summaries in the sample if type information is available
    summaries_sample = [item['text'] for item in texts_for_qid if item.get('type') == 'summary' and item.get('text')]
    passages_sample = [item['text'] for item in texts_for_qid if item.get('type') == 'passages' and item.get('text')]

    combined_sample = summaries_sample[:sample_size//2] # Prioritize summaries
    remaining_needed = sample_size - len(combined_sample)
    if remaining_needed > 0:
        combined_sample.extend(passages_sample[:remaining_needed])

    if not combined_sample and texts_for_qid: # Fallback if no type info or no summaries
        combined_sample = [item['text'] for item in texts_for_qid[:sample_size] if item.get('text')]

    output["sample_texts_for_review"] = combined_sample

    # Candidate N-grams
    if combined_sample: # Use the sample for n-gram generation to speed things up
        output["candidate_ngrams"] = extract_candidate_phrases_ngrams(combined_sample, n_values=(2,3,4), top_k=15)

    # Candidate Noun Phrases (Optional, using spaCy)
    # if nlp_spacy_model and combined_sample:
    #     output["candidate_noun_phrases_spacy"] = extract_candidate_phrases_spacy_np(combined_sample, nlp_spacy_model, top_k=20)

    return output


# --- Main execution block for the Keyphrase Helper Script ---
if __name__ == "__main__":
    # --- Configuration ---
    # Ensure FileLogger is initialized if you uncomment logger lines
    SCRIPT_LOG_FILE_KP_HELPER = 'stage4a_keyphrase_helper.log'
    logger = FileLogger(SCRIPT_LOG_FILE_KP_HELPER)

    INPUT_COLLATED_TEXTS_JSON = 'stage4a_collated_texts_for_thematic_analysis_20250515_060005.json' # From Stage 4a
    OUTPUT_DRAFT_KEYPHRASES_JSON = 'stage4a_helper_draft_keyphrases.json'

    SAMPLE_SIZE_PER_QID = 10 # Number of text samples to include for review for each QID

    # Optional: spaCy for Noun Phrase chunking (more advanced)
    # try:
    #     import spacy
    #     # You might need to download the model if running for the first time in Colab:
    #     # !python -m spacy download en_core_web_sm
    #     nlp_spacy = spacy.load("en_core_web_sm")
    #     logger.log("INFO: spaCy model loaded for Noun Phrase extraction.")
    # except ImportError:
    #     logger.log("WARN: spaCy not installed or model not found. Noun Phrase extraction will be skipped.")
    #     nlp_spacy = None
    nlp_spacy = None # Keeping it simple for now

    # --- Load data from Stage 4a ---
    try:
        with open(INPUT_COLLATED_TEXTS_JSON, 'r', encoding='utf-8') as f:
            stage4a_data = json.load(f)
        collated_content_by_qid = stage4a_data.get("aggregated_content_by_qid", {})
        # metadata_from_4a = stage4a_data.get("metadata", {}) # For question texts if stored there
    except Exception as e:
        logger.log(f"FATAL: Could not load Stage 4a input JSON '{INPUT_COLLATED_TEXTS_JSON}': {e}")
        print(f"FATAL: Could not load Stage 4a input JSON '{INPUT_COLLATED_TEXTS_JSON}': {e}")
        # import sys; sys.exit(1) # if running as standalone script
        raise # if running in Colab, let the error stop the cell

    if not collated_content_by_qid:
        logger.log(f"FATAL: No 'aggregated_content_by_qid' found in {INPUT_COLLATED_TEXTS_JSON}. Cannot proceed.")
        print(f"FATAL: No 'aggregated_content_by_qid' found in {INPUT_COLLATED_TEXTS_JSON}. Cannot proceed.")
        raise ValueError("Missing aggregated content in input file.")


    all_qids_draft_keyphrases = {}
    qids_to_process = sorted(collated_content_by_qid.keys())

    logger.log(f"INFO: Starting draft keyphrase generation for {len(qids_to_process)} QIDs.")

    for i, qid in enumerate(qids_to_process):
        logger.log(f"INFO: Generating draft keyphrases for QID {i+1}/{len(qids_to_process)}: {qid}...")
        texts_for_this_qid = collated_content_by_qid.get(qid, [])

        # Each item in texts_for_this_qid is like:
        # {'source_file': 'file1.json', 'response_id': 'id_A', 'type': 'summary', 'text': '...'}

        draft_output_for_qid = generate_draft_keyphrases_for_qid(
            qid,
            texts_for_this_qid,
            sample_size=SAMPLE_SIZE_PER_QID,
            nlp_spacy_model=nlp_spacy
        )
        all_qids_draft_keyphrases[qid] = draft_output_for_qid
        logger.log(f"INFO: Draft keyphrases generated for QID: {qid}. Sample texts: {len(draft_output_for_qid['sample_texts_for_review'])}, N-grams generated.")

    # --- Save the draft keyphrases ---
    logger.log(f"\nINFO: Saving all draft keyphrases to '{OUTPUT_DRAFT_KEYPHRASES_JSON}'...")
    try:
        final_output_structure = {
            "metadata": {
                "generation_timestamp": time.strftime('%Y-%m-%d %H:%M:%S %Z', time.gmtime()),
                "source_collated_texts_file": INPUT_COLLATED_TEXTS_JSON,
                "sample_size_per_qid_for_review_texts": SAMPLE_SIZE_PER_QID,
                "notes": "This file contains DRAFT keyphrase suggestions and sample texts. MANUAL REVIEW AND REFINEMENT IS ESSENTIAL before using these in the main thematic analysis."
            },
            "draft_keyphrases_by_qid": all_qids_draft_keyphrases
        }
        with open(OUTPUT_DRAFT_KEYPHRASES_JSON, 'w', encoding='utf-8') as f_out:
            json.dump(final_output_structure, f_out, indent=2, default=logger._get_serializer()) # Using logger's serializer
        logger.log(f"INFO: Successfully saved draft keyphrases to '{OUTPUT_DRAFT_KEYPHRASES_JSON}'.")
    except Exception as e:
        logger.log(f"ERROR: Could not write draft keyphrases JSON output: {e}")
        logger.log(f"TRACEBACK:\n{traceback.format_exc()}")

    logger.log("\nINFO: Draft keyphrase generation process complete.")

# Remember to update paths and ensure FileLogger is available when running in Colab.

"""## Stage 4b: Thematic Analysis

You are absolutely right to think that parts of what I described can be automated or at least semi-automated in another script! This would be a logical "Stage 4b" or "Stage 5 - Thematic Analysis Module."

Let's break down which parts are more amenable to automation and which still benefit greatly from human oversight, especially for a nuanced task like identifying "common points" in qualitative data.

**Parts That Can Be Automated or Semi-Automated in a New Script:**

1.  **Loading Data:**
    *   Loading the `stage4a_collated_texts_for_thematic_analysis_...json` file.
    *   Selecting a specific QID to analyze.
    *   Extracting the list of text items for that QID.

2.  **Initial Filtering (Automated with Rules/Keywords):**
    *   **Removing Placeholder/Error Summaries:**
        *   Script can automatically filter out text items where `text` matches known placeholders like "Please provide the responses..." or "This response doesn't address the specific question..."
    *   **Filtering Potentially Off-Topic Summaries (Heuristic):**
        *   This is trickier but can be attempted. For Q4 ("...exceptions...in the employment context?"), you could define:
            *   **Positive keywords:** `employ`, `worker`, `staff`, `workplace`, `HR`, `personnel`, `exception`, `right`, `APP12`, `APP13`, `proposal 18`
            *   **Negative keywords (for Q4):** `small business support`, `child`, `age verification` (if these consistently indicate a summary meant for another question).
        *   The script could flag summaries that have a low count of positive keywords or a high count of negative keywords for a given QID for manual review, or even automatically set them aside. This is a heuristic and needs care.

3.  **N-gram Frequency Analysis (Automated):**
    *   For the filtered texts of a given QID, the script can:
        *   Preprocess the text (lowercase, remove punctuation, potentially remove common stop words, though be careful with domain-specific stop words).
        *   Calculate and list the most frequent unigrams (single words), bi-grams (2-word phrases), and tri-grams (3-word phrases).
    *   **Output:** A ranked list of common phrases. This doesn't give "points" directly but provides strong clues to human reviewers about recurring terminology and potential themes.

4.  **Keyword/Keyphrase Counting (Semi-Automated):**
    *   **After initial human review identifies potential "points" or keyphrases representing those points:**
    *   The script can take a list of these keyphrases (e.g., "employee records exemption," "balance privacy public interest," "international standards alignment").
    *   It can then search for these exact phrases (or variants using regular expressions) within the text items for the QID.
    *   **Counting (with de-duplication):**
        *   For each keyphrase found in a text item, record the `response_id`.
        *   The count for the "point" represented by that keyphrase would be the number of unique `response_id`s associated with it.

5.  **Basic Topic Modeling (Automated, but requires human interpretation):**
    *   As discussed before, you could run LDA or NMF on the collected texts for a QID.
    *   The script automates the model training and outputting the top words for each topic.
    *   **Human step:** Someone still needs to look at the word lists for each topic and assign a meaningful "point" or "theme" label to it. The script can then count how many documents (and unique `response_id`s) are strongly associated with each (now labeled) topic.

6.  **LLM-based Point Extraction and Clustering (More Complex Automation):**
    *   This is the most advanced automation.
    *   **Automated parts:** Sending texts to an LLM for point extraction, embedding those points, running clustering algorithms.
    *   **Human/Semi-Automated parts:** Crafting the initial LLM prompt for point extraction, interpreting/labeling the resulting clusters. The script could then count items per (labeled) cluster.

**What Remains Primarily Human-Driven (even with automation support):**

1.  **Defining "Points" / Creating the Codebook:** Even if n-grams or topic models suggest themes, a human needs to synthesize these into coherent, distinct "points" or arguments relevant to the policy question. For example, "employee records" and "exemption" might be frequent bi-grams; a human decides the "point" is "Discussion around the employee records exemption."
2.  **Interpreting Nuance and Context:** Language is complex. Automated methods might miss sarcasm, implied meanings, or the subtle ways different respondents frame similar ideas.
3.  **Handling Ambiguity:** Some responses might be vague or touch on multiple points. Humans are better at disambiguating these.
4.  **Validating Automated Outputs:** If you use topic modeling or LLM clustering, a human needs to review the results to ensure they make sense and accurately reflect the data.
5.  **Deciding on Thresholds for "Common":** What constitutes "common"? Top 3 by count? Or points mentioned by >X% of unique respondents? This is a human judgment call.

**How this Script Automates and What it Achieves:**

1.  **Loads Stage 4a Output:** Takes the JSON from the previous collation step.
2.  **Focuses per QID:** Designed to be run (or looped) for each QID you want to analyze.
3.  **Automated Filtering:** Includes basic filtering for known placeholder summaries.
4.  **Automated N-gram Generation:** Provides top bi-grams and tri-grams as exploratory data to *aid human theme identification*.
5.  **Semi-Automated Keyphrase Counting:**
    *   The script includes `count_keyphrases`. This function is automated.
    *   **The human input needed:** Defining the `keyphrases_dict`. This dictionary maps a "Point Label" (e.g., "Concerns about Employee Records Exemption") to a list of actual phrases/keywords that indicate this point (e.g., `["employee records exemption", "record exemption"]`). This dictionary would be built iteratively by humans looking at the texts, n-grams, etc.
    *   It counts unique `response_id`s for each point.
6.  **Structured Output:** Saves the n-grams and the keyphrase counts into a new JSON file, structured by QID.

**Iterative Workflow for this New Script:**

1.  **Run Stage 4a** (the script from our previous discussion) to get `stage4a_collated_texts_for_thematic_analysis_...json`.
2.  **Run this new "Thematic Analysis" script for a target QID (e.g., Q4).**
    *   Initially, the `keyphrases_dict` might be empty or have just a few guesses.
3.  **Review the output:**
    *   Look at the `top_bigrams` and `top_trigrams`. Do they suggest common themes?
    *   Manually read a sample of the actual texts for Q4 from the Stage 4a output.
4.  **Refine `keyphrases_dict`:** Based on your review, update the `keyphrases_dict` within the script (or load it from an external config file for better management) with more accurate and comprehensive phrases for the points you're identifying for Q4.
5.  **Re-run this "Thematic Analysis" script for Q4.** The `identified_points_and_counts` will now be based on your refined keyphrases.
6.  Repeat steps 3-5 until you are satisfied with the identified points and their counts for Q4.
7.  Once you have good counts, select the top 3 (or more) for your report.
8.  Move to the next QID and repeat the process.

This script provides a framework. The keyphrase definition is the human-in-the-loop part that makes the "point" identification accurate. The automation helps with processing the text once those definitions are established. You can make the keyphrase definition more sophisticated (e.g., using regex for variations) as needed.

"""

import json
from collections import Counter, defaultdict
import re
# Potentially: from sklearn.feature_extraction.text import CountVectorizer (for n-grams)
# Potentially: from nltk.corpus import stopwords (if doing stopword removal)
# Potentially: LLM libraries if going that route

# (Assume FileLogger is available and initialized as 'logger')

def preprocess_text_for_ngram(text, custom_stopwords=None):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text) # Remove punctuation
    text = re.sub(r'\d+', '', text) # Remove numbers
    # Optional: stopword removal
    if custom_stopwords:
        words = text.split()
        words = [word for word in words if word not in custom_stopwords]
        text = " ".join(words)
    return text

def get_ngrams(text_list, n, top_k=20):
    # Simplified n-gram counter (sklearn's CountVectorizer is more robust)
    all_ngrams = []
    for text in text_list:
        processed_text = preprocess_text_for_ngram(text) # Basic preprocessing
        words = processed_text.split()
        if len(words) < n:
            continue
        for i in range(len(words) - n + 1):
            all_ngrams.append(" ".join(words[i:i+n]))
    return Counter(all_ngrams).most_common(top_k)

def count_keyphrases(text_list_with_ids, keyphrases_dict):
    """
    Counts unique response_ids associated with each keyphrase.
    keyphrases_dict: {'Point Label 1': ['keyword1a', 'keyword1b'], 'Point Label 2': ['keyword2a']}
    text_list_with_ids: [{'response_id': 'id1', 'text': '...'}, ...]
    """
    point_counts = defaultdict(lambda: {"count": 0, "response_ids": set()})
    for item in text_list_with_ids:
        text_lower = item['text'].lower()
        response_id = item['response_id']
        for point_label, phrases in keyphrases_dict.items():
            for phrase in phrases:
                if phrase.lower() in text_lower: # Simple substring match
                    point_counts[point_label]["response_ids"].add(response_id)
                    break # Count response once per point label, even if multiple phrases match

    # Finalize counts
    for point_label in point_counts:
        point_counts[point_label]["count"] = len(point_counts[point_label]["response_ids"])
    return point_counts


def thematic_analysis_stage(input_collated_texts_json, output_analysis_json, qid_to_analyze):
    logger.log(f"INFO: Starting Thematic Analysis for QID: {qid_to_analyze}")

    try:
        with open(input_collated_texts_json, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        logger.log(f"ERROR: Could not load input JSON '{input_collated_texts_json}': {e}")
        return

    aggregated_content = data.get("aggregated_content_by_qid", {})
    if qid_to_analyze not in aggregated_content:
        logger.log(f"ERROR: QID '{qid_to_analyze}' not found in the input JSON.")
        return

    text_items_for_qid = aggregated_content[qid_to_analyze]
    logger.log(f"INFO: Found {len(text_items_for_qid)} text items for QID '{qid_to_analyze}'.")

    # 1. Automated Filtering (Example)
    filtered_text_items = []
    placeholders_to_remove = [
        "please provide the responses",
        "doesn't address the specific question",
        "i need the text of the responses"
    ]
    for item in text_items_for_qid:
        text_lower = item['text'].lower()
        if not any(placeholder in text_lower for placeholder in placeholders_to_remove):
            filtered_text_items.append(item)
    logger.log(f"INFO: After filtering placeholders, {len(filtered_text_items)} items remain for QID '{qid_to_analyze}'.")

    # Extract just the text for n-gram analysis
    texts_for_ngram = [item['text'] for item in filtered_text_items]

    # 2. N-gram Frequency Analysis (Automated)
    top_bigrams = get_ngrams(texts_for_ngram, 2, top_k=30)
    top_trigrams = get_ngrams(texts_for_ngram, 3, top_k=30)
    logger.log(f"INFO: Top Bigrams for {qid_to_analyze}: {top_bigrams}")
    logger.log(f"INFO: Top Trigrams for {qid_to_analyze}: {top_trigrams}")

    # 3. Keyphrase Counting (Semi-Automated - requires human-defined keyphrases)
    # This part would typically be iterative.
    # Iteration 1: Human reviews n-grams, CAR_35, and a sample of texts, then defines initial keyphrases.
    # Example keyphrases for Q4 (these would come from human analysis):
    q4_keyphrases_from_human_analysis = {
        "Employee Records Exemption Discussion": ["employee records exemption", "employee record exemption"],
        "Balancing Privacy with Public/Business Interests": ["balance privacy", "competing public interest", "business practicalit", "operational necessit"],
        "Alignment with International Standards/GDPR": ["international standard", "gdpr", "global data protection", "harmonisation"],
        "Exceptions for Legal Obligations/Investigations": ["legal obligation", "law enforcement", "investigation"],
        "Concerns about Small Business Impact": ["small business", "burden on smes"],
        "Protection for Children/Vulnerable Individuals": ["children", "vulnerable individuals", "under 18"] # Check relevance to Q4 context
    }
    # For a generic script, you might load these keyphrases from a config file per QID

    # Use keyphrases relevant to the qid_to_analyze
    # For this example, let's assume qid_to_analyze is Q4
    current_qid_keyphrases = {}
    if qid_to_analyze == "Q4":
        current_qid_keyphrases = q4_keyphrases_from_human_analysis
    else:
        logger.log(f"WARN: No predefined keyphrases for {qid_to_analyze}. Keyphrase counting will be skipped or use a generic set.")
        # You could have a default empty dict or a mechanism to load them dynamically

    counted_points = {}
    if current_qid_keyphrases:
        # Prepare list of dicts with 'response_id' and 'text' for count_keyphrases
        text_list_with_ids_for_counting = [{'response_id': item['response_id'], 'text': item['text']} for item in filtered_text_items]
        counted_points = count_keyphrases(text_list_with_ids_for_counting, current_qid_keyphrases)
        logger.log(f"INFO: Keyphrase Counts (unique responses) for {qid_to_analyze}:")
        for point, data in sorted(counted_points.items(), key=lambda item: item[1]["count"], reverse=True):
            logger.log(f"  - \"{point}\": {data['count']} unique responses")

    # 4. Prepare Output
    analysis_output = {
        "qid": qid_to_analyze,
        "question_text": "Get this from CAR_35 or Stage4a output metadata if available", # TODO: Get actual Q text
        "total_items_analyzed_for_qid": len(filtered_text_items),
        "top_bigrams": top_bigrams,
        "top_trigrams": top_trigrams,
        "identified_points_and_counts": dict(counted_points), # Convert defaultdict
        "notes": "Counts for 'identified_points_and_counts' are based on unique response_ids mentioning predefined keyphrases. Keyphrases require human definition and iteration."
    }

    # Append to a JSONL file for multiple QIDs, or save one JSON per QID
    # For simplicity, let's assume one main output JSON that can be updated
    # This part needs refinement based on how you want to store results for ALL QIDs

    # For now, let's just log the intended output for this QID
    logger.log_json(analysis_output, prefix=f"ThematicAnalysis_{qid_to_analyze}")

    # If saving to a single aggregated output file:
    try:
        # Load existing data if file exists, otherwise start fresh
        all_qids_analysis_data = {}
        if os.path.exists(output_analysis_json):
            with open(output_analysis_json, 'r', encoding='utf-8') as f_in:
                all_qids_analysis_data = json.load(f_in)

        all_qids_analysis_data[qid_to_analyze] = analysis_output # Add/update analysis for current QID

        with open(output_analysis_json, 'w', encoding='utf-8') as f_out:
            json.dump(all_qids_analysis_data, f_out, indent=2, default=logger._get_serializer())
        logger.log(f"INFO: Successfully saved/updated thematic analysis to '{output_analysis_json}' for QID {qid_to_analyze}")
    except Exception as e:
        logger.log(f"ERROR: Could not write thematic analysis output for {qid_to_analyze} to '{output_analysis_json}': {e}")

    logger.log(f"INFO: Thematic Analysis for QID: {qid_to_analyze} complete.")


# --- Main execution block for the new Thematic Analysis Stage ---
# (Assume FileLogger class is defined and 'logger' instance is created as in previous script)

# Example usage:
SCRIPT_LOG_FILE_THEMATIC = 'stage4b_thematic_analysis.log'
logger = FileLogger(SCRIPT_LOG_FILE_THEMATIC) # Assuming logger is re-initialized or passed

INPUT_COLLATED_TEXTS_JSON = 'stage4a_collated_texts_for_thematic_analysis_20250515_060005.json' # From Stage 4a
OUTPUT_THEMATIC_ANALYSIS_JSON = 'stage4b_thematic_analysis_results.json'

QID_TO_PROCESS = "Q4" # Example: Process Q4

if 'logger' not in globals() or not isinstance(logger, FileLogger):
  print("CRITICAL: 'logger' is not defined as a FileLogger instance. Please initialize it.")
else:
  if not os.path.exists(INPUT_COLLATED_TEXTS_JSON):
    logger.log(f"ERROR: Input file for thematic analysis not found: {INPUT_COLLATED_TEXTS_JSON}")
  else:
    thematic_analysis_stage(INPUT_COLLATED_TEXTS_JSON, OUTPUT_THEMATIC_ANALYSIS_JSON, QID_TO_PROCESS)
# You would loop or call this for multiple QIDs



"""## Stage 4a: Text Collation for Thematic Analysis

This stage processes the output of Stage 4 (pdf_passage_analysis_by_response_minilm_gemma.json) to create consolidated textual datasets for each consultation question, prioritizing the AI-generated summaries. The output of Stage 4a, stage4a_collated_texts_for_thematic_analysis.json, will serve as the primary input for the subsequent manual and semi-automated thematic analysis aimed at identifying common points...
"""

# Extract summaries or passages from Stage 4 output file
import json
import os
import glob
from collections import defaultdict
from typing import Dict, Any, List, Tuple, Optional
import re # For cleaning up multiple newlines/spaces if needed
import time # For timestamp in output filename and logger
import sys # For checking sys.modules in FileLogger (already there)
# The FileLogger class you provided also implicitly uses numpy and torch if available.
# Ensure these are installed in your Colab environment if you expect to log objects of these types.
# For this specific script's core logic, numpy and torch are not directly used.
import numpy as np # Assuming FileLogger might encounter these
import torch # Assuming FileLogger might encounter these


# --- Assuming FileLogger class is defined in a previous cell or imported ---
class FileLogger:
    def __init__(self, log_file):
        self.log_file = log_file; log_dir = os.path.dirname(log_file)
        if log_dir and not os.path.exists(log_dir): os.makedirs(log_dir, exist_ok=True)
        try:
            with open(self.log_file, 'w', encoding='utf-8') as f: f.write(f"{self._get_timestamp()}: Starting new log...\n")
        except IOError as e: print(f"CRITICAL: Failed to initialize log file {self.log_file}: {e}")
    def _get_timestamp(self): return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())
    def log(self, message: str):
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(f"{self._get_timestamp()}: {message}\n")
        except Exception as e: print(f"!!! LOGGER FAILED TO WRITE: {e} !!!"); print(f"{self._get_timestamp()}: {message}")
    def _get_serializer(self):
        def default_serializer(obj):
            if isinstance(obj, (np.integer, np.floating)): return obj.item()
            if isinstance(obj, np.ndarray): return obj.tolist()
            if isinstance(obj, set): return sorted(list(obj))
            if 'torch' in sys.modules and isinstance(obj, torch.Tensor): return obj.detach().cpu().numpy().tolist()
            try: return json.JSONEncoder().default(obj)
            except TypeError: return f"UNSERIALIZABLE:{type(obj).__name__}"
        return default_serializer
    def log_json(self, data: Dict[str, Any], prefix="JSON_DATA"):
        serializer = self._get_serializer(); log_entry = { "timestamp": self._get_timestamp(), "type": prefix, "data": data }
        try:
            json_string = json.dumps(log_entry, default=serializer, indent=2)
            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(json_string + "\n")
        except Exception as e: self.log(f"ERROR: Unexpected error during log_json (prefix={prefix}): {e}")
# --- End of FileLogger assumption ---


# --- Helper Functions ---
def clean_text(text):
    """Optional: Basic text cleaning."""
    if not text or not isinstance(text, str):
        return ""
    text = re.sub(r'\s*\n\s*', '\n', text) # Consolidate newlines
    text = re.sub(r'[ \t]+', ' ', text)    # Consolidate spaces/tabs
    return text.strip()


def extract_and_aggregate_content_to_json(input_path, output_json_file, logger_instance):
    """
    Extracts summaries (prioritized) or passages from Stage 4 output files
    and aggregates them per QID into a single JSON output file.

    Args:
        input_path (str): Path to a single Stage 4 JSON file or a directory
                          containing multiple Stage 4 JSON files.
        output_json_file (str): Path to the output JSON file where aggregated
                                texts per QID will be saved.
        logger_instance (FileLogger): An instance of the FileLogger class.
    """
    global logger # Make logger available globally within this function if not passed
    logger = logger_instance

    if not os.path.exists(input_path):
        logger.log(f"ERROR: Input path '{input_path}' does not exist.")
        return

    output_dir = os.path.dirname(output_json_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        logger.log(f"INFO: Created output directory: '{output_dir}'")

    if os.path.isfile(input_path):
        files_to_process = [input_path]
    elif os.path.isdir(input_path):
        files_to_process = glob.glob(os.path.join(input_path, 'pdf_passage_analysis*.json'))
        if not files_to_process:
            files_to_process = glob.glob(os.path.join(input_path, '*.json'))
    else:
        logger.log(f"ERROR: Input path '{input_path}' is neither a file nor a directory.")
        return

    if not files_to_process:
        logger.log(f"INFO: No JSON files found to process in '{input_path}'.")
        return

    logger.log(f"INFO: Found {len(files_to_process)} file(s) to process: {files_to_process}")

    qid_collected_content = defaultdict(list)
    total_responses_processed_count = 0 # This will count unique response_content dicts encountered
    processed_response_ids_from_all_files = set() # To track unique response IDs across files for counting
    total_content_items_collected = 0
    summaries_collected_count = 0
    passages_collected_count = 0

    for file_idx, file_path in enumerate(files_to_process):
        logger.log(f"INFO: Processing file {file_idx + 1}/{len(files_to_process)}: '{os.path.basename(file_path)}'...")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                stage4_data_full = json.load(f)

            stage4_data = stage4_data_full.get("pdf_analysis_by_response")
            if stage4_data is None:
                logger.log(f"  WARN: 'pdf_analysis_by_response' key not found in '{file_path}'. Skipping this file.")
                continue
            if not isinstance(stage4_data, dict):
                logger.log(f"  WARN: 'pdf_analysis_by_response' is not a dictionary in '{file_path}'. Skipping this file.")
                continue

            file_responses_count = 0
            for response_id, response_content in stage4_data.items():
                if not isinstance(response_content, dict):
                    continue # Skip status entries

                file_responses_count +=1
                processed_response_ids_from_all_files.add(response_id)


                for qid, q_data in response_content.items():
                    if not isinstance(q_data, dict):
                        continue

                    content_item = {
                        "source_file": os.path.basename(file_path),
                        "response_id": response_id,
                        "question_id": qid,
                        "type": None,
                        "text": None
                    }
                    text_to_add = None

                    summary_text = q_data.get("summary")
                    is_summary_valid = (
                        summary_text is not None and
                        isinstance(summary_text, str) and
                        summary_text.strip()
                    )

                    if is_summary_valid:
                        text_to_add = clean_text(summary_text)
                        content_item["type"] = "summary"
                        summaries_collected_count +=1
                    else:
                        passages = q_data.get("extracted_passages")
                        if passages and isinstance(passages, list):
                            valid_passages = [p for p in passages if isinstance(p, str) and p.strip()]
                            if valid_passages:
                                passage_block = "\n\n".join(valid_passages)
                                text_to_add = clean_text(passage_block)
                                content_item["type"] = "passages"
                                passages_collected_count +=1

                    if text_to_add:
                        content_item["text"] = text_to_add
                        qid_collected_content[qid].append(content_item)
                        total_content_items_collected += 1

            logger.log(f"  INFO: Finished processing {file_responses_count} response entries from '{os.path.basename(file_path)}'.")

        except FileNotFoundError:
            logger.log(f"  ERROR: File not found: '{file_path}'. Skipping.")
        except json.JSONDecodeError as e:
            logger.log(f"  ERROR: Invalid JSON in file '{file_path}': {e}. Skipping.")
        except Exception as e:
            logger.log(f"  ERROR: Unexpected error processing file '{file_path}': {e}")
            import traceback
            logger.log(f"TRACEBACK:\n{traceback.format_exc()}")

    total_responses_processed_count = len(processed_response_ids_from_all_files)


    final_output_data = {
        "metadata": {
            "creation_timestamp": time.strftime('%Y-%m-%d %H:%M:%S %Z', time.gmtime()),
            "source_input_path": input_path,
            "total_files_processed": len(files_to_process),
            "total_unique_qids_found": len(qid_collected_content),
            "total_content_items_collected": total_content_items_collected,
            "summaries_collected_count": summaries_collected_count,
            "passages_collected_count": passages_collected_count,
            "total_unique_responses_with_content_processed": total_responses_processed_count,
        },
        "aggregated_content_by_qid": dict(qid_collected_content)
    }

    logger.log(f"\n--- Aggregation Summary ---")
    for key, value in final_output_data["metadata"].items():
        logger.log(f"  {key}: {value}")

    logger.log(f"\nINFO: Saving aggregated content to '{output_json_file}'...")
    try:
        with open(output_json_file, 'w', encoding='-utf-8') as outfile:
            # Use the logger's serializer if it's more robust for your data types
            # otherwise, standard json.dump is fine for dicts of strings.
            json.dump(final_output_data, outfile, indent=2, default=logger._get_serializer() if hasattr(logger, '_get_serializer') else None)
        logger.log(f"INFO: Successfully saved aggregated content to '{output_json_file}'.")
    except Exception as e:
        logger.log(f"ERROR: Could not write output JSON file '{output_json_file}': {e}")
        logger.log(f"TRACEBACK:\n{traceback.format_exc()}")


    logger.log("\nINFO: Extraction process complete.")


# --- How to use the script (when running as a Colab cell) ---
# Make sure FileLogger class is defined in a previous cell or imported.

# Define where the log file for this script will be saved
SCRIPT_LOG_FILE = 'stage4a_text_collation.log' # UPDATE THIS PATH
logger = FileLogger(SCRIPT_LOG_FILE) # Initialize the logger

# OPTION 1: Process a single Stage 4 output file
# INPUT_STAGE4_PATH = '/content/drive/MyDrive/Colab Notebooks/Legal/pdf_passage_analysis_by_response_minilm_gemma.json' # UPDATE

# OPTION 2: Process all relevant .json files in a directory
INPUT_STAGE4_PATH = 'Stage4_Outputs' # UPDATE / CREATE THIS FOLDER

timestamp_str = time.strftime("%Y%m%d_%H%M%S")
OUTPUT_AGGREGATED_JSON = f'stage4a_collated_texts_for_thematic_analysis_{timestamp_str}.json' # UPDATE


# --- Safety Check ---
if '/Stage4_Outputs/' in INPUT_STAGE4_PATH and not os.path.exists(INPUT_STAGE4_PATH) and os.path.isdir(INPUT_STAGE4_PATH):
    logger.log("="*60)
    logger.log("!!! WARNING: The example INPUT_STAGE4_PATH directory for multiple files:")
    logger.log(f"!!! '{INPUT_STAGE4_PATH}'")
    logger.log("!!! does not exist. Please create it and place your Stage 4 JSON output")
    logger.log("!!! files there, or update the path to a single file if preferred.")
    logger.log("!!! Script will not run effectively until paths are correct.")
    logger.log("="*60)
elif 'path/to/your' in INPUT_STAGE4_PATH:
    logger.log("="*60)
    logger.log("!!! IMPORTANT: Please update 'INPUT_STAGE4_PATH' with the actual path")
    logger.log("!!! to your Stage 4 output file or directory.")
    logger.log("!!! Also, update 'OUTPUT_AGGREGATED_JSON' and 'SCRIPT_LOG_FILE' if needed.")
    logger.log("="*60)
else:
    try:
        extract_and_aggregate_content_to_json(INPUT_STAGE4_PATH, OUTPUT_AGGREGATED_JSON, logger)
    except NameError as ne:
        if 'FileLogger' in str(ne):
            print("="*70)
            print("CRITICAL ERROR: The 'FileLogger' class is not defined in the current scope.")
            print("Please ensure the FileLogger class definition is executed in a cell before this one.")
            print("="*70)
        else:
            raise # Re-raise other NameErrors
    except Exception as e_main:
        # Use logger if available, otherwise print
        if 'logger' in globals() and logger:
            logger.log(f"FATAL ERROR in script execution: {e_main}")
            logger.log(f"TRACEBACK:\n{traceback.format_exc()}")
        else:
            print(f"FATAL ERROR in script execution: {e_main}")
            import traceback
            traceback.print_exc()

"""# 14th May

The plan is to automate merging of existing partial Stage 4 work, then Complete Stage 4, then Thematic Reporting as per initial problem statement given on 28th Feb
"""

# @title --- Cell: Stage 4 Merge Script ---
import json
import os
import traceback
from collections import defaultdict # Not strictly needed if just overwriting, but good for other merge strategies

# Ensure FileLogger class is defined in a cell above or imported
# For example:
# class FileLogger:
#     def __init__(self, log_file): self.log_file = log_file; # ... (rest of FileLogger) ...
#     def _get_serializer(self): # Ensure this handles numpy if it might appear
#         import numpy as np # Import if numpy types might be in your data
#         def default_serializer(obj):
#             if isinstance(obj, (np.integer, np.floating)): return obj.item()
#             if isinstance(obj, np.ndarray): return obj.tolist()
#             # Add other type handling if necessary (e.g., torch.Tensor)
#             try: return json.JSONEncoder().default(obj)
#             except TypeError: return f"UNSERIALIZABLE:{type(obj).__name__}"
#         return default_serializer


# --- Configuration for Merging ---
PARTIAL_STATES_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/Legal/partial_stage4_states_for_merge/' # ADJUST THIS PATH
CONSOLIDATED_STATE_FILE = '/content/drive/MyDrive/Colab Notebooks/Legal/consolidated_stage4_processing_state.json' # ADJUST THIS PATH
CONSOLIDATED_OUTPUT_JSON = '/content/drive/MyDrive/Colab Notebooks/Legal/pdf_passage_analysis_by_response_minilm_gemma.json' # ADJUST THIS PATH
MERGE_LOG_FILE = '/content/drive/MyDrive/Colab Notebooks/Legal/stage4_merge_process.log'

# --- Initialize Logger for this script ---
merge_logger = FileLogger(MERGE_LOG_FILE)
merge_logger.log("INFO: Starting Stage 4 partial state and output merge process.")

def merge_partial_stage4_data(partial_states_dir, cons_state_file, cons_output_json):
    master_processed_ids = set()
    # master_results_dict holds: {response_id: {qid_data_for_that_response}}
    # This comes from the "results" key in the state files.
    master_results_payload = {}

    if not os.path.isdir(partial_states_dir):
        merge_logger.log(f"ERROR: Partial states directory not found: {partial_states_dir}")
        return False

    # Ensure files are processed in a somewhat consistent order (e.g., by name)
    # This helps if there are true overlaps and you want the "latest" version based on filename.
    partial_state_files = sorted([
        os.path.join(partial_states_dir, f) for f in os.listdir(partial_states_dir)
        if f.startswith("stage4_processing_state") and f.endswith(".json")
    ])

    if not partial_state_files:
        merge_logger.log(f"WARN: No partial state files matching pattern found in {partial_states_dir}")
        # If no partials, you might want to check if a single, older consolidated file exists
        # or if this is truly a fresh start for the consolidation.
        # For now, assume if no files, nothing to merge, but this might be an error condition.
        return True # No files to merge, so technically "successful" in doing nothing.

    merge_logger.log(f"INFO: Found {len(partial_state_files)} partial state files to merge.")

    for state_file_path in partial_state_files:
        merge_logger.log(f"INFO: Processing partial state file: {state_file_path}")
        try:
            with open(state_file_path, 'r', encoding='utf-8') as f_partial_state:
                partial_state_data = json.load(f_partial_state)

            batch_ids = set(partial_state_data.get("processed_responses_ids", []))
            # 'results' in the state file is the {response_id: {qid_data}} structure
            batch_results_payload = partial_state_data.get("results", {})

            master_processed_ids.update(batch_ids)

            for resp_id, resp_data in batch_results_payload.items():
                if resp_id in master_results_payload and master_results_payload[resp_id] != resp_data:
                    # This indicates a response ID was processed differently in different batches,
                    # or the same response was processed in multiple overlapping batches.
                    # The simple overwrite here takes the version from the "later" file (due to sorting).
                    merge_logger.log(f"WARN: Response ID {resp_id} found in multiple partial states with potentially different data. Overwriting with data from {state_file_path}.")
                master_results_payload[resp_id] = resp_data
            merge_logger.log(f"INFO: Merged {len(batch_ids)} IDs and data for {len(batch_results_payload)} responses from {state_file_path}")

        except FileNotFoundError:
            merge_logger.log(f"ERROR: File not found during merge loop: {state_file_path}. Skipping.")
        except json.JSONDecodeError:
            merge_logger.log(f"ERROR: JSON decode error in {state_file_path}. Skipping.")
        except Exception as e_merge:
            merge_logger.log(f"ERROR: Failed to process partial state file {state_file_path}: {e_merge}")
            merge_logger.log(traceback.format_exc())
            # Decide: continue or stop on error? For now, continue to try and salvage as much as possible.

    merge_logger.log(f"INFO: Merge complete. Total unique processed IDs collected: {len(master_processed_ids)}. Total unique response result entries: {len(master_results_payload)}.")

    # Save Consolidated State File
    consolidated_state_data_to_save = {
        "processed_responses_ids": sorted(list(master_processed_ids)),
        "results": master_results_payload # This is the merged content
    }
    try:
        with open(cons_state_file, 'w', encoding='utf-8') as f_state_out:
            # Use FileLogger's serializer if available and handles complex types like numpy
            # If merge_logger._get_serializer exists and handles types from Stage 4 output
            json.dump(consolidated_state_data_to_save, f_state_out, indent=2, default=getattr(merge_logger, '_get_serializer', None))
        merge_logger.log(f"INFO: Created consolidated Stage 4 state file: {cons_state_file}")
    except Exception as e_save_state:
        merge_logger.log(f"ERROR: Failed to save consolidated state file {cons_state_file}: {e_save_state}")
        merge_logger.log(traceback.format_exc())
        return False

    # Save Consolidated Stage 4 Output JSON (which is used by Stage 5)
    # The structure is {"pdf_analysis_by_response": master_results_payload}
    final_output_structure_s4 = {"pdf_analysis_by_response": master_results_payload}
    try:
        with open(cons_output_json, 'w', encoding='utf-8') as f_json_out:
            json.dump(final_output_structure_s4, f_json_out, indent=2, default=getattr(merge_logger, '_get_serializer', None))
        merge_logger.log(f"INFO: Created consolidated Stage 4 output JSON: {cons_output_json}")
    except Exception as e_save_json:
        merge_logger.log(f"ERROR: Failed to save consolidated output JSON {cons_output_json}: {e_save_json}")
        merge_logger.log(traceback.format_exc())
        return False

    return True

# --- Main call to the merge function ---
if __name__ == "__main__": # This guard prevents execution if just importing
    merge_success = merge_partial_stage4_data(
        PARTIAL_STATES_DIRECTORY,
        CONSOLIDATED_STATE_FILE,
        CONSOLIDATED_OUTPUT_JSON
    )
    if merge_success:
        merge_logger.log("SUCCESS: Stage 4 partial files merged successfully.")
        merge_logger.log(f"Next Step: Configure your main Stage 4 script to use:")
        merge_logger.log(f"  STATE_FILE = '{CONSOLIDATED_STATE_FILE}'")
        merge_logger.log(f"  PDF_ANALYSIS_OUTPUT_FILE = '{CONSOLIDATED_OUTPUT_JSON}'")
        merge_logger.log(f"  And then run it to process any remaining responses.")
    else:
        merge_logger.log("ERROR: Merging Stage 4 partial files failed. Please check logs at {MERGE_LOG_FILE}.")

"""# SEPARATOR

---

# 18th Apr
"""

# =====================================================
# @title Stage 5: Comprehensive Analysis Report Generator
# =====================================================
# Purpose: Loads the final output from Stage 4 (Retrieval + Summary attempts),
#          aggregates relevant content per question (prioritizing non-blank summaries,
#          falling back to passages), performs sentiment analysis and keyword extraction,
#          and reports overall statistics.

# =====================================================
#                      Imports
# =====================================================
import json
import os
import sys
import traceback
import time
from collections import defaultdict, Counter
from typing import Dict, Any, List, Tuple, Optional
import re
import string
import numpy as np

# --- NLP Imports ---
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.sentiment.vader import SentimentIntensityAnalyzer
    from sklearn.feature_extraction.text import TfidfVectorizer
except ImportError as e:
    print(f"CRITICAL ERROR: Missing essential NLP libraries. Install them. Details: {e}")
    print("\nTry: pip install nltk scikit-learn numpy")
    # If using VADER: nltk.download('vader_lexicon') needs to be run once
    sys.exit(1)

# Import logger (assuming FileLogger class is available from previous stages)
# If not available, use the SimpleLogger placeholder
try:
    # Assuming FileLogger class is defined in a file named 'file_logger_module.py'
    # from file_logger_module import FileLogger
    # Placeholder if FileLogger isn't easily importable:
    class SimpleLogger:
        def __init__(self, log_file=None): self.log_file=log_file # Optional log file
        def log(self, msg): print(msg) # Simple print for now
    logger = SimpleLogger()
except ImportError:
     class SimpleLogger:
        def __init__(self, log_file=None): self.log_file=log_file
        def log(self, msg): print(msg)
     logger = SimpleLogger()

class FileLogger:
    def __init__(self, log_file):
        self.log_file = log_file; log_dir = os.path.dirname(log_file)
        if log_dir and not os.path.exists(log_dir): os.makedirs(log_dir, exist_ok=True)
        try:
            # Use 'a' to append if resuming is common, 'w' to start fresh each time
            # Let's use 'w' for now, assuming state file handles resume logic primarily
            with open(self.log_file, 'w', encoding='utf-8') as f: f.write(f"{self._get_timestamp()}: Starting new log...\n")
        except IOError as e: print(f"CRITICAL: Failed to initialize log file {self.log_file}: {e}")
    def _get_timestamp(self): return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())
    def log(self, message: str):
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(f"{self._get_timestamp()}: {message}\n")
        except Exception as e: print(f"!!! LOGGER FAILED TO WRITE: {e} !!!"); print(f"{self._get_timestamp()}: {message}") # Fallback print
    def _get_serializer(self):
        # Define a nested function to handle custom serialization types
        def default_serializer(obj):
            if isinstance(obj, (np.integer, np.floating)): return obj.item() # Convert to standard Python int/float
            if isinstance(obj, np.ndarray): return obj.tolist() # Convert to Python list
            if isinstance(obj, set): return sorted(list(obj)) # Convert to sorted list for consistent output
            # Handle torch tensors if torch is imported
            if 'torch' in sys.modules and isinstance(obj, torch.Tensor): return obj.detach().cpu().numpy().tolist()
            # If none of the above, try the default JSON encoder
            try: return json.JSONEncoder().default(obj) # Will raise TypeError if not serializable
            except TypeError: return f"UNSERIALIZABLE:{type(obj).__name__}" # Return string representation
        return default_serializer # Return the nested function
    def log_json(self, data: Dict[str, Any], prefix="JSON_DATA"):
        serializer = self._get_serializer(); log_entry = { "timestamp": self._get_timestamp(), "type": prefix, "data": data }
        try:
            json_string = json.dumps(log_entry, default=serializer, indent=2)
            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(json_string + "\n")
        except Exception as e: self.log(f"ERROR: Unexpected error during log_json (prefix={prefix}): {e}") # Note: Uses self.log here


# =====================================================
#              Pipeline Configuration Constants (Stage 5)
# =====================================================
# Input: Final JSON output from the *full* Stage 4 run (with summary attempts)
FULL_STAGE4_OUTPUT_FILE = 'pdf_passage_analysis_by_response_minilm_gemma.json'
# Output: JSON file containing the structured analysis results per question
ANALYSIS_OUTPUT_FILE = 'comprehensive_analysis_report.json'
# Log file for this stage
STAGE5_LOG_FILE = "stage5_comprehensive_analysis.log"
# Parameters for Analysis
NUM_TOP_KEYWORDS = 20 # Extract more keywords for better insight
USE_VADER_SENTIMENT = True # Keep using VADER for speed

# =====================================================
#                 Logger Initialization
# =====================================================
# logger = FileLogger(STAGE5_LOG_FILE) # Use actual FileLogger if imported
logger = FileLogger(STAGE5_LOG_FILE)

# =====================================================
#         NLTK Setup (Ensure necessary resources)
# =====================================================
def download_nltk_resources_stage5():
    resources = {
        'corpora/stopwords': 'stopwords',
        'tokenizers/punkt': 'punkt',
        'sentiment/vader_lexicon': 'vader_lexicon'
    }
    for resource_id, download_name in resources.items():
        try: nltk.data.find(resource_id)
        except LookupError:
            logger.log(f"INFO: Downloading NLTK resource: {download_name}")
            try: nltk.download(download_name, quiet=True)
            except Exception as e: logger.log(f"ERROR: Failed NLTK download {download_name}: {e}")

logger.log("INFO: Checking NLTK resources for Stage 5...")
download_nltk_resources_stage5()
logger.log("INFO: NLTK check complete.")

# =====================================================
#       Helper Functions for Analysis
# =====================================================
def preprocess_text(text):
    """Basic text cleaning: lowercasing, remove punctuation/numbers, remove stopwords."""
    if not text or not isinstance(text, str): return ""
    text = text.lower()
    # Remove punctuation more thoroughly
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    try:
        stop_words = set(stopwords.words('english'))
        tokens = word_tokenize(text)
        # Keep only alphabetic tokens (removes single letters often left over)
        filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]
        return " ".join(filtered_tokens)
    except Exception as e:
        logger.log(f"WARN: Preprocessing failed: {e}. Text starts: '{text[:50]}...'")
        return "" # Return empty on failure to avoid polluting analysis

def get_sentiment_vader(text):
    """Calculates sentiment using VADER."""
    if not text or not isinstance(text, str): return 'neutral', 0.0
    try:
        # Lazy load analyzer if needed
        if not hasattr(get_sentiment_vader, 'analyzer'):
             get_sentiment_vader.analyzer = SentimentIntensityAnalyzer()
        vs = get_sentiment_vader.analyzer.polarity_scores(text)
        # Classify based on compound score
        if vs['compound'] >= 0.05: sentiment = 'positive'
        elif vs['compound'] <= -0.05: sentiment = 'negative'
        else: sentiment = 'neutral'
        return sentiment, vs['compound']
    except Exception as e:
        logger.log(f"ERROR: VADER sentiment failed: {e}. Text starts: '{text[:50]}...'")
        return 'error', 0.0

def get_top_keywords_tfidf(texts: List[str], num_keywords: int) -> List[Tuple[str, float]]:
    """Extracts top keywords using TF-IDF."""
    if not texts: return []
    valid_texts = [t for t in texts if isinstance(t, str) and t.strip()]
    if not valid_texts: return []
    try:
        processed_texts = [preprocess_text(text) for text in valid_texts]
        processed_texts = [t for t in processed_texts if t]
        if not processed_texts:
             logger.log("WARN: No processable text left after preprocessing for keyword extraction.")
             return []

        vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), stop_words='english') # Use built-in stopwords too
        tfidf_matrix = vectorizer.fit_transform(processed_texts)
        feature_names = vectorizer.get_feature_names_out()
        summed_tfidf = tfidf_matrix.sum(axis=0)
        summed_tfidf_array = np.asarray(summed_tfidf).flatten()
        actual_num_keywords = min(num_keywords, len(feature_names))
        if actual_num_keywords <= 0: return []
        top_indices = np.argsort(summed_tfidf_array)[-actual_num_keywords:][::-1]
        # Ensure scores are standard floats for JSON
        keywords = [(feature_names[i], float(summed_tfidf_array[i])) for i in top_indices]
        return keywords
    except ValueError as ve: logger.log(f"WARN: TF-IDF ValueError: {ve}"); return []
    except Exception as e: logger.log(f"ERROR: TF-IDF extraction failed: {e}"); return []

# =====================================================
#                   Main Execution Block (Stage 5)
# =====================================================
if __name__ == "__main__":
    logger.log("=" * 50); logger.log(" INFO: Starting Stage 5: Comprehensive Analysis Report "); logger.log("=" * 50)
    logger.log(f"INFO: Input Stage 4 Data: {FULL_STAGE4_OUTPUT_FILE}")
    logger.log(f"INFO: Output Analysis File: {ANALYSIS_OUTPUT_FILE}")

    # --- Overall Counters ---
    overall_stats = {
        "total_responses_in_input": 0,
        "responses_with_pdf_content_analyzed": 0,
        "questions_analyzed_count": 0,
        "total_content_items_analyzed": 0, # Sum of summaries + passages used
        "total_non_blank_summaries_used": 0,
        "total_passages_used_as_fallback": 0,
        "overall_sentiment_distribution": Counter(),
        "average_sentiment_compound_overall": [], # Collect all compound scores
    }

    analysis_results_per_qid = {} # Main output structure
    main_success = False
    start_time = time.time()

    try:
        # 1. Load Full Data from Stage 4 Output
        logger.log(f"INFO: Loading full Stage 4 data from {FULL_STAGE4_OUTPUT_FILE}...")
        try:
            with open(FULL_STAGE4_OUTPUT_FILE, 'r', encoding='utf-8') as f: full_stage4_data = json.load(f)
            retrieval_data = full_stage4_data.get("pdf_analysis_by_response") # Use the correct top-level key
            if retrieval_data is None: raise ValueError("'pdf_analysis_by_response' key not found.")
            overall_stats["total_responses_in_input"] = len(retrieval_data)
            logger.log(f"INFO: Loaded Stage 4 data for {overall_stats['total_responses_in_input']} responses.")
        except FileNotFoundError: logger.log(f"ERROR: Stage 4 input file not found: {FULL_STAGE4_OUTPUT_FILE}"); sys.exit(1)
        except (json.JSONDecodeError, ValueError) as e: logger.log(f"ERROR: Failed parsing JSON/invalid format in {FULL_STAGE4_OUTPUT_FILE}: {e}"); sys.exit(1)
        except Exception as e: logger.log(f"ERROR: Unexpected error loading {FULL_STAGE4_OUTPUT_FILE}: {e}"); tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}"); sys.exit(1)

        # 2. Aggregate Content (Summaries or Passages) per Question
        logger.log("INFO: Aggregating content (prioritizing summaries) per question...")
        # Structure: qid -> {"question_text": str, "content_items": list, "source_types": Counter}
        aggregated_content_by_qid = defaultdict(lambda: {"question_text": "", "content_items": [], "source_types": Counter()})
        question_texts = {}

        for response_id, response_data in retrieval_data.items():
            if not isinstance(response_data, dict): continue # Skip simple status entries like {"status": "pdf_missing_or_short"}
            if "status" in response_data and response_data["status"] in ["pdf_missing_or_short", "error_processing_response", "error_pdf_embedding", "pdf_no_sentences", "error_json_decode"]:
                continue # Skip responses where high-level PDF processing failed

            found_content_flag = False
            for qid, q_data in response_data.items():
                if isinstance(q_data, dict):
                    # Store question text once
                    if qid not in question_texts and "question_text" in q_data:
                         question_texts[qid] = q_data["question_text"]

                    content_to_add = None; source_type = "none"

                    # --- Prioritize Non-Blank Summary ---
                    summary_text = q_data.get("summary")
                    if q_data.get("status") == "success_summarized" and summary_text is not None and isinstance(summary_text, str) and summary_text.strip():
                        content_to_add = summary_text.strip()
                        source_type = "summary"
                        overall_stats["total_non_blank_summaries_used"] += 1
                    # --- Fallback to Extracted Passages ---
                    elif q_data.get("extracted_passages"):
                        passages = q_data["extracted_passages"]
                        if isinstance(passages, list) and passages:
                            # Join passages into a single block for analysis
                            passage_block = "\n\n".join(p for p in passages if isinstance(p, str) and p.strip())
                            if passage_block.strip():
                                content_to_add = passage_block
                                source_type = "passages"
                                overall_stats["total_passages_used_as_fallback"] += 1

                    # If content was found, aggregate it
                    if content_to_add:
                        aggregated_content_by_qid[qid]["content_items"].append(content_to_add)
                        aggregated_content_by_qid[qid]["source_types"][source_type] += 1
                        found_content_flag = True

            if found_content_flag:
                overall_stats["responses_with_pdf_content_analyzed"] += 1
        # --- End Aggregation Loop ---

        overall_stats["questions_analyzed_count"] = len(aggregated_content_by_qid)
        logger.log(f"INFO: Aggregated content from {overall_stats['responses_with_pdf_content_analyzed']} responses across {overall_stats['questions_analyzed_count']} questions.")
        logger.log(f"INFO: Total non-blank summaries used: {overall_stats['total_non_blank_summaries_used']}")
        logger.log(f"INFO: Total passage blocks used as fallback: {overall_stats['total_passages_used_as_fallback']}")

        # 3. Perform Analysis on Aggregated Content per Question
        logger.log("INFO: Performing sentiment and keyword analysis per question...")
        for qid, data in aggregated_content_by_qid.items():
            analysis_results_per_qid[qid] = {}
            analysis_results_per_qid[qid]["question_text"] = question_texts.get(qid, f"Unknown Text for {qid}")
            all_content_items = data["content_items"]
            analysis_results_per_qid[qid]["content_analyzed_count"] = len(all_content_items)
            analysis_results_per_qid[qid]["source_distribution"] = dict(data["source_types"])

            if not all_content_items:
                logger.log(f"WARN: No content items to analyze for QID {qid}.")
                analysis_results_per_qid[qid]["sentiment"] = {"distribution": {}, "average_compound": 0.0}
                analysis_results_per_qid[qid]["top_keywords_tfidf"] = []
                continue

            overall_stats["total_content_items_analyzed"] += len(all_content_items)

            # Sentiment Analysis
            sentiments = []; compound_scores = []
            for item in all_content_items:
                sentiment_label, compound_score = get_sentiment_vader(item)
                sentiments.append(sentiment_label)
                compound_scores.append(compound_score)
                # Add to overall counters/lists
                overall_stats["overall_sentiment_distribution"][sentiment_label] += 1
                overall_stats["average_sentiment_compound_overall"].append(compound_score)

            sentiment_distribution = Counter(sentiments)
            average_compound = np.mean(compound_scores) if compound_scores else 0.0
            analysis_results_per_qid[qid]["sentiment"] = {
                "distribution": dict(sentiment_distribution),
                "average_compound": float(average_compound)
            }

            # Keyword Extraction
            top_keywords = get_top_keywords_tfidf(all_content_items, NUM_TOP_KEYWORDS)
            analysis_results_per_qid[qid]["top_keywords_tfidf"] = top_keywords

            if (len(analysis_results_per_qid) % 5 == 0): logger.log(f"INFO: ...analyzed {len(analysis_results_per_qid)}/{len(aggregated_content_by_qid)} questions.")

        logger.log("INFO: Analysis complete.")

        # Calculate final overall stats
        if overall_stats["average_sentiment_compound_overall"]:
            overall_stats["average_sentiment_compound_overall"] = float(np.mean(overall_stats["average_sentiment_compound_overall"]))
        else:
             overall_stats["average_sentiment_compound_overall"] = 0.0
        overall_stats["overall_sentiment_distribution"] = dict(overall_stats["overall_sentiment_distribution"])


        # 4. Prepare and Save Final Report Data
        final_report_data = {
            "overall_analysis_statistics": overall_stats,
            "analysis_by_question": analysis_results_per_qid
        }

        with open(ANALYSIS_OUTPUT_FILE, 'w', encoding='utf-8') as f:
             # Use a serializer that handles numpy floats potentially left from np.mean
             json.dump(final_report_data, f, indent=2, default=logger._get_serializer())
        logger.log(f"INFO: Comprehensive analysis report saved to {ANALYSIS_OUTPUT_FILE}")
        main_success = True

    except Exception as main_e:
        logger.log(f"CRITICAL: FATAL ERROR during analysis: {main_e}")
        tb_str = traceback.format_exc(); logger.log(f"CRITICAL TRACEBACK:\n{tb_str}")

    finally:
        end_time = time.time()
        # Log final overall stats
        logger.log("=" * 20 + " Overall Statistics Summary " + "=" * 20)
        for key, value in overall_stats.items():
            if key == "average_sentiment_compound_overall":
                logger.log(f"  {key}: {value:.4f}")
            else:
                logger.log(f"  {key}: {value}")
        logger.log("=" * 66)
        logger.log(f"INFO: Stage 5 finished in {end_time - start_time:.2f} seconds.")
        logger.log("=" * 50)
        if main_success: logger.log(" INFO: Stage 5 Analysis Finished Successfully ")
        else: logger.log(" ERROR: Stage 5 Analysis Finished With Errors ")
        logger.log("=" * 50)

"""# 17th Apr"""

# =====================================================
# @title Stage 4: PDF Content Analysis (Retrieval & Selective Summarization)
# =====================================================
# Purpose: Processes original responses, extracts PDF text, retrieves
#          relevant passages using local embeddings (MiniLM), selectively
#          generates LLM summaries (Gemma 2B) for retrieved passages,
#          and includes runtime limits with resume capability.

# =====================================================
#                      Imports
# =====================================================
import json
import os
import sys
import traceback
import time
import gc
import re
from collections import defaultdict
from typing import Dict, Any, List, Tuple, Optional
import warnings
import argparse # Kept for potential future use
import tempfile # Needed for test mode

# Suppress specific warnings if needed
warnings.filterwarnings("ignore", message=".*Using the latest cached version.*")
warnings.filterwarnings("ignore", message=".*torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.*")


# --- ML/NLP Imports ---
try:
    import torch
    import numpy as np
    from sentence_transformers import SentenceTransformer, util
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
    import nltk
    from nltk.tokenize import sent_tokenize
except ImportError as e:
    # Cannot use logger before it's initialized
    print(f"CRITICAL ERROR: Missing essential ML/NLP libraries. Install them. Details: {e}")
    print("\nTry: pip install torch numpy sentence-transformers transformers accelerate bitsandbytes nltk scikit-learn")
    sys.exit(1)

# --- Optional Imports ---
try:
    # Only required if USE_QUANTIZATION is True
    from bitsandbytes.cextension import ColabNotSupportedError
except ImportError:
    ColabNotSupportedError = None # Define dummy class if not importable
    pass # Silently ignore if bitsandbytes is not installed but quantization is False


# =====================================================
#                Custom FileLogger Class
# =====================================================
# (Using simplified version compatible with logger.log("LEVEL: ..."))
class FileLogger:
    def __init__(self, log_file):
        self.log_file = log_file; log_dir = os.path.dirname(log_file)
        if log_dir and not os.path.exists(log_dir): os.makedirs(log_dir, exist_ok=True)
        try:
            # Use 'a' to append if resuming is common, 'w' to start fresh each time
            # Let's use 'w' for now, assuming state file handles resume logic primarily
            with open(self.log_file, 'w', encoding='utf-8') as f: f.write(f"{self._get_timestamp()}: Starting new log...\n")
        except IOError as e: print(f"CRITICAL: Failed to initialize log file {self.log_file}: {e}")
    def _get_timestamp(self): return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())
    def log(self, message: str):
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(f"{self._get_timestamp()}: {message}\n")
        except Exception as e: print(f"!!! LOGGER FAILED TO WRITE: {e} !!!"); print(f"{self._get_timestamp()}: {message}") # Fallback print
    def _get_serializer(self):
        # Define a nested function to handle custom serialization types
        def default_serializer(obj):
            if isinstance(obj, (np.integer, np.floating)): return obj.item() # Convert to standard Python int/float
            if isinstance(obj, np.ndarray): return obj.tolist() # Convert to Python list
            if isinstance(obj, set): return sorted(list(obj)) # Convert to sorted list for consistent output
            # Handle torch tensors if torch is imported
            if 'torch' in sys.modules and isinstance(obj, torch.Tensor): return obj.detach().cpu().numpy().tolist()
            # If none of the above, try the default JSON encoder
            try: return json.JSONEncoder().default(obj) # Will raise TypeError if not serializable
            except TypeError: return f"UNSERIALIZABLE:{type(obj).__name__}" # Return string representation
        return default_serializer # Return the nested function
    def log_json(self, data: Dict[str, Any], prefix="JSON_DATA"):
        serializer = self._get_serializer(); log_entry = { "timestamp": self._get_timestamp(), "type": prefix, "data": data }
        try:
            json_string = json.dumps(log_entry, default=serializer, indent=2)
            with open(self.log_file, 'a', encoding='utf-8') as f: f.write(json_string + "\n")
        except Exception as e: self.log(f"ERROR: Unexpected error during log_json (prefix={prefix}): {e}") # Note: Uses self.log here

# =====================================================
#              Pipeline Configuration Constants (Stage 4)
# =====================================================

# --- Input/Output Files ---
ORIGINAL_RESPONSES_FILE = 'processed_responses.jsonl' # Source for raw data + PDF text
STAGE2_AGGREGATION_FILE = 'question_centric_aggregation_output.json' # Source for QID->Text map
PDF_ANALYSIS_OUTPUT_FILE = 'pdf_passage_analysis_by_response_minilm_gemma.json' # Final output
STATE_FILE = 'stage4_processing_state.json' # For resuming partial runs

# --- Model Configuration ---
EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2' # Sticking with MiniLM based on tests
LLM_MODEL_ID = 'google/gemma-3-4b-it' # Corrected to Gemma 2B Instruct
USE_QUANTIZATION = True # Enable 4-bit quantization for LLM

# --- Retrieval & Summarization Parameters ---
MIN_SIMILARITY_SCORE = 0.5    # Threshold for passage retrieval
CONTEXT_WINDOW_SIZE = 1       # Sentences before/after matched sentence for passage context
TOP_K_PASSAGES = 3            # Max relevant passages to retrieve/store per Q
NUM_SUMMARY_SENTENCES = 3     # Target sentences for LLM summary
MIN_ANSWERS_FOR_SUMMARY = 1   # Min number of text answers needed before attempting summary (can increase)
MIN_CHARS_FOR_SUMMARY = 150   # Min total chars in combined passages to attempt summary
LLM_MAX_NEW_TOKENS = 350      # Max tokens for the generated summary (adjust based on NUM_SUMMARY_SENTENCES)

# --- PDF Processing ---
MIN_PDF_TEXT_LENGTH = 100 # Ignore PDFs shorter than this

# --- Runtime & Logging ---
MAX_RUNTIME_MINUTES = 60 # 60 * 10 # e.g., 10 hours (adjust based on Colab limits/needs)
SAVE_STATE_INTERVAL_SECONDS = 60 * 15 # Save progress every 15 minutes
STAGE4_LOG_FILE = "stage4_retrieval_summary_minilm_gemma.log"
ENABLE_DEBUG_LOGGING = False # True # Enable detailed logs, including summary debugging
TEST_SINGLE_RESPONSE_ID = None # Set to integer ID for testing, None for full run

# --- Manual Question Selection for Summarization ---
# Dictionary mapping Question ID -> True/False (whether to summarize)
QUESTIONS_TO_SUMMARIZE = {
    "Q1": True, "Q2": True, "Q3": True, "Q4": True, "Q5": True, "Q6": True, "Q7": True,
    "Q8": True, "Q9": True, "Q10": True,"Q11": True, "Q12": True, "Q13": True, "Q14": True,
    "Q15": True, "Q16": False,"Q17": True, "Q18": True, "Q19": True,"Q20": False, "Q21": True,
    "Q22": True, "Q23": True, "Q24": True, "Q25": True, "Q26": True, "Q27": True, "Q28": True,
    "Q29": True, "Q30": True, "Q31": True,"Q32": False,"Q33": False,"Q34": False,"Q35": False,
    "Q36": False,
    # Add any other QIDs if applicable, defaulting to False if not present
}
# =====================================================
#                 Logger Initialization
# =====================================================
logger = FileLogger(STAGE4_LOG_FILE)
# =====================================================
#         NLTK Setup
# =====================================================
def download_nltk_resource(resource_id, download_name):
    """Checks for an NLTK resource and downloads it if missing."""
    try: nltk.data.find(resource_id); # logger.log(f"INFO: NLTK resource '{download_name}' already present.") # Less verbose
    except LookupError:
        logger.log(f"INFO: NLTK resource '{download_name}' not found. Downloading...");
        try: nltk.download(download_name, quiet=True); logger.log(f"INFO: Downloaded NLTK resource: {download_name}")
        except Exception as dl_e: logger.log(f"ERROR: Failed NLTK download {download_name}: {dl_e}")
    except Exception as e: logger.log(f"ERROR: NLTK check error {download_name}: {e}")
logger.log("INFO: Checking NLTK resources for sentence tokenization...")
download_nltk_resource('tokenizers/punkt', 'punkt')
download_nltk_resource('tokenizers/punkt_tab', 'punkt_tab')
logger.log("INFO: NLTK check complete.")
# =====================================================
#       Helper Functions
# =====================================================
def save_stage4_output(output_file: str, output_data: Dict[str, Any]):
    """Saves the final Stage 4 output JSON."""
    logger.log(f"INFO: Saving Stage 4 output to {output_file}...")
    serializer = logger._get_serializer()
    try:
        # Wrap results in a top-level key for valid JSON
        final_output = {"pdf_analysis_by_response": output_data}
        with open(output_file, 'w', encoding='utf-8') as f: json.dump(final_output, f, indent=2, default=serializer)
        logger.log(f"INFO: Successfully saved Stage 4 output.")
    except Exception as e: logger.log(f"ERROR: Saving Stage 4 output failed: {e}"); tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")

def save_state(state_file: str, processed_ids: set, current_results: Dict[str, Any]):
    """Saves the current processing state to allow resumption."""
    logger.log(f"INFO: Saving processing state to {state_file}...")
    try:
        # Ensure results dict doesn't contain problematic references if possible
        # Convert set to list for JSON compatibility
        state_data = {"processed_responses_ids": sorted(list(processed_ids)), "results": current_results}
        temp_state_file = state_file + ".tmp" # Save to temp first
        with open(temp_state_file, 'w', encoding='utf-8') as f: json.dump(state_data, f, indent=2, default=logger._get_serializer())
        os.replace(temp_state_file, state_file) # Atomic rename/replace
        logger.log(f"INFO: State saved successfully ({len(processed_ids)} responses processed).")
    except Exception as e: logger.log(f"ERROR: Failed to save state to {state_file}: {e}"); tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")

def load_state(state_file: str) -> Tuple[set, Dict[str, Any]]:
    """Loads processing state from a file."""
    if os.path.exists(state_file):
        logger.log(f"INFO: Found state file {state_file}. Attempting to load state...")
        try:
            with open(state_file, 'r', encoding='utf-8') as f: state_data = json.load(f)
            # Convert list back to set, ensure results is a dict
            processed_ids = set(state_data.get("processed_responses_ids", []));
            results = state_data.get("results", {})
            if not isinstance(results, dict): raise TypeError("Loaded 'results' is not a dictionary.")
            logger.log(f"INFO: Successfully loaded state. Resuming after {len(processed_ids)} processed responses.")
            return processed_ids, results
        except Exception as e: logger.log(f"ERROR: Failed to load or parse state from {state_file}: {e}. Corrupted file? Starting fresh."); return set(), {}
    else: logger.log("INFO: No state file found. Starting fresh."); return set(), {}

def get_local_embeddings(texts: List[str], model: SentenceTransformer, device: str, batch_size: int = 64) -> Optional[np.ndarray]:
    """Generates embeddings using a local SentenceTransformer model."""
    if not texts: return None
    try:
        # Corrected Debug Log
        if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG: Generating local embeddings for {len(texts)} texts on {device}...")
        start_emb = time.time(); model.to(device)
        embeddings = model.encode(texts, convert_to_tensor=True, device=device, show_progress_bar=False, batch_size=batch_size)
        emb_time = time.time() - start_emb
        if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG: Local embedding generation took {emb_time:.2f}s")
        return embeddings.detach().cpu().numpy() # Detach before moving to CPU
    except Exception as e: logger.log(f"ERROR: Failed to generate local embeddings: {e}"); tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}"); return None

def retrieve_relevant_passages(question_embedding: np.ndarray, sentences: List[str], sentence_embeddings: np.ndarray, threshold: float, top_k: int, context_window: int) -> Tuple[List[str], float | None]:
    """Finds top K relevant passages above threshold using PyTorch cosine similarity."""
    # (Using corrected version with explicit torch.float32 conversion)
    relevant_passages = []; highest_score_found = None
    if question_embedding is None or sentence_embeddings is None or not sentences: return relevant_passages, highest_score_found
    if question_embedding.ndim == 1: question_embedding = question_embedding.reshape(1, -1)
    if sentence_embeddings.ndim == 1: sentence_embeddings = sentence_embeddings.reshape(1, -1)
    if sentence_embeddings.shape[0] != len(sentences): logger.log(f"ERROR: Sent/Emb count mismatch!"); return [], None
    if question_embedding.shape[1] != sentence_embeddings.shape[1]: logger.log(f"ERROR: Emb dim mismatch!"); return [], None
    try:
        target_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        q_tensor = torch.from_numpy(question_embedding).to(dtype=torch.float32, device=target_device)
        s_tensor = torch.from_numpy(sentence_embeddings).to(dtype=torch.float32, device=target_device)
        cosine_scores_tensor = util.cos_sim(q_tensor, s_tensor)[0]
        cosine_scores = cosine_scores_tensor.cpu().numpy()
        sorted_indices = np.argsort(cosine_scores)[::-1]; added_passage_hashes = set()
        for idx in sorted_indices:
            score = cosine_scores[idx]
            if score < threshold: break
            if highest_score_found is None: highest_score_found = float(score)
            start_idx = max(0, idx - context_window); end_idx = min(len(sentences), idx + context_window + 1)
            if start_idx >= end_idx or start_idx >= len(sentences): continue
            context_sentences = sentences[start_idx:end_idx]; passage_text = " ".join(context_sentences).strip(); passage_hash = hash(passage_text)
            if passage_text and passage_hash not in added_passage_hashes:
                relevant_passages.append(passage_text); added_passage_hashes.add(passage_hash)
                if len(relevant_passages) >= top_k: break
        return relevant_passages, highest_score_found
    except Exception as e: logger.log(f"ERROR: Failed during passage retrieval: {e}"); tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}"); return [], None

def summarize_context_block(text_chunk: str, question_text: str, qid: str, summarization_pipe: pipeline, num_sentences: int) -> Optional[str]:
    """Generates a summary for a given text chunk using the LLM pipeline."""
    # Added qid for specific debugging if needed
    if not text_chunk or not summarization_pipe: return None
    # Using the detailed prompt structure
    messages = [{"role": "user", "content": f"Carefully review all the following responses to the question: \"{question_text}\". Then, concisely summarize the main themes or points expressed across these responses in exactly {num_sentences} distinct sentences. Focus on capturing the essence of the input.\n\nResponses:\n\"\"\"\n{text_chunk}\n\"\"\""}]
    try:
        prompt_formatted = summarization_pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        # Explicitly set generation parameters known to work better for summarization/instruction following
        generation_args = {
            "max_new_tokens": LLM_MAX_NEW_TOKENS,
            "do_sample": False, # More deterministic summaries
            # "temperature": 0.7, # Typically not needed with do_sample=False
            # "top_k": 50,
            # "top_p": 0.95,
            # Add pad_token_id if tokenizer has one and pipeline doesn't handle it
            "pad_token_id": summarization_pipe.tokenizer.pad_token_id if summarization_pipe.tokenizer.pad_token_id else summarization_pipe.tokenizer.eos_token_id
        }
        if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG: Calling LLM for summary QID {qid} ({len(text_chunk)} chars input)...")
        start_llm = time.time(); outputs = summarization_pipe(prompt_formatted, **generation_args); llm_time = time.time() - start_llm
        if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG: LLM summary generation QID {qid} took {llm_time:.2f}s")

        # --- Debugging Area ---
        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):
            logger.log(f"ERROR [Summarize QID {qid}]: LLM pipeline returned unexpected/empty output: {outputs}")
            return None
        generated_text_full = outputs[0]['generated_text']
        if ENABLE_DEBUG_LOGGING: # Log raw output only in debug mode to avoid huge logs
             logger.log(f"DEBUG [Summarize QID {qid}]: Raw generated_text_full (first 500 chars):\n'''\n{generated_text_full[:500]}\n'''")
        # --- End Debugging Area ---

        # Attempt to parse using the expected marker for Gemma
        response_marker = "<start_of_turn>model\n"; summary_start_index = generated_text_full.rfind(response_marker)
        if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG [Summarize QID {qid}]: response_marker found at index: {summary_start_index}")

        summary_text = ""
        if summary_start_index != -1:
            summary_text = generated_text_full[summary_start_index + len(response_marker):].strip()
            if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG [Summarize QID {qid}]: Extracted summary via marker (first 300 chars):\n'''\n{summary_text[:300]}\n'''")
        else:
            logger.log(f"WARN [Summarize QID {qid}]: Could not find standard model response marker. Attempting prompt removal fallback.")
            # Fallback: try removing the prompt
            if generated_text_full.startswith(prompt_formatted):
                 summary_text = generated_text_full[len(prompt_formatted):].strip()
                 if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG [Summarize QID {qid}]: Extracted summary via fallback (first 300 chars):\n'''\n{summary_text[:300]}\n'''")
            else:
                 summary_text = f"Error: Could not reliably extract summary (marker missing & output mismatch)."
                 logger.log(f"ERROR [Summarize QID {qid}]: Fallback failed. Output mismatch. Starts: {generated_text_full[:100]}")

        # Final check and return
        if summary_text.startswith("Error:") or not summary_text.strip():
             logger.log(f"WARN [Summarize QID {qid}]: Final extracted summary is empty or indicates error.")
             return "" # Return empty string to signify failure to summarize meaningfully
        return summary_text

    except Exception as pipe_err:
        logger.log(f"ERROR: Exception during LLM pipeline execution/parsing QID {qid}: {pipe_err}")
        logger.log(traceback.format_exc())
        return None # Return None on exception

# =====================================================
#                   Main Execution Block (Stage 4)
# =====================================================
if __name__ == "__main__":

    logger.log("=" * 50); logger.log(" INFO: Starting Stage 4: PDF Retrieval & Summarization (MiniLM + Gemma) "); logger.log("=" * 50)
    logger.log(f"INFO: Input Original Responses (JSONL): {ORIGINAL_RESPONSES_FILE}")
    logger.log(f"INFO: Input Stage 2 Aggregation (Q Map): {STAGE2_AGGREGATION_FILE}")
    logger.log(f"INFO: Output PDF Analysis File (JSON): {PDF_ANALYSIS_OUTPUT_FILE}")
    logger.log(f"INFO: State File for Resumption: {STATE_FILE}")
    logger.log(f"INFO: Embedding Model: {EMBEDDING_MODEL_NAME}")
    logger.log(f"INFO: LLM Model: {LLM_MODEL_ID}")
    logger.log(f"INFO: Use Quantization: {USE_QUANTIZATION}")
    logger.log(f"INFO: Min Similarity Score: {MIN_SIMILARITY_SCORE}")
    logger.log(f"INFO: Runtime Limit (Minutes): {MAX_RUNTIME_MINUTES}")
    logger.log(f"INFO: Test Mode Single Response ID: {TEST_SINGLE_RESPONSE_ID}")

    # --- Load LLM and Embedding Models ---
    embedding_model = None; loaded_llm_tokenizer = None; model_instance_llm = None; summarization_pipe = None
    models_loaded_success = False; device = "cuda" if torch.cuda.is_available() else "cpu"
    try:
        logger.log(f"INFO: Loading embedding model: {EMBEDDING_MODEL_NAME} onto {device}...")
        embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)
        logger.log("INFO: Embedding model loaded successfully.")
        logger.log(f"INFO: Loading LLM tokenizer for: {LLM_MODEL_ID}...")
        loaded_llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)
        if getattr(loaded_llm_tokenizer, 'pad_token', None) is None and getattr(loaded_llm_tokenizer, 'eos_token', None):
            logger.log(f"WARN: LLM Tokenizer missing pad token; setting to eos_token.")
            loaded_llm_tokenizer.pad_token = loaded_llm_tokenizer.eos_token
            loaded_llm_tokenizer.padding_side = "left" # Usually needed for generation tasks
        elif getattr(loaded_llm_tokenizer, 'pad_token', None) is None:
             logger.log(f"WARN: LLM Tokenizer missing pad token and eos_token. Padding may fail.")
        logger.log("INFO: LLM Tokenizer loaded.")
        bnb_config = None; effective_quantization = USE_QUANTIZATION
        if USE_QUANTIZATION:
            logger.log("INFO: Setting up 4-bit quantization...")
            try:
                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)
                logger.log(f"INFO: 4-bit quantization configured (Compute dtype: {compute_dtype}).")
            except ImportError: logger.log("ERROR: bitsandbytes not found. Install it for quantization. Disabling quantization."); effective_quantization = False
            except ColabNotSupportedError: logger.log("ERROR: Bits and Bytes Colab issue detected. Disabling quantization."); effective_quantization = False
            except Exception as q_err: logger.log(f"ERROR: Failed BitsAndBytesConfig: {q_err}. Disabling quantization."); effective_quantization = False
        logger.log(f"INFO: Loading LLM model: {LLM_MODEL_ID} (Quantization: {effective_quantization})...")
        if not torch.cuda.is_available() and effective_quantization: logger.log("WARN: Quantization enabled but CUDA not available! Model will load on CPU, likely very slow.")
        # Load LLM
        model_instance_llm = AutoModelForCausalLM.from_pretrained(
            LLM_MODEL_ID,
            quantization_config=bnb_config if effective_quantization else None,
            torch_dtype=compute_dtype if not effective_quantization else None, # Set dtype if not quantizing
            device_map="auto", # Let accelerate handle device placement
            trust_remote_code=True # Needed for some models like Gemma 2
        )
        logger.log("INFO: LLM model loaded.")
        logger.log("INFO: Creating text-generation pipeline...")
        summarization_pipe = pipeline(
            "text-generation",
            model=model_instance_llm,
            tokenizer=loaded_llm_tokenizer,
            # device arg is usually not needed here when device_map="auto" is used in from_pretrained
        )
        logger.log("INFO: LLM Pipeline created successfully.")
        models_loaded_success = True
    except Exception as load_err:
        logger.log(f"CRITICAL: Failed to load models/pipeline: {load_err}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
        sys.exit(1) # Exit if models fail to load

    # --- Main Processing Steps ---
    main_success = False; results = {}; processed_responses_ids = set(); qid_to_embedding = {}
    try:
        # 1. Load State if not in test mode
        if TEST_SINGLE_RESPONSE_ID is None:
            processed_responses_ids, results = load_state(STATE_FILE)
        else:
            logger.log("INFO: Test mode enabled. Ignoring state file.")
            processed_responses_ids, results = set(), {} # Ensure clean start for test

        # 2. Load Question Texts (always needed)
        logger.log(f"INFO: Loading question texts from {STAGE2_AGGREGATION_FILE}...")
        qid_to_text = {}
        try:
            with open(STAGE2_AGGREGATION_FILE, 'r', encoding='utf-8') as f: stage2_data = json.load(f)
            question_data_from_stage2 = stage2_data.get("question_data", {})
            for qid, data in question_data_from_stage2.items():
                if "question_text" in data: qid_to_text[qid] = data["question_text"]
            if not qid_to_text: raise ValueError("No question texts found.")
            logger.log(f"INFO: Loaded text for {len(qid_to_text)} questions.")
        except Exception as e: logger.log(f"ERROR: Failed loading question texts: {e}"); sys.exit(1)

        # 3. Pre-compute Question Embeddings (always needed)
        logger.log("INFO: Pre-computing embeddings for all target questions...")
        target_qids = list(qid_to_text.keys()); target_qtexts = list(qid_to_text.values())
        question_embeddings_all = get_local_embeddings(target_qtexts, embedding_model, device)
        if question_embeddings_all is None: logger.log("CRITICAL: Failed to generate question embeddings."); sys.exit(1)
        qid_to_embedding = {qid: emb for qid, emb in zip(target_qids, question_embeddings_all)}
        logger.log(f"INFO: Generated embeddings for {len(qid_to_embedding)} questions.")

        # 4. Determine Input File (Original or Temp for Test)
        input_file_path = ORIGINAL_RESPONSES_FILE; temp_file_for_testing = None
        if TEST_SINGLE_RESPONSE_ID is not None:
            logger.log(f"WARN: --- RUNNING IN SINGLE RESPONSE TEST MODE (ID: {TEST_SINGLE_RESPONSE_ID}) ---")
            try:
                with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.jsonl', encoding='utf-8') as temp_file:
                    temp_file_for_testing = temp_file.name; logger.log(f"INFO: Created temporary test file: {temp_file_for_testing}"); found = False
                    try:
                        with open(ORIGINAL_RESPONSES_FILE, 'r', encoding='utf-8') as original_file:
                            for line in original_file:
                                try:
                                    response = json.loads(line)
                                    if str(response.get("id")) == str(TEST_SINGLE_RESPONSE_ID):
                                        temp_file.write(line); found = True; logger.log(f"INFO: Copied response ID {TEST_SINGLE_RESPONSE_ID}."); break
                                except json.JSONDecodeError: continue # Skip bad lines in source
                    except FileNotFoundError: logger.log(f"ERROR: Original file not found: {ORIGINAL_RESPONSES_FILE}"); sys.exit(1)
                    if not found: logger.log(f"ERROR: Test response ID {TEST_SINGLE_RESPONSE_ID} not found."); sys.exit(1)
                input_file_path = temp_file_for_testing # Override input path
            except Exception as temp_err: logger.log(f"ERROR: Error setting up test mode: {temp_err}"); tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}"); sys.exit(1)
        else: logger.log(f"INFO: --- RUNNING IN FULL/RESUME MODE (Input: {input_file_path}) ---")

        # 5. Main Processing Loop
        processed_in_this_run = 0; pdfs_processed_this_run = 0; pdfs_skipped_this_run = 0; summarization_tasks_run = 0; summarization_tasks_skipped_time = 0
        start_processing_time = time.time(); last_save_time = start_processing_time
        MAX_RUNTIME_SECONDS = MAX_RUNTIME_MINUTES * 60; hit_time_limit = False

        logger.log(f"INFO: Starting processing loop for {input_file_path}...")
        try:
            with open(input_file_path, 'r', encoding='utf-8') as infile:
                line_num_processed = 0 # Counter for lines actually processed (not skipped by state)
                for line_num, line in enumerate(infile, 1):
                    # --- Runtime Limit Check (Only in full run mode) ---
                    if TEST_SINGLE_RESPONSE_ID is None:
                         elapsed_seconds = time.time() - start_processing_time
                         if elapsed_seconds >= MAX_RUNTIME_SECONDS:
                             logger.log(f"WARN: Reached runtime limit ({elapsed_seconds:.0f}s >= {MAX_RUNTIME_SECONDS}s). Stopping processing loop.")
                             hit_time_limit = True; break # Exit the loop
                         # --- Periodic State Saving (Only in full run mode) ---
                         current_time = time.time()
                         if current_time - last_save_time >= SAVE_STATE_INTERVAL_SECONDS:
                             save_state(STATE_FILE, processed_responses_ids, results)
                             last_save_time = current_time

                    # --- Initialize variables for this response ---
                    response_id = f"Unknown_Line_{line_num}" # Default ID
                    try:
                        line_strip = line.strip();
                        if not line_strip: continue # Skip empty lines
                        response_object = json.loads(line_strip);
                        response_id = str(response_object.get("id", response_id)) # Get actual ID, ensure string

                        # --- Skip if already processed (Resume Logic) ---
                        if response_id in processed_responses_ids:
                             # Log skipping less frequently in full runs
                             if ENABLE_DEBUG_LOGGING and (line_num_processed % 1000 == 0) and TEST_SINGLE_RESPONSE_ID is None:
                                 logger.log(f"DEBUG: Skipping already processed response ID {response_id}")
                             continue # Move to next line

                        # If not skipped, process it
                        processed_in_this_run += 1
                        line_num_processed +=1 # Increment counter for lines attempted in this run

                        # Initialize entry in results dict (overwrites if loaded from state)
                        results[response_id] = {}

                        if ENABLE_DEBUG_LOGGING and (processed_in_this_run % 100 == 0 or processed_in_this_run <= 5 or TEST_SINGLE_RESPONSE_ID is not None) : logger.log(f"DEBUG: Processing response {processed_in_this_run} (ID: {response_id}, Line: {line_num})...")

                        # --- Check for PDF Text ---
                        pdf_text = response_object.get("pdf_text")
                        if not pdf_text or len(pdf_text) < MIN_PDF_TEXT_LENGTH:
                            pdfs_skipped_this_run += 1
                            results[response_id] = {"status": "pdf_missing_or_short"} # Set top-level status
                            processed_responses_ids.add(response_id) # Mark as processed
                            continue # Skip to the next response line

                        # --- Process PDF ---
                        pdf_processed_flag = True
                        cleaned_pdf_text = re.sub(r'\s+', ' ', pdf_text).strip()
                        sentences = nltk.sent_tokenize(cleaned_pdf_text); sentences = [s.strip() for s in sentences if s.strip()]

                        if not sentences: status_msg = "pdf_no_sentences"; logger.log(f"WARN: {status_msg} for Resp ID {response_id}.")
                        else: sentence_embeddings = get_local_embeddings(sentences, embedding_model, device)

                        if not sentences or sentence_embeddings is None:
                            status_msg = "error_pdf_processing_or_embedding" if sentences else "pdf_no_sentences"
                            results[response_id] = {"status": status_msg} # Set top-level status
                            processed_responses_ids.add(response_id) # Mark as processed
                            continue # Skip to next response line

                        # --- Loop through Questions for this PDF ---
                        for qid in target_qids:
                            question_text = qid_to_text[qid]; q_embedding = qid_to_embedding.get(qid)
                            result_for_qid = {"question_text": question_text} # Initialize result for this QID
                            results[response_id][qid] = result_for_qid # Store immediately

                            if q_embedding is None:
                                result_for_qid["status"] = "error_missing_q_embedding"
                                continue # Go to next question

                            relevant_passages, top_score = retrieve_relevant_passages(q_embedding, sentences, sentence_embeddings, MIN_SIMILARITY_SCORE, TOP_K_PASSAGES, CONTEXT_WINDOW_SIZE)

                            if relevant_passages:
                                result_for_qid["status"] = "passages_extracted"
                                result_for_qid["extracted_passages"] = relevant_passages
                                result_for_qid["passages_found"] = len(relevant_passages)
                                result_for_qid["top_passage_score"] = float(top_score) if top_score is not None else None

                                # --- Conditional Summarization ---
                                should_summarize_config = QUESTIONS_TO_SUMMARIZE.get(qid, False)
                                # Check time limit again *before* calling LLM (only needed in full run)
                                can_summarize_time = True
                                if TEST_SINGLE_RESPONSE_ID is None:
                                     elapsed_seconds = time.time() - start_processing_time
                                     if elapsed_seconds >= MAX_RUNTIME_SECONDS:
                                         if not hit_time_limit: logger.log(f"INFO: Time limit reached during Q loop ({elapsed_seconds:.0f}s). Skipping summary for QID {qid}."); hit_time_limit = True
                                         can_summarize_time = False
                                     else:
                                         can_summarize_time = True

                                combined_passage_text = "\n\n".join(relevant_passages)
                                passage_quality_ok = len(combined_passage_text) >= MIN_CHARS_FOR_SUMMARY

                                if should_summarize_config and passage_quality_ok and can_summarize_time:
                                    summarization_tasks_run += 1
                                    summary = summarize_context_block(combined_passage_text, question_text, qid, summarization_pipe, NUM_SUMMARY_SENTENCES) # Pass qid for logging
                                    if summary is not None: # Check for None explicitly (indicates error)
                                        result_for_qid["status"] = "success_summarized"
                                        result_for_qid["summary"] = summary
                                    else: # Summarization function returned None (error)
                                        result_for_qid["status"] = "error_llm_pipe_post_retrieval"
                                elif should_summarize_config and not passage_quality_ok:
                                    result_for_qid["status"] = "skipped_passage_quality"
                                    if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG: Skipped summary QID {qid}, Resp {response_id} due to passage quality.")
                                elif should_summarize_config and not can_summarize_time:
                                    summarization_tasks_skipped_time += 1
                                    result_for_qid["status"] = "skipped_runtime_limit"
                                # --- End Summarization Logic ---

                            else: # No relevant passages found
                                result_for_qid["status"] = "no_relevant_passages_found"; result_for_qid["passages_found"] = 0; result_for_qid["top_passage_score"] = None

                        # --- End of question loop ---
                        if pdf_processed_flag: pdfs_processed_this_run += 1
                        processed_responses_ids.add(response_id) # Mark response as fully processed

                    except json.JSONDecodeError as json_err:
                        logger.log(f"WARN: Skipping line {line_num} due to JSON parse error: {json_err}")
                        results[response_id] = {"status": "error_json_decode"} # Store error status
                        processed_responses_ids.add(response_id) # Mark as processed to avoid retry
                    except Exception as resp_err:
                        logger.log(f"ERROR: Processing response line {line_num} (ID: {response_id}) failed: {resp_err}")
                        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
                        results[response_id] = {"status": "error_processing_response", "error_message": str(resp_err)}
                        processed_responses_ids.add(response_id) # Mark as processed
                    finally:
                        # Cleanup per-response temporary variables conditionally
                        if 'sentence_embeddings' in locals() and sentence_embeddings is not None:
                            del sentence_embeddings
                        if 'sentences' in locals():
                            del sentences
                        # We know gc exists, so call it directly
                        gc.collect()                # --- End of file processing loop ---

            logger.log(f"INFO: Finished processing file loop. Processed in this run: {processed_in_this_run}, PDFs analyzed: {pdfs_processed_this_run}, PDFs skipped: {pdfs_skipped_this_run}, Summaries attempted: {summarization_tasks_run}.")
            if hit_time_limit: logger.log("WARN: Processing stopped due to runtime limit.")

        except FileNotFoundError: logger.log(f"ERROR: Input file not found: {input_file_path}"); sys.exit(1)
        except Exception as file_err: logger.log(f"ERROR: Unhandled error during file processing: {file_err}"); tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")

        # 6. Final Save (State and Output)
        # Save state one last time if in full run mode and processed something
        if TEST_SINGLE_RESPONSE_ID is None and processed_in_this_run > 0:
             save_state(STATE_FILE, processed_responses_ids, results)
        # Save the final accumulated results
        save_stage4_output(PDF_ANALYSIS_OUTPUT_FILE, results)
        main_success = True

    except Exception as main_e:
        logger.log(f"CRITICAL: FATAL ERROR in main execution block: {main_e}")
        tb_str = traceback.format_exc(); logger.log(f"CRITICAL TRACEBACK:\n{tb_str}")
        # Attempt to save state even on fatal error if possible
        if TEST_SINGLE_RESPONSE_ID is None and 'processed_responses_ids' in locals() and 'results' in locals():
            logger.log("CRITICAL: Attempting to save state before exiting due to fatal error...")
            save_state(STATE_FILE, processed_responses_ids, results)

    finally:
        # --- Cleanup ---
        # Clean up the temporary file if it was created for test mode
        if temp_file_for_testing and os.path.exists(temp_file_for_testing):
            try: os.unlink(temp_file_for_testing); logger.log(f"INFO: Deleted temporary test file: {temp_file_for_testing}")
            except OSError as e: logger.log(f"WARN: Could not delete temporary test file {temp_file_for_testing}: {e}")

        logger.log("INFO: Cleaning up models and resources...")
        del embedding_model; del loaded_llm_tokenizer; del model_instance_llm; del summarization_pipe
        del qid_to_embedding; del question_embeddings_all; del results; del processed_responses_ids; gc.collect()
        if torch.cuda.is_available():
             logger.log("INFO: Emptying CUDA cache...")
             try: torch.cuda.empty_cache()
             except Exception as cuda_err: logger.log(f"WARN: Error emptying CUDA cache: {cuda_err}")
        logger.log("INFO: Cleanup complete.")
        # --- Final Status Log ---
        logger.log("=" * 50)
        if main_success: logger.log(" INFO: Stage 4 Processing Finished Successfully ")
        else: logger.log(" ERROR: Stage 4 Processing Finished With Errors or Did Not Complete ")
        logger.log("=" * 50)

"""# 8th Apr (Form Data Only)

There are typically two main stages involving these UDS structures in the simplified flow (focusing only on form data analysis):

Stage 1: Generate Simplified Response-Centric UDS

- Input: The original raw responses data (e.g., processed_responses.jsonl), which might contain structured form data and (now ignored) PDF information.

- Process: Run the Simplified Response-Centric UDS Script. This script:

 - Reads the original raw data to discover all unique questions and create the template_questions map (create_template_questions).

 - Iterates through each response in the raw data.

 - For each response, it processes only the structured form answers using process_form_answers.

 - It skips all PDF processing (process_pdf_answers is commented out or removed).

 - It outputs a data structure for each response, containing the processed form answers, metadata, and statistics reflecting only the form data (PDF counts/details will be zero or absent).

- Output: A file containing the Simplified Response-Centric UDS (e.g., response_centric_uds_analysis_simplified.json). This file lists individual responses with their processed form data.

Stage 2: Generate Question-Centric Aggregation (from Form Data)

- Input:

 - The Simplified Response-Centric UDS file generated in Stage 1.

 - The original raw responses data again (needed to recreate the exact same template_questions map using create_template_questions for consistency).

- Process: Run the Simplified Question-Centric Aggregation Script. This script:

 - Recreates the template_questions map from the original raw data.

 - Loads the entire list of responses from the Simplified Response-Centric UDS file.

 - Iterates through this list and aggregates the results by question ID, using the simplified aggregate_responses_by_question function which only considers the form_answers field from its input.

 - Calculates counts (like answered_form_count, unanswered_by_form_count) and collects examples for each question based only on the form data.

- Output: A file containing the Question-Centric Aggregation (e.g., question_centric_aggregation_output.json). This file structures the data by question, summarizing the form answer statistics across all responses.

In Summary:

the order is:

1. Process raw data -> Simplified Response-Centric UDS (capturing only form data per response).

1. Process Simplified Response-Centric UDS -> Question-Centric Aggregation (summarizing form data across responses, grouped by question).

This two-stage approach allows you to first normalize and structure the data per response (handling only the desired input source - forms) and then perform the cross-response analysis by pivoting to a question-centric view.

## Step 1: Response-Centric UDS
"""

# @title Pipeline Configuration Constants
# =====================================================
#              Pipeline Configuration Constants
# =====================================================

# --- Core Input/Output File Definitions ---

# The absolute starting point: Original raw responses data file (JSON Lines format assumed)
# Used by Stage 1 for processing, and Stage 2 for consistent template creation.
ORIGINAL_RESPONSES_FILE = 'processed_responses.jsonl'

# The intermediate file: Output of Stage 1 (simplified response-centric UDS)
# and Input for Stage 2. (JSON Lines format recommended).
SIMPLIFIED_RESPONSE_UDS_FILE = 'response_centric_uds_simplified.jsonl'

# The final output file: Result of Stage 2 (question-centric aggregation)
# (Single JSON object format).
QUESTION_CENTRIC_UDS_FILE = 'question_centric_aggregation_output.json'


# --- Logging Configuration ---

# Log file specific to Stage 1 processing.
STAGE1_LOG_FILE = "stage1_response_centric_processing.log"
# Log file specific to Stage 2 aggregation.
STAGE2_LOG_FILE = "stage2_question_centric_aggregation.log"
# Global toggle for detailed logging in both stages.
ENABLE_DEBUG_LOGGING = True


# --- Stage 1 Specific Configuration ---

# Set to an integer ID to process only that single response in Stage 1,
# helpful for debugging. Set to None to process the full input file.
TEST_SINGLE_RESPONSE_ID = None # Example: 838011019


# --- Stage 2 Specific Configuration ---

# Limits the number of example form answers stored per question
# in the final question-centric output file.
MAX_EXAMPLE_ANSWERS = 10

# =====================================================
#             End of Configuration Constants
# =====================================================

# @title Helper Functions
# =====================================================
#                      Imports
# =====================================================
# REMOVED: re, nltk, sklearn, transformers, torch
import json
import numpy as np
import time
import os
import traceback # For manual traceback logging
import sys # For potential fallback print
from typing import Dict, Any, Tuple
from collections import defaultdict # Keep if used elsewhere

# =====================================================
#                Custom FileLogger Class
# =====================================================
class FileLogger:
    def __init__(self, log_file):
        self.log_file = log_file
        log_dir = os.path.dirname(log_file)
        if log_dir and not os.path.exists(log_dir):
             os.makedirs(log_dir, exist_ok=True)
        try:
            # Use 'a' for append mode to avoid overwriting logs on restart if desired,
            # or keep 'w' to start fresh each run. Let's keep 'w' for now.
            with open(self.log_file, 'w', encoding='utf-8') as f:
                f.write(f"{self._get_timestamp()}: Starting new log...\n")
        except IOError as e:
            print(f"CRITICAL: Failed to initialize log file {self.log_file}: {e}")
            # Optionally exit if logging is critical
            # sys.exit(1)

    def _get_timestamp(self):
        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())

    def log(self, message: str):
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(f"{self._get_timestamp()}: {message}\n")
        except Exception as e:
            # Fallback print if logging fails
            print(f"!!! LOGGER FAILED TO WRITE: {e} !!!")
            print(f"{self._get_timestamp()}: {message}") # Log to console as fallback

    def log_json(self, data: Dict[str, Any], prefix="JSON_DATA"):
        serializer = self._get_serializer()
        try:
            log_entry = {
                 "timestamp": self._get_timestamp(),
                 "type": prefix,
                 "data": data
            }
            # Use the custom serializer for dumping
            json_string = json.dumps(log_entry, default=serializer, indent=2)
            with open(self.log_file, 'a', encoding='utf-8') as f:
                 f.write(json_string + "\n")
        except TypeError as e:
            # Log the serialization error using the standard log method
            self.log(f"ERROR: Failed to serialize data for JSON logging: {e} - Data Keys: {list(data.keys())}")
            # Optionally log traceback for serialization errors
            # tb_str = traceback.format_exc(); self.log(f"TRACEBACK:\n{tb_str}")
        except Exception as e:
             # Log any other unexpected errors during JSON logging
             self.log(f"ERROR: Unexpected error during log_json: {e}")
             tb_str = traceback.format_exc(); self.log(f"TRACEBACK:\n{tb_str}")

    def _get_serializer(self):
        # Define a nested function to handle custom serialization types
        def default_serializer(obj):
            # Handle NumPy numeric types
            if isinstance(obj, (np.integer, np.floating)):
                 return obj.item() # Convert to standard Python int/float
            # Handle NumPy arrays
            if isinstance(obj, np.ndarray):
                return obj.tolist() # Convert to Python list
            # REMOVED TORCH TENSOR HANDLING BLOCK:
            # if isinstance(obj, (torch.Tensor)):
            #      return obj.detach().cpu().numpy().tolist()
            # Handle sets
            if isinstance(obj, set):
                return sorted(list(obj)) # Convert to sorted list for consistent output
            # If none of the above, try the default JSON encoder
            try:
                # This will raise TypeError if the object is not serializable by default
                return json.JSONEncoder().default(obj)
            except TypeError:
                # If default encoder fails, return a string representation
                # You could log the unserializable type here if needed for debugging
                # self.log(f"DEBUG: Unserializable type encountered: {type(obj).__name__}")
                return f"UNSERIALIZABLE:{type(obj).__name__}"
        return default_serializer # Return the nested function

# =====================================================
#                 Global Configuration (Simplified)
# =====================================================
# REMOVED: PDF_ANSWER_CONFIDENCE_THRESHOLD
# REMOVED: CHUNK_MERGE_SIMILARITY_THRESHOLD


def create_template_questions(original_responses_file: str) -> Tuple[Dict[str, str], Dict[str, str]]:
    """
    Creates template questions map (ID -> Text) and reverse map (Text -> ID)
    by reading the *original* source responses file (JSON Lines format assumed).
    """
    template_questions = {} # id -> text
    text_to_id = {}       # text -> id
    unique_questions = set()
    logger.log(f"INFO: Creating template questions map from original source: {original_responses_file}")
    try:
        with open(original_responses_file, 'r', encoding='utf-8') as f:
            processed_lines = 0
            for line_num, line in enumerate(f, 1):
                try:
                    response = json.loads(line)
                    # Adjust 'responses' key if your original source structure is different
                    form_responses = response.get('responses', {})
                    for question_text in form_responses.keys():
                        if question_text not in unique_questions:
                             unique_questions.add(question_text)
                             question_id = f"Q{len(template_questions) + 1}"
                             template_questions[question_id] = question_text
                             text_to_id[question_text] = question_id
                    processed_lines += 1
                except json.JSONDecodeError:
                    logger.log(f"WARN: Skipping invalid JSON line {line_num} in {original_responses_file}")
            logger.log(f"INFO: Processed {processed_lines} lines from original file. Found {len(template_questions)} unique questions.")
    except FileNotFoundError:
        logger.log(f"ERROR: Original source responses file not found: {original_responses_file}")
        return {}, {}
    except Exception as e:
        logger.log(f"ERROR: Failed template creation from original file: {e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
        return {}, {}
    if not template_questions:
        logger.log("WARN: No template questions generated. Check original source file format/content.")
    return template_questions, text_to_id



# Example Usage (Optional - for testing)
# if __name__ == '__main__':
#     logger.log("This is a standard log message.")
#     my_data = {"a": 1, "b": np.array([1, 2, 3]), "c": {4, 5, 2}, "d": np.int64(10)}
#     logger.log_json(my_data, prefix="TEST_DATA")
#     logger.log("Test complete.")

# @title Process Form Answers
def process_form_answers(
    response: Dict[str, Any],
    text_to_id: Dict[str, str],
    template_questions: Dict[str, str] # <-- Add template_questions
    ) -> Tuple[Dict[str, Any], Dict[str, Dict[str, Any]]]:
    """Processes form answers and includes the canonical question text.""" # Updated docstring
    form_meta = {'text_answers': 0, 'selection_answers': 0}; form_answers = {}
    for question_text_from_form, data in response.get("responses", {}).items():
        # Find the canonical QID using the text_to_id map
        if question_text_from_form in text_to_id:
             qid = text_to_id[question_text_from_form]
             # Get the canonical question text using the QID
             question_text_canonical = template_questions.get(qid, f"QUESTION TEXT NOT FOUND FOR QID {qid}") # Safeguard

             if "text" in data and data["text"] is not None:
                 form_answers[qid] = {
                     "question_text": question_text_canonical, # Add question text
                     "type": "text",
                     "content": data["text"]
                     }
                 form_meta['text_answers'] += 1
             elif "selected" in data and data["selected"] is not None:
                 form_answers[qid] = {
                      "question_text": question_text_canonical, # Add question text
                      "type": "selection",
                      "content": data["selected"]
                      }
                 form_meta['selection_answers'] += 1
        # Optional: Log if a question text from the form doesn't map to a known QID
        # else:
        #    logger.log(f"WARN: Question text from form '{question_text_from_form[:50]}...' not found in text_to_id map for response {response.get('id', 'N/A')}.")

    return form_meta, form_answers

# @title Disable Process-PDF-Answers
def process_response(
    response: Dict[str, Any],
    template_questions: Dict[str, str],
    text_to_id: Dict[str, str]
    # PDF_CONFIDENCE_THRESHOLD is global
    ) -> Dict[str, Any]:
    """
    Processes a single response, including form and PDF answers (now with question text embedded),
    and calculates comparison statistics.
    """
    response_id = response.get('id', 'UNKNOWN_ID')
    logger.log(f"INFO: Processing response ID: {response_id}")

    # Pass template_questions to process_form_answers so it can embed the text
    form_meta, form_answers = process_form_answers(response, text_to_id, template_questions)

    # process_pdf_answers now returns pdf_answers with question text embedded
    # pdf_meta, pdf_answers, pdf_confidence_scores = process_pdf_answers(response, template_questions)

    # Provide default empty values since PDF processing is skipped
    logger.log(f"INFO: PDF processing skipped for response ID: {response_id}")
    pdf_meta = {"processing_skipped_reason": "PDF processing disabled by configuration"} # Or simply {}
    pdf_answers = {}
    pdf_confidence_scores = {}

    # --- Calculate Comparison Statistics using Sets ---
    # (No changes needed here, still based on QIDs)
    all_template_qids = set(template_questions.keys())
    form_answered_qids = set(form_answers.keys())
    pdf_answered_qids = set(pdf_answers.keys()) # pdf_answers still only contains keys >= threshold

    answered_by_both_set = form_answered_qids.intersection(pdf_answered_qids)
    answered_by_form_only_set = form_answered_qids.difference(pdf_answered_qids)
    answered_by_pdf_only_set = pdf_answered_qids.difference(form_answered_qids)
    total_answered_set = form_answered_qids.union(pdf_answered_qids)
    unanswered_set = all_template_qids.difference(total_answered_set)

    # Populate the response_stats dictionary
    # (No changes needed here)
    response_stats = {
        'total_template_questions': len(template_questions),
        'form_text_answered_count': form_meta.get('text_answers', 0),
        'form_selection_answered_count': form_meta.get('selection_answers', 0),
        'pdf_answered_count': len(pdf_answered_qids),
        'answered_by_form_only_count': len(answered_by_form_only_set),
        'answered_by_pdf_only_count': len(answered_by_pdf_only_set),
        'answered_by_both_count': len(answered_by_both_set),
        'total_unique_answered_count': len(total_answered_set),
        'unanswered_count': len(unanswered_set),
        'pdf_confidence_scores': pdf_confidence_scores
    }
    # --- End Statistics Calculation ---

    # Compile the final UDS entry for this response
    # No extra enrichment step needed here now
    response_data = {
        "response_id": response_id,
        "form_meta": form_meta,
        "form_answers": form_answers, # Already contains question_text
        "pdf_meta": pdf_meta,
        "pdf_answers": pdf_answers,   # Already contains question_text
        "response_stats": response_stats
    }

    # Log the key outcomes (counts remain useful for a quick check)
    logger.log(f"INFO: Finished response ID: {response_id}. "
               f"Form Ans: {len(form_answered_qids)}, "
               f"PDF Ans (>=thr): {response_stats['pdf_answered_count']}, "
               f"Both: {response_stats['answered_by_both_count']}, "
               f"FormOnly: {response_stats['answered_by_form_only_count']}, "
               f"PDFOnly: {response_stats['answered_by_pdf_only_count']}, "
               f"Unanswered: {response_stats['unanswered_count']}")

    return response_data

# @title Main Execution Block
import json
import os
import sys
import tempfile
import traceback
# Assume FileLogger, create_template_questions, process_form_answers are defined above
# Assume a simplified process_response function is also defined (as shown below for context)

# =====================================================
#  Main Execution Block (Simplified for Stage 1)
# =====================================================
if __name__ == "__main__":
    # --- Configuration --- (Referencing the constants above)
    INPUT_RESPONSES_FILE = ORIGINAL_RESPONSES_FILE # Use the consistent name
    OUTPUT_UDS_FILE = SIMPLIFIED_RESPONSE_UDS_FILE # Use the consistent name
    TEST_ID = TEST_SINGLE_RESPONSE_ID # Use the consistent name
    LOG_FILE = STAGE1_LOG_FILE # Use the stage-specific log file

    # --- Logger Initialization --- (Assuming logger class is defined)
    logger = FileLogger(LOG_FILE)
    # (Enable debug logging can be checked within logger calls if needed)

    logger.log("=" * 50); logger.log(f" INFO: Starting Stage 1: Simplified Response-Centric UDS Creation (Log: {LOG_FILE})"); logger.log("=" * 50)
    logger.log(f"INFO: Debug logging enabled: {ENABLE_DEBUG_LOGGING}")
    logger.log(f"INFO: Input Raw Responses: {INPUT_RESPONSES_FILE}")
    logger.log(f"INFO: Output Simplified UDS (JSONL): {OUTPUT_UDS_FILE}")

    input_file_to_process = INPUT_RESPONSES_FILE
    temp_filename = None

    # --- Test Mode Setup (Optional) ---
    if TEST_SINGLE_RESPONSE_ID is not None:
        logger.log(f"WARN: --- RUNNING IN SINGLE RESPONSE TEST MODE (ID: {TEST_SINGLE_RESPONSE_ID}) ---")
        try:
            # Create a temporary file to hold only the test response
            with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.jsonl', encoding='utf-8') as temp_file:
                temp_filename = temp_file.name
                found = False
                try:
                    with open(INPUT_RESPONSES_FILE, 'r', encoding='utf-8') as original_file:
                        for line_num, line in enumerate(original_file, 1):
                            try:
                                response = json.loads(line)
                                if response.get("id") == TEST_SINGLE_RESPONSE_ID:
                                    json.dump(response, temp_file); temp_file.write('\n')
                                    logger.log(f"INFO: Copied response ID {TEST_SINGLE_RESPONSE_ID} to temp file: {temp_filename}")
                                    found = True; break
                            except json.JSONDecodeError: logger.log(f"WARN: Skipping invalid JSON line {line_num} in original file during test setup.")
                except FileNotFoundError: logger.log(f"ERROR: Original file not found for test setup: {INPUT_RESPONSES_FILE}"); sys.exit(1)
                if not found: logger.log(f"ERROR: Test response ID {TEST_SINGLE_RESPONSE_ID} not found in {INPUT_RESPONSES_FILE}."); sys.exit(1)

            input_file_to_process = temp_filename # Process only the temp file
            logger.log(f"INFO: Using temporary file as input: {input_file_to_process}")
        except Exception as temp_err:
            logger.log(f"ERROR: Error setting up test mode: {temp_err}")
            tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
            if temp_filename and os.path.exists(temp_filename): os.unlink(temp_filename) # Cleanup attempt
            sys.exit(1)
    else:
         logger.log(f"INFO: --- RUNNING IN FULL BATCH MODE (Input: {input_file_to_process}) ---")

    # --- Main Processing Steps ---
    main_success = False
    try:
        # 1. Create Template Questions Map from FULL ORIGINAL dataset
        logger.log(f"INFO: Reading full dataset ({INPUT_RESPONSES_FILE}) to create template questions...")
        # Assumes create_template_questions returns dicts: id->text, text->id
        template_questions, text_to_id = create_template_questions(INPUT_RESPONSES_FILE)
        if not template_questions:
            logger.log("ERROR: No template questions created from original file. Cannot proceed.")
            sys.exit(1) # Essential for mapping answers
        logger.log(f"INFO: Discovered {len(template_questions)} unique questions in template.")

        # 2. Process Input File Line-by-Line
        processed_count = 0
        logger.log(f"INFO: Processing responses from '{input_file_to_process}' and writing to '{OUTPUT_UDS_FILE}'...")
        # Open output file for writing JSON Lines
        with open(OUTPUT_UDS_FILE, 'w', encoding='utf-8') as outfile:
            # Open input file (original or temp) for reading
            with open(input_file_to_process, 'r', encoding='utf-8') as infile:
                for line_num, line in enumerate(infile, 1):
                    try:
                        raw_response = json.loads(line)
                        # Process using the simplified function
                        processed_data = process_response(raw_response, template_questions, text_to_id)
                        # Write the result as one line in the output JSONL file
                        json.dump(processed_data, outfile) # Uses standard JSON encoder
                        outfile.write('\n')
                        processed_count += 1
                        # Log progress occasionally for large files
                        if processed_count % 200 == 0:
                             logger.log(f"INFO: ... processed {processed_count} responses")
                    except json.JSONDecodeError:
                        logger.log(f"WARN: Skipping invalid JSON on line {line_num} of {input_file_to_process}")
                    except Exception as proc_err:
                        # Try to get ID for better error logging
                        response_id_for_error = 'UNKNOWN_ID_ON_ERROR'
                        try: response_id_for_error = json.loads(line).get('id', response_id_for_error)
                        except: pass
                        logger.log(f"ERROR: Failed to process response ID {response_id_for_error} on line {line_num}: {proc_err}")
                        # Optionally log full traceback for debugging specific response errors
                        # tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")

            logger.log(f"INFO: Finished processing. Processed {processed_count} responses.")
            main_success = True # Mark success if the loop finished

    except FileNotFoundError as fnf_err:
        logger.log(f"CRITICAL: Input file not found: {fnf_err}")
    except Exception as main_e: # Catch other fatal errors (e.g., template creation failure, file open errors)
        logger.log("CRITICAL:" + "="*10 + " FATAL ERROR in main execution block " + "="*10)
        logger.log(f"CRITICAL: Error: {main_e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
        logger.log("CRITICAL:" + "="*50)

    finally:
        # Clean up the temporary file if it was created
        if temp_filename and os.path.exists(temp_filename):
            try:
                os.unlink(temp_filename)
                logger.log(f"INFO: Deleted temporary file: {temp_filename}")
            except OSError as e:
                logger.log(f"WARN: Could not delete temporary file {temp_filename}: {e}")

        logger.log("=" * 50)
        if main_success: logger.log(" INFO: Simplified Response-Centric Processing Finished Successfully ")
        else: logger.log(" ERROR: Simplified Response-Centric Processing Finished With Errors ")
        logger.log("=" * 50)

For Reference only
# @title Main Execution Block (For Reference only)
# --- Imports --- (Keep only necessary ones: json, os, sys, traceback, time, potentially numpy, your logger)
import json
import sys
import traceback
import time
# ... other necessary standard library imports ...
# from your_helper_module import logger, create_template_questions, aggregate_responses_by_question, save_output # Assuming these are defined elsewhere

if __name__ == "__main__":

    logger.log("=" * 50); logger.log(" INFO: Starting Question-Centric UDS Aggregation (No Summarization)"); logger.log("=" * 50)
    # Log key config parameters (Only relevant ones like input/output files)
    # logger.log(f"INFO: Original Responses File: {ORIGINAL_RESPONSES_FILE}") # Assuming these vars are defined
    # logger.log(f"INFO: Response-Centric UDS Input: {RESPONSE_CENTRIC_UDS_FILE}")
    # logger.log(f"INFO: Question-Centric UDS Output: {QUESTION_CENTRIC_UDS_FILE}")

    # --- Main Processing Steps ---
    main_success = False
    try:
        # 1. Create Template Questions Map (Needs original responses file)
        # Make sure ORIGINAL_RESPONSES_FILE variable is defined somewhere
        template_questions, text_to_id = create_template_questions(ORIGINAL_RESPONSES_FILE)
        if not template_questions:
            logger.log("ERROR: Failed to create template questions map. Cannot proceed.")
            sys.exit(1)
        logger.log(f"INFO: Template created with {len(template_questions)} unique questions.")

        # 2. Load Response-Centric UDS Data (Contains processed form answers)
        response_centric_input_data = None
        try:
            # Make sure RESPONSE_CENTRIC_UDS_FILE variable is defined somewhere
            with open(RESPONSE_CENTRIC_UDS_FILE, 'r', encoding='utf-8') as f:
                response_centric_input_data = json.load(f)
            logger.log(f"INFO: Successfully loaded response-centric data from {RESPONSE_CENTRIC_UDS_FILE}.")
            if "response_data" not in response_centric_input_data or not isinstance(response_centric_input_data.get("response_data"), list):
                 logger.log("ERROR: Input file format error: 'response_data' key missing or not a list.")
                 sys.exit(1)
        except FileNotFoundError:
            logger.log(f"ERROR: Response-centric input file not found: {RESPONSE_CENTRIC_UDS_FILE}")
            sys.exit(1)
        # ... other loading error handling ...

        # 3. Perform Aggregation (NO Summarization)
        # **IMPORTANT**: aggregate_responses_by_question MUST be modified
        #               to *only* group data and not expect/use models/tokenizer.
        response_data_list = response_centric_input_data.get("response_data", [])
        logger.log(f"INFO: Starting aggregation for {len(response_data_list)} responses...")
        question_centric_data = aggregate_responses_by_question(
            response_data_list,
            template_questions
            # NO model/tokenizer/device arguments passed here
        )
        logger.log("INFO: Aggregation complete.")

        # 4. Save the Question-Centric Output
        if question_centric_data:
            # Make sure QUESTION_CENTRIC_UDS_FILE variable is defined somewhere
            save_output(QUESTION_CENTRIC_UDS_FILE, question_centric_data)
            main_success = True
        else:
            logger.log("WARN: Aggregation resulted in empty data. No output file saved.")

    except Exception as main_e:
        logger.log("CRITICAL:" + "="*10 + " FATAL ERROR in main execution block " + "="*10)
        logger.log(f"CRITICAL: Error: {main_e}")
        logger.log(traceback.format_exc())
        logger.log("CRITICAL:" + "="*50)

    finally:
        # Minimal cleanup if any (e.g., close files if not using 'with')
        logger.log("INFO: Cleanup complete (minimal cleanup needed).")

        logger.log("=" * 50)
        if main_success: logger.log(" INFO: Question-Centric Aggregation Finished Successfully ")
        else: logger.log(" ERROR: Question-Centric Aggregation Finished With Errors ")
        logger.log("=" * 50)

# Commented out IPython magic to ensure Python compatibility.
# %tb

"""## Step 2: Question-Centric UDS

1.  **Single JSON Object:** The `save_output` function in the Stage 2 script takes the final `question_centric_data` dictionary (which itself contains question IDs as keys) and wraps it under a top-level key (e.g., `"question_data"`). It then uses `json.dump` to write this *entire wrapped structure* as **one single, valid JSON object** into the file specified by `QUESTION_CENTRIC_UDS_FILE`. This is why the extension is typically `.json`.

2.  **Questions as Keys/Sub-tags:** Inside that single JSON object (specifically within the value associated with `"question_data"`), each unique question identified in the survey (`Q1`, `Q2`, `Q3`, etc.) becomes a **key**. The value associated with each question key is another dictionary holding all the aggregated statistics (`answered_form_count`, `unanswered_by_form_count`, `form_answer_examples`, etc.) *for that specific question* across the entire set of responses processed.

So, the structure looks conceptually like this:

```json
// Contents of question_centric_aggregation_output.json
{
  "question_data": { // Top-level wrapper
    "Q1": { // Key = Question ID
      "question_text": "What is your name?",
      "total_responses_processed_for_question": 500,
      "answered_form_count": 498,
      "unanswered_by_form_count": 2,
      "form_answer_examples": [
        {"response_id": 123, "content": "Alice", "type": "text"},
        {"response_id": 456, "content": "Bob", "type": "text"},
        // ... up to MAX_EXAMPLE_ANSWERS
      ]
    },
    "Q2": { // Key = Question ID
      "question_text": "What is your favorite color?",
      "total_responses_processed_for_question": 500,
      "answered_form_count": 450,
      "unanswered_by_form_count": 50,
      "form_answer_examples": [
         {"response_id": 123, "content": "Blue", "type": "selection"},
         {"response_id": 789, "content": "Green", "type": "selection"},
         // ...
      ]
    },
    // ... more entries for Q3, Q4, etc. ...
  }
}
```

This structure is ideal for the purpose of Stage 2: providing a **holistic, aggregated view of the answers to each specific question** across the entire survey dataset, making cross-response analysis easy for individual questions. It contrasts with the Stage 1 output (JSON Lines), where each line/object represented a single response's data across multiple questions.
"""

# @title Aggregate Responses by Question
# =====================================================
#   Question-Centric UDS Aggregation (Form Data Only)
# =====================================================
# Purpose: Aggregates pre-processed response-centric data
#          (containing only form answers) into a
#          question-centric structure for analysis.

# =====================================================
#                      Imports
# =====================================================
import json
import os
import numpy as np # Keep for potential use in helpers/future stats
import time
import traceback
import sys
from collections import defaultdict
from typing import Dict, Any, List, Tuple


# =====================================================
#       Helper Functions
# =====================================================


def get_json_serializer():
    """Returns a robust JSON serializer function (handles numpy, sets)."""
    def default_serializer(obj):
        if isinstance(obj, (np.integer, np.floating)): return obj.item()
        if isinstance(obj, np.ndarray): return obj.tolist()
        if isinstance(obj, set): return sorted(list(obj))
        return json.JSONEncoder().default(obj)
    return default_serializer

def save_output(output_file: str, question_centric_data: Dict[str, Any]):
    """Saves the final question-centric output JSON."""
    logger.log(f"INFO: Saving question-centric aggregation output to {output_file}...")
    serializer = get_json_serializer()
    try:
        # Structure the final output with a top-level key
        final_output = {"question_data": question_centric_data}
        with open(output_file, 'w', encoding='utf-8') as f:
             json.dump(final_output, f, default=serializer, indent=2)
        logger.log(f"INFO: Successfully saved question-centric aggregation output.")
    except IOError as e:
        logger.log(f"ERROR: Could not write to output file {output_file}: {e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
    except TypeError as e:
        logger.log(f"ERROR: Failed to serialize data for output file {output_file}: {e}")
        problematic_keys = []
        # Attempt to identify problematic keys during serialization failure
        if isinstance(question_centric_data, dict):
            for k, v in question_centric_data.items():
                try: json.dumps({k: v}, default=serializer)
                except TypeError: problematic_keys.append(k)
        if problematic_keys: logger.log(f"ERROR: Problematic top-level keys during serialization: {problematic_keys}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
    except Exception as e:
        logger.log(f"ERROR: An unexpected error occurred during saving: {e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")

# =====================================================
#      Core Question-Centric Aggregation Logic (SIMPLIFIED)
# =====================================================

def aggregate_responses_by_question(
    response_centric_data: List[Dict[str, Any]],
    template_questions: Dict[str, str]
    ) -> Dict[str, Any]:
    """
    Aggregates data from the response-centric list into a question-centric structure,
    focusing ONLY on structured form answers.
    """
    if not response_centric_data:
        logger.log("WARN: No response data provided for aggregation.")
        return {}
    if not template_questions:
        logger.log("ERROR: Template questions map is empty. Cannot perform aggregation.")
        return {}

    # Initialize the main aggregation dictionary (Simplified structure)
    # Defines the default structure for each new question ID encountered
    question_aggregation = defaultdict(lambda: {
        "question_text": "",
        "total_responses_processed_for_question": 0, # Responses where this Q was potentially applicable
        "answered_form_count": 0,
        "unanswered_by_form_count": 0, # Count where no form answer was present
        "form_answer_examples": []      # Store {response_id, content, type}
    })

    logger.log(f"INFO: Starting aggregation for {len(template_questions)} questions across {len(response_centric_data)} responses (Form data only).")

    # --- Pre-populate basic info for all template questions ---
    total_responses_in_batch = len(response_centric_data)
    for qid, qtext in template_questions.items():
        question_aggregation[qid]["question_text"] = qtext
        # Set the total number of responses this question could have appeared in
        question_aggregation[qid]["total_responses_processed_for_question"] = total_responses_in_batch

    # --- Iterate through each response from the input list ---
    for i, response in enumerate(response_centric_data):
        response_id = response.get("response_id", f"UNKNOWN_{i}")
        # Extract form answers (assuming structure QID -> {question_text, type, content})
        form_answers = response.get("form_answers", {})

        # PDF related data from the input file is ignored in this simplified logic

        # --- Iterate through each *template* question ID to update its stats ---
        for qid in template_questions.keys():
            # Check if *this* response contains an answer for *this* template question
            has_form_answer = qid in form_answers

            # Increment counts based solely on form answer presence
            if has_form_answer:
                question_aggregation[qid]["answered_form_count"] += 1
                # Store form examples if limit not reached
                if len(question_aggregation[qid]["form_answer_examples"]) < MAX_EXAMPLE_ANSWERS:
                    answer_detail = form_answers.get(qid, {}) # Get the details for this QID
                    # Store relevant parts (adjust keys if needed based on input structure)
                    question_aggregation[qid]["form_answer_examples"].append({
                        "response_id": response_id,
                        "content": answer_detail.get("content"),
                        "type": answer_detail.get("type")
                    })
            else:
                # No form answer was found for this QID in this response
                question_aggregation[qid]["unanswered_by_form_count"] += 1

    logger.log("INFO: Aggregation loop finished. No further per-question stats calculated.")

    # --- No final calculations needed (fractions removed) ---

    return dict(question_aggregation) # Convert defaultdict back to dict for output

# @title Main Execution Block
# =====================================================
#                   Main Execution Block (Stage 2 - Question-Centric Aggregation)
# =====================================================
if __name__ == "__main__":
    # --- Configuration --- (Referencing the constants above)
    RESPONSE_CENTRIC_UDS_FILE = SIMPLIFIED_RESPONSE_UDS_FILE # Use the consistent name (Input for this stage)
    ORIGINAL_RESPONSES_SRC = ORIGINAL_RESPONSES_FILE # Use the consistent name (Needed for template)
    QUESTION_CENTRIC_OUTPUT_FILE = QUESTION_CENTRIC_UDS_FILE # Use the consistent name
    LOG_FILE = STAGE2_LOG_FILE # Use the stage-specific log file
    # MAX_EXAMPLE_ANSWERS is used within the aggregate_responses_by_question function

    # --- Logger Initialization --- (Assuming logger class is defined)
    logger = FileLogger(LOG_FILE)
    # (Enable debug logging can be checked within logger calls if needed)

    logger.log("=" * 50); logger.log(f" INFO: Starting Stage 2: Question-Centric Aggregation (Log: {LOG_FILE})"); logger.log("=" * 50)
    logger.log(f"INFO: Input Simplified Response UDS: {RESPONSE_CENTRIC_UDS_FILE}")
    logger.log(f"INFO: Original Responses Source (for Template): {ORIGINAL_RESPONSES_SRC}")
    logger.log(f"INFO: Output Question-Centric Aggregation: {QUESTION_CENTRIC_OUTPUT_FILE}")

    main_success = False
    try:
        # 1. Create the Question ID <-> Text Mapping from original source file
        logger.log(f"INFO: Reading {ORIGINAL_RESPONSES_FILE} to create template questions...")
        template_questions, text_to_id = create_template_questions(ORIGINAL_RESPONSES_FILE)

        if not template_questions:
            logger.log("ERROR: Failed to create template questions map from original file. Cannot proceed.")
            sys.exit(1)
        logger.log(f"INFO: Template map created with {len(template_questions)} unique questions.")

        # 2. Load the (Simplified) Response-Centric UDS Data (from JSON Lines)
        response_data_list = []
        try:
            logger.log(f"INFO: Loading response-centric data from JSON Lines file: {RESPONSE_CENTRIC_UDS_FILE}")
            with open(RESPONSE_CENTRIC_UDS_FILE, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        line_strip = line.strip()
                        if not line_strip: continue # Skip empty lines
                        response_object = json.loads(line_strip)
                        # Optional validation: check for expected keys
                        if not all(k in response_object for k in ["response_id", "form_answers", "response_stats"]):
                            logger.log(f"WARN: Skipping line {line_num} in {RESPONSE_CENTRIC_UDS_FILE} due to missing expected keys.")
                            continue
                        response_data_list.append(response_object)
                    except json.JSONDecodeError as e:
                        logger.log(f"ERROR: Failed to parse JSON on line {line_num} of {RESPONSE_CENTRIC_UDS_FILE}: {e}. Skipping line.")

            logger.log(f"INFO: Successfully loaded {len(response_data_list)} responses from {RESPONSE_CENTRIC_UDS_FILE}.")
            if not response_data_list:
                 logger.log(f"ERROR: No valid response data loaded from {RESPONSE_CENTRIC_UDS_FILE}. Cannot aggregate.")
                 sys.exit(1)

        except FileNotFoundError:
            logger.log(f"ERROR: Response-centric input file not found: {RESPONSE_CENTRIC_UDS_FILE}")
            sys.exit(1)
        except Exception as e:
            logger.log(f"ERROR: Unexpected error loading {RESPONSE_CENTRIC_UDS_FILE}: {e}")
            tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
            sys.exit(1)

        # 3. Perform Aggregation (using the simplified function)
        logger.log(f"INFO: Starting aggregation on {len(response_data_list)} responses...")
        question_centric_data = aggregate_responses_by_question( # Calls the simplified version
            response_data_list,
            template_questions
        )
        logger.log(f"INFO: Aggregation complete. Aggregated data for {len(question_centric_data)} questions.")

        # 4. Save the Question-Centric Output
        if question_centric_data:
            save_output(QUESTION_CENTRIC_UDS_FILE, question_centric_data) # Uses standard json.dump
            main_success = True
        else:
            logger.log("WARN: Aggregation resulted in empty data. No output file saved.")

    except Exception as main_e: # Catch unexpected errors in main script flow
        logger.log("CRITICAL:" + "="*10 + " FATAL ERROR in main execution block " + "="*10)
        logger.log(f"CRITICAL: Error: {main_e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
        logger.log("CRITICAL:" + "="*50)

    finally:
        logger.log("=" * 50)
        if main_success: logger.log(" INFO: Question-Centric Aggregation Finished Successfully ")
        else: logger.log(" ERROR: Question-Centric Aggregation Finished With Errors ")
        logger.log("=" * 50)

"""## Step 3:"""

from huggingface_hub import notebook_login

notebook_login()

# @title Question-Centric Summarization using LLM
# =====================================================
#   Stage 3: Question-Centric Summarization using LLM
# =====================================================
# Purpose: Loads aggregated counts from Stage 2, reads full
#          answers from Stage 1 for selected questions,
#          generates LLM summaries, and saves them.

# =====================================================
#                      Imports
# =====================================================
import json
import os
import sys
import traceback # For traceback logging
import time
import gc # Garbage Collection
from collections import defaultdict
from typing import Dict, Any, List

# Potentially heavy imports - keep them together
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
    # accelerate is usually needed for device_map='auto'
    import accelerate
except ImportError as e:
    print(f"CRITICAL ERROR: Missing essential libraries (torch, transformers, accelerate). Install them. Details: {e}")
    sys.exit(1)

try:
    # Only required if USE_QUANTIZATION is True
    from bitsandbytes.cextension import ColabNotSupportedError
except ImportError:
    # Define dummy class if bitsandbytes is not installed but quantization is False
    ColabNotSupportedError = type('ColabNotSupportedError', (Exception,), {})
    print("WARN: bitsandbytes not found. Quantization will be disabled if requested.")
    pass

# =====================================================
#                Custom FileLogger Class
# =====================================================
# Reusing the simplified FileLogger (no torch dependency)
class FileLogger:
    def __init__(self, log_file):
        self.log_file = log_file
        log_dir = os.path.dirname(log_file)
        if log_dir and not os.path.exists(log_dir):
             os.makedirs(log_dir, exist_ok=True)
        try:
            with open(self.log_file, 'w', encoding='utf-8') as f:
                f.write(f"{self._get_timestamp()}: Starting new log...\n")
        except IOError as e:
            print(f"CRITICAL: Failed to initialize log file {self.log_file}: {e}")

    def _get_timestamp(self):
        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())

    def log(self, message: str):
        """Logs a message to the file."""
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(f"{self._get_timestamp()}: {message}\n")
        except Exception as e:
            print(f"!!! LOGGER FAILED TO WRITE: {e} !!!")
            print(f"{self._get_timestamp()}: {message}") # Fallback print

    def log_json(self, data: Dict[str, Any], prefix="JSON_DATA"):
        """Logs a dictionary as formatted JSON."""
        serializer = self._get_serializer()
        try:
            log_entry = {
                 "timestamp": self._get_timestamp(),
                 "type": prefix,
                 "data": data
            }
            json_string = json.dumps(log_entry, default=serializer, indent=2)
            with open(self.log_file, 'a', encoding='utf-8') as f:
                 f.write(json_string + "\n")
        except TypeError as e:
            self.log(f"ERROR: Failed to serialize data for JSON logging: {e} - Data Keys: {list(data.keys())}")
        except Exception as e:
             self.log(f"ERROR: Unexpected error during log_json: {e}")
             tb_str = traceback.format_exc(); self.log(f"TRACEBACK:\n{tb_str}")

    def _get_serializer(self):
        """Returns a serializer function for json.dump."""
        def default_serializer(obj):
            # NumPy types can sometimes appear via libraries, keep if needed.
            # import numpy as np # Import only if needed
            # if isinstance(obj, (np.integer, np.floating)): return obj.item()
            # if isinstance(obj, np.ndarray): return obj.tolist()
            if isinstance(obj, set): return sorted(list(obj))
            try: return json.JSONEncoder().default(obj)
            except TypeError: return f"UNSERIALIZABLE:{type(obj).__name__}"
        return default_serializer

# =====================================================
#              Pipeline Configuration Constants
# =====================================================

# --- Input/Output Files ---
# Input 1: Aggregated data from Stage 2 (Single JSON object, contains counts, examples, texts)
STAGE2_AGGREGATION_FILE = 'question_centric_aggregation_output.json'
# Input 2: Simplified response data from Stage 1 (JSON Lines, used to get ALL answers)
STAGE1_SIMPLIFIED_UDS_FILE = 'response_centric_uds_simplified.jsonl'
# Output: Summaries generated by this script (Single JSON object)
SUMMARIES_OUTPUT_FILE = 'question_centric_summaries_output.json'

# --- LLM Configuration ---
MODEL_ID = "google/gemma-2-2b-it" # Recommend 2B for Colab Free Tier
# MODEL_ID = "google/gemma-2-9b-it" # Use if you have sufficient GPU RAM (>18GB ideal)
USE_QUANTIZATION = True # Set to False if bitsandbytes is unavailable or causing issues
# os.environ['HF_TOKEN'] = "hf_..." # Set if needed

# --- Summarization Parameters ---
NUM_SUMMARY_SENTENCES = 3 # Target sentences for the summary
MIN_ANSWERS_FOR_SUMMARY = 3 # Min *total* answers reported by Stage 2 to even *attempt* reading Stage 1 file

# --- Logging ---
STAGE3_LOG_FILE = "stage3_question_summarization.log"
ENABLE_DEBUG_LOGGING = True # Controls some optional logging calls

# --- Manual Question Selection (REVISED as per user) ---
QUESTIONS_TO_SUMMARIZE = {
    "Q1": True, "Q2": True, "Q3": True, "Q4": True, "Q5": True,
    "Q6": True, "Q7": True, "Q8": True, "Q9": True, "Q10": True,
    "Q11": True, "Q12": True, "Q13": True, "Q14": True, "Q15": True,
    "Q16": False, "Q17": True, "Q18": True, "Q19": True, "Q20": True,
    "Q21": True, "Q22": True, "Q23": True, "Q24": True, "Q25": True,
    "Q26": True, "Q27": True, "Q28": True, "Q29": True, "Q30": True,
    "Q31": True, "Q32": False, "Q33": False, "Q34": False, "Q35": False,
    "Q36": False,
}

# =====================================================
#                 Logger Initialization
# =====================================================
logger = FileLogger(STAGE3_LOG_FILE)

# =====================================================
#       Helper Function (for saving final output)
# =====================================================
def save_summaries_output(output_file: str, summaries_data: Dict[str, Any]):
    """Saves the final summaries output JSON."""
    logger.log(f"INFO: Saving summaries output to {output_file}...")
    try:
        final_output = {"question_summaries": summaries_data}
        with open(output_file, 'w', encoding='utf-8') as f:
             json.dump(final_output, f, indent=2)
        logger.log(f"INFO: Successfully saved summaries output.")
    except IOError as e:
        logger.log(f"ERROR: Could not write to output file {output_file}: {e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
    except TypeError as e:
        logger.log(f"ERROR: Failed to serialize summary data for output file {output_file}: {e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
    except Exception as e:
        logger.log(f"ERROR: An unexpected error occurred during saving summaries: {e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")


# =====================================================
#                   Main Execution Block (Stage 3)
# =====================================================
if __name__ == "__main__":

    # --- Log Initial Configuration ---
    logger.log("=" * 50); logger.log(" INFO: Starting Stage 3: Question Summarization "); logger.log("=" * 50)
    logger.log(f"INFO: Input Aggregated UDS (Stage 2): {STAGE2_AGGREGATION_FILE}")
    logger.log(f"INFO: Input Simplified UDS (Stage 1): {STAGE1_SIMPLIFIED_UDS_FILE}")
    logger.log(f"INFO: Output Summaries File: {SUMMARIES_OUTPUT_FILE}")
    logger.log(f"INFO: LLM Model ID: {MODEL_ID}")
    logger.log(f"INFO: Use Quantization: {USE_QUANTIZATION}")
    logger.log(f"INFO: Min Answers for Summary Attempt: {MIN_ANSWERS_FOR_SUMMARY}")
    logger.log(f"INFO: Target Summary Sentences: {NUM_SUMMARY_SENTENCES}")
    logger.log(f"INFO: Debug Logging Enabled: {ENABLE_DEBUG_LOGGING}")

    # --- Load LLM Resources ---
    loaded_tokenizer = None
    model_instance = None
    summarization_pipe = None
    llm_load_success = False
    compute_dtype = torch.float16 # Default compute type

    try:
        # --- Tokenizer ---
        logger.log(f"INFO: Loading tokenizer for: {MODEL_ID}...")
        loaded_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)

        if getattr(loaded_tokenizer, 'pad_token', None) is None:
             logger.log("WARN: Tokenizer missing pad token; setting to eos_token.")
             if getattr(loaded_tokenizer, 'eos_token', None) is not None:
                 loaded_tokenizer.pad_token = loaded_tokenizer.eos_token
                 loaded_tokenizer.padding_side = "left"
                 logger.log("INFO: Set pad_token to eos_token and padding_side to left.")
             else:
                 logger.log("ERROR: Tokenizer missing both pad_token and eos_token! Cannot set padding.")
                 sys.exit("CRITICAL: Tokenizer setup failed.")

        logger.log("INFO: Tokenizer loaded.")

        # --- Quantization Config (Optional) ---
        bnb_config = None
        effective_quantization = USE_QUANTIZATION
        if USE_QUANTIZATION:
            logger.log("INFO: Setting up 4-bit quantization...")
            try:
                compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16
                logger.log(f"INFO: Using compute dtype: {compute_dtype}")

                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=compute_dtype,
                    bnb_4bit_use_double_quant=True,
                )
                logger.log("INFO: 4-bit quantization configured.")
            except ImportError:
                logger.log("ERROR: bitsandbytes not found. Install it for quantization. Disabling quantization.")
                effective_quantization = False
            except ColabNotSupportedError:
                 logger.log("ERROR: Bits and Bytes Colab issue detected. Disabling quantization.")
                 effective_quantization = False
            except Exception as q_err:
                 logger.log(f"ERROR: Failed to configure BitsAndBytes: {q_err}. Disabling quantization.")
                 tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
                 effective_quantization = False

        # --- Main Model Loading ---
        logger.log(f"INFO: Loading main model: {MODEL_ID} (Quantization: {effective_quantization})...")
        if not torch.cuda.is_available():
            logger.log("WARN: CUDA not available, device_map='auto' will use CPU. Model loading/inference will be very slow.")

        model_instance = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            quantization_config=bnb_config if effective_quantization else None,
            device_map="auto",
            torch_dtype=compute_dtype if not effective_quantization else None,
            trust_remote_code=True # Recommended for Gemma 2
        )
        logger.log("INFO: Main model loaded.")

        # --- Pipeline Creation ---
        logger.log("INFO: Creating text-generation pipeline...")
        summarization_pipe = pipeline(
            "text-generation",
            model=model_instance,
            tokenizer=loaded_tokenizer,
        )
        logger.log("INFO: Pipeline created successfully.")
        llm_load_success = True

    except ImportError as ie:
         logger.log(f"ERROR: Missing libraries needed for LLM. Install requirements (torch, transformers, accelerate, bitsandbytes?). Details: {ie}")
         sys.exit(1)
    except Exception as e:
        logger.log(f"CRITICAL: Failed to load LLM models/pipeline: {e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
        logger.log("CRITICAL: Cannot proceed with summarization.")
        sys.exit(1)


    # --- Main Processing Steps ---
    main_success = False
    summaries_data = {} # Dictionary to store final results: QID -> {question_text, status, summary, ...}

    try:
        # 1. Load Aggregated Data (Counts & Texts) from Stage 2
        logger.log(f"INFO: Loading aggregated data from {STAGE2_AGGREGATION_FILE}...")
        try:
            with open(STAGE2_AGGREGATION_FILE, 'r', encoding='utf-8') as f:
                aggregated_data = json.load(f)

            if "question_data" not in aggregated_data or not isinstance(aggregated_data.get("question_data"), dict):
                logger.log(f"ERROR: Input file {STAGE2_AGGREGATION_FILE} missing 'question_data' key or it's not a dictionary.")
                sys.exit(1)
            question_data_from_stage2 = aggregated_data["question_data"]
            logger.log(f"INFO: Successfully loaded aggregated data for {len(question_data_from_stage2)} questions.")
        except FileNotFoundError:
            logger.log(f"ERROR: Aggregated input file not found: {STAGE2_AGGREGATION_FILE}")
            sys.exit(1)
        except json.JSONDecodeError as e:
            logger.log(f"ERROR: Failed to parse JSON from {STAGE2_AGGREGATION_FILE}: {e}")
            sys.exit(1)
        except Exception as e:
            logger.log(f"ERROR: Unexpected error loading {STAGE2_AGGREGATION_FILE}: {e}")
            tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
            sys.exit(1)


        # 2. Iterate through Questions, Filter, Read Full Answers, and Generate Summaries
        if not llm_load_success:
             logger.log("WARN: LLM failed to load earlier. Skipping summarization loop.")
        else:
            logger.log(f"INFO: Starting summarization processing...")
            processed_q_count = 0
            summarized_q_count = 0
            skipped_q_count = 0
            error_q_count = 0

            # Iterate through the questions defined in the Stage 2 aggregated data
            for qid, q_data in question_data_from_stage2.items():
                processed_q_count += 1
                question_text = q_data.get("question_text", f"Unknown Text for {qid}")

                # --- Initial Filtering (Before Reading Stage 1 File) ---
                should_summarize_flag = QUESTIONS_TO_SUMMARIZE.get(qid, False)
                if not should_summarize_flag:
                    if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG: Skipping QID {qid} (Marked False in config).")
                    skipped_q_count += 1
                    summaries_data[qid] = {"question_text": question_text, "status": "skipped_config", "summary": None}
                    continue

                # Check count reported by Stage 2
                total_form_answers_count_s2 = q_data.get("answered_form_count", 0)
                if total_form_answers_count_s2 < MIN_ANSWERS_FOR_SUMMARY:
                    logger.log(f"INFO: Skipping QID {qid} - Insufficient total answers reported by Stage 2 ({total_form_answers_count_s2} < {MIN_ANSWERS_FOR_SUMMARY}).")
                    skipped_q_count += 1
                    summaries_data[qid] = {
                        "question_text": question_text,
                        "status": "skipped_low_answers_s2_count",
                        "summary": None,
                        "total_answer_count_reported_s2": total_form_answers_count_s2
                    }
                    continue
                # --- End Initial Filtering ---

                # --- Collect ALL Answers for this QID from Stage 1 File ---
                logger.log(f"INFO: QID {qid} selected. Reading {STAGE1_SIMPLIFIED_UDS_FILE} to collect all answers...")
                all_text_answers_for_qid = []
                read_error_occurred = False
                try:
                    with open(STAGE1_SIMPLIFIED_UDS_FILE, 'r', encoding='utf-8') as infile:
                        for line_num, line in enumerate(infile, 1):
                            try:
                                line_strip = line.strip()
                                if not line_strip: continue
                                response_object = json.loads(line_strip)
                                form_answers_in_resp = response_object.get("form_answers", {})
                                if qid in form_answers_in_resp:
                                    answer_detail = form_answers_in_resp[qid]
                                    # Check type and content validity
                                    if answer_detail.get('type') == 'text' and answer_detail.get('content') is not None:
                                        content_str = str(answer_detail['content']).strip()
                                        if content_str: # Ensure it's not just whitespace
                                            all_text_answers_for_qid.append(content_str)
                            except json.JSONDecodeError as json_err:
                                logger.log(f"WARN: Skipping line {line_num} in {STAGE1_SIMPLIFIED_UDS_FILE} due to JSON parse error: {json_err}")
                            except Exception as inner_read_err:
                                logger.log(f"WARN: Error processing line {line_num} in {STAGE1_SIMPLIFIED_UDS_FILE} for QID {qid}: {inner_read_err}")
                                # Continue reading other lines if possible

                except FileNotFoundError:
                    logger.log(f"ERROR: Stage 1 file not found ({STAGE1_SIMPLIFIED_UDS_FILE}) while collecting answers for QID {qid}.")
                    read_error_occurred = True
                except Exception as read_err:
                     logger.log(f"ERROR: Failed reading Stage 1 file ({STAGE1_SIMPLIFIED_UDS_FILE}) for QID {qid}: {read_err}")
                     tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
                     read_error_occurred = True

                if read_error_occurred:
                     summaries_data[qid] = {
                          "question_text": question_text,
                          "status": "error_reading_stage1_answers",
                          "summary": None
                     }
                     error_q_count += 1
                     continue # Skip to next QID if we couldn't read the answers

                actual_answers_collected_count = len(all_text_answers_for_qid)
                logger.log(f"INFO: Collected {actual_answers_collected_count} valid text answers for QID {qid} from Stage 1 file.")
                # --- End Collecting ALL answers ---

                # --- Final Check on Collected Answer Count ---
                if actual_answers_collected_count < MIN_ANSWERS_FOR_SUMMARY:
                     logger.log(f"INFO: Skipping QID {qid} - Insufficient valid text answers found after reading Stage 1 file ({actual_answers_collected_count} < {MIN_ANSWERS_FOR_SUMMARY}).")
                     skipped_q_count += 1
                     summaries_data[qid] = {
                          "question_text": question_text,
                          "status": "skipped_low_answers_post_read",
                          "summary": None,
                          "actual_answers_collected": actual_answers_collected_count
                     }
                     continue
                # --- End Final Check ---

                # --- Proceed with Summarization ---
                logger.log(f"INFO: Summarizing {actual_answers_collected_count} answers for QID {qid}...")
                start_q_time = time.time()
                summary_text = None # Initialize summary variable

                try:
                    # Prepare text chunk using ALL collected answers
                    text_chunk = "\n\n".join(all_text_answers_for_qid)
                    # Optional: Add truncation for extremely long combined text
                    # MAX_INPUT_LEN = 30000 # Example character limit
                    # if len(text_chunk) > MAX_INPUT_LEN:
                    #    logger.log(f"WARN: Truncating input text for QID {qid} to {MAX_INPUT_LEN} chars.")
                    #    text_chunk = text_chunk[:MAX_INPUT_LEN]

                    # Format prompt for Gemma instruct model
                    messages = [
                        {"role": "user",
                         "content": f"Carefully review all the following responses to the question: \"{question_text}\". Then, concisely summarize the main themes or points expressed across these responses in exactly {NUM_SUMMARY_SENTENCES} distinct sentences.\n\nResponses:\n\"\"\"\n{text_chunk}\n\"\"\""}
                    ]

                    prompt_formatted = summarization_pipe.tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )

                    generation_args = {
                        "max_new_tokens": 200 + NUM_SUMMARY_SENTENCES * 50, # Generous token allowance
                        "do_sample": False,
                        "temperature": None, "top_p": None,
                        }
                    if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG: Calling LLM for QID {qid}...")
                    outputs = summarization_pipe(prompt_formatted, **generation_args)

                    # --- Extract the generated summary ---
                    generated_text_full = outputs[0]['generated_text']
                    if generated_text_full.startswith(prompt_formatted):
                        summary_text = generated_text_full[len(prompt_formatted):].strip()
                        status = "success"
                    else:
                        # Fallback extraction (less reliable)
                        response_marker = "<start_of_turn>model\n"
                        summary_start_index = generated_text_full.rfind(response_marker)
                        if summary_start_index != -1:
                             summary_text = generated_text_full[summary_start_index + len(response_marker):].strip()
                             status = "success"
                             logger.log(f"INFO: Used fallback marker extraction for QID {qid}.")
                        else:
                             logger.log(f"WARN: Could not reliably extract summary for QID {qid}. Output may contain prompt.")
                             summary_text = generated_text_full # Return full output
                             status = "success_extraction_uncertain"
                    # --- End Extraction ---

                    end_q_time = time.time()
                    logger.log(f"INFO: Successfully processed summary for QID {qid} in {end_q_time - start_q_time:.2f} seconds.")
                    summaries_data[qid] = {
                         "question_text": question_text,
                         "status": status,
                         "summary": summary_text,
                         "total_answers_summarized": actual_answers_collected_count
                     }
                    summarized_q_count += 1

                except Exception as pipe_err:
                    end_q_time = time.time()
                    logger.log(f"ERROR: LLM pipeline failed for QID {qid} after {end_q_time - start_q_time:.2f} seconds: {pipe_err}")
                    tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
                    summaries_data[qid] = {
                        "question_text": question_text,
                        "status": "error_llm_pipe",
                        "summary": None,
                        "error_message": str(pipe_err),
                        "answers_attempted": actual_answers_collected_count
                        }
                    error_q_count += 1

                # --- Progress Logging ---
                if (processed_q_count % 5 == 0) or (processed_q_count == len(question_data_from_stage2)):
                     logger.log(f"INFO: ...progress: processed {processed_q_count}/{len(question_data_from_stage2)} questions (Summarized: {summarized_q_count}, Skipped: {skipped_q_count}, Errors: {error_q_count}).")


            logger.log(f"INFO: Summarization loop finished. Processed: {processed_q_count}, Summarized: {summarized_q_count}, Skipped: {skipped_q_count}, Errors: {error_q_count}.")

        # 3. Save the Summaries
        save_summaries_output(SUMMARIES_OUTPUT_FILE, summaries_data)
        main_success = True # Mark success if loop completed or was skipped gracefully


    except Exception as main_e: # Catch unexpected errors in main flow
        logger.log("CRITICAL:" + "="*10 + " FATAL ERROR in main execution block " + "="*10)
        logger.log(f"CRITICAL: Error: {main_e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
        logger.log("CRITICAL:" + "="*50)

    finally:
        # --- Cleanup ---
        logger.log("INFO: Cleaning up LLM resources...")
        # Place deletion in try-except blocks in case variables were never assigned
        try: del summarization_pipe
        except NameError: pass
        try: del model_instance
        except NameError: pass
        try: del loaded_tokenizer
        except NameError: pass

        gc.collect() # Explicitly call garbage collector
        if torch.cuda.is_available():
             logger.log("INFO: Emptying CUDA cache...")
             try:
                 torch.cuda.empty_cache()
             except Exception as cuda_err:
                  logger.log(f"WARN: Error emptying CUDA cache: {cuda_err}")
        logger.log("INFO: Cleanup complete.")

        logger.log("=" * 50)
        if main_success: logger.log(" INFO: Stage 3 Summarization Finished Successfully ")
        else: logger.log(" ERROR: Stage 3 Summarization Finished With Errors or Did Not Complete ")
        logger.log("=" * 50)

SUPERCEDED
# @title Question-Centric Summarization using LLM (superceded)
# =====================================================
#   Stage 3: Question-Centric Summarization using LLM
# =====================================================
# Purpose: Loads aggregated data from Stage 2, and for a
#          manually selected subset of questions, generates
#          LLM-based summaries of the collected form answers.

# =====================================================
#                      Imports
# =====================================================
import json
import os
import sys
import traceback # For traceback logging
import time
import gc # Garbage Collection
from collections import defaultdict
from typing import Dict, Any, List

# Potentially heavy imports - keep them together
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
    # You might need 'accelerate' implicitly for device_map='auto'
    # import accelerate
except ImportError as e:
    # Cannot log yet as logger isn't initialized, print critical error and exit
    print(f"CRITICAL ERROR: Missing essential libraries (torch, transformers). Install them. Details: {e}")
    sys.exit(1)

try:
    # Only required if USE_QUANTIZATION is True
    from bitsandbytes.cextension import ColabNotSupportedError
except ImportError:
    # Define dummy class if bitsandbytes is not installed but quantization is False
    ColabNotSupportedError = type('ColabNotSupportedError', (Exception,), {})
    pass # Silently ignore if not needed

# =====================================================
#                Custom FileLogger Class
# =====================================================
# Reusing the simplified FileLogger (no torch dependency)
class FileLogger:
    def __init__(self, log_file):
        self.log_file = log_file
        log_dir = os.path.dirname(log_file)
        if log_dir and not os.path.exists(log_dir):
             os.makedirs(log_dir, exist_ok=True)
        try:
            # Open in append mode 'a' to preserve logs across restarts if needed,
            # or 'w' to start fresh each run. Using 'w' for now.
            with open(self.log_file, 'w', encoding='utf-8') as f:
                f.write(f"{self._get_timestamp()}: Starting new log...\n")
        except IOError as e:
            print(f"CRITICAL: Failed to initialize log file {self.log_file}: {e}")
            # Optionally exit if logging is absolutely critical
            # sys.exit(1)

    def _get_timestamp(self):
        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())

    def log(self, message: str):
        """Logs a message to the file."""
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(f"{self._get_timestamp()}: {message}\n")
        except Exception as e:
            # Fallback print if logging to file fails
            print(f"!!! LOGGER FAILED TO WRITE: {e} !!!")
            print(f"{self._get_timestamp()}: {message}") # Log to console as fallback

    def log_json(self, data: Dict[str, Any], prefix="JSON_DATA"):
        """Logs a dictionary as formatted JSON."""
        serializer = self._get_serializer()
        try:
            log_entry = {
                 "timestamp": self._get_timestamp(),
                 "type": prefix,
                 "data": data
            }
            # Use the custom serializer for dumping
            json_string = json.dumps(log_entry, default=serializer, indent=2)
            with open(self.log_file, 'a', encoding='utf-8') as f:
                 f.write(json_string + "\n")
        except TypeError as e:
            # Log the serialization error using the standard log method
            self.log(f"ERROR: Failed to serialize data for JSON logging: {e} - Data Keys: {list(data.keys())}")
        except Exception as e:
             # Log any other unexpected errors during JSON logging
             self.log(f"ERROR: Unexpected error during log_json: {e}")
             # Log traceback for unexpected errors
             tb_str = traceback.format_exc()
             self.log(f"TRACEBACK:\n{tb_str}")

    def _get_serializer(self):
        """Returns a serializer function for json.dump."""
        # Define a nested function to handle custom serialization types
        def default_serializer(obj):
            # Handle NumPy numeric types if numpy is imported and used
            # import numpy as np # Import locally if needed just here
            # if isinstance(obj, (np.integer, np.floating)): return obj.item()
            # if isinstance(obj, np.ndarray): return obj.tolist()

            # Handle sets
            if isinstance(obj, set):
                return sorted(list(obj)) # Convert to sorted list
            # If none of the above, try the default JSON encoder
            try:
                # This will raise TypeError if the object is not serializable by default
                return json.JSONEncoder().default(obj)
            except TypeError:
                # If default encoder fails, return a string representation
                return f"UNSERIALIZABLE:{type(obj).__name__}"
        return default_serializer # Return the nested function

# =====================================================
#              Pipeline Configuration Constants
# =====================================================

# --- Input/Output Files ---
# Input: Aggregated data from Stage 2 (Single JSON object)
QUESTION_CENTRIC_UDS_FILE = 'question_centric_aggregation_output.json'
# Output: Summaries generated by this script (Single JSON object)
SUMMARIES_OUTPUT_FILE = 'question_centric_summaries_output.json'

# --- LLM Configuration ---
# Choose model based on available resources (Colab free tier might struggle with 9B)
MODEL_ID = "google/gemma-2-2b-it" # Smaller Gemma 2 model, better for Colab free
# MODEL_ID = "google/gemma-2-9b-it" # Larger model, needs more GPU RAM
USE_QUANTIZATION = True # Set to False if bitsandbytes is unavailable or causing issues
# Consider setting HF token if needed for gated models: os.environ['HF_TOKEN'] = "hf_..."

# --- Summarization Parameters ---
NUM_SUMMARY_SENTENCES = 3 # How many sentences to ask for in the summary
MIN_ANSWERS_FOR_SUMMARY = 3 # Minimum number of text answers needed to attempt summary

# --- Logging ---
STAGE3_LOG_FILE = "stage3_question_summarization.log"
ENABLE_DEBUG_LOGGING = True # Controls some optional logging calls

# --- Manual Question Selection (REVISED as per user) ---
# Dictionary mapping Question ID -> True/False (whether to summarize)
QUESTIONS_TO_SUMMARIZE = {
    "Q1": True,   # "Should there be a criminal offence..."
    "Q2": True,   # "Should consent be required for other tracking data..."
    "Q3": True,   # "How should employers provide enhanced transparency..."
    "Q4": True,   # "Noting the current individual rights..."
    "Q5": True,   # "If privacy protections for employees were introduced..."
    "Q6": True,   # "If you are a small business operator..."
    "Q7": True,   # "What additional requirements should apply to mitigate privacy risks (biometric)..."
    "Q8": True,   # "What would the impact of the proposed individual rights be..."
    "Q9": True,   # "Are further exceptions required for any of the proposed individual rights?"
    "Q10": True,  # "What baseline privacy outcomes should be included in APP 11?"
    "Q11": True,  # "What are the barriers APP entities face to minimise collection (identity credential)..."
    "Q12": True,  # "Should APP entities be required to take reasonable steps (Notifiable Data Breach)..."
    "Q13": True,  # "What additional support... needed to assist smaller media organisations..."
    "Q14": True,  # "Should the scope of research permitted without consent be broadened?"
    "Q15": True,  # "Should there be a single exception for research without consent..."
    "Q16": False, # "Which entity is the most appropriate body to develop guidelines..."
    "Q17": True,  # "What privacy-related issues do APP entities face (financial abuse)..."
    "Q18": True,  # "How can financial institutions act in the interests of customers (financial abuse/capacity)..."
    "Q19": True,  # "Should the permitted general situations... be amended (safeguarding)..."
    "Q20": True,  # "What types of decisions are likely to have a legal or similarly significant effect..."
    "Q21": True,  # "Should there be exceptions to a right for individuals to request meaningful information (automated decisions)..."
    "Q22": True,  # "What would be the impact... in relation to direct marketing..."
    "Q23": True,  # "What would be the impact... in relation to targeting..."
    "Q24": True,  # "What would be the impact... in relation to sale of personal information..."
    "Q25": True,  # "Are there any technical or other challenges... providing information about algorithms..."
    "Q26": True,  # "Please share any examples of situations where greater transparency... is not necessary..."
    "Q27": True,  # "If small business non-APP entities... brought into the scope..."
    "Q28": True,  # "Should the extraterritorial scope... be amended..."
    "Q29": True,  # "Should disclosures... to overseas recipients via publication... be subject to an exception..."
    "Q30": True,  # "How can reporting processes for Notifiable Data Breaches be streamlined..."
    "Q31": True,  # "If you would like to provide general feedback..."
    "Q32": False, # "What is your name?"
    "Q33": False, # "Who are you making this submission for?"
    "Q34": False, # "What is your organisation?"
    "Q35": False, # "What sector is your organisation a part of?"
    "Q36": False, # "What is your position in the organisation?"
}

# =====================================================
#                 Logger Initialization
# =====================================================
logger = FileLogger(STAGE3_LOG_FILE)

# =====================================================
#       Helper Function (for saving final output)
# =====================================================
def save_summaries_output(output_file: str, summaries_data: Dict[str, Any]):
    """Saves the final summaries output JSON."""
    logger.log(f"INFO: Saving summaries output to {output_file}...")
    # No complex types expected in final summary dict, standard encoder should work
    try:
        # Structure the final output with a top-level key
        final_output = {"question_summaries": summaries_data}
        with open(output_file, 'w', encoding='utf-8') as f:
             json.dump(final_output, f, indent=2)
        logger.log(f"INFO: Successfully saved summaries output.")
    except IOError as e:
        # Corrected logger usage
        logger.log(f"ERROR: Could not write to output file {output_file}: {e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
    except TypeError as e:
        # Corrected logger usage
        logger.log(f"ERROR: Failed to serialize summary data for output file {output_file}: {e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
    except Exception as e:
        # Corrected logger usage
        logger.log(f"ERROR: An unexpected error occurred during saving summaries: {e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")


# =====================================================
#                   Main Execution Block (Stage 3)
# =====================================================
if __name__ == "__main__":

    # --- Log Initial Configuration ---
    logger.log("=" * 50); logger.log(" INFO: Starting Stage 3: Question Summarization "); logger.log("=" * 50)
    logger.log(f"INFO: Input Aggregated UDS: {QUESTION_CENTRIC_UDS_FILE}")
    logger.log(f"INFO: Output Summaries File: {SUMMARIES_OUTPUT_FILE}")
    logger.log(f"INFO: LLM Model ID: {MODEL_ID}")
    logger.log(f"INFO: Use Quantization: {USE_QUANTIZATION}")
    logger.log(f"INFO: Min Answers for Summary: {MIN_ANSWERS_FOR_SUMMARY}")
    logger.log(f"INFO: Target Summary Sentences: {NUM_SUMMARY_SENTENCES}")
    logger.log(f"INFO: Debug Logging Enabled: {ENABLE_DEBUG_LOGGING}")

    # --- Load LLM Resources ---
    loaded_tokenizer = None
    model_instance = None
    summarization_pipe = None
    llm_load_success = False

    try:
        # --- Tokenizer ---
        logger.log(f"INFO: Loading tokenizer for: {MODEL_ID}...")
        loaded_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)

        if getattr(loaded_tokenizer, 'pad_token', None) is None:
             # Corrected logger usage
             logger.log("WARN: Tokenizer missing pad token; setting to eos_token.")
             if getattr(loaded_tokenizer, 'eos_token', None) is not None:
                 loaded_tokenizer.pad_token = loaded_tokenizer.eos_token
                 loaded_tokenizer.padding_side = "left" # Important for batch generation if used later
                 logger.log("INFO: Set pad_token to eos_token and padding_side to left.")
             else:
                 # Corrected logger usage
                 logger.log("ERROR: Tokenizer missing both pad_token and eos_token! Cannot set padding.")
                 sys.exit("CRITICAL: Tokenizer setup failed.") # Exit cleanly

        logger.log("INFO: Tokenizer loaded.")

        # --- Quantization Config (Optional) ---
        bnb_config = None
        effective_quantization = USE_QUANTIZATION # Track if quantization is actually used
        if USE_QUANTIZATION:
            logger.log("INFO: Setting up 4-bit quantization...")
            try:
                # Check torch version and CUDA availability for bfloat16 support
                compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16
                logger.log(f"INFO: Using compute dtype: {compute_dtype}")

                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=compute_dtype,
                    bnb_4bit_use_double_quant=True,
                )
                logger.log("INFO: 4-bit quantization configured.")
            except ImportError:
                # Corrected logger usage
                logger.log("ERROR: bitsandbytes not found. Install it for quantization. Disabling quantization.")
                effective_quantization = False
            except ColabNotSupportedError:
                 # Corrected logger usage
                 logger.log("ERROR: Bits and Bytes Colab issue detected (often related to older versions or specific hardware). Disabling quantization.")
                 effective_quantization = False
            except Exception as q_err:
                 # Corrected logger usage
                 logger.log(f"ERROR: Failed to configure BitsAndBytes: {q_err}. Disabling quantization.")
                 tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
                 effective_quantization = False

        # --- Main Model Loading ---
        logger.log(f"INFO: Loading main model: {MODEL_ID} (Quantization: {effective_quantization})...")
        if not torch.cuda.is_available():
            # Corrected logger usage
            logger.log("WARN: CUDA not available, device_map='auto' will use CPU. Model loading/inference will be very slow.")

        model_instance = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            quantization_config=bnb_config if effective_quantization else None,
            device_map="auto",
            torch_dtype=compute_dtype if not effective_quantization else None, # Set dtype if not quantizing
            trust_remote_code=True # Often needed for newer models like Gemma 2
        )
        logger.log("INFO: Main model loaded.")

        # --- Pipeline Creation ---
        logger.log("INFO: Creating text-generation pipeline...")
        summarization_pipe = pipeline(
            "text-generation",
            model=model_instance,
            tokenizer=loaded_tokenizer,
            # device is handled by device_map
            # Consider adding default max_length here if desired
            # max_length=512 # Example
        )
        logger.log("INFO: Pipeline created successfully.")
        llm_load_success = True

    except ImportError as ie:
         # Corrected logger usage
         logger.log(f"ERROR: Missing libraries needed for LLM. Install requirements (torch, transformers, accelerate, bitsandbytes?). Details: {ie}")
         sys.exit(1)
    except Exception as e:
        # Corrected logger usage
        logger.log(f"CRITICAL: Failed to load LLM models/pipeline: {e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
        logger.log("CRITICAL: Cannot proceed with summarization.")
        sys.exit(1)


    # --- Main Processing Steps ---
    main_success = False
    summaries_data = {} # Dictionary to store QID -> {status, summary, ...}

    try:
        # 1. Load Aggregated Data from Stage 2
        logger.log(f"INFO: Loading aggregated data from {QUESTION_CENTRIC_UDS_FILE}...")
        try:
            with open(QUESTION_CENTRIC_UDS_FILE, 'r', encoding='utf-8') as f:
                aggregated_data = json.load(f)

            if "question_data" not in aggregated_data or not isinstance(aggregated_data.get("question_data"), dict):
                # Corrected logger usage
                logger.log(f"ERROR: Input file {QUESTION_CENTRIC_UDS_FILE} missing 'question_data' key or it's not a dictionary.")
                sys.exit(1)
            question_data = aggregated_data["question_data"]
            logger.log(f"INFO: Successfully loaded aggregated data for {len(question_data)} questions.")
        except FileNotFoundError:
            # Corrected logger usage
            logger.log(f"ERROR: Aggregated input file not found: {QUESTION_CENTRIC_UDS_FILE}")
            sys.exit(1)
        except json.JSONDecodeError as e:
            # Corrected logger usage
            logger.log(f"ERROR: Failed to parse JSON from {QUESTION_CENTRIC_UDS_FILE}: {e}")
            sys.exit(1)
        except Exception as e:
            # Corrected logger usage
            logger.log(f"ERROR: Unexpected error loading {QUESTION_CENTRIC_UDS_FILE}: {e}")
            tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
            sys.exit(1)


        # 2. Iterate through Questions and Generate Summaries (only if LLM loaded)
        if not llm_load_success:
             # Corrected logger usage
             logger.log("WARN: LLM failed to load. Skipping summarization loop.")
        else:
            logger.log(f"INFO: Starting summarization loop for selected questions...")
            processed_q_count = 0
            summarized_q_count = 0
            skipped_q_count = 0
            error_q_count = 0

            # Iterate through the questions found in the Stage 2 data
            for qid, q_data in question_data.items():
                processed_q_count += 1
                question_text = q_data.get("question_text", f"Unknown Text for {qid}")

                # --- Filtering Logic ---
                should_summarize_flag = QUESTIONS_TO_SUMMARIZE.get(qid, False) # Check manual config
                if not should_summarize_flag:
                    if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG: Skipping QID {qid} (Marked False in config).")
                    skipped_q_count += 1
                    summaries_data[qid] = {"status": "skipped_config", "summary": None}
                    continue

                answers = q_data.get("form_answer_examples", [])
                # Filter for non-empty text answers
                text_answers = [str(ans['content']) for ans in answers
                                if ans.get('type') == 'text' and ans.get('content') is not None and str(ans['content']).strip()]

                if len(text_answers) < MIN_ANSWERS_FOR_SUMMARY:
                    logger.log(f"INFO: Skipping QID {qid} - Insufficient valid text answers ({len(text_answers)} found < {MIN_ANSWERS_FOR_SUMMARY} required).")
                    skipped_q_count += 1
                    summaries_data[qid] = {"status": "skipped_low_answers", "summary": None, "answer_count": len(text_answers)}
                    continue
                # --- End Filtering Logic ---

                logger.log(f"INFO: Processing QID {qid} for summarization ({len(text_answers)} answers)...")
                start_q_time = time.time()

                # Prepare text chunk for LLM
                text_chunk = "\n\n".join(text_answers)

                # Format prompt for Gemma instruct model
                # Note: Gemma-2 templates might differ slightly, check model card if issues arise
                messages = [
                    # Removed role for Gemma 2 - it might prefer just user/model turns or specific syntax
                    {"role": "user",
                     "content": f"Carefully review all the following responses to the question: \"{question_text}\". Then, concisely summarize the main themes or points expressed across these responses in exactly {NUM_SUMMARY_SENTENCES} distinct sentences.\n\nResponses:\n\"\"\"\n{text_chunk}\n\"\"\""}
                ]

                try:
                    # Generate prompt string using tokenizer
                    prompt_formatted = summarization_pipe.tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True # Add prompt for generation turn
                    )

                    # Call the LLM pipeline
                    # Adjust max_new_tokens based on expected summary length + buffer
                    # Setting temperature low for more factual summary
                    generation_args = {
                        "max_new_tokens": 150 + NUM_SUMMARY_SENTENCES * 40, # Estimate tokens needed
                        "do_sample": False, # Turn off sampling for deterministic output
                        "temperature": None, # Not needed if do_sample=False
                        "top_p": None, # Not needed if do_sample=False
                        }
                    if ENABLE_DEBUG_LOGGING: logger.log(f"DEBUG: Calling LLM for QID {qid}...")
                    outputs = summarization_pipe(prompt_formatted, **generation_args)

                    # Extract the generated summary
                    generated_text_full = outputs[0]['generated_text']

                    # Robust extraction: Remove the input prompt part
                    # Check if the generated text starts with the formatted prompt
                    if generated_text_full.startswith(prompt_formatted):
                        summary_text = generated_text_full[len(prompt_formatted):].strip()
                    else:
                        # Fallback if template format caused unexpected output start
                        # Try finding common model response markers (may vary)
                        response_marker = "<start_of_turn>model\n" # Common marker
                        summary_start_index = generated_text_full.rfind(response_marker)
                        if summary_start_index != -1:
                             summary_text = generated_text_full[summary_start_index + len(response_marker):].strip()
                        else:
                             # If still not found, log warning and maybe return full output for inspection
                             # Corrected logger usage
                             logger.log(f"WARN: Could not reliably extract summary for QID {qid} using standard markers/prompt removal. Output may contain prompt residue.")
                             summary_text = generated_text_full # Return full output as best guess

                    end_q_time = time.time()
                    logger.log(f"INFO: Successfully generated summary for QID {qid} in {end_q_time - start_q_time:.2f} seconds.")
                    summaries_data[qid] = {"status": "success", "summary": summary_text, "answer_count": len(text_answers)}
                    summarized_q_count += 1

                except Exception as pipe_err:
                    end_q_time = time.time()
                    # Corrected logger usage
                    logger.log(f"ERROR: LLM pipeline failed for QID {qid} after {end_q_time - start_q_time:.2f} seconds: {pipe_err}")
                    tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
                    summaries_data[qid] = {"status": "error_llm_pipe", "summary": None, "error_message": str(pipe_err)}
                    error_q_count += 1 # Count errors separately from skipped

                # Log progress within loop
                if (processed_q_count % 5 == 0) or (processed_q_count == len(question_data)):
                     logger.log(f"INFO: ...progress: processed {processed_q_count}/{len(question_data)} questions (Summarized: {summarized_q_count}, Skipped: {skipped_q_count}, Errors: {error_q_count}).")


            logger.log(f"INFO: Summarization loop finished. Processed: {processed_q_count}, Summarized: {summarized_q_count}, Skipped: {skipped_q_count}, Errors: {error_q_count}.")

        # 3. Save the Summaries
        save_summaries_output(SUMMARIES_OUTPUT_FILE, summaries_data)
        main_success = True # Mark success if loop completed or was skipped gracefully


    except Exception as main_e: # Catch unexpected errors in main flow
        # Corrected logger usage
        logger.log("CRITICAL:" + "="*10 + " FATAL ERROR in main execution block " + "="*10)
        logger.log(f"CRITICAL: Error: {main_e}")
        tb_str = traceback.format_exc(); logger.log(f"TRACEBACK:\n{tb_str}")
        logger.log("CRITICAL:" + "="*50)

    finally:
        # --- Cleanup ---
        logger.log("INFO: Cleaning up LLM resources...")
        del summarization_pipe
        del model_instance
        del loaded_tokenizer
        gc.collect() # Explicitly call garbage collector
        if torch.cuda.is_available():
             logger.log("INFO: Emptying CUDA cache...")
             try:
                 torch.cuda.empty_cache()
             except Exception as cuda_err:
                  # Corrected logger usage
                  logger.log(f"WARN: Error emptying CUDA cache: {cuda_err}")
        logger.log("INFO: Cleanup complete.")

        logger.log("=" * 50)
        if main_success: logger.log(" INFO: Stage 3 Summarization Finished Successfully ")
        # Corrected logger usage
        else: logger.log(" ERROR: Stage 3 Summarization Finished With Errors or Did Not Complete ")
        logger.log("=" * 50)

"""# END"""
