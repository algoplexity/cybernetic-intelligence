{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-requisites"
      ],
      "metadata": {
        "id": "KO-6XBvavdzQ"
      },
      "id": "KO-6XBvavdzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pybdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaW9O1LiviQM",
        "outputId": "a8af3ce0-59d8-4d92-8c90-af22ebeb8084"
      },
      "id": "gaW9O1LiviQM",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybdm\n",
            "  Downloading pybdm-0.1.0-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.11/dist-packages (from pybdm) (2.0.2)\n",
            "Downloading pybdm-0.1.0-py2.py3-none-any.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybdm\n",
            "Successfully installed pybdm-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94RukJ_YITAA",
        "outputId": "622755a4-3f1c-452d-d38c-2f1e1408faff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.2\n",
            "    Uninstalling transformers-4.52.2:\n",
            "      Successfully uninstalled transformers-4.52.2\n",
            "Successfully installed bitsandbytes-0.46.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-4.52.4\n"
          ]
        }
      ],
      "source": [
        "pip install -U transformers bitsandbytes accelerate"
      ],
      "id": "94RukJ_YITAA"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlRaGVHDebOm",
        "outputId": "19330b03-cf9b-48be-b819-018a0bed24a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "mlRaGVHDebOm"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwkuBBweecko",
        "outputId": "806e29f2-ee11-48d6-dc9e-ace299e26dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Legal\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/Legal"
      ],
      "id": "bwkuBBweecko"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3rd June"
      ],
      "metadata": {
        "id": "gJCMwMyO473F"
      },
      "id": "gJCMwMyO473F"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1: Configuration\n",
        "\n",
        "import os\n",
        "from typing import List, Dict, Set\n",
        "from collections import Counter\n",
        "\n",
        "# --- Project Paths ---\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"]\n",
        "\n",
        "# --- BDM Configuration ---\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "# MAX_TEXT_FOR_BDM_HASH = 2000 # This will now be controlled by BDM_SEGMENT_LENGTH\n",
        "BDM_SEGMENT_LENGTH = 2000 # NEW: Length of segments for calculating full corpus BDM\n",
        "\n",
        "# --- LLM Configuration ---\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "LLM_BATCH_SIZE_RESPONSES = 5\n",
        "LLM_RETRY_ATTEMPTS = 2\n",
        "MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 1000\n",
        "MAX_MOTIFS_PER_CHUNK = 3\n",
        "\n",
        "# --- Token-Based L(H) Configuration (SFs cost zeroed for current experiment) ---\n",
        "MOTIF_SYMBOLIC_LABEL_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TOKEN_COST = 0.1\n",
        "MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.0\n",
        "MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.0\n",
        "\n",
        "# --- Surface Form Filtering Configuration ---\n",
        "MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "MAX_SF_TOKEN_LENGTH_FOR_FINAL_MOTIF = 6\n",
        "\n",
        "# --- Logging File ---\n",
        "LLM_DEBUG_LOG_FILE = os.path.join(BASE_PROJECT_DIR, \"llm_motif_debug_log_refactored_v10_segmented_bdm.txt\")\n",
        "\n",
        "print(f\"Cell 1: Configuration loaded. LOCAL_LLM_MODEL_ID set to '{LOCAL_LLM_MODEL_ID}'.\")\n",
        "print(f\"BDM Strategy: Segmented BDM with segment length = {BDM_SEGMENT_LENGTH}\")\n",
        "# ... (other existing print statements for config)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4d8lOWOx45_l"
      },
      "id": "4d8lOWOx45_l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4: Motif Processing and Validation\n",
        "\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict\n",
        "# from collections import Counter # Defined in Cell 1, used by extract_actual_phrases (Cell 2)\n",
        "\n",
        "# Assume constants from Cell 1 are in global scope\n",
        "# Assume text_utils functions (preprocess_corpus_for_motif_extraction, count_sf_occurrences, tokenize_phrase) from Cell 2 are in scope\n",
        "# Assume LLM interaction functions (create_enhanced_motif_prompt, call_local_llm_for_raw_response) from Cell 3 are in scope\n",
        "\n",
        "try:\n",
        "    LLM_DEBUG_LOG_FILE; LLM_RETRY_ATTEMPTS; MIN_SF_FREQUENCY_IN_FULL_CORPUS; MAX_SF_TOKEN_LENGTH_FOR_FINAL_MOTIF\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 4): Key config constants not found from Cell 1. Using fallbacks.\")\n",
        "    LLM_DEBUG_LOG_FILE = \"temp_llm_debug_log_cell4.txt\"; LLM_RETRY_ATTEMPTS = 2\n",
        "    MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2; MAX_SF_TOKEN_LENGTH_FOR_FINAL_MOTIF = 6\n",
        "\n",
        "\n",
        "def parse_and_validate_llm_json_response(\n",
        "    llm_raw_response_text: str,\n",
        "    qid_for_log:str,\n",
        "    chunk_idx_for_log:int,\n",
        "    prompt_sent_to_llm:str\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"Parses LLM JSON, attempts label fixing, validates schema.\"\"\"\n",
        "    json_str_candidate = llm_raw_response_text.strip()\n",
        "\n",
        "    # Attempt to remove markdown fences if LLM adds them\n",
        "    if json_str_candidate.startswith(\"```json\"):\n",
        "        json_str_candidate = json_str_candidate[len(\"```json\"):].strip()\n",
        "    if json_str_candidate.startswith(\"```\"):\n",
        "        json_str_candidate = json_str_candidate[len(\"```\"):].strip()\n",
        "    # VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\n",
        "    # CORRECTED TYPO HERE\n",
        "    if json_str_candidate.endswith(\"```\"):\n",
        "        json_str_candidate = json_str_candidate[:-len(\"```\")].strip() # Was json_str, now json_str_candidate\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "    # print(f\"    DEBUG (parse_validate): QID {qid_for_log}, Chunk {chunk_idx_for_log}, JSON candidate for parsing:\\n{json_str_candidate[:500]}...\")\n",
        "\n",
        "    if not json_str_candidate or json_str_candidate.lower() == \"[]\" or \\\n",
        "       \"no_themes_found\" in json_str_candidate.lower() or \\\n",
        "       \"no clear motifs\" in json_str_candidate.lower():\n",
        "        return []\n",
        "\n",
        "    # Attempt to fix common JSON error: trailing commas before closing brackets/braces\n",
        "    json_str_candidate_fixed = re.sub(r',\\s*([\\}\\]])', r'\\1', json_str_candidate)\n",
        "    if json_str_candidate_fixed != json_str_candidate:\n",
        "        # print(f\"    DEBUG (parse_validate): Applied trailing comma fix for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        json_str_candidate = json_str_candidate_fixed\n",
        "\n",
        "    try:\n",
        "        parsed_data = json.loads(json_str_candidate)\n",
        "        if isinstance(parsed_data, dict): parsed_data = [parsed_data]\n",
        "        if not isinstance(parsed_data, list):\n",
        "            raise ValueError(\"Parsed JSON from LLM is not a list or a single object.\")\n",
        "\n",
        "        valid_motifs_from_json = []\n",
        "        for item_idx, item in enumerate(parsed_data):\n",
        "            if not isinstance(item, dict) or not item: continue\n",
        "\n",
        "            label_str_original = item.get('label', \"\")\n",
        "            label_str_processed = \"\"\n",
        "            if isinstance(label_str_original, str) and label_str_original.strip():\n",
        "                temp_label_stripped = label_str_original.strip()\n",
        "                match_strict_bracketed = re.fullmatch(r\"\\[([A-Z0-9_]+)\\]\", temp_label_stripped)\n",
        "                if match_strict_bracketed:\n",
        "                    label_str_processed = temp_label_stripped\n",
        "                else:\n",
        "                    match_bracketed_part = re.search(r\"(\\[[A-Z0-9_]+(?:_[A-Z0-9_]+)*\\])\", temp_label_stripped)\n",
        "                    if match_bracketed_part:\n",
        "                        label_str_processed = match_bracketed_part.group(1)\n",
        "                    else:\n",
        "                        sanitized_content = re.sub(r'\\s+|-', '_', temp_label_stripped)\n",
        "                        sanitized_content = re.sub(r'[^a-zA-Z0-9_]', '', sanitized_content).upper()\n",
        "                        sanitized_content = \"_\".join(sanitized_content.split('_')[:4])\n",
        "                        if sanitized_content:\n",
        "                            label_str_processed = f\"[{sanitized_content}]\"\n",
        "\n",
        "            item['label'] = label_str_processed\n",
        "\n",
        "            label_to_validate = item.get('label',\"\")\n",
        "            desc_str = item.get('description',\"\")\n",
        "            sf_list = item.get('surface_forms', [])\n",
        "\n",
        "            has_all_keys = all(k in item for k in [\"label\", \"description\", \"surface_forms\"])\n",
        "            label_is_valid_format = isinstance(label_to_validate, str) and \\\n",
        "                                    bool(label_to_validate) and \\\n",
        "                                    label_to_validate.startswith('[') and \\\n",
        "                                    label_to_validate.endswith(']') and \\\n",
        "                                    re.fullmatch(r\"\\[[A-Z0-9_]+\\]\", label_to_validate)\n",
        "            desc_is_valid = isinstance(item.get('description'), str)\n",
        "            sfs_list_is_valid = isinstance(item.get('surface_forms'), list) and \\\n",
        "                                 all(isinstance(sf_item, str) for sf_item in item.get('surface_forms',[]))\n",
        "\n",
        "            if has_all_keys and label_is_valid_format and desc_is_valid and sfs_list_is_valid:\n",
        "                valid_motifs_from_json.append({\n",
        "                    \"label\": label_to_validate,\n",
        "                    \"description\": item['description'].strip(),\n",
        "                    \"surface_forms\": [s.strip() for s in item['surface_forms'] if isinstance(s, str) and s.strip()]\n",
        "                })\n",
        "            else: # Item failed detailed schema validation\n",
        "                print(f\"    [WARN] Invalid motif object schema for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}. Skipping.\")\n",
        "                with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} --- ITEM_SCHEMA_FAILURE (Item {item_idx+1}) ---\\n\")\n",
        "                    f.write(f\"Original Label: '{label_str_original}', Processed Label for Validation: '{label_to_validate}'\\n\")\n",
        "                    f.write(f\"Item Content: {json.dumps(item, indent=2)}\\n\")\n",
        "                    f.write(f\"Validation Checks: has_keys={has_all_keys}, label_valid_fmt={label_is_valid_format}, desc_valid={desc_is_valid}, sfs_list_valid={sfs_list_is_valid}\\n\")\n",
        "        return valid_motifs_from_json\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        print(f\"    [WARN] Motif JSON parsing or core structure issue for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} --- JSON_PARSE_VALUE_ERROR ---\\n\")\n",
        "            prompt_parts = prompt_sent_to_llm.split('Set of comments to analyze:')\n",
        "            user_content_for_log = prompt_parts[1][:500] if len(prompt_parts) > 1 else (prompt_sent_to_llm[:500] if prompt_sent_to_llm else \"PROMPT_EMPTY\")\n",
        "            f.write(f\"PROMPT USER CONTENT (approx first 500 chars):\\n{user_content_for_log}...\\n\")\n",
        "            f.write(f\"RAW LLM RESPONSE (Error: {type(e).__name__} - {str(e)}):\\n{llm_raw_response_text}\\n\")\n",
        "            f.write(f\"EXTRACTED JSON CANDIDATE (Error: {type(e).__name__}):\\n{json_str_candidate}\\n\")\n",
        "        return []\n",
        "\n",
        "def get_motifs_for_qid_batched(\n",
        "    list_of_individual_response_texts: List[str],\n",
        "    responses_per_batch: int,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str\n",
        "    ) -> List[Dict]:\n",
        "    all_raw_motifs_from_chunks = []\n",
        "    batched_text_chunks_for_llm = []\n",
        "    for i in range(0, len(list_of_individual_response_texts), responses_per_batch):\n",
        "        batch_responses = list_of_individual_response_texts[i:i + responses_per_batch]\n",
        "        chunk_text_for_llm = preprocess_corpus_for_motif_extraction(\"\\n\\n<RSP_SEP>\\n\\n\".join(batch_responses)) # from Cell 2\n",
        "        batched_text_chunks_for_llm.append(chunk_text_for_llm)\n",
        "    print(f\"  QID {qid_for_log}: Processing {len(list_of_individual_response_texts)} responses in {len(batched_text_chunks_for_llm)} preprocessed chunks.\")\n",
        "    for chunk_idx, text_chunk_to_analyze_processed in enumerate(batched_text_chunks_for_llm):\n",
        "        print(f\"    Analyzing chunk {chunk_idx + 1}/{len(batched_text_chunks_for_llm)} for QID {qid_for_log} (len: {len(text_chunk_to_analyze_processed)} chars)...\")\n",
        "        if len(text_chunk_to_analyze_processed.strip()) < 50: print(f\"      Chunk {chunk_idx+1} too short, skipping.\"); continue\n",
        "\n",
        "        # create_enhanced_motif_prompt is from Cell 3\n",
        "        prompt_for_llm = create_enhanced_motif_prompt(text_chunk_to_analyze_processed)\n",
        "\n",
        "        motifs_from_this_chunk = []\n",
        "        for attempt in range(LLM_RETRY_ATTEMPTS): # from config\n",
        "            # call_local_llm_for_raw_response is from Cell 3\n",
        "            raw_llm_response = call_local_llm_for_raw_response(\n",
        "                prompt_for_llm, hf_pipeline_instance, hf_tokenizer_instance, qid_for_log, chunk_idx + 1\n",
        "            )\n",
        "            if not raw_llm_response:\n",
        "                print(f\"      LLM call attempt {attempt + 1} for chunk {chunk_idx+1} returned empty. Retrying...\");\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1); continue\n",
        "\n",
        "            # parse_and_validate_llm_json_response is defined in this cell\n",
        "            parsed_motifs_from_this_attempt = parse_and_validate_llm_json_response(\n",
        "                raw_llm_response, qid_for_log, chunk_idx+1, prompt_for_llm\n",
        "            )\n",
        "            if parsed_motifs_from_this_attempt:\n",
        "                motifs_from_this_chunk = parsed_motifs_from_this_attempt; break\n",
        "            else:\n",
        "                print(f\"      Motif parsing/validation attempt {attempt + 1} yielded no motifs for chunk {chunk_idx+1}. Retrying...\");\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "\n",
        "        if motifs_from_this_chunk:\n",
        "            print(f\"      Extracted {len(motifs_from_this_chunk)} motifs from chunk {chunk_idx+1} (QID {qid_for_log}).\")\n",
        "            all_raw_motifs_from_chunks.extend(motifs_from_this_chunk)\n",
        "        else:\n",
        "            print(f\"      No valid motifs from chunk {chunk_idx+1} (QID {qid_for_log}) after {LLM_RETRY_ATTEMPTS} attempts.\")\n",
        "    return all_raw_motifs_from_chunks\n",
        "\n",
        "def consolidate_raw_motifs(list_of_all_raw_motifs: List[Dict]) -> List[Dict]:\n",
        "    # (Same as last version)\n",
        "    if not list_of_all_raw_motifs: return []\n",
        "    consolidated_motifs_map = {}\n",
        "    for motif_obj in list_of_all_raw_motifs:\n",
        "        label = motif_obj.get(\"label\",\"\").strip(); description = motif_obj.get(\"description\",\"\").strip()\n",
        "        surface_forms = motif_obj.get(\"surface_forms\", [])\n",
        "        if not (label and isinstance(surface_forms, list)): continue\n",
        "        current_sfs_set = set(sf.lower().strip() for sf in surface_forms if isinstance(sf, str) and sf.strip())\n",
        "        if label not in consolidated_motifs_map:\n",
        "            consolidated_motifs_map[label] = {\"label\": label, \"description\": description, \"surface_forms\": sorted(list(current_sfs_set))}\n",
        "        else:\n",
        "            existing_sfs_set = set(consolidated_motifs_map[label].get(\"surface_forms\", []))\n",
        "            consolidated_motifs_map[label][\"surface_forms\"] = sorted(list(existing_sfs_set.union(current_sfs_set)))\n",
        "    return list(consolidated_motifs_map.values())\n",
        "\n",
        "def filter_surface_forms_by_global_frequency(\n",
        "    consolidated_motifs_list: List[Dict],\n",
        "    full_qid_corpus_text: str,\n",
        "    min_global_freq: int = MIN_SF_FREQUENCY_IN_FULL_CORPUS,\n",
        "    max_sf_len_tokens: int = MAX_SF_TOKEN_LENGTH_FOR_FINAL_MOTIF # Uses constant from Cell 1\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"Filters SFs by global frequency AND token length.\"\"\"\n",
        "    if not consolidated_motifs_list: return []\n",
        "    final_globally_filtered_motifs = []\n",
        "    for motif_obj in consolidated_motifs_list:\n",
        "        globally_frequent_and_short_sfs = []\n",
        "        original_sfs_for_this_motif = motif_obj.get(\"surface_forms\", [])\n",
        "        for sf_str_lower in original_sfs_for_this_motif:\n",
        "            count = count_sf_occurrences(full_qid_corpus_text, sf_str_lower) # from Cell 2\n",
        "            # tokenize_phrase from Cell 2\n",
        "            if count >= min_global_freq and len(tokenize_phrase(sf_str_lower)) <= max_sf_len_tokens:\n",
        "                globally_frequent_and_short_sfs.append(sf_str_lower)\n",
        "        if globally_frequent_and_short_sfs:\n",
        "            filtered_motif_entry = motif_obj.copy()\n",
        "            filtered_motif_entry[\"surface_forms\"] = sorted(list(set(globally_frequent_and_short_sfs)))\n",
        "            final_globally_filtered_motifs.append(filtered_motif_entry)\n",
        "    return final_globally_filtered_motifs\n",
        "\n",
        "print(\"Cell 4: Motif Processing and Validation Utilities loaded (with NameError fix in parse_and_validate).\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QETkPit3FtOb"
      },
      "id": "QETkPit3FtOb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5: MDL Calculations\n",
        "\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# Assume constants from Cell 1 are in global scope\n",
        "# Assume tokenize_phrase from Cell 2 is in global scope\n",
        "try:\n",
        "    MATRIX_SIZE_GLOBAL; MAX_TEXT_FOR_BDM_HASH; MOTIF_SYMBOLIC_LABEL_COST # Check a few\n",
        "    MOTIF_DESCRIPTION_TEXT_BASE_COST; MOTIF_DESCRIPTION_TOKEN_COST\n",
        "    MOTIF_SURFACE_FORMS_LIST_BASE_COST; MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH\n",
        "    BDM_SEGMENT_LENGTH # From last config update\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 5): Key config constants not found from Cell 1. Using fallbacks or expecting errors.\")\n",
        "    MATRIX_SIZE_GLOBAL = (8, 8); MAX_TEXT_FOR_BDM_HASH = 2000; BDM_SEGMENT_LENGTH = 2000\n",
        "    MOTIF_SYMBOLIC_LABEL_COST = 0.5; MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "    MOTIF_DESCRIPTION_TOKEN_COST = 0.1; MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.0\n",
        "    MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.0\n",
        "\n",
        "\n",
        "def initialize_bdm_instance():\n",
        "    \"\"\"Initializes and returns a BDM instance.\"\"\"\n",
        "    print(\"Initializing BDM instance...\")\n",
        "    try:\n",
        "        bdm_instance = BDM(ndim=2)\n",
        "        print(\"BDM instance initialized successfully (ndim=2, default CTM-based).\")\n",
        "        return bdm_instance\n",
        "    except Exception as e_bdm_init:\n",
        "        print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\")\n",
        "        if \"CTM data files\" in str(e_bdm_init).lower() or \"dataset\" in str(e_bdm_init).lower():\n",
        "            print(\"  BDM Error Hint: This might be related to missing/corrupted CTM data files for PyBDM.\")\n",
        "            print(\"  Ensure PyBDM is installed correctly and can access/download its data.\")\n",
        "            print(\"  You might need to run once: from pybdm import get_ctm_dataset; get_ctm_dataset()\")\n",
        "        return None\n",
        "\n",
        "def text_to_binary_matrix(text_input_segment: str, size: tuple = MATRIX_SIZE_GLOBAL) -> np.ndarray:\n",
        "    \"\"\"Converts a text segment to a binary matrix using its SHA256 hash.\"\"\"\n",
        "    if not isinstance(text_input_segment, str) or not text_input_segment.strip():\n",
        "        return np.zeros(size, dtype=int)\n",
        "    hash_obj = hashlib.sha256(text_input_segment.encode('utf-8', 'ignore'))\n",
        "    hash_digest = hash_obj.hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string_from_hash = bin(int(hash_digest, 16))[2:].zfill(256)\n",
        "    binary_string_for_matrix = binary_string_from_hash[:required_bits] if required_bits <= 256 else binary_string_from_hash.ljust(required_bits, '0')\n",
        "    bits_for_matrix = [int(b) for b in binary_string_for_matrix]\n",
        "    return np.array(bits_for_matrix).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text_segment(\n",
        "    text_segment: str,\n",
        "    bdm_instance: BDM,\n",
        "    matrix_s: tuple = MATRIX_SIZE_GLOBAL\n",
        "    ) -> float:\n",
        "    \"\"\"Computes BDM for a given text segment.\"\"\"\n",
        "    if not isinstance(text_segment, str) or not text_segment.strip() :\n",
        "        return 0.0\n",
        "    binary_matrix = text_to_binary_matrix(text_segment, size=matrix_s) # text_segment is passed directly\n",
        "    try:\n",
        "        bdm_value = bdm_instance.bdm(binary_matrix)\n",
        "        return bdm_value\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM segment calculation for text (segment len {len(text_segment)}): {e_bdm}\")\n",
        "        return -1.0\n",
        "\n",
        "def calculate_full_corpus_bdm_segmented(\n",
        "    corpus_str: str,\n",
        "    bdm_instance: BDM,\n",
        "    segment_len: int = BDM_SEGMENT_LENGTH,\n",
        "    matrix_s: tuple = MATRIX_SIZE_GLOBAL\n",
        "    ) -> float:\n",
        "    \"\"\"Calculates total BDM for a corpus by summing BDM of its segments.\"\"\"\n",
        "    if not isinstance(corpus_str, str) or not corpus_str.strip():\n",
        "        return 0.0\n",
        "    total_bdm = 0.0\n",
        "    num_segments_processed = 0\n",
        "    for i in range(0, len(corpus_str), segment_len):\n",
        "        segment = corpus_str[i:i+segment_len]\n",
        "        if segment.strip():\n",
        "            bdm_val_segment = compute_bdm_for_text_segment(segment, bdm_instance, matrix_s)\n",
        "            if bdm_val_segment < 0:\n",
        "                print(f\"    ERROR (calculate_full_corpus_bdm_segmented): BDM error in segment {num_segments_processed + 1}. Aborting QID.\")\n",
        "                return -1.0\n",
        "            total_bdm += bdm_val_segment\n",
        "            num_segments_processed += 1\n",
        "    if num_segments_processed == 0 and len(corpus_str) > 0 :\n",
        "        return 0.0\n",
        "    return total_bdm\n",
        "\n",
        "def calculate_L_H_token_based_structured(structured_motifs_list: List[Dict]) -> float:\n",
        "    # (This function is the corrected one from the previous UnboundLocalError fix for 'current_motif_lh')\n",
        "    if not structured_motifs_list: return 0.0\n",
        "    total_lh_cost = 0.0\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        current_motif_lh = 0.0\n",
        "        label_str = motif_obj.get('label', \"\")\n",
        "        if isinstance(label_str, str) and label_str.strip(): current_motif_lh += MOTIF_SYMBOLIC_LABEL_COST\n",
        "        description_str = motif_obj.get('description', \"\")\n",
        "        if isinstance(description_str, str) and description_str.strip():\n",
        "            current_motif_lh += MOTIF_DESCRIPTION_TEXT_BASE_COST\n",
        "            current_motif_lh += len(tokenize_phrase(description_str)) * MOTIF_DESCRIPTION_TOKEN_COST\n",
        "        surface_forms_list = motif_obj.get('surface_forms', [])\n",
        "        if isinstance(surface_forms_list, list) and surface_forms_list:\n",
        "            valid_sfs_for_lh = [sf for sf in surface_forms_list if isinstance(sf, str) and sf.strip()]\n",
        "            if valid_sfs_for_lh:\n",
        "                current_motif_lh += MOTIF_SURFACE_FORMS_LIST_BASE_COST\n",
        "                for sf_str in valid_sfs_for_lh:\n",
        "                    current_motif_lh += len(tokenize_phrase(sf_str)) * MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH\n",
        "        total_lh_cost += current_motif_lh\n",
        "    return total_lh_cost\n",
        "\n",
        "# VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\n",
        "# CORRECTED VERSION OF THIS FUNCTION\n",
        "# VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\n",
        "def llm_compress_text_structured(text_to_compress: str, structured_motifs_list: List[Dict]) -> str:\n",
        "    \"\"\"Compresses text by replacing occurrences of motif surface forms with their symbolic labels.\"\"\"\n",
        "    if not isinstance(text_to_compress, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Initialize compressed_text immediately after confirming text_to_compress is a string\n",
        "    # and before checking if structured_motifs_list is empty.\n",
        "    compressed_text = text_to_compress.lower()\n",
        "\n",
        "    if not structured_motifs_list:\n",
        "        return compressed_text # Return the lowercased original if no motifs\n",
        "\n",
        "    # Now proceed with replacements\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict):\n",
        "            continue\n",
        "\n",
        "        label = motif_obj.get('label', None)\n",
        "        surface_forms = motif_obj.get('surface_forms', [])\n",
        "\n",
        "        if not (isinstance(label, str) and label.strip()) or \\\n",
        "           not (isinstance(surface_forms, list) and surface_forms):\n",
        "            continue\n",
        "\n",
        "        placeholder = label\n",
        "\n",
        "        sorted_sfs_for_this_motif = sorted(\n",
        "            [sf for sf in surface_forms if isinstance(sf, str) and sf.strip()],\n",
        "            key=len,\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        for sf_str_lower in sorted_sfs_for_this_motif: # Assumes sf_str is already lowercased\n",
        "            try:\n",
        "                # compressed_text is used on the right here, and assigned to itself\n",
        "                compressed_text = re.sub(r'\\b' + re.escape(sf_str_lower) + r'\\b', placeholder, compressed_text)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for SF '{sf_str_lower}' of motif '{label}': {re_e}. Skipping.\")\n",
        "                continue\n",
        "    return compressed_text\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "# END OF CORRECTION\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "def compute_mdl_cost_for_text_block(\n",
        "    full_qid_corpus_str: str,\n",
        "    final_motifs_to_evaluate: List[Dict],\n",
        "    bdm_instance: BDM,\n",
        "    matrix_s: tuple = MATRIX_SIZE_GLOBAL,\n",
        "    segment_len_for_bdm: int = BDM_SEGMENT_LENGTH # Use from config\n",
        "    ) -> tuple[float, float, float]:\n",
        "    \"\"\"Computes L(H), L(D|H) (using segmented BDM), and Total MDL.\"\"\"\n",
        "    if not isinstance(full_qid_corpus_str, str): full_qid_corpus_str = \"\"\n",
        "    l_h = calculate_L_H_token_based_structured(final_motifs_to_evaluate)\n",
        "    compressed_text_block = llm_compress_text_structured(full_qid_corpus_str, final_motifs_to_evaluate)\n",
        "\n",
        "    l_d_h = calculate_full_corpus_bdm_segmented( # Using segmented BDM\n",
        "        compressed_text_block,\n",
        "        bdm_instance,\n",
        "        segment_len=segment_len_for_bdm,\n",
        "        matrix_s=matrix_s\n",
        "    )\n",
        "\n",
        "    if l_d_h < 0: return l_h, -1.0, -1.0\n",
        "    total_mdl_cost = l_h + l_d_h\n",
        "    return l_h, l_d_h, total_mdl_cost\n",
        "\n",
        "print(\"Cell 5: MDL Calculation Utilities loaded (with UnboundLocalError fix in llm_compress_text_structured and segmented BDM).\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qeO3auNWRIGJ"
      },
      "id": "qeO3auNWRIGJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6: Main Pipeline Orchestration\n",
        "\n",
        "# Assume functions from previous cells are defined\n",
        "script_version_name_for_run_message = \"Refactored MWP v10 (Segmented BDM)\"\n",
        "\n",
        "def main():\n",
        "    script_version_name = \"Refactored MWP v10 (Segmented BDM, Simpler Prompt, Label Fix, L(H) SF Cost Zero, SF Len Filter)\"\n",
        "    print(f\"--- {script_version_name} ---\")\n",
        "    # ... (Print config params including BDM_SEGMENT_LENGTH - same as before) ...\n",
        "    print(f\"Timestamp: {time.asctime()}\")\n",
        "    print(\"\\n--- Configuration Summary ---\"); print(f\"LLM Model: {LOCAL_LLM_MODEL_ID}, Quantization: {USE_QUANTIZATION_FOR_LOCAL_LLM}\")\n",
        "    print(f\"LLM Batch Size: {LLM_BATCH_SIZE_RESPONSES}, Retries: {LLM_RETRY_ATTEMPTS}, Max Chars/Chunk: {MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK}, Max New Tokens: {LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION}, Max Motifs/Chunk: {MAX_MOTIFS_PER_CHUNK}\")\n",
        "    print(f\"L(H) Costs: Label={MOTIF_SYMBOLIC_LABEL_COST}, DescBase={MOTIF_DESCRIPTION_TEXT_BASE_COST}, DescToken={MOTIF_DESCRIPTION_TOKEN_COST}, SFListBase={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, SFTokenInLH={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "    print(f\"Global SF Filtering: MinFreq={MIN_SF_FREQUENCY_IN_FULL_CORPUS}, MaxSFLen={MAX_SF_TOKEN_LENGTH_FOR_FINAL_MOTIF}\")\n",
        "    print(f\"BDM Segment Length: {BDM_SEGMENT_LENGTH}, BDM Matrix: {MATRIX_SIZE_GLOBAL}\"); print(f\"Debug Log File: {LLM_DEBUG_LOG_FILE}\"); print(\"--- End Config ---\\n\")\n",
        "\n",
        "    # ... (Initialize debug log file - same as before) ...\n",
        "    # ... (Initialize LLM Pipeline - same as before) ...\n",
        "    # ... (Initialize BDM - same as before) ...\n",
        "    # ... (Load Phase 2 Data - same as before) ...\n",
        "    # ... (QID selection logic - same as before) ...\n",
        "    try:\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"w\", encoding=\"utf-8\") as f: f.write(f\"LLM Log - {time.asctime()}\\nVersion: {script_version_name}\\nModel: {LOCAL_LLM_MODEL_ID}\\n---\\n\")\n",
        "    except Exception as e_log: print(f\"WARN: Log init failed: {e_log}\")\n",
        "    hf_pipeline_instance, hf_tokenizer_instance = initialize_llm_pipeline()\n",
        "    if not hf_pipeline_instance: print(\"CRITICAL: LLM init failed.\"); return\n",
        "    bdm_instance_main = initialize_bdm_instance()\n",
        "    if not bdm_instance_main: print(\"CRITICAL: BDM init failed.\"); return\n",
        "    if not os.path.exists(P2_COLLATED_FILE): print(f\"ERROR: File {P2_COLLATED_FILE} not found.\"); return\n",
        "    print(f\"Loading data from: {P2_COLLATED_FILE}...\"); phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f: phase2_data_content = json.load(f)\n",
        "    except Exception as e_load: print(f\"Error loading {P2_COLLATED_FILE}: {e_load}\"); return\n",
        "    all_qid_mdl_results_list = []\n",
        "    aggregated_content_by_qid_from_file = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "    if not aggregated_content_by_qid_from_file: print(f\"No 'aggregated_pdf_content_by_qid' in {P2_COLLATED_FILE}.\"); return\n",
        "    qids_to_process_this_run = [qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY if qid in aggregated_content_by_qid_from_file] if (P3_QIDS_TO_PROCESS_THEMATICALLY and isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and P3_QIDS_TO_PROCESS_THEMATICALLY) else list(aggregated_content_by_qid_from_file.keys())[:1]\n",
        "    if not qids_to_process_this_run: print(f\"No QIDs to process. Exiting.\"); return\n",
        "    print(f\"\\nMDL analysis for QIDs: {qids_to_process_this_run}\\n\")\n",
        "\n",
        "\n",
        "    for qid_identifier_str in qids_to_process_this_run:\n",
        "        print(f\"--- Analyzing Data for QID: {qid_identifier_str} ---\")\n",
        "        # ... (Get actual_response_texts_for_qid and full_corpus_text_for_qid) ...\n",
        "        list_of_individual_response_structs = aggregated_content_by_qid_from_file.get(qid_identifier_str, [])\n",
        "        actual_response_texts_for_qid = [item.get(\"text\", \"\") for item in list_of_individual_response_structs if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()]\n",
        "        if not actual_response_texts_for_qid: print(f\"  No valid text for QID {qid_identifier_str}. Skipping.\"); print(\"-\" * 50); continue\n",
        "        full_corpus_text_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(actual_response_texts_for_qid)\n",
        "        if len(full_corpus_text_for_qid.strip()) < 100: print(f\"  Skipping QID {qid_identifier_str}: text too short.\"); print(\"-\" * 50); continue\n",
        "        num_total_responses_for_qid = len(actual_response_texts_for_qid)\n",
        "        print(f\"  Corpus for QID {qid_identifier_str}: {len(full_corpus_text_for_qid)} chars, {num_total_responses_for_qid} responses.\")\n",
        "\n",
        "        # MODIFIED: Calculate baseline L(D) using segmented BDM\n",
        "        baseline_bdm_original_corpus = calculate_full_corpus_bdm_segmented(\n",
        "            full_corpus_text_for_qid,\n",
        "            bdm_instance_main,\n",
        "            segment_len=BDM_SEGMENT_LENGTH, # Use constant from Cell 1\n",
        "            matrix_s=MATRIX_SIZE_GLOBAL\n",
        "        )\n",
        "        if baseline_bdm_original_corpus < 0:\n",
        "            print(f\"  Error computing segmented baseline BDM for QID {qid_identifier_str}. Skipping this QID.\")\n",
        "            # Log error if needed\n",
        "            continue\n",
        "        current_qid_baseline_mdl_cost = baseline_bdm_original_corpus\n",
        "        print(f\"  Baseline MDL for QID {qid_identifier_str} (L(D_orig) - Segmented BDM): {current_qid_baseline_mdl_cost:.4f}\")\n",
        "\n",
        "        # ... (The rest of the loop: get_motifs_for_qid_batched, consolidation,\n",
        "        #      global SF filtering with length, final MDL calc, logging results -\n",
        "        #      this part remains the same as your last working main() function,\n",
        "        #      but compute_mdl_cost_for_text_block will now internally use segmented BDM\n",
        "        #      for L(D|H) because it calls calculate_full_corpus_bdm_segmented)\n",
        "        raw_motifs_from_chunks = get_motifs_for_qid_batched(actual_response_texts_for_qid, LLM_BATCH_SIZE_RESPONSES, hf_pipeline_instance, hf_tokenizer_instance, qid_identifier_str)\n",
        "        current_qid_result_entry = {\n",
        "            \"qid\": qid_identifier_str, \"corpus_len_chars\": len(full_corpus_text_for_qid), \"num_responses\": num_total_responses_for_qid,\n",
        "            \"baseline_mdl\": current_qid_baseline_mdl_cost, \"final_refined_motifs\": [], \"l_h_final_motifs\": 0.0,\n",
        "            \"l_d_h_final_motifs\": current_qid_baseline_mdl_cost, \"total_mdl_with_final_motifs\": current_qid_baseline_mdl_cost,\n",
        "            \"compression_achieved\": 0.0, \"num_raw_motifs_extracted\": len(raw_motifs_from_chunks),\n",
        "            \"num_consolidated_motifs\": 0, \"num_globally_refined_motifs\": 0\n",
        "        }\n",
        "        if not raw_motifs_from_chunks: print(f\"  No raw motifs by LLM for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "        print(f\"  Extracted {len(raw_motifs_from_chunks)} raw motif objects for QID {qid_identifier_str}.\")\n",
        "        consolidated_motifs_list = consolidate_raw_motifs(raw_motifs_from_chunks)\n",
        "        current_qid_result_entry[\"num_consolidated_motifs\"] = len(consolidated_motifs_list)\n",
        "        print(f\"  Consolidated into {len(consolidated_motifs_list)} unique motifs for QID {qid_identifier_str}.\")\n",
        "        if not consolidated_motifs_list: print(f\"  No unique motifs after consolidation for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        globally_refined_motifs = filter_surface_forms_by_global_frequency(\n",
        "            consolidated_motifs_list, full_corpus_text_for_qid,\n",
        "            min_global_freq=MIN_SF_FREQUENCY_IN_FULL_CORPUS,\n",
        "            max_sf_len_tokens=MAX_SF_TOKEN_LENGTH_FOR_FINAL_MOTIF # Pass this new arg\n",
        "        )\n",
        "        current_qid_result_entry[\"num_globally_refined_motifs\"] = len(globally_refined_motifs)\n",
        "        print(f\"  Globally refined (freq & len) into {len(globally_refined_motifs)} motifs for QID {qid_identifier_str}.\")\n",
        "        if not globally_refined_motifs: print(f\"  No motifs left after GLOBAL SF refinement for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  Final Globally Refined Motifs for QID {qid_identifier_str}:\")\n",
        "        for idx, mo_final in enumerate(globally_refined_motifs): print(f\"    Refined Motif {idx+1}: L='{mo_final.get('label')}', D='{mo_final.get('description','N/A')[:60]}...', SFs({len(mo_final.get('surface_forms',[]))})='{mo_final.get('surface_forms',[])}'\")\n",
        "\n",
        "        l_h_final, l_d_h_final, total_mdl_final = compute_mdl_cost_for_text_block( # This now uses segmented BDM for L(D|H)\n",
        "            full_corpus_text_for_qid, globally_refined_motifs, bdm_instance_main\n",
        "        )\n",
        "        current_qid_result_entry.update({\n",
        "            \"final_refined_motifs\": globally_refined_motifs, \"l_h_final_motifs\": l_h_final,\n",
        "            \"l_d_h_final_motifs\": l_d_h_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"total_mdl_with_final_motifs\": total_mdl_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"compression_achieved\": \"BDM_ERROR\" if l_d_h_final < 0 else (current_qid_baseline_mdl_cost - total_mdl_final)\n",
        "        })\n",
        "        if l_d_h_final < 0: print(f\"  Error computing MDL cost (BDM error) for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "        print(f\"  L(H) final motifs: {l_h_final:.4f} (SFs definition cost in L(H) is ZEROED)\")\n",
        "        print(f\"  L(D|H) (Segmented BDM) compressed full corpus: {l_d_h_final:.4f}\") # Clarify it's segmented\n",
        "        print(f\"  Total MDL cost with final motifs: {total_mdl_final:.4f}\")\n",
        "        compression_val = current_qid_result_entry[\"compression_achieved\"]\n",
        "        result_status_str = f\"SUCCESS: Comp: {compression_val:.4f}\" if isinstance(compression_val, float) and compression_val > 0.0001 else f\"NOTE: No sig. comp. Diff: {compression_val if isinstance(compression_val, str) else compression_val:.4f}\"\n",
        "        print(f\"  {result_status_str}\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50)\n",
        "\n",
        "    # --- Summary Printing and Saving Results ---\n",
        "    # ... (Same summary logic as before, just ensure filename is updated) ...\n",
        "    print(f\"\\n--- Overall QID-based MDL Analysis Summary ({script_version_name}) ---\")\n",
        "    if not all_qid_mdl_results_list: print(\"No QIDs processed.\")\n",
        "    else:\n",
        "        valid_results = [r for r in all_qid_mdl_results_list if isinstance(r.get('compression_achieved'), float) and r.get('l_h_final_motifs', -1.0) >= 0]\n",
        "        num_qids_ok = len(valid_results); num_comp = sum(1 for r in valid_results if r['compression_achieved'] > 0.0001)\n",
        "        print(f\"Targeted QIDs: {len(qids_to_process_this_run)}, Results logged: {len(all_qid_mdl_results_list)}, Valid MDL: {num_qids_ok}, QIDs compressed: {num_comp}\")\n",
        "        if num_comp > 0:\n",
        "            comp_vals = [r['compression_achieved'] for r in valid_results if r['compression_achieved'] > 0.0001]\n",
        "            print(f\"  Avg compression: {np.mean(comp_vals):.4f}, Max compression: {np.max(comp_vals):.4f}\")\n",
        "        else: print(\"  No compression achieved.\")\n",
        "        output_filename = os.path.join(BASE_PROJECT_DIR, \"mdl_analysis_refactored_v10_segmentedBDM.json\")\n",
        "        try:\n",
        "            with open(output_filename, \"w\", encoding=\"utf-8\") as f_out: json.dump(all_qid_mdl_results_list, f_out, indent=2, ensure_ascii=False)\n",
        "            print(f\"Detailed results saved to {output_filename}\")\n",
        "        except Exception as e_s: print(f\"Error saving results: {e_s}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Executing main MDL pipeline ({script_version_name_for_run_message}) at {time.asctime()}...\") # Define script_version_name_for_run_message or use fixed string\n",
        "    main()\n",
        "    print(f\"Main MDL pipeline execution finished at {time.asctime()}.\")\n",
        "\n",
        "# Define this at the top of Cell 6 or globally if Cell 6 is the only one with if __name__ == \"__main__\":\n",
        "script_version_name_for_run_message = \"Refactored MWP v10 (Segmented BDM)\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "6-NvLs3s5fGn"
      },
      "id": "6-NvLs3s5fGn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3rd June\n",
        "Thematic phrase extractor completed !"
      ],
      "metadata": {
        "id": "cQmXAzm0c065"
      },
      "id": "cQmXAzm0c065"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1: Configuration\n",
        "\n",
        "import os\n",
        "from typing import List, Dict, Set\n",
        "from collections import Counter\n",
        "\n",
        "# --- Project Paths ---\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"]\n",
        "\n",
        "# --- BDM Configuration ---\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "MAX_TEXT_FOR_BDM_HASH = 2000\n",
        "\n",
        "# --- LLM Configuration ---\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "LLM_BATCH_SIZE_RESPONSES = 5\n",
        "LLM_RETRY_ATTEMPTS = 2\n",
        "MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 1000 # Kept increased for JSON completeness\n",
        "MAX_MOTIFS_PER_CHUNK = 3 # REDUCED: Ask LLM for fewer motifs per chunk\n",
        "\n",
        "# --- Token-Based L(H) Configuration (SFs cost zeroed for current experiment) ---\n",
        "MOTIF_SYMBOLIC_LABEL_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TOKEN_COST = 0.1\n",
        "MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.0\n",
        "MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.0\n",
        "\n",
        "# --- Surface Form Filtering Configuration ---\n",
        "MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "\n",
        "# --- Logging File ---\n",
        "LLM_DEBUG_LOG_FILE = os.path.join(BASE_PROJECT_DIR, \"llm_motif_debug_log_refactored_v8_strict_prompt.txt\") # New log filename\n",
        "\n",
        "print(f\"Cell 1: Configuration loaded. LOCAL_LLM_MODEL_ID set to '{LOCAL_LLM_MODEL_ID}'.\")\n",
        "print(f\"LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION: {LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION}, MAX_MOTIFS_PER_CHUNK: {MAX_MOTIFS_PER_CHUNK}\")\n",
        "print(f\"L(H) SF Costs: Base={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, Token={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "print(f\"Debug log will be: {LLM_DEBUG_LOG_FILE}\")\n",
        "# ... (Path existence checks - same as before)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fIi6v-z_c2-t"
      },
      "id": "fIi6v-z_c2-t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2: Text Utilities\n",
        "\n",
        "import re\n",
        "from typing import List, Dict\n",
        "from collections import Counter\n",
        "\n",
        "try:\n",
        "    MIN_SF_FREQ_IN_CHUNK_VALIDATION\n",
        "except NameError:\n",
        "    MIN_SF_FREQ_IN_CHUNK_VALIDATION = 2\n",
        "\n",
        "\n",
        "def tokenize_phrase(phrase_text: str) -> List[str]:\n",
        "    if not isinstance(phrase_text, str) or not phrase_text.strip():\n",
        "        return []\n",
        "    return phrase_text.lower().split()\n",
        "\n",
        "def preprocess_corpus_for_motif_extraction(text_corpus: str) -> str:\n",
        "    if not isinstance(text_corpus, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text_corpus)\n",
        "    text = re.sub(r' {2,}', ' ', text)\n",
        "    lines = text.split('\\n')\n",
        "    filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10 or not line.strip()]\n",
        "    return '\\n'.join(filtered_lines)\n",
        "\n",
        "def count_sf_occurrences(corpus_text: str, surface_form: str) -> int:\n",
        "    if not corpus_text or not surface_form or \\\n",
        "       not isinstance(corpus_text, str) or not isinstance(surface_form, str) or \\\n",
        "       not surface_form.strip():\n",
        "        return 0\n",
        "    try:\n",
        "        return len(re.findall(re.escape(surface_form.lower()), corpus_text.lower(), flags=re.IGNORECASE))\n",
        "    except re.error as e:\n",
        "        print(f\"    [WARN] Regex error in count_sf_occurrences for SF '{surface_form}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def extract_actual_phrases_from_text(\n",
        "    text: str,\n",
        "    min_phrase_len: int = 2,\n",
        "    max_phrase_len: int = 6,\n",
        "    min_freq: int = MIN_SF_FREQ_IN_CHUNK_VALIDATION\n",
        "    ) -> Dict[str, int]:\n",
        "    if not isinstance(text, str) or not text.strip(): return {}\n",
        "    text_cleaned = text.lower()\n",
        "    text_cleaned = re.sub(r'[^\\w\\s\\']', ' ', text_cleaned)\n",
        "    text_cleaned = re.sub(r'\\s+', ' ', text_cleaned).strip()\n",
        "    words = text_cleaned.split()\n",
        "    if not words or len(words) < min_phrase_len: return {}\n",
        "    phrase_counts = Counter()\n",
        "    for n in range(min_phrase_len, max_phrase_len + 1):\n",
        "        if n > len(words): continue\n",
        "        for i in range(len(words) - n + 1):\n",
        "            phrase = ' '.join(words[i:i+n])\n",
        "            if phrase: phrase_counts[phrase] += 1\n",
        "    return {phrase: count for phrase, count in phrase_counts.items() if count >= min_freq}\n",
        "\n",
        "print(\"Cell 2: Text Utilities loaded.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5P1jJi-Cc6VE"
      },
      "id": "5P1jJi-Cc6VE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3: LLM Interaction\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import time\n",
        "from typing import List, Dict\n",
        "import os\n",
        "import re\n",
        "\n",
        "try:\n",
        "    LOCAL_LLM_MODEL_ID; USE_QUANTIZATION_FOR_LOCAL_LLM; MAX_MOTIFS_PER_CHUNK\n",
        "    MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK; LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION; LLM_DEBUG_LOG_FILE\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 3): Key config constants not found from Cell 1. Using fallbacks.\")\n",
        "    LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'; USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "    MAX_MOTIFS_PER_CHUNK = 3; MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "    LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 1000; LLM_DEBUG_LOG_FILE = \"temp_llm_debug_log_cell3.txt\"\n",
        "\n",
        "\n",
        "def initialize_llm_pipeline( # Same as last version\n",
        "    model_id: str = LOCAL_LLM_MODEL_ID,\n",
        "    use_quantization: bool = USE_QUANTIZATION_FOR_LOCAL_LLM,\n",
        "    pipeline_return_full_text: bool = False\n",
        "    ):\n",
        "    print(f\"--- Initializing LLM Pipeline (model: {model_id}, quantization: {use_quantization}, return_full_text: {pipeline_return_full_text}) ---\")\n",
        "    hf_pipeline_instance = None; hf_tokenizer_instance = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {model_id}...\"); hf_tokenizer_instance = AutoTokenizer.from_pretrained(model_id)\n",
        "        if hf_tokenizer_instance.pad_token is None:\n",
        "            if hf_tokenizer_instance.eos_token is not None: print(\"Tokenizer setting pad_token = eos_token.\"); hf_tokenizer_instance.pad_token = hf_tokenizer_instance.eos_token\n",
        "            else: print(\"WARN (initialize_llm): Tokenizer has no pad_token and no eos_token.\")\n",
        "        bnb_config = None; quant_active = False\n",
        "        if use_quantization and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if (device.type == 'cuda' and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n",
        "                quant_active = True; print(f\"BNB config created, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb: print(f\"WARN: Failed BitsAndBytesConfig: {e_bnb}. Quantization may be disabled.\"); quant_active = False\n",
        "        print(f\"Loading model {model_id} (Quantization: {quant_active})...\"); model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "        if quant_active and bnb_config: model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        elif device.type == 'cuda': model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "        hf_model_instance = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "        if hf_tokenizer_instance.pad_token_id is not None:\n",
        "            if hf_model_instance.config.pad_token_id is None or hf_model_instance.config.pad_token_id != hf_tokenizer_instance.pad_token_id:\n",
        "                hf_model_instance.config.pad_token_id = hf_tokenizer_instance.pad_token_id\n",
        "        hf_pipeline_instance = pipeline(\"text-generation\", model=hf_model_instance, tokenizer=hf_tokenizer_instance, return_full_text=pipeline_return_full_text)\n",
        "        print(f\"LLM pipeline initialized for {model_id}.\"); return hf_pipeline_instance, hf_tokenizer_instance\n",
        "    except Exception as e: print(f\"CRITICAL: LLM pipeline init failed: {e}\"); import traceback; traceback.print_exc(); return None, None\n",
        "\n",
        "\n",
        "def create_enhanced_motif_prompt(text_corpus_chunk: str, max_motifs_to_extract: int = MAX_MOTIFS_PER_CHUNK) -> str:\n",
        "    \"\"\"Revised prompt with stronger emphasis on JSON syntax and label format.\"\"\"\n",
        "    if len(text_corpus_chunk) > MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK:\n",
        "        text_corpus_chunk = text_corpus_chunk[:MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK]\n",
        "\n",
        "    prompt = f\"\"\"You are a highly precise assistant for thematic analysis. Your task is to extract key recurring themes from the provided text.\n",
        "\n",
        "STRICT OUTPUT REQUIREMENTS:\n",
        "1.  Your entire response MUST be a single, valid JSON list.\n",
        "2.  Each element in the list MUST be a JSON object.\n",
        "3.  Each JSON object MUST contain exactly three keys: \"label\", \"description\", and \"surface_forms\".\n",
        "4.  The value for \"label\" MUST be a string, IN ALL_CAPITAL_SNAKE_CASE, AND enclosed in square brackets. Example of a correct label: \"[DATA_SECURITY_POLICY]\". Example of an incorrect label: \"Data Security Policy\".\n",
        "5.  The value for \"description\" MUST be a single, concise sentence (string).\n",
        "6.  The value for \"surface_forms\" MUST be a JSON list of 2 to 3 short (2-6 words) VERBATIM phrases extracted DIRECTLY from the 'Text to analyze'. These phrases must be strong examples of the theme. If no suitable verbatim phrases are found, provide an empty list `[]`. The \"surface_forms\" key MUST always be present.\n",
        "7.  Identify up to {max_motifs_to_extract} themes. If fewer are clear, provide fewer objects. Do NOT include empty JSON objects `{{}}` in the list.\n",
        "8.  Do NOT include any text, explanations, apologies, or markdown (like ```json) before or after the main JSON list.\n",
        "9.  CRITICAL JSON SYNTAX: Ensure all strings are double-quoted. All lists (like \"surface_forms\") MUST start with '[' and end with ']', with elements comma-separated. Do NOT use trailing commas before a closing ']' or '}}'. For example, `[\"item1\", \"item2\",]` is WRONG; it should be `[\"item1\", \"item2\"]`.\n",
        "\n",
        "INSTRUCTIONS FOR THEME IDENTIFICATION:\n",
        "- Focus on meaningful recurring concepts directly stated or strongly implied in the 'Text to analyze'.\n",
        "- For 'surface_forms', prioritize phrases that appear to be REPEATED or are highly characteristic of the theme within THIS text.\n",
        "- Avoid generic labels like [EXAMPLE_THEME] or [GENERAL_TOPIC]. Make labels specific.\n",
        "\n",
        "Text to analyze:\n",
        "\\\"\\\"\\\"\n",
        "{text_corpus_chunk}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Your valid JSON response (ONLY the JSON list):\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def call_local_llm_for_raw_response( # Using this name to match Cell 4 calls\n",
        "    prompt_content_for_user_turn: str,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str,\n",
        "    chunk_idx_for_log: int\n",
        "    ) -> str:\n",
        "    # (Same as your last working version - uses do_sample=False)\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance:\n",
        "        print(f\"    ERROR (call_local_llm): LLM pipeline/tokenizer not initialized for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return \"\"\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": prompt_content_for_user_turn}]\n",
        "    try:\n",
        "        prompt_formatted_for_llm = hf_tokenizer_instance.apply_chat_template(\n",
        "            messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception as e_template:\n",
        "        print(f\"    ERROR (call_local_llm): Applying chat template for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_template}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm) ---\\nERROR APPLYING CHAT TEMPLATE: {e_template}\\nPrompt content (first 300): {prompt_content_for_user_turn[:300]}...\\n\")\n",
        "        return \"\"\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION,\n",
        "        \"do_sample\": False,\n",
        "        \"pad_token_id\": hf_tokenizer_instance.pad_token_id\n",
        "    }\n",
        "    try:\n",
        "        outputs = hf_pipeline_instance(prompt_formatted_for_llm, **generation_args)\n",
        "        if outputs and isinstance(outputs, list) and len(outputs) > 0 and \\\n",
        "           outputs[0] and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
        "            return outputs[0]['generated_text'].strip()\n",
        "        else:\n",
        "            print(f\"    WARN (call_local_llm): LLM pipeline unexpected structure for QID {qid_for_log}, Chunk {chunk_idx_for_log}. Output: {outputs}\")\n",
        "            return \"\"\n",
        "    except Exception as e_pipeline:\n",
        "        print(f\"    ERROR (call_local_llm): Exception during hf_pipeline call for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_pipeline}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm) ---\\nERROR PIPELINE CALL: {e_pipeline}\\nFormatted prompt (first 300): {prompt_formatted_for_llm[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"Cell 3: LLM Interaction Utilities loaded (with stricter JSON prompt).\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eXlfkwcsc_Ik"
      },
      "id": "eXlfkwcsc_Ik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4: Motif Processing and Validation\n",
        "\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict\n",
        "# from collections import Counter # Defined in Cell 1, used by extract_actual_phrases (Cell 2)\n",
        "\n",
        "# Assume constants from Cell 1\n",
        "# Assume text_utils functions from Cell 2\n",
        "# Assume LLM interaction functions from Cell 3\n",
        "\n",
        "try:\n",
        "    LLM_DEBUG_LOG_FILE; LLM_RETRY_ATTEMPTS; MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "except NameError:\n",
        "    LLM_DEBUG_LOG_FILE = \"temp_llm_debug_log_cell4.txt\"; LLM_RETRY_ATTEMPTS = 2; MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "\n",
        "def parse_and_validate_llm_json_response(\n",
        "    llm_raw_response_text: str,\n",
        "    qid_for_log:str,\n",
        "    chunk_idx_for_log:int,\n",
        "    prompt_sent_to_llm:str\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"Parses LLM JSON, attempts label fixing, validates schema.\"\"\"\n",
        "    json_str_candidate = llm_raw_response_text.strip()\n",
        "    if json_str_candidate.startswith(\"```json\"): json_str_candidate = json_str_candidate[len(\"```json\"):].strip()\n",
        "    if json_str_candidate.startswith(\"```\"): json_str_candidate = json_str_candidate[len(\"```\"):].strip()\n",
        "    if json_str_candidate.endswith(\"```\"): json_str_candidate = json_str_candidate[:-len(\"```\")].strip()\n",
        "\n",
        "    if not json_str_candidate or json_str_candidate.lower() == \"[]\" or \\\n",
        "       \"no_themes_found\" in json_str_candidate.lower() or \\\n",
        "       \"no clear motifs\" in json_str_candidate.lower():\n",
        "        return []\n",
        "\n",
        "    # Attempt to fix common JSON error: trailing commas before closing brackets/braces\n",
        "    json_str_candidate_fixed = re.sub(r',\\s*([\\}\\]])', r'\\1', json_str_candidate)\n",
        "    if json_str_candidate_fixed != json_str_candidate:\n",
        "        # print(f\"    DEBUG (parse_validate): Applied trailing comma fix for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        json_str_candidate = json_str_candidate_fixed\n",
        "\n",
        "    try:\n",
        "        parsed_data = json.loads(json_str_candidate)\n",
        "        if isinstance(parsed_data, dict): parsed_data = [parsed_data]\n",
        "        if not isinstance(parsed_data, list):\n",
        "            raise ValueError(\"Parsed JSON from LLM is not a list or a single object.\")\n",
        "\n",
        "        valid_motifs_from_json = []\n",
        "        for item_idx, item in enumerate(parsed_data):\n",
        "            if not isinstance(item, dict) or not item: continue\n",
        "\n",
        "            label_str_original = item.get('label', \"\")\n",
        "            label_str_processed = \"\"\n",
        "            if isinstance(label_str_original, str) and label_str_original.strip():\n",
        "                temp_label_stripped = label_str_original.strip()\n",
        "                match_strict_bracketed = re.fullmatch(r\"\\[([A-Z0-9_]+)\\]\", temp_label_stripped)\n",
        "                if match_strict_bracketed:\n",
        "                    label_str_processed = temp_label_stripped\n",
        "                else:\n",
        "                    match_bracketed_part = re.search(r\"(\\[[A-Z0-9_]+(?:_[A-Z0-9_]+)*\\])\", temp_label_stripped) # Allow multiple words in snake case\n",
        "                    if match_bracketed_part:\n",
        "                        label_str_processed = match_bracketed_part.group(1)\n",
        "                    else:\n",
        "                        sanitized_content = re.sub(r'\\s+|-', '_', temp_label_stripped)\n",
        "                        sanitized_content = re.sub(r'[^a-zA-Z0-9_]', '', sanitized_content).upper()\n",
        "                        sanitized_content = \"_\".join(sanitized_content.split('_')[:4])\n",
        "                        if sanitized_content:\n",
        "                            label_str_processed = f\"[{sanitized_content}]\"\n",
        "\n",
        "            # Check if all essential keys are present before trying to access them\n",
        "            if not all(k in item for k in [\"label\", \"description\", \"surface_forms\"]):\n",
        "                print(f\"    [WARN] Item missing required keys for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}. Item: {str(item)[:200]}...\")\n",
        "                # ... (log to file) ...\n",
        "                continue # Skip this malformed item\n",
        "\n",
        "            # Validate using the processed label\n",
        "            label_to_validate = label_str_processed # Use the processed one\n",
        "            desc_str = item.get('description',\"\")\n",
        "            sf_list = item.get('surface_forms', [])\n",
        "\n",
        "            label_is_valid_format = isinstance(label_to_validate, str) and \\\n",
        "                                    bool(label_to_validate) and \\\n",
        "                                    label_to_validate.startswith('[') and \\\n",
        "                                    label_to_validate.endswith(']') and \\\n",
        "                                    re.fullmatch(r\"\\[[A-Z0-9_]+\\]\", label_to_validate)\n",
        "            desc_is_valid = isinstance(desc_str, str)\n",
        "            sfs_list_is_valid = isinstance(sf_list, list) and \\\n",
        "                                 all(isinstance(sf_item, str) for sf_item in sf_list)\n",
        "\n",
        "            if label_is_valid_format and desc_is_valid and sfs_list_is_valid:\n",
        "                valid_motifs_from_json.append({\n",
        "                    \"label\": label_to_validate,\n",
        "                    \"description\": desc_str.strip(),\n",
        "                    \"surface_forms\": [s.strip() for s in sf_list if isinstance(s, str) and s.strip()]\n",
        "                })\n",
        "            else:\n",
        "                print(f\"    [WARN] Invalid motif object schema after label processing for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}. Skipping.\")\n",
        "                # ... (log to file with details of which validation failed) ...\n",
        "        return valid_motifs_from_json\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        print(f\"    [WARN] Motif JSON parsing or core structure issue for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e}\")\n",
        "        # ... (log to file, include json_str_candidate) ...\n",
        "        return []\n",
        "\n",
        "def get_motifs_for_qid_batched(\n",
        "    list_of_individual_response_texts: List[str],\n",
        "    responses_per_batch: int,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str\n",
        "    ) -> List[Dict]:\n",
        "    # (This function remains structurally the same, calling the updated helpers)\n",
        "    all_raw_motifs_from_chunks = []\n",
        "    batched_text_chunks_for_llm = []\n",
        "    for i in range(0, len(list_of_individual_response_texts), responses_per_batch):\n",
        "        batch_responses = list_of_individual_response_texts[i:i + responses_per_batch]\n",
        "        chunk_text_for_llm = preprocess_corpus_for_motif_extraction(\"\\n\\n<RSP_SEP>\\n\\n\".join(batch_responses))\n",
        "        batched_text_chunks_for_llm.append(chunk_text_for_llm)\n",
        "    print(f\"  QID {qid_for_log}: Processing {len(list_of_individual_response_texts)} responses in {len(batched_text_chunks_for_llm)} preprocessed chunks.\")\n",
        "    for chunk_idx, text_chunk_to_analyze_processed in enumerate(batched_text_chunks_for_llm):\n",
        "        print(f\"    Analyzing chunk {chunk_idx + 1}/{len(batched_text_chunks_for_llm)} for QID {qid_for_log} (len: {len(text_chunk_to_analyze_processed)} chars)...\")\n",
        "        if len(text_chunk_to_analyze_processed.strip()) < 50: print(f\"      Chunk {chunk_idx+1} too short, skipping.\"); continue\n",
        "        prompt_for_llm = create_enhanced_motif_prompt(text_chunk_to_analyze_processed)\n",
        "        motifs_from_this_chunk = []\n",
        "        for attempt in range(LLM_RETRY_ATTEMPTS):\n",
        "            raw_llm_response = call_local_llm_for_raw_response(\n",
        "                prompt_for_llm, hf_pipeline_instance, hf_tokenizer_instance, qid_for_log, chunk_idx + 1\n",
        "            )\n",
        "            if not raw_llm_response:\n",
        "                print(f\"      LLM call attempt {attempt + 1} for chunk {chunk_idx+1} returned empty. Retrying...\");\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1); continue\n",
        "            parsed_motifs_from_this_attempt = parse_and_validate_llm_json_response(\n",
        "                raw_llm_response, qid_for_log, chunk_idx+1, prompt_for_llm\n",
        "            )\n",
        "            if parsed_motifs_from_this_attempt: motifs_from_this_chunk = parsed_motifs_from_this_attempt; break\n",
        "            else:\n",
        "                print(f\"      Motif parsing/validation attempt {attempt + 1} yielded no motifs for chunk {chunk_idx+1}. Retrying...\");\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "        if motifs_from_this_chunk:\n",
        "            print(f\"      Extracted {len(motifs_from_this_chunk)} motifs from chunk {chunk_idx+1} (QID {qid_for_log}).\")\n",
        "            all_raw_motifs_from_chunks.extend(motifs_from_this_chunk)\n",
        "        else:\n",
        "            print(f\"      No valid motifs from chunk {chunk_idx+1} (QID {qid_for_log}) after {LLM_RETRY_ATTEMPTS} attempts.\")\n",
        "    return all_raw_motifs_from_chunks\n",
        "\n",
        "def consolidate_raw_motifs(list_of_all_raw_motifs: List[Dict]) -> List[Dict]:\n",
        "    # (Same as last version)\n",
        "    if not list_of_all_raw_motifs: return []\n",
        "    consolidated_motifs_map = {}\n",
        "    for motif_obj in list_of_all_raw_motifs:\n",
        "        label = motif_obj.get(\"label\",\"\").strip(); description = motif_obj.get(\"description\",\"\").strip()\n",
        "        surface_forms = motif_obj.get(\"surface_forms\", [])\n",
        "        if not (label and isinstance(surface_forms, list)): continue\n",
        "        current_sfs_set = set(sf.lower().strip() for sf in surface_forms if isinstance(sf, str) and sf.strip())\n",
        "        if label not in consolidated_motifs_map:\n",
        "            consolidated_motifs_map[label] = {\"label\": label, \"description\": description, \"surface_forms\": sorted(list(current_sfs_set))}\n",
        "        else:\n",
        "            existing_sfs_set = set(consolidated_motifs_map[label].get(\"surface_forms\", []))\n",
        "            consolidated_motifs_map[label][\"surface_forms\"] = sorted(list(existing_sfs_set.union(current_sfs_set)))\n",
        "    return list(consolidated_motifs_map.values())\n",
        "\n",
        "def filter_surface_forms_by_global_frequency(\n",
        "    consolidated_motifs_list: List[Dict],\n",
        "    full_qid_corpus_text: str,\n",
        "    min_global_freq: int = MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "    ) -> List[Dict]:\n",
        "    # (Same as last version)\n",
        "    if not consolidated_motifs_list: return []\n",
        "    final_globally_filtered_motifs = []\n",
        "    for motif_obj in consolidated_motifs_list:\n",
        "        globally_frequent_sfs_for_this_motif = []\n",
        "        original_sfs_for_this_motif = motif_obj.get(\"surface_forms\", [])\n",
        "        for sf_str_lower in original_sfs_for_this_motif:\n",
        "            count = count_sf_occurrences(full_qid_corpus_text, sf_str_lower)\n",
        "            if count >= min_global_freq:\n",
        "                globally_frequent_sfs_for_this_motif.append(sf_str_lower)\n",
        "        if globally_frequent_sfs_for_this_motif:\n",
        "            filtered_motif_entry = motif_obj.copy()\n",
        "            filtered_motif_entry[\"surface_forms\"] = sorted(list(set(globally_frequent_sfs_for_this_motif)))\n",
        "            final_globally_filtered_motifs.append(filtered_motif_entry)\n",
        "    return final_globally_filtered_motifs\n",
        "\n",
        "print(\"Cell 4: Motif Processing and Validation Utilities loaded (with enhanced prompt strategy and robust parsing).\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cMnfEmoYdG00"
      },
      "id": "cMnfEmoYdG00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5: MDL Calculations\n",
        "\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# Assume constants from Cell 1 are in global scope\n",
        "# Assume tokenize_phrase from Cell 2 is in global scope\n",
        "try:\n",
        "    MATRIX_SIZE_GLOBAL; MAX_TEXT_FOR_BDM_HASH; MOTIF_SYMBOLIC_LABEL_COST # Check a few\n",
        "    MOTIF_DESCRIPTION_TEXT_BASE_COST; MOTIF_DESCRIPTION_TOKEN_COST\n",
        "    MOTIF_SURFACE_FORMS_LIST_BASE_COST; MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 5): Key config constants not found from Cell 1. Using fallbacks or expecting errors.\")\n",
        "    MATRIX_SIZE_GLOBAL = (8, 8); MAX_TEXT_FOR_BDM_HASH = 2000\n",
        "    MOTIF_SYMBOLIC_LABEL_COST = 0.5; MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "    MOTIF_DESCRIPTION_TOKEN_COST = 0.1; MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.0\n",
        "    MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.0\n",
        "\n",
        "\n",
        "def initialize_bdm_instance():\n",
        "    \"\"\"Initializes and returns a BDM instance.\"\"\"\n",
        "    print(\"Initializing BDM instance...\")\n",
        "    try:\n",
        "        bdm_instance = BDM(ndim=2)\n",
        "        print(\"BDM instance initialized successfully (ndim=2, default CTM-based).\")\n",
        "        return bdm_instance\n",
        "    except Exception as e_bdm_init:\n",
        "        print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\")\n",
        "        if \"CTM data files\" in str(e_bdm_init).lower() or \"dataset\" in str(e_bdm_init).lower():\n",
        "            print(\"  BDM Error Hint: This might be related to missing/corrupted CTM data files for PyBDM.\")\n",
        "            print(\"  Ensure PyBDM is installed correctly and can access/download its data.\")\n",
        "            print(\"  You might need to run once: from pybdm import get_ctm_dataset; get_ctm_dataset()\")\n",
        "        return None\n",
        "\n",
        "def text_to_binary_matrix(text_input: str, size: tuple = MATRIX_SIZE_GLOBAL) -> np.ndarray:\n",
        "    \"\"\"Converts a text string to a binary matrix using its SHA256 hash.\"\"\"\n",
        "    if not isinstance(text_input, str) or not text_input.strip():\n",
        "        return np.zeros(size, dtype=int)\n",
        "    hash_obj = hashlib.sha256(text_input.encode('utf-8', 'ignore'))\n",
        "    hash_digest = hash_obj.hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string_from_hash = bin(int(hash_digest, 16))[2:].zfill(256)\n",
        "    binary_string_for_matrix = binary_string_from_hash[:required_bits] if required_bits <= 256 else binary_string_from_hash.ljust(required_bits, '0')\n",
        "    bits_for_matrix = [int(b) for b in binary_string_for_matrix]\n",
        "    return np.array(bits_for_matrix).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input: str, bdm_instance: BDM, matrix_s: tuple = MATRIX_SIZE_GLOBAL) -> float:\n",
        "    \"\"\"Computes BDM for a given text string using a prefix for hashing.\"\"\"\n",
        "    if not isinstance(text_input, str) or not text_input.strip() :\n",
        "        return 0.0\n",
        "    text_for_hash = text_input[:MAX_TEXT_FOR_BDM_HASH] if len(text_input) > MAX_TEXT_FOR_BDM_HASH else text_input\n",
        "    if not text_for_hash.strip():\n",
        "        return 0.0\n",
        "    binary_matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        bdm_value = bdm_instance.bdm(binary_matrix)\n",
        "        return bdm_value\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (full len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0 # Indicate error\n",
        "\n",
        "# VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\n",
        "# CORRECTED VERSION OF THIS FUNCTION\n",
        "# VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\n",
        "def calculate_L_H_token_based_structured(structured_motifs_list: List[Dict]) -> float:\n",
        "    \"\"\"Calculates L(H) - the cost of defining the list of structured motifs.\"\"\"\n",
        "    if not structured_motifs_list:\n",
        "        return 0.0\n",
        "\n",
        "    total_lh_cost = 0.0\n",
        "    for motif_obj in structured_motifs_list: # For each motif\n",
        "        if not isinstance(motif_obj, dict):\n",
        "            continue\n",
        "\n",
        "        current_motif_lh = 0.0 # INITIALIZE PER MOTIF\n",
        "\n",
        "        label_str = motif_obj.get('label', \"\")\n",
        "        if isinstance(label_str, str) and label_str.strip():\n",
        "            current_motif_lh += MOTIF_SYMBOLIC_LABEL_COST\n",
        "\n",
        "        description_str = motif_obj.get('description', \"\")\n",
        "        if isinstance(description_str, str) and description_str.strip():\n",
        "            current_motif_lh += MOTIF_DESCRIPTION_TEXT_BASE_COST\n",
        "            current_motif_lh += len(tokenize_phrase(description_str)) * MOTIF_DESCRIPTION_TOKEN_COST # tokenize_phrase from Cell 2\n",
        "\n",
        "        surface_forms_list = motif_obj.get('surface_forms', [])\n",
        "        if isinstance(surface_forms_list, list) and surface_forms_list:\n",
        "            valid_sfs_for_lh = [sf for sf in surface_forms_list if isinstance(sf, str) and sf.strip()]\n",
        "            if valid_sfs_for_lh: # Only add base cost if there are actual SFs\n",
        "                current_motif_lh += MOTIF_SURFACE_FORMS_LIST_BASE_COST # From Cell 1 config\n",
        "                for sf_str in valid_sfs_for_lh:\n",
        "                    current_motif_lh += len(tokenize_phrase(sf_str)) * MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH # From Cell 1 config\n",
        "\n",
        "        total_lh_cost += current_motif_lh\n",
        "    return total_lh_cost\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "# END OF CORRECTION\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "\n",
        "def llm_compress_text_structured(text_to_compress: str, structured_motifs_list: List[Dict]) -> str:\n",
        "    \"\"\"Compresses text by replacing occurrences of motif surface forms with their symbolic labels.\"\"\"\n",
        "    if not isinstance(text_to_compress, str): return \"\"\n",
        "    if not structured_motifs_list: return text_to_compress.lower()\n",
        "    compressed_text = text_to_compress.lower()\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        label = motif_obj.get('label', None)\n",
        "        surface_forms = motif_obj.get('surface_forms', [])\n",
        "        if not (isinstance(label, str) and label.strip()) or \\\n",
        "           not (isinstance(surface_forms, list) and surface_forms):\n",
        "            continue\n",
        "        placeholder = label\n",
        "        sorted_sfs_for_this_motif = sorted(\n",
        "            [sf for sf in surface_forms if isinstance(sf, str) and sf.strip()], key=len, reverse=True\n",
        "        )\n",
        "        for sf_str_lower in sorted_sfs_for_this_motif: # Assumes sf_str is already lowercased from consolidation/filtering\n",
        "            try:\n",
        "                # Using word boundaries for more precise replacement\n",
        "                compressed_text = re.sub(r'\\b' + re.escape(sf_str_lower) + r'\\b', placeholder, compressed_text)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for SF '{sf_str_lower}' of motif '{label}': {re_e}. Skipping.\")\n",
        "                continue\n",
        "    return compressed_text\n",
        "\n",
        "def compute_mdl_cost_for_text_block(\n",
        "    full_qid_corpus_str: str,\n",
        "    final_motifs_to_evaluate: List[Dict],\n",
        "    bdm_instance: BDM,\n",
        "    matrix_s: tuple = MATRIX_SIZE_GLOBAL\n",
        "    ) -> tuple[float, float, float]:\n",
        "    \"\"\"Computes L(H), L(D|H), and Total MDL for a text block given a final set of motifs.\"\"\"\n",
        "    if not isinstance(full_qid_corpus_str, str): full_qid_corpus_str = \"\"\n",
        "    l_h = calculate_L_H_token_based_structured(final_motifs_to_evaluate) # Uses the corrected version\n",
        "    compressed_text_block = llm_compress_text_structured(full_qid_corpus_str, final_motifs_to_evaluate)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "    if l_d_h < 0: return l_h, -1.0, -1.0\n",
        "    total_mdl_cost = l_h + l_d_h\n",
        "    return l_h, l_d_h, total_mdl_cost\n",
        "\n",
        "print(\"Cell 5: MDL Calculation Utilities loaded (with L(H) fix).\")"
      ],
      "metadata": {
        "id": "0A0P51iEvkTV"
      },
      "id": "0A0P51iEvkTV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6: Main Pipeline Orchestration\n",
        "\n",
        "# Assume functions from previous cells are defined\n",
        "\n",
        "def main():\n",
        "    script_version_name = \"Refactored MWP v8 (Stricter Prompt, Robust Label Fix, L(H) SF Cost Zero)\"\n",
        "    print(f\"--- {script_version_name} ---\")\n",
        "    print(f\"Timestamp: {time.asctime()}\")\n",
        "    print(\"\\n--- Configuration Summary ---\") # ... (print all config constants from Cell 1)\n",
        "    print(f\"LLM Model: {LOCAL_LLM_MODEL_ID}, Quantization: {USE_QUANTIZATION_FOR_LOCAL_LLM}\")\n",
        "    print(f\"LLM Batch Size (Responses): {LLM_BATCH_SIZE_RESPONSES}, Retries: {LLM_RETRY_ATTEMPTS}\")\n",
        "    print(f\"Max Text Chars per LLM Prompt Chunk: {MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK}\")\n",
        "    print(f\"Max New Tokens for LLM Motif Extraction: {LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION}\")\n",
        "    print(f\"Max Motifs to Request per Chunk: {MAX_MOTIFS_PER_CHUNK}\")\n",
        "    print(f\"L(H) Costs: Label={MOTIF_SYMBOLIC_LABEL_COST}, DescBase={MOTIF_DESCRIPTION_TEXT_BASE_COST}, DescToken={MOTIF_DESCRIPTION_TOKEN_COST}, SFListBase={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, SFTokenInLH={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "    print(f\"Global SF Filtering Min Freq: {MIN_SF_FREQUENCY_IN_FULL_CORPUS}\")\n",
        "    print(f\"BDM Hash Prefix Length: {MAX_TEXT_FOR_BDM_HASH}, BDM Matrix: {MATRIX_SIZE_GLOBAL}\")\n",
        "    print(f\"Debug Log File: {LLM_DEBUG_LOG_FILE}\")\n",
        "    print(\"--- End Configuration Summary ---\\n\")\n",
        "\n",
        "    try: # Initialize debug log\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"LLM Motif Debug Log - Run Started: {time.asctime()}\\n\")\n",
        "            f.write(f\"Script Version: {script_version_name}\\n\")\n",
        "            f.write(f\"Model ID: {LOCAL_LLM_MODEL_ID}\\n\")\n",
        "            f.write(f\"Pipeline Config: return_full_text=False\\n\")\n",
        "            f.write(f\"L(H) SF Costs: Base={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, Token={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\\n---\\n\")\n",
        "    except Exception as e_log: print(f\"WARN: Could not initialize debug log file {LLM_DEBUG_LOG_FILE}: {e_log}\")\n",
        "\n",
        "    hf_pipeline_instance, hf_tokenizer_instance = initialize_llm_pipeline() # From Cell 3\n",
        "    if not hf_pipeline_instance: print(\"CRITICAL: Exiting: LLM init failure.\"); return\n",
        "    bdm_instance_main = initialize_bdm_instance() # From Cell 5\n",
        "    if not bdm_instance_main: print(\"CRITICAL: Exiting: BDM init failure.\"); return\n",
        "\n",
        "    if not os.path.exists(P2_COLLATED_FILE): print(f\"ERROR: File {P2_COLLATED_FILE} not found.\"); return\n",
        "    print(f\"Loading data from: {P2_COLLATED_FILE}...\"); phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f: phase2_data_content = json.load(f)\n",
        "    except Exception as e_load: print(f\"Error loading {P2_COLLATED_FILE}: {e_load}\"); return\n",
        "\n",
        "    all_qid_mdl_results_list = []\n",
        "    aggregated_content_by_qid_from_file = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "    if not aggregated_content_by_qid_from_file: print(f\"No 'aggregated_pdf_content_by_qid' in {P2_COLLATED_FILE}.\"); return\n",
        "\n",
        "    qids_to_process_this_run = [] # From Cell 1 (config)\n",
        "    if P3_QIDS_TO_PROCESS_THEMATICALLY and isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and P3_QIDS_TO_PROCESS_THEMATICALLY:\n",
        "        qids_to_process_this_run = [qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY if qid in aggregated_content_by_qid_from_file]\n",
        "        if not qids_to_process_this_run: print(f\"Warning: QIDs {P3_QIDS_TO_PROCESS_THEMATICALLY} not found. Exiting.\"); return\n",
        "    else:\n",
        "        qids_to_process_limit_fallback = 1\n",
        "        print(f\"P3_QIDS_TO_PROCESS_THEMATICALLY not set/empty. Processing up to {qids_to_process_limit_fallback} QID(s) as fallback.\")\n",
        "        qids_to_process_this_run = list(aggregated_content_by_qid_from_file.keys())[:qids_to_process_limit_fallback]\n",
        "        if not qids_to_process_this_run: print(\"No QIDs in data for fallback. Exiting.\"); return\n",
        "    if not qids_to_process_this_run: print(\"No QIDs selected. Exiting.\"); return\n",
        "    print(f\"\\nMDL analysis will run for QIDs: {qids_to_process_this_run}\\n\")\n",
        "\n",
        "    for qid_identifier_str in qids_to_process_this_run:\n",
        "        print(f\"--- Analyzing Data for QID: {qid_identifier_str} ---\")\n",
        "        list_of_individual_response_structs = aggregated_content_by_qid_from_file.get(qid_identifier_str, [])\n",
        "        actual_response_texts_for_qid = [item.get(\"text\", \"\") for item in list_of_individual_response_structs if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()]\n",
        "        if not actual_response_texts_for_qid: print(f\"  No valid text for QID {qid_identifier_str}. Skipping.\"); print(\"-\" * 50); continue\n",
        "        full_corpus_text_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(actual_response_texts_for_qid)\n",
        "        if len(full_corpus_text_for_qid.strip()) < 100: print(f\"  Skipping QID {qid_identifier_str}: text too short.\"); print(\"-\" * 50); continue\n",
        "        num_total_responses_for_qid = len(actual_response_texts_for_qid)\n",
        "        print(f\"  Corpus for QID {qid_identifier_str}: {len(full_corpus_text_for_qid)} chars, {num_total_responses_for_qid} responses.\")\n",
        "\n",
        "        baseline_bdm_original_corpus = compute_bdm_for_text(full_corpus_text_for_qid, bdm_instance_main)\n",
        "        if baseline_bdm_original_corpus < 0: print(f\"  Error computing baseline BDM for QID {qid_identifier_str}. Skipping.\"); print(\"-\" * 50); continue\n",
        "        current_qid_baseline_mdl_cost = baseline_bdm_original_corpus\n",
        "        print(f\"  Baseline MDL for QID {qid_identifier_str} (L(D_orig)): {current_qid_baseline_mdl_cost:.4f}\")\n",
        "\n",
        "        raw_motifs_from_chunks = get_motifs_for_qid_batched( # From Cell 4\n",
        "            actual_response_texts_for_qid, LLM_BATCH_SIZE_RESPONSES,\n",
        "            hf_pipeline_instance, hf_tokenizer_instance, qid_identifier_str\n",
        "        )\n",
        "        current_qid_result_entry = { # Init result entry\n",
        "            \"qid\": qid_identifier_str, \"corpus_len_chars\": len(full_corpus_text_for_qid), \"num_responses\": num_total_responses_for_qid,\n",
        "            \"baseline_mdl\": current_qid_baseline_mdl_cost, \"final_refined_motifs\": [], \"l_h_final_motifs\": 0.0,\n",
        "            \"l_d_h_final_motifs\": current_qid_baseline_mdl_cost, \"total_mdl_with_final_motifs\": current_qid_baseline_mdl_cost,\n",
        "            \"compression_achieved\": 0.0, \"num_raw_motifs_extracted\": len(raw_motifs_from_chunks),\n",
        "            \"num_consolidated_motifs\": 0, \"num_globally_refined_motifs\": 0\n",
        "        }\n",
        "        if not raw_motifs_from_chunks: print(f\"  No raw motifs by LLM for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "        print(f\"  Extracted {len(raw_motifs_from_chunks)} raw motif objects for QID {qid_identifier_str}.\")\n",
        "\n",
        "        consolidated_motifs_list = consolidate_raw_motifs(raw_motifs_from_chunks) # From Cell 4\n",
        "        current_qid_result_entry[\"num_consolidated_motifs\"] = len(consolidated_motifs_list)\n",
        "        print(f\"  Consolidated into {len(consolidated_motifs_list)} unique motifs for QID {qid_identifier_str}.\")\n",
        "        if not consolidated_motifs_list: print(f\"  No unique motifs after consolidation for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        # print(f\"  Consolidated Motifs (BEFORE Global SF refinement):\")\n",
        "        # for idx, mo_con in enumerate(consolidated_motifs_list): print(f\"    Cons. Motif {idx+1}: L='{mo_con.get('label')}', D='{mo_con.get('description','N/A')[:30]}...', SFs({len(mo_con.get('surface_forms',[]))})='{mo_con.get('surface_forms',[])[:2]}...'\")\n",
        "\n",
        "        globally_refined_motifs = filter_surface_forms_by_global_frequency( # From Cell 4\n",
        "            consolidated_motifs_list, full_corpus_text_for_qid, MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "        )\n",
        "        current_qid_result_entry[\"num_globally_refined_motifs\"] = len(globally_refined_motifs)\n",
        "        print(f\"  Globally refined into {len(globally_refined_motifs)} motifs for QID {qid_identifier_str}.\")\n",
        "        if not globally_refined_motifs: print(f\"  No motifs left after GLOBAL SF refinement for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  Final Globally Refined Motifs for QID {qid_identifier_str}:\")\n",
        "        for idx, mo_final in enumerate(globally_refined_motifs): print(f\"    Refined Motif {idx+1}: L='{mo_final.get('label')}', D='{mo_final.get('description','N/A')[:60]}...', SFs({len(mo_final.get('surface_forms',[]))})='{mo_final.get('surface_forms',[])}'\")\n",
        "\n",
        "        l_h_final, l_d_h_final, total_mdl_final = compute_mdl_cost_for_text_block( # From Cell 5\n",
        "            full_corpus_text_for_qid, globally_refined_motifs, bdm_instance_main\n",
        "        )\n",
        "        current_qid_result_entry.update({\n",
        "            \"final_refined_motifs\": globally_refined_motifs, \"l_h_final_motifs\": l_h_final,\n",
        "            \"l_d_h_final_motifs\": l_d_h_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"total_mdl_with_final_motifs\": total_mdl_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"compression_achieved\": \"BDM_ERROR\" if l_d_h_final < 0 else (current_qid_baseline_mdl_cost - total_mdl_final)\n",
        "        })\n",
        "        if l_d_h_final < 0: print(f\"  Error computing MDL cost (BDM error) for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  L(H) final motifs: {l_h_final:.4f} (SFs definition cost in L(H) is ZEROED for this run)\")\n",
        "        print(f\"  L(D|H) compressed full corpus: {l_d_h_final:.4f}\")\n",
        "        print(f\"  Total MDL cost with final motifs: {total_mdl_final:.4f}\")\n",
        "        compression_val = current_qid_result_entry[\"compression_achieved\"]\n",
        "        result_status_str = f\"SUCCESS: Comp: {compression_val:.4f}\" if isinstance(compression_val, float) and compression_val > 0.0001 else f\"NOTE: No sig. comp. Diff: {compression_val if isinstance(compression_val, str) else compression_val:.4f}\"\n",
        "        print(f\"  {result_status_str}\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50)\n",
        "\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary ---\")\n",
        "    if not all_qid_mdl_results_list: print(\"No QIDs processed.\")\n",
        "    else:\n",
        "        valid_results = [r for r in all_qid_mdl_results_list if isinstance(r.get('compression_achieved'), float) and r.get('l_h_final_motifs', -1.0) >= 0]\n",
        "        num_qids_ok = len(valid_results); num_comp = sum(1 for r in valid_results if r['compression_achieved'] > 0.0001)\n",
        "        print(f\"Targeted QIDs: {len(qids_to_process_this_run)}, Results logged: {len(all_qid_mdl_results_list)}, Valid MDL: {num_qids_ok}, QIDs compressed: {num_comp}\")\n",
        "        if num_comp > 0:\n",
        "            comp_vals = [r['compression_achieved'] for r in valid_results if r['compression_achieved'] > 0.0001]\n",
        "            print(f\"  Avg compression: {np.mean(comp_vals):.4f}, Max compression: {np.max(comp_vals):.4f}\")\n",
        "        else: print(\"  No compression achieved.\")\n",
        "\n",
        "        output_filename = os.path.join(BASE_PROJECT_DIR, \"mdl_analysis_refactored_v8_strictprompt_LHSFzero.json\")\n",
        "        try:\n",
        "            with open(output_filename, \"w\", encoding=\"utf-8\") as f_out: json.dump(all_qid_mdl_results_list, f_out, indent=2, ensure_ascii=False)\n",
        "            print(f\"Detailed results saved to {output_filename}\")\n",
        "        except Exception as e_s: print(f\"Error saving results: {e_s}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Executing main MDL pipeline (Refactored MWP v8 - Stricter Prompt, Label Fix, L(H) SF Cost Zero) at {time.asctime()}...\")\n",
        "    main()\n",
        "    print(f\"Main MDL pipeline execution finished at {time.asctime()}.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sUYNLEFSdTgE"
      },
      "id": "sUYNLEFSdTgE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3rd June"
      ],
      "metadata": {
        "id": "f-XwmuWnRz0K"
      },
      "id": "f-XwmuWnRz0K"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1: Configuration\n",
        "\n",
        "import os\n",
        "from typing import List, Dict, Set\n",
        "from collections import Counter\n",
        "\n",
        "# --- Project Paths ---\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"]\n",
        "\n",
        "# --- BDM Configuration ---\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "MAX_TEXT_FOR_BDM_HASH = 2000\n",
        "\n",
        "# --- LLM Configuration ---\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "LLM_BATCH_SIZE_RESPONSES = 5\n",
        "LLM_RETRY_ATTEMPTS = 2\n",
        "MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 1000 # INCREASED FOR TESTING JSON COMPLETENESS\n",
        "MAX_MOTIFS_PER_CHUNK = 5\n",
        "\n",
        "# --- Token-Based L(H) Configuration (SFs cost zeroed for current experiment) ---\n",
        "MOTIF_SYMBOLIC_LABEL_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TOKEN_COST = 0.1\n",
        "MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.0\n",
        "MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.0\n",
        "\n",
        "# --- Surface Form Filtering Configuration ---\n",
        "MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "\n",
        "# --- Logging File ---\n",
        "LLM_DEBUG_LOG_FILE = os.path.join(BASE_PROJECT_DIR, \"llm_motif_debug_log_refactored_v7_prompt_jsonfix.txt\")\n",
        "\n",
        "print(f\"Cell 1: Configuration loaded. LOCAL_LLM_MODEL_ID set to '{LOCAL_LLM_MODEL_ID}'.\")\n",
        "print(f\"LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION set to: {LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION}\")\n",
        "print(f\"L(H) SF Costs: Base={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, Token={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "print(f\"Debug log will be: {LLM_DEBUG_LOG_FILE}\")\n",
        "if not os.path.exists(BASE_PROJECT_DIR):\n",
        "    print(f\"WARNING: BASE_PROJECT_DIR '{BASE_PROJECT_DIR}' does not exist.\")\n",
        "if P2_COLLATED_FILE and not os.path.exists(P2_COLLATED_FILE):\n",
        "     print(f\"WARNING: P2_COLLATED_FILE '{P2_COLLATED_FILE}' does not exist. Data loading may fail.\")"
      ],
      "metadata": {
        "id": "VJtyyJdwRysZ"
      },
      "id": "VJtyyJdwRysZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2: Text Utilities\n",
        "\n",
        "import re\n",
        "from typing import List, Dict\n",
        "from collections import Counter\n",
        "\n",
        "# Constants this cell might use if run independently\n",
        "try:\n",
        "    MIN_SF_FREQ_IN_CHUNK_VALIDATION # Check if defined from config\n",
        "except NameError:\n",
        "    MIN_SF_FREQ_IN_CHUNK_VALIDATION = 2\n",
        "\n",
        "\n",
        "def tokenize_phrase(phrase_text: str) -> List[str]:\n",
        "    if not isinstance(phrase_text, str) or not phrase_text.strip():\n",
        "        return []\n",
        "    return phrase_text.lower().split()\n",
        "\n",
        "def preprocess_corpus_for_motif_extraction(text_corpus: str) -> str:\n",
        "    if not isinstance(text_corpus, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text_corpus)\n",
        "    text = re.sub(r' {2,}', ' ', text)\n",
        "    lines = text.split('\\n')\n",
        "    filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10 or not line.strip()]\n",
        "    return '\\n'.join(filtered_lines)\n",
        "\n",
        "def count_sf_occurrences(corpus_text: str, surface_form: str) -> int:\n",
        "    if not corpus_text or not surface_form or \\\n",
        "       not isinstance(corpus_text, str) or not isinstance(surface_form, str) or \\\n",
        "       not surface_form.strip():\n",
        "        return 0\n",
        "    try:\n",
        "        return len(re.findall(re.escape(surface_form.lower()), corpus_text.lower(), flags=re.IGNORECASE))\n",
        "    except re.error as e:\n",
        "        print(f\"    [WARN] Regex error in count_sf_occurrences for SF '{surface_form}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def extract_actual_phrases_from_text( # Not actively used in current main flow\n",
        "    text: str,\n",
        "    min_phrase_len: int = 2,\n",
        "    max_phrase_len: int = 6,\n",
        "    min_freq: int = MIN_SF_FREQ_IN_CHUNK_VALIDATION\n",
        "    ) -> Dict[str, int]:\n",
        "    if not isinstance(text, str) or not text.strip(): return {}\n",
        "    text_cleaned = text.lower()\n",
        "    text_cleaned = re.sub(r'[^\\w\\s\\']', ' ', text_cleaned)\n",
        "    text_cleaned = re.sub(r'\\s+', ' ', text_cleaned).strip()\n",
        "    words = text_cleaned.split()\n",
        "    if not words or len(words) < min_phrase_len: return {}\n",
        "    phrase_counts = Counter()\n",
        "    for n in range(min_phrase_len, max_phrase_len + 1):\n",
        "        if n > len(words): continue\n",
        "        for i in range(len(words) - n + 1):\n",
        "            phrase = ' '.join(words[i:i+n])\n",
        "            if phrase: phrase_counts[phrase] += 1\n",
        "    return {phrase: count for phrase, count in phrase_counts.items() if count >= min_freq}\n",
        "\n",
        "print(\"Cell 2: Text Utilities loaded.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-DO-_i-GR-3g"
      },
      "id": "-DO-_i-GR-3g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3: LLM Interaction\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import time\n",
        "from typing import List, Dict\n",
        "import os\n",
        "import re\n",
        "\n",
        "try:\n",
        "    LOCAL_LLM_MODEL_ID; USE_QUANTIZATION_FOR_LOCAL_LLM; MAX_MOTIFS_PER_CHUNK\n",
        "    MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK; LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION; LLM_DEBUG_LOG_FILE\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 3): Key config constants not found from Cell 1. Using fallbacks.\")\n",
        "    LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'; USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "    MAX_MOTIFS_PER_CHUNK = 5; MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "    LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 1000 # Matching Cell 1\n",
        "    LLM_DEBUG_LOG_FILE = \"temp_llm_debug_log_cell3.txt\"\n",
        "\n",
        "\n",
        "def initialize_llm_pipeline(\n",
        "    model_id: str = LOCAL_LLM_MODEL_ID,\n",
        "    use_quantization: bool = USE_QUANTIZATION_FOR_LOCAL_LLM,\n",
        "    pipeline_return_full_text: bool = False\n",
        "    ):\n",
        "    # (Same as your last working version of this function)\n",
        "    print(f\"--- Initializing LLM Pipeline (model: {model_id}, quantization: {use_quantization}, return_full_text: {pipeline_return_full_text}) ---\")\n",
        "    hf_pipeline_instance = None; hf_tokenizer_instance = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {model_id}...\"); hf_tokenizer_instance = AutoTokenizer.from_pretrained(model_id)\n",
        "        if hf_tokenizer_instance.pad_token is None:\n",
        "            if hf_tokenizer_instance.eos_token is not None: print(\"Tokenizer setting pad_token = eos_token.\"); hf_tokenizer_instance.pad_token = hf_tokenizer_instance.eos_token\n",
        "            else: print(\"WARN (initialize_llm): Tokenizer has no pad_token and no eos_token.\")\n",
        "        bnb_config = None; quant_active = False\n",
        "        if use_quantization and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if (device.type == 'cuda' and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n",
        "                quant_active = True; print(f\"BNB config created, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb: print(f\"WARN: Failed BitsAndBytesConfig: {e_bnb}. Quantization may be disabled.\"); quant_active = False\n",
        "        print(f\"Loading model {model_id} (Quantization: {quant_active})...\"); model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "        if quant_active and bnb_config: model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        elif device.type == 'cuda': model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "        hf_model_instance = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "        if hf_tokenizer_instance.pad_token_id is not None:\n",
        "            if hf_model_instance.config.pad_token_id is None or hf_model_instance.config.pad_token_id != hf_tokenizer_instance.pad_token_id:\n",
        "                hf_model_instance.config.pad_token_id = hf_tokenizer_instance.pad_token_id\n",
        "        hf_pipeline_instance = pipeline(\"text-generation\", model=hf_model_instance, tokenizer=hf_tokenizer_instance, return_full_text=pipeline_return_full_text)\n",
        "        print(f\"LLM pipeline initialized for {model_id}.\"); return hf_pipeline_instance, hf_tokenizer_instance\n",
        "    except Exception as e: print(f\"CRITICAL: LLM pipeline init failed: {e}\"); import traceback; traceback.print_exc(); return None, None\n",
        "\n",
        "\n",
        "def create_enhanced_motif_prompt(text_block_for_prompt: str, max_motifs_to_extract: int = MAX_MOTIFS_PER_CHUNK) -> str:\n",
        "    \"\"\"Revised prompt with stronger emphasis on JSON syntax and label format.\"\"\"\n",
        "    if len(text_block_for_prompt) > MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK:\n",
        "        text_block_for_prompt = text_block_for_prompt[:MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK]\n",
        "\n",
        "    prompt = f\"\"\"You are a precise assistant for thematic analysis. Your task is to extract key recurring themes from the provided text.\n",
        "\n",
        "STRICT OUTPUT REQUIREMENTS:\n",
        "1.  Your entire response MUST be a single, valid JSON list.\n",
        "2.  Each element in the list MUST be a JSON object.\n",
        "3.  Each JSON object MUST contain exactly three keys: \"label\", \"description\", and \"surface_forms\".\n",
        "4.  The value for \"label\" MUST be a string, IN ALL_CAPITAL_SNAKE_CASE, AND enclosed in square brackets. Example of a correct label: \"[DATA_SECURITY_POLICY]\". Example of an incorrect label: \"Data Security Policy\".\n",
        "5.  The value for \"description\" MUST be a single, concise sentence (string).\n",
        "6.  The value for \"surface_forms\" MUST be a JSON list of 2 to 3 short (2-6 words) VERBATIM phrases extracted DIRECTLY from the 'Text to analyze'. These phrases must be strong examples of the theme. If no suitable verbatim phrases are found, provide an empty list `[]`.\n",
        "7.  Identify up to {max_motifs_to_extract} themes. If fewer are clear, provide fewer objects. If no themes are clear, output an empty JSON list: `[]`.\n",
        "8.  Do NOT include any text, explanations, apologies, or markdown (like ```json) before or after the main JSON list. Ensure all strings in the JSON are properly double-quoted and terminated, and all lists/objects are correctly bracketed with elements comma-separated. Avoid trailing commas before a closing ']' or '}}'.\n",
        "\n",
        "INSTRUCTIONS FOR THEME IDENTIFICATION:\n",
        "- Focus on meaningful recurring concepts directly stated or strongly implied in the 'Text to analyze'.\n",
        "- For 'surface_forms', prioritize phrases that appear to be REPEATED or are highly characteristic of the theme within THIS text.\n",
        "- Avoid generic labels like [EXAMPLE_THEME] or [GENERAL_TOPIC]. Make labels specific.\n",
        "\n",
        "Text to analyze:\n",
        "\\\"\\\"\\\"\n",
        "{text_block_for_prompt}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Your valid JSON response (ONLY the JSON list):\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def call_local_llm_for_raw_response(\n",
        "    prompt_content_for_user_turn: str,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str,\n",
        "    chunk_idx_for_log: int\n",
        "    ) -> str:\n",
        "    # (Same as your last working version - uses do_sample=False)\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance:\n",
        "        print(f\"    ERROR (call_local_llm): LLM pipeline/tokenizer not initialized for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return \"\"\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": prompt_content_for_user_turn}]\n",
        "    try:\n",
        "        prompt_formatted_for_llm = hf_tokenizer_instance.apply_chat_template(\n",
        "            messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception as e_template:\n",
        "        print(f\"    ERROR (call_local_llm): Applying chat template for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_template}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm) ---\\nERROR APPLYING CHAT TEMPLATE: {e_template}\\nPrompt content (first 300): {prompt_content_for_user_turn[:300]}...\\n\")\n",
        "        return \"\"\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION,\n",
        "        \"do_sample\": False,\n",
        "        \"pad_token_id\": hf_tokenizer_instance.pad_token_id\n",
        "    }\n",
        "    try:\n",
        "        outputs = hf_pipeline_instance(prompt_formatted_for_llm, **generation_args)\n",
        "        if outputs and isinstance(outputs, list) and len(outputs) > 0 and \\\n",
        "           outputs[0] and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
        "            return outputs[0]['generated_text'].strip()\n",
        "        else:\n",
        "            print(f\"    WARN (call_local_llm): LLM pipeline unexpected structure for QID {qid_for_log}, Chunk {chunk_idx_for_log}. Output: {outputs}\")\n",
        "            return \"\"\n",
        "    except Exception as e_pipeline:\n",
        "        print(f\"    ERROR (call_local_llm): Exception during hf_pipeline call for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_pipeline}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm) ---\\nERROR PIPELINE CALL: {e_pipeline}\\nFormatted prompt (first 300): {prompt_formatted_for_llm[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"Cell 3: LLM Interaction Utilities loaded (with more explicit JSON prompt).\")"
      ],
      "metadata": {
        "id": "PLuQS5ahSC1Z"
      },
      "id": "PLuQS5ahSC1Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4: Motif Processing and Validation\n",
        "\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict\n",
        "\n",
        "# Assume constants (LLM_DEBUG_LOG_FILE, etc.) from Cell 1 are in global scope\n",
        "# Assume text_utils (preprocess_corpus_for_motif_extraction, count_sf_occurrences) from Cell 2\n",
        "# Assume LLM interaction (build_llm_prompt_for_motifs, call_local_llm_for_raw_response) from Cell 3\n",
        "\n",
        "try:\n",
        "    LLM_DEBUG_LOG_FILE; LLM_RETRY_ATTEMPTS; MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "except NameError:\n",
        "    LLM_DEBUG_LOG_FILE = \"temp_llm_debug_log_cell4.txt\"; LLM_RETRY_ATTEMPTS = 2; MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "\n",
        "\n",
        "def parse_and_validate_llm_json_response(\n",
        "    llm_raw_response_text: str,\n",
        "    qid_for_log:str,\n",
        "    chunk_idx_for_log:int,\n",
        "    prompt_sent_to_llm:str # For logging context\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"Parses LLM JSON, attempts label fixing, validates schema.\"\"\"\n",
        "    json_str_candidate = llm_raw_response_text.strip()\n",
        "    if json_str_candidate.startswith(\"```json\"): json_str_candidate = json_str_candidate[len(\"```json\"):].strip()\n",
        "    if json_str_candidate.startswith(\"```\"): json_str_candidate = json_str_candidate[len(\"```\"):].strip()\n",
        "    if json_str_candidate.endswith(\"```\"): json_str_candidate = json_str_candidate[:-len(\"```\")].strip()\n",
        "\n",
        "    if not json_str_candidate or json_str_candidate.lower() == \"[]\" or \\\n",
        "       \"no_themes_found\" in json_str_candidate.lower() or \\\n",
        "       \"no clear motifs\" in json_str_candidate.lower():\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        parsed_data = json.loads(json_str_candidate)\n",
        "        if isinstance(parsed_data, dict): parsed_data = [parsed_data]\n",
        "        if not isinstance(parsed_data, list):\n",
        "            raise ValueError(\"Parsed JSON is not a list or single object.\")\n",
        "\n",
        "        valid_motifs_from_json = []\n",
        "        for item_idx, item in enumerate(parsed_data):\n",
        "            if not isinstance(item, dict) or not item: continue\n",
        "\n",
        "            # --- Robust Label Processing ---\n",
        "            label_str_original = item.get('label', \"\")\n",
        "            label_str_processed = \"\"\n",
        "            if isinstance(label_str_original, str) and label_str_original.strip():\n",
        "                temp_label_stripped = label_str_original.strip()\n",
        "                # Priority 1: Extract existing well-formed [UPPER_SNAKE_CASE]\n",
        "                match_strict_bracketed = re.fullmatch(r\"\\[([A-Z0-9_]+)\\]\", temp_label_stripped)\n",
        "                if match_strict_bracketed:\n",
        "                    label_str_processed = temp_label_stripped\n",
        "                else:\n",
        "                    # Priority 2: Extract bracketed part even if extra text (e.g., \"[LABEL] explanation\")\n",
        "                    match_bracketed_part = re.search(r\"(\\[[A-Z0-9_]+\\])\", temp_label_stripped)\n",
        "                    if match_bracketed_part:\n",
        "                        label_str_processed = match_bracketed_part.group(1)\n",
        "                    else: # Priority 3: No brackets found, try to create one\n",
        "                        sanitized_content = re.sub(r'\\s+|-', '_', temp_label_stripped) # Replace space or hyphen\n",
        "                        sanitized_content = re.sub(r'[^a-zA-Z0-9_]', '', sanitized_content).upper()\n",
        "                        sanitized_content = \"_\".join(sanitized_content.split('_')[:4]) # Limit length\n",
        "                        if sanitized_content:\n",
        "                            label_str_processed = f\"[{sanitized_content}]\"\n",
        "            item['label'] = label_str_processed # Use the processed label for the item\n",
        "            # --- End Label Processing ---\n",
        "\n",
        "            label_to_validate = item.get('label',\"\")\n",
        "            desc_str = item.get('description',\"\")\n",
        "            sf_list = item.get('surface_forms', [])\n",
        "\n",
        "            has_all_keys = all(k in item for k in [\"label\", \"description\", \"surface_forms\"])\n",
        "            label_is_valid_format = isinstance(label_to_validate, str) and \\\n",
        "                                    bool(label_to_validate) and \\\n",
        "                                    label_to_validate.startswith('[') and \\\n",
        "                                    label_to_validate.endswith(']') and \\\n",
        "                                    re.fullmatch(r\"\\[[A-Z0-9_]+\\]\", label_to_validate) # Final check on processed label\n",
        "            desc_is_valid = isinstance(desc_str, str)\n",
        "            sfs_list_is_valid = isinstance(sf_list, list) and \\\n",
        "                                 all(isinstance(sf_item, str) for sf_item in sf_list)\n",
        "\n",
        "            if has_all_keys and label_is_valid_format and desc_is_valid and sfs_list_is_valid:\n",
        "                valid_motifs_from_json.append({\n",
        "                    \"label\": label_to_validate,\n",
        "                    \"description\": desc_str.strip(),\n",
        "                    \"surface_forms\": [s.strip() for s in sf_list if isinstance(s, str) and s.strip()]\n",
        "                })\n",
        "            else:\n",
        "                print(f\"    [WARN] Invalid motif object schema for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}. Skipping.\")\n",
        "                with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} --- ITEM_SCHEMA_FAILURE (Item {item_idx+1}) ---\\n\")\n",
        "                    f.write(f\"Original Label: '{label_str_original}', Processed Label for Validation: '{label_to_validate}'\\n\")\n",
        "                    f.write(f\"Item Content After Label Proc: {json.dumps(item, indent=2)}\\n\")\n",
        "                    f.write(f\"Validation Checks: has_keys={has_all_keys}, label_valid_fmt={label_is_valid_format}, desc_valid={desc_is_valid}, sfs_list_valid={sfs_list_is_valid}\\n\")\n",
        "        return valid_motifs_from_json\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        print(f\"    [WARN] Motif JSON parsing/core structure issue for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} --- JSON_PARSE_VALUE_ERROR ---\\n\")\n",
        "            prompt_parts = prompt_sent_to_llm.split('Set of comments to analyze:')\n",
        "            user_content_for_log = prompt_parts[1][:500] if len(prompt_parts) > 1 else (prompt_sent_to_llm[:500] if prompt_sent_to_llm else \"PROMPT_EMPTY\")\n",
        "            f.write(f\"PROMPT USER CONTENT (approx first 500 chars):\\n{user_content_for_log}...\\n\")\n",
        "            f.write(f\"RAW LLM RESPONSE (Error: {type(e).__name__} - {str(e)}):\\n{llm_raw_response_text}\\n\")\n",
        "            f.write(f\"EXTRACTED JSON CANDIDATE (Error: {type(e).__name__}):\\n{json_str_candidate}\\n\")\n",
        "        return []\n",
        "\n",
        "def get_motifs_for_qid_batched(\n",
        "    list_of_individual_response_texts: List[str],\n",
        "    responses_per_batch: int,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str\n",
        "    ) -> List[Dict]:\n",
        "    # (This function remains the same as your last correct version:\n",
        "    #  It batches text, calls create_enhanced_motif_prompt, call_local_llm_for_raw_response,\n",
        "    #  and parse_and_validate_llm_json_response with retries)\n",
        "    all_raw_motifs_from_chunks = []\n",
        "    batched_text_chunks_for_llm = []\n",
        "    for i in range(0, len(list_of_individual_response_texts), responses_per_batch):\n",
        "        batch_responses = list_of_individual_response_texts[i:i + responses_per_batch]\n",
        "        chunk_text_for_llm = preprocess_corpus_for_motif_extraction(\"\\n\\n<RSP_SEP>\\n\\n\".join(batch_responses)) # from Cell 2\n",
        "        batched_text_chunks_for_llm.append(chunk_text_for_llm)\n",
        "    print(f\"  QID {qid_for_log}: Processing {len(list_of_individual_response_texts)} responses in {len(batched_text_chunks_for_llm)} preprocessed chunks (batch size: {responses_per_batch} responses).\")\n",
        "    for chunk_idx, text_chunk_to_analyze_processed in enumerate(batched_text_chunks_for_llm):\n",
        "        print(f\"    Analyzing chunk {chunk_idx + 1}/{len(batched_text_chunks_for_llm)} for QID {qid_for_log} (processed chunk len: {len(text_chunk_to_analyze_processed)} chars)...\")\n",
        "        if len(text_chunk_to_analyze_processed.strip()) < 50:\n",
        "            print(f\"      Chunk {chunk_idx+1} (QID {qid_for_log}) too short, skipping.\"); continue\n",
        "        prompt_for_llm = create_enhanced_motif_prompt(text_chunk_to_analyze_processed) # from Cell 3\n",
        "        motifs_from_this_chunk = []\n",
        "        for attempt in range(LLM_RETRY_ATTEMPTS): # from config\n",
        "            raw_llm_response = call_local_llm_for_raw_response( # from Cell 3\n",
        "                prompt_for_llm, hf_pipeline_instance, hf_tokenizer_instance, qid_for_log, chunk_idx + 1\n",
        "            )\n",
        "            if not raw_llm_response:\n",
        "                print(f\"      LLM call attempt {attempt + 1} for chunk {chunk_idx+1} (QID {qid_for_log}) returned empty. Retrying if possible...\");\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1); continue\n",
        "            parsed_motifs_from_this_attempt = parse_and_validate_llm_json_response( # from this cell\n",
        "                raw_llm_response, qid_for_log, chunk_idx+1, prompt_for_llm\n",
        "            )\n",
        "            if parsed_motifs_from_this_attempt:\n",
        "                motifs_from_this_chunk = parsed_motifs_from_this_attempt; break\n",
        "            else:\n",
        "                print(f\"      Motif parsing/validation attempt {attempt + 1} yielded no structured motifs for chunk {chunk_idx+1}. Retrying if possible...\");\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "        if motifs_from_this_chunk:\n",
        "            print(f\"      Extracted {len(motifs_from_this_chunk)} structured motif objects from chunk {chunk_idx+1} (QID {qid_for_log}).\")\n",
        "            all_raw_motifs_from_chunks.extend(motifs_from_this_chunk)\n",
        "        else:\n",
        "            print(f\"      No valid structured motifs from chunk {chunk_idx+1} (QID {qid_for_log}) after {LLM_RETRY_ATTEMPTS} attempts.\")\n",
        "    return all_raw_motifs_from_chunks\n",
        "\n",
        "\n",
        "def consolidate_raw_motifs(list_of_all_raw_motifs: List[Dict]) -> List[Dict]:\n",
        "    # (Same as your last correct version)\n",
        "    if not list_of_all_raw_motifs: return []\n",
        "    consolidated_motifs_map = {}\n",
        "    for motif_obj in list_of_all_raw_motifs:\n",
        "        label = motif_obj.get(\"label\",\"\").strip()\n",
        "        description = motif_obj.get(\"description\",\"\").strip()\n",
        "        surface_forms = motif_obj.get(\"surface_forms\", [])\n",
        "        if not (label and isinstance(surface_forms, list)): continue\n",
        "        current_sfs_set = set(sf.lower().strip() for sf in surface_forms if isinstance(sf, str) and sf.strip())\n",
        "        if label not in consolidated_motifs_map:\n",
        "            consolidated_motifs_map[label] = {\"label\": label, \"description\": description, \"surface_forms\": sorted(list(current_sfs_set))}\n",
        "        else:\n",
        "            existing_sfs_set = set(consolidated_motifs_map[label].get(\"surface_forms\", []))\n",
        "            consolidated_motifs_map[label][\"surface_forms\"] = sorted(list(existing_sfs_set.union(current_sfs_set)))\n",
        "    return list(consolidated_motifs_map.values())\n",
        "\n",
        "def filter_surface_forms_by_global_frequency(\n",
        "    consolidated_motifs_list: List[Dict],\n",
        "    full_qid_corpus_text: str,\n",
        "    min_global_freq: int = MIN_SF_FREQUENCY_IN_FULL_CORPUS # From config\n",
        "    ) -> List[Dict]:\n",
        "    # (Same as your last correct version - uses count_sf_occurrences from Cell 2)\n",
        "    if not consolidated_motifs_list: return []\n",
        "    final_globally_filtered_motifs = []\n",
        "    for motif_obj in consolidated_motifs_list:\n",
        "        globally_frequent_sfs_for_this_motif = []\n",
        "        original_sfs_for_this_motif = motif_obj.get(\"surface_forms\", [])\n",
        "        for sf_str_lower in original_sfs_for_this_motif:\n",
        "            count = count_sf_occurrences(full_qid_corpus_text, sf_str_lower) # from Cell 2\n",
        "            if count >= min_global_freq:\n",
        "                globally_frequent_sfs_for_this_motif.append(sf_str_lower)\n",
        "        if globally_frequent_sfs_for_this_motif:\n",
        "            filtered_motif_entry = motif_obj.copy()\n",
        "            filtered_motif_entry[\"surface_forms\"] = sorted(list(set(globally_frequent_sfs_for_this_motif)))\n",
        "            final_globally_filtered_motifs.append(filtered_motif_entry)\n",
        "    return final_globally_filtered_motifs\n",
        "\n",
        "print(\"Cell 4: Motif Processing and Validation Utilities loaded (with enhanced label processing).\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AMp2Y1IqSINd"
      },
      "id": "AMp2Y1IqSINd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5: MDL Calculations\n",
        "\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# Assume constants from Cell 1 are in global scope\n",
        "# Assume tokenize_phrase from Cell 2 is in global scope\n",
        "try:\n",
        "    MATRIX_SIZE_GLOBAL; MAX_TEXT_FOR_BDM_HASH; MOTIF_SYMBOLIC_LABEL_COST\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 5): Key config constants not found. Using fallbacks or expecting errors.\")\n",
        "    MATRIX_SIZE_GLOBAL = (8, 8); MAX_TEXT_FOR_BDM_HASH = 2000\n",
        "    MOTIF_SYMBOLIC_LABEL_COST = 0.5; MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "    MOTIF_DESCRIPTION_TOKEN_COST = 0.1; MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.0\n",
        "    MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.0\n",
        "\n",
        "\n",
        "def initialize_bdm_instance():\n",
        "    print(\"Initializing BDM instance...\")\n",
        "    try:\n",
        "        bdm_instance = BDM(ndim=2)\n",
        "        print(\"BDM instance initialized successfully (ndim=2, default CTM-based).\")\n",
        "        return bdm_instance\n",
        "    except Exception as e_bdm_init:\n",
        "        print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\")\n",
        "        if \"CTM data files\" in str(e_bdm_init).lower() or \"dataset\" in str(e_bdm_init).lower():\n",
        "            print(\"  BDM Error Hint: This might be related to missing/corrupted CTM data files for PyBDM.\")\n",
        "        return None\n",
        "\n",
        "def text_to_binary_matrix(text_input: str, size: tuple = MATRIX_SIZE_GLOBAL) -> np.ndarray:\n",
        "    if not isinstance(text_input, str) or not text_input.strip():\n",
        "        return np.zeros(size, dtype=int)\n",
        "    hash_obj = hashlib.sha256(text_input.encode('utf-8', 'ignore'))\n",
        "    hash_digest = hash_obj.hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string_from_hash = bin(int(hash_digest, 16))[2:].zfill(256)\n",
        "    binary_string_for_matrix = binary_string_from_hash[:required_bits] if required_bits <= 256 else binary_string_from_hash.ljust(required_bits, '0')\n",
        "    bits_for_matrix = [int(b) for b in binary_string_for_matrix]\n",
        "    return np.array(bits_for_matrix).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input: str, bdm_instance: BDM, matrix_s: tuple = MATRIX_SIZE_GLOBAL) -> float:\n",
        "    if not isinstance(text_input, str) or not text_input.strip() : return 0.0\n",
        "    text_for_hash = text_input[:MAX_TEXT_FOR_BDM_HASH] if len(text_input) > MAX_TEXT_FOR_BDM_HASH else text_input\n",
        "    if not text_for_hash.strip(): return 0.0\n",
        "    binary_matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        return bdm_instance.bdm(binary_matrix)\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0\n",
        "\n",
        "def calculate_L_H_token_based_structured(structured_motifs_list: List[Dict]) -> float:\n",
        "    # This function uses MOTIF_* constants from Cell 1.\n",
        "    # The change to make SFs \"free\" in L(H) is done by setting those constants to 0 in Cell 1.\n",
        "    if not structured_motifs_list: return 0.0\n",
        "    total_lh_cost = 0.0\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        current_motif_lh = 0.0\n",
        "        label_str = motif_obj.get('label', \"\")\n",
        "        if isinstance(label_str, str) and label_str.strip():\n",
        "            current_motif_lh += MOTIF_SYMBOLIC_LABEL_COST\n",
        "        description_str = motif_obj.get('description', \"\")\n",
        "        if isinstance(description_str, str) and description_str.strip():\n",
        "            current_motif_lh += MOTIF_DESCRIPTION_TEXT_BASE_COST\n",
        "            current_motif_lh += len(tokenize_phrase(description_str)) * MOTIF_DESCRIPTION_TOKEN_COST # tokenize_phrase from Cell 2\n",
        "        surface_forms_list = motif_obj.get('surface_forms', [])\n",
        "        if isinstance(surface_forms_list, list) and surface_forms_list:\n",
        "            valid_sfs_for_lh = [sf for sf in surface_forms_list if isinstance(sf, str) and sf.strip()]\n",
        "            if valid_sfs_for_lh:\n",
        "                current_motif_lh += MOTIF_SURFACE_FORMS_LIST_BASE_COST\n",
        "                for sf_str in valid_sfs_for_lh:\n",
        "                    current_motif_lh += len(tokenize_phrase(sf_str)) * MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH\n",
        "        total_lh_cost += current_motif_lh\n",
        "    return total_lh_cost\n",
        "\n",
        "def llm_compress_text_structured(text_to_compress: str, structured_motifs_list: List[Dict]) -> str:\n",
        "    if not isinstance(text_to_compress, str): return \"\"\n",
        "    if not structured_motifs_list: return text_to_compress.lower()\n",
        "    compressed_text = text_to_compress.lower()\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        label = motif_obj.get('label', None)\n",
        "        surface_forms = motif_obj.get('surface_forms', [])\n",
        "        if not (isinstance(label, str) and label.strip()) or \\\n",
        "           not (isinstance(surface_forms, list) and surface_forms):\n",
        "            continue\n",
        "        placeholder = label\n",
        "        sorted_sfs_for_this_motif = sorted(\n",
        "            [sf for sf in surface_forms if isinstance(sf, str) and sf.strip()], key=len, reverse=True\n",
        "        )\n",
        "        for sf_str_lower in sorted_sfs_for_this_motif:\n",
        "            try:\n",
        "                compressed_text = re.sub(r'\\b' + re.escape(sf_str_lower) + r'\\b', placeholder, compressed_text)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for SF '{sf_str_lower}' of motif '{label}': {re_e}. Skipping.\")\n",
        "                continue\n",
        "    return compressed_text\n",
        "\n",
        "def compute_mdl_cost_for_text_block(\n",
        "    full_qid_corpus_str: str,\n",
        "    final_motifs_to_evaluate: List[Dict],\n",
        "    bdm_instance: BDM,\n",
        "    matrix_s: tuple = MATRIX_SIZE_GLOBAL\n",
        "    ) -> tuple[float, float, float]:\n",
        "    if not isinstance(full_qid_corpus_str, str): full_qid_corpus_str = \"\"\n",
        "    l_h = calculate_L_H_token_based_structured(final_motifs_to_evaluate)\n",
        "    compressed_text_block = llm_compress_text_structured(full_qid_corpus_str, final_motifs_to_evaluate)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "    if l_d_h < 0: return l_h, -1.0, -1.0\n",
        "    total_mdl_cost = l_h + l_d_h\n",
        "    return l_h, l_d_h, total_mdl_cost\n",
        "\n",
        "print(\"Cell 5: MDL Calculation Utilities loaded.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bZv8Az1VSNx6"
      },
      "id": "bZv8Az1VSNx6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6: Main Pipeline Orchestration\n",
        "\n",
        "# Assume functions from previous cells are defined\n",
        "\n",
        "def main():\n",
        "    script_version_name = \"Refactored MWP v7 (Simpler Prompt, Enhanced Label Fix, L(H) SF Cost Zero)\"\n",
        "    print(f\"--- {script_version_name} ---\")\n",
        "    print(f\"Timestamp: {time.asctime()}\")\n",
        "    print(\"\\n--- Configuration Summary ---\")\n",
        "    print(f\"LLM Model: {LOCAL_LLM_MODEL_ID}, Quantization: {USE_QUANTIZATION_FOR_LOCAL_LLM}\")\n",
        "    print(f\"LLM Batch Size (Responses): {LLM_BATCH_SIZE_RESPONSES}, Retries: {LLM_RETRY_ATTEMPTS}\")\n",
        "    print(f\"Max Text Chars per LLM Prompt Chunk: {MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK}\")\n",
        "    print(f\"Max New Tokens for LLM Motif Extraction: {LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION}\")\n",
        "    print(f\"Max Motifs to Request per Chunk: {MAX_MOTIFS_PER_CHUNK}\")\n",
        "    print(f\"L(H) Costs: Label={MOTIF_SYMBOLIC_LABEL_COST}, DescBase={MOTIF_DESCRIPTION_TEXT_BASE_COST}, DescToken={MOTIF_DESCRIPTION_TOKEN_COST}, SFListBase={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, SFTokenInLH={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "    print(f\"Global SF Filtering Min Freq: {MIN_SF_FREQUENCY_IN_FULL_CORPUS}\")\n",
        "    print(f\"BDM Hash Prefix Length: {MAX_TEXT_FOR_BDM_HASH}, BDM Matrix: {MATRIX_SIZE_GLOBAL}\")\n",
        "    print(f\"Debug Log File: {LLM_DEBUG_LOG_FILE}\")\n",
        "    print(\"--- End Configuration Summary ---\\n\")\n",
        "\n",
        "    try:\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"LLM Motif Debug Log - Run Started: {time.asctime()}\\n\")\n",
        "            f.write(f\"Script Version: {script_version_name}\\n\")\n",
        "            f.write(f\"Model ID: {LOCAL_LLM_MODEL_ID}\\n\")\n",
        "            f.write(f\"Pipeline Config: return_full_text=False\\n\")\n",
        "            f.write(f\"L(H) SF Costs: Base={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, Token={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\\n---\\n\")\n",
        "    except Exception as e_log: print(f\"WARN: Could not initialize debug log file {LLM_DEBUG_LOG_FILE}: {e_log}\")\n",
        "\n",
        "    hf_pipeline_instance, hf_tokenizer_instance = initialize_llm_pipeline()\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance: print(\"CRITICAL: Exiting: LLM init failure.\"); return\n",
        "\n",
        "    bdm_instance_main = initialize_bdm_instance()\n",
        "    if not bdm_instance_main: print(\"CRITICAL: Exiting: BDM init failure.\"); return\n",
        "\n",
        "    if not os.path.exists(P2_COLLATED_FILE): print(f\"ERROR: File {P2_COLLATED_FILE} not found.\"); return\n",
        "    print(f\"Loading data from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f: phase2_data_content = json.load(f)\n",
        "    except Exception as e_load: print(f\"Error loading {P2_COLLATED_FILE}: {e_load}\"); return\n",
        "\n",
        "    all_qid_mdl_results_list = []\n",
        "    aggregated_content_by_qid_from_file = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "    if not aggregated_content_by_qid_from_file: print(f\"No 'aggregated_pdf_content_by_qid' in {P2_COLLATED_FILE}.\"); return\n",
        "\n",
        "    qids_to_process_this_run = []\n",
        "    if P3_QIDS_TO_PROCESS_THEMATICALLY and isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and P3_QIDS_TO_PROCESS_THEMATICALLY:\n",
        "        qids_to_process_this_run = [qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY if qid in aggregated_content_by_qid_from_file]\n",
        "        if not qids_to_process_this_run: print(f\"Warning: QIDs {P3_QIDS_TO_PROCESS_THEMATICALLY} not found. Exiting.\"); return\n",
        "    else:\n",
        "        qids_to_process_limit_fallback = 1\n",
        "        print(f\"P3_QIDS_TO_PROCESS_THEMATICALLY not set/empty. Processing up to {qids_to_process_limit_fallback} QID(s) as fallback.\")\n",
        "        qids_to_process_this_run = list(aggregated_content_by_qid_from_file.keys())[:qids_to_process_limit_fallback]\n",
        "        if not qids_to_process_this_run: print(\"No QIDs in data for fallback. Exiting.\"); return\n",
        "    if not qids_to_process_this_run: print(\"No QIDs selected. Exiting.\"); return\n",
        "    print(f\"\\nMDL analysis will run for QIDs: {qids_to_process_this_run}\\n\")\n",
        "\n",
        "    for qid_identifier_str in qids_to_process_this_run:\n",
        "        print(f\"--- Analyzing Data for QID: {qid_identifier_str} ---\")\n",
        "        list_of_individual_response_structs = aggregated_content_by_qid_from_file.get(qid_identifier_str, [])\n",
        "        actual_response_texts_for_qid = [item.get(\"text\", \"\") for item in list_of_individual_response_structs if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()]\n",
        "        if not actual_response_texts_for_qid: print(f\"  No valid text for QID {qid_identifier_str}. Skipping.\"); print(\"-\" * 50); continue\n",
        "        full_corpus_text_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(actual_response_texts_for_qid)\n",
        "        if len(full_corpus_text_for_qid.strip()) < 100: print(f\"  Skipping QID {qid_identifier_str}: text too short.\"); print(\"-\" * 50); continue\n",
        "        num_total_responses_for_qid = len(actual_response_texts_for_qid)\n",
        "        print(f\"  Corpus for QID {qid_identifier_str}: {len(full_corpus_text_for_qid)} chars, {num_total_responses_for_qid} responses.\")\n",
        "\n",
        "        baseline_bdm_original_corpus = compute_bdm_for_text(full_corpus_text_for_qid, bdm_instance_main)\n",
        "        if baseline_bdm_original_corpus < 0: print(f\"  Error computing baseline BDM for QID {qid_identifier_str}. Skipping.\"); print(\"-\" * 50); continue\n",
        "        current_qid_baseline_mdl_cost = baseline_bdm_original_corpus\n",
        "        print(f\"  Baseline MDL for QID {qid_identifier_str} (L(D_orig)): {current_qid_baseline_mdl_cost:.4f}\")\n",
        "\n",
        "        raw_motifs_from_chunks = get_motifs_for_qid_batched(\n",
        "            actual_response_texts_for_qid, LLM_BATCH_SIZE_RESPONSES,\n",
        "            hf_pipeline_instance, hf_tokenizer_instance, qid_identifier_str\n",
        "        )\n",
        "        current_qid_result_entry = {\n",
        "            \"qid\": qid_identifier_str, \"corpus_len_chars\": len(full_corpus_text_for_qid), \"num_responses\": num_total_responses_for_qid,\n",
        "            \"baseline_mdl\": current_qid_baseline_mdl_cost, \"final_refined_motifs\": [], \"l_h_final_motifs\": 0.0,\n",
        "            \"l_d_h_final_motifs\": current_qid_baseline_mdl_cost, \"total_mdl_with_final_motifs\": current_qid_baseline_mdl_cost,\n",
        "            \"compression_achieved\": 0.0, \"num_raw_motifs_extracted\": len(raw_motifs_from_chunks),\n",
        "            \"num_consolidated_motifs\": 0, \"num_globally_refined_motifs\": 0\n",
        "        }\n",
        "        if not raw_motifs_from_chunks: print(f\"  No raw motifs by LLM for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "        print(f\"  Extracted {len(raw_motifs_from_chunks)} raw motif objects for QID {qid_identifier_str}.\")\n",
        "\n",
        "        consolidated_motifs_list = consolidate_raw_motifs(raw_motifs_from_chunks)\n",
        "        current_qid_result_entry[\"num_consolidated_motifs\"] = len(consolidated_motifs_list)\n",
        "        print(f\"  Consolidated into {len(consolidated_motifs_list)} unique motifs for QID {qid_identifier_str}.\")\n",
        "        if not consolidated_motifs_list: print(f\"  No unique motifs after consolidation for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        # print(f\"  Consolidated Motifs (BEFORE Global SF refinement):\")\n",
        "        # for idx, mo_con in enumerate(consolidated_motifs_list): print(f\"    Cons. Motif {idx+1}: L='{mo_con.get('label')}', D='{mo_con.get('description','N/A')[:30]}...', SFs({len(mo_con.get('surface_forms',[]))})='{mo_con.get('surface_forms',[])[:2]}...'\")\n",
        "\n",
        "        globally_refined_motifs = filter_surface_forms_by_global_frequency(\n",
        "            consolidated_motifs_list, full_corpus_text_for_qid, MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "        )\n",
        "        current_qid_result_entry[\"num_globally_refined_motifs\"] = len(globally_refined_motifs)\n",
        "        print(f\"  Globally refined into {len(globally_refined_motifs)} motifs for QID {qid_identifier_str}.\")\n",
        "        if not globally_refined_motifs: print(f\"  No motifs left after GLOBAL SF refinement for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  Final Globally Refined Motifs for QID {qid_identifier_str}:\")\n",
        "        for idx, mo_final in enumerate(globally_refined_motifs): print(f\"    Refined Motif {idx+1}: L='{mo_final.get('label')}', D='{mo_final.get('description','N/A')[:60]}...', SFs({len(mo_final.get('surface_forms',[]))})='{mo_final.get('surface_forms',[])}'\")\n",
        "\n",
        "        l_h_final, l_d_h_final, total_mdl_final = compute_mdl_cost_for_text_block(\n",
        "            full_corpus_text_for_qid, globally_refined_motifs, bdm_instance_main\n",
        "        )\n",
        "        current_qid_result_entry.update({\n",
        "            \"final_refined_motifs\": globally_refined_motifs, \"l_h_final_motifs\": l_h_final,\n",
        "            \"l_d_h_final_motifs\": l_d_h_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"total_mdl_with_final_motifs\": total_mdl_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"compression_achieved\": \"BDM_ERROR\" if l_d_h_final < 0 else (current_qid_baseline_mdl_cost - total_mdl_final)\n",
        "        })\n",
        "        if l_d_h_final < 0: print(f\"  Error computing MDL cost (BDM error) for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  L(H) final motifs: {l_h_final:.4f} (SFs definition cost in L(H) is ZEROED for this run)\")\n",
        "        print(f\"  L(D|H) compressed full corpus: {l_d_h_final:.4f}\")\n",
        "        print(f\"  Total MDL cost with final motifs: {total_mdl_final:.4f}\")\n",
        "        compression_val = current_qid_result_entry[\"compression_achieved\"]\n",
        "        result_status_str = f\"SUCCESS: Comp: {compression_val:.4f}\" if isinstance(compression_val, float) and compression_val > 0.0001 else f\"NOTE: No sig. comp. Diff: {compression_val if isinstance(compression_val, str) else compression_val:.4f}\"\n",
        "        print(f\"  {result_status_str}\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50)\n",
        "\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary ---\")\n",
        "    if not all_qid_mdl_results_list: print(\"No QIDs processed.\")\n",
        "    else:\n",
        "        valid_results = [r for r in all_qid_mdl_results_list if isinstance(r.get('compression_achieved'), float) and r.get('l_h_final_motifs', -1.0) >= 0]\n",
        "        num_qids_ok = len(valid_results); num_comp = sum(1 for r in valid_results if r['compression_achieved'] > 0.0001)\n",
        "        print(f\"Targeted QIDs: {len(qids_to_process_this_run)}, Results logged: {len(all_qid_mdl_results_list)}, Valid MDL: {num_qids_ok}, QIDs compressed: {num_comp}\")\n",
        "        if num_comp > 0:\n",
        "            comp_vals = [r['compression_achieved'] for r in valid_results if r['compression_achieved'] > 0.0001]\n",
        "            print(f\"  Avg compression: {np.mean(comp_vals):.4f}, Max compression: {np.max(comp_vals):.4f}\")\n",
        "        else: print(\"  No compression achieved.\")\n",
        "\n",
        "        output_filename = os.path.join(BASE_PROJECT_DIR, \"mdl_analysis_refactored_v7_LHSFzero_labelfix.json\")\n",
        "        try:\n",
        "            with open(output_filename, \"w\", encoding=\"utf-8\") as f_out: json.dump(all_qid_mdl_results_list, f_out, indent=2, ensure_ascii=False)\n",
        "            print(f\"Detailed results saved to {output_filename}\")\n",
        "        except Exception as e_s: print(f\"Error saving results: {e_s}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Executing main MDL pipeline (Refactored MWP v7 - L(H) SF Cost Zero, Label Fix) at {time.asctime()}...\")\n",
        "    main()\n",
        "    print(f\"Main MDL pipeline execution finished at {time.asctime()}.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kbb6PNfDSTdw"
      },
      "id": "kbb6PNfDSTdw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2nd June"
      ],
      "metadata": {
        "id": "tK5a4RI8_0uc"
      },
      "id": "tK5a4RI8_0uc"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1: Configuration\n",
        "\n",
        "import os\n",
        "from typing import List, Dict, Set # Moved typing here for early availability\n",
        "from collections import Counter # Moved Counter here\n",
        "\n",
        "# --- Project Paths ---\n",
        "# !!! IMPORTANT: UPDATE BASE_PROJECT_DIR TO YOUR ACTUAL PATH !!!\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n",
        "# BASE_PROJECT_DIR = './' # For local testing if files are relative\n",
        "\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"]\n",
        "\n",
        "# --- BDM Configuration ---\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "MAX_TEXT_FOR_BDM_HASH = 2000\n",
        "\n",
        "# --- LLM Configuration ---\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "LLM_BATCH_SIZE_RESPONSES = 5\n",
        "LLM_RETRY_ATTEMPTS = 2\n",
        "MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 700 # For simpler prompt\n",
        "MAX_MOTIFS_PER_CHUNK = 5 # For simpler prompt\n",
        "\n",
        "# --- Token-Based L(H) Configuration ---\n",
        "MOTIF_SYMBOLIC_LABEL_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TOKEN_COST = 0.1\n",
        "MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.0 # EXPERIMENT: Zero cost for SF list itself\n",
        "MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.0 # EXPERIMENT: Zero cost for SF tokens in L(H)\n",
        "\n",
        "# --- Surface Form Filtering Configuration ---\n",
        "MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "\n",
        "# --- Logging File ---\n",
        "LLM_DEBUG_LOG_FILE = os.path.join(BASE_PROJECT_DIR, \"llm_motif_debug_log_refactored_v6_reset.txt\")\n",
        "\n",
        "print(f\"Cell 1: Configuration loaded. LOCAL_LLM_MODEL_ID set to '{LOCAL_LLM_MODEL_ID}'.\")\n",
        "print(f\"L(H) SF Costs: Base={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, Token={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "print(f\"Debug log will be: {LLM_DEBUG_LOG_FILE}\")\n",
        "if not os.path.exists(BASE_PROJECT_DIR):\n",
        "    print(f\"WARNING: BASE_PROJECT_DIR '{BASE_PROJECT_DIR}' does not exist.\")\n",
        "if P2_COLLATED_FILE and not os.path.exists(P2_COLLATED_FILE):\n",
        "     print(f\"WARNING: P2_COLLATED_FILE '{P2_COLLATED_FILE}' does not exist. Data loading may fail.\")"
      ],
      "metadata": {
        "id": "hDx5g9KU_3dS"
      },
      "id": "hDx5g9KU_3dS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2: Text Utilities\n",
        "\n",
        "import re\n",
        "from typing import List, Dict # Redundant if Cell 1 has it, but good for cell independence\n",
        "from collections import Counter # Redundant if Cell 1 has it\n",
        "\n",
        "# Constants this cell might use if run independently (though ideally from Cell 1)\n",
        "try:\n",
        "    MIN_SF_FREQ_IN_CHUNK_VALIDATION # Check if defined from config\n",
        "except NameError:\n",
        "    MIN_SF_FREQ_IN_CHUNK_VALIDATION = 2 # Default if not from config (not used in current main flow)\n",
        "\n",
        "\n",
        "def tokenize_phrase(phrase_text: str) -> List[str]:\n",
        "    \"\"\"Simple tokenizer for phrases, definitions, or surface forms.\"\"\"\n",
        "    if not isinstance(phrase_text, str) or not phrase_text.strip():\n",
        "        return []\n",
        "    return phrase_text.lower().split()\n",
        "\n",
        "def preprocess_corpus_for_motif_extraction(text_corpus: str) -> str:\n",
        "    \"\"\"Preprocesses a text corpus before sending to LLM or for n-gram extraction.\"\"\"\n",
        "    if not isinstance(text_corpus, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text_corpus)\n",
        "    text = re.sub(r' {2,}', ' ', text)\n",
        "    lines = text.split('\\n')\n",
        "    filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10 or not line.strip()]\n",
        "    return '\\n'.join(filtered_lines)\n",
        "\n",
        "def count_sf_occurrences(corpus_text: str, surface_form: str) -> int:\n",
        "    \"\"\"Counts case-insensitive occurrences of a surface_form within the corpus_text.\"\"\"\n",
        "    if not corpus_text or not surface_form or \\\n",
        "       not isinstance(corpus_text, str) or not isinstance(surface_form, str) or \\\n",
        "       not surface_form.strip():\n",
        "        return 0\n",
        "    try:\n",
        "        return len(re.findall(re.escape(surface_form.lower()), corpus_text.lower(), flags=re.IGNORECASE))\n",
        "    except re.error as e:\n",
        "        print(f\"    [WARN] Regex error in count_sf_occurrences for SF '{surface_form}': {e}\")\n",
        "        return 0\n",
        "\n",
        "# This function is defined here as a utility, but not actively called in the primary\n",
        "# motif extraction flow of this \"reverted simpler prompt\" version.\n",
        "def extract_actual_phrases_from_text(\n",
        "    text: str,\n",
        "    min_phrase_len: int = 2,\n",
        "    max_phrase_len: int = 6,\n",
        "    min_freq: int = MIN_SF_FREQ_IN_CHUNK_VALIDATION\n",
        "    ) -> Dict[str, int]:\n",
        "    \"\"\"Extracts n-gram phrases and their frequencies from text.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip(): return {}\n",
        "    text_cleaned = text.lower()\n",
        "    text_cleaned = re.sub(r'[^\\w\\s\\']', ' ', text_cleaned)\n",
        "    text_cleaned = re.sub(r'\\s+', ' ', text_cleaned).strip()\n",
        "    words = text_cleaned.split()\n",
        "    if not words or len(words) < min_phrase_len: return {}\n",
        "    phrase_counts = Counter()\n",
        "    for n in range(min_phrase_len, max_phrase_len + 1):\n",
        "        if n > len(words): continue\n",
        "        for i in range(len(words) - n + 1):\n",
        "            phrase = ' '.join(words[i:i+n])\n",
        "            if phrase: phrase_counts[phrase] += 1\n",
        "    return {phrase: count for phrase, count in phrase_counts.items() if count >= min_freq}\n",
        "\n",
        "print(\"Cell 2: Text Utilities loaded.\")"
      ],
      "metadata": {
        "id": "10kMZvcsAU2J"
      },
      "id": "10kMZvcsAU2J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3: LLM Interaction\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import time\n",
        "from typing import List, Dict\n",
        "import os\n",
        "import re # Needed for label sanitization if moved here, but parsing is in Cell 4\n",
        "\n",
        "# Assume constants from Cell 1 are in global scope\n",
        "try:\n",
        "    LOCAL_LLM_MODEL_ID; USE_QUANTIZATION_FOR_LOCAL_LLM; MAX_MOTIFS_PER_CHUNK\n",
        "    MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK; LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION; LLM_DEBUG_LOG_FILE\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 3): Key config constants not found from Cell 1. Using fallbacks.\")\n",
        "    LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'; USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "    MAX_MOTIFS_PER_CHUNK = 5; MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "    LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 700; LLM_DEBUG_LOG_FILE = \"temp_llm_debug_log_cell3.txt\"\n",
        "\n",
        "\n",
        "def initialize_llm_pipeline(\n",
        "    model_id: str = LOCAL_LLM_MODEL_ID,\n",
        "    use_quantization: bool = USE_QUANTIZATION_FOR_LOCAL_LLM,\n",
        "    pipeline_return_full_text: bool = False\n",
        "    ):\n",
        "    \"\"\"Initializes and returns the Hugging Face pipeline and tokenizer.\"\"\"\n",
        "    print(f\"--- Initializing LLM Pipeline (model: {model_id}, quantization: {use_quantization}, return_full_text: {pipeline_return_full_text}) ---\")\n",
        "    hf_pipeline_instance = None; hf_tokenizer_instance = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {model_id}...\"); hf_tokenizer_instance = AutoTokenizer.from_pretrained(model_id)\n",
        "        if hf_tokenizer_instance.pad_token is None:\n",
        "            if hf_tokenizer_instance.eos_token is not None: print(\"Tokenizer setting pad_token = eos_token.\"); hf_tokenizer_instance.pad_token = hf_tokenizer_instance.eos_token\n",
        "            else: print(\"WARN (initialize_llm): Tokenizer has no pad_token and no eos_token.\")\n",
        "        bnb_config = None; quant_active = False\n",
        "        if use_quantization and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if (device.type == 'cuda' and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n",
        "                quant_active = True; print(f\"BNB config created, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb: print(f\"WARN: Failed BitsAndBytesConfig: {e_bnb}. Quantization may be disabled.\"); quant_active = False\n",
        "        print(f\"Loading model {model_id} (Quantization: {quant_active})...\"); model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "        if quant_active and bnb_config: model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        elif device.type == 'cuda': model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "        hf_model_instance = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "        if hf_tokenizer_instance.pad_token_id is not None:\n",
        "            if hf_model_instance.config.pad_token_id is None or hf_model_instance.config.pad_token_id != hf_tokenizer_instance.pad_token_id:\n",
        "                hf_model_instance.config.pad_token_id = hf_tokenizer_instance.pad_token_id\n",
        "        hf_pipeline_instance = pipeline(\"text-generation\", model=hf_model_instance, tokenizer=hf_tokenizer_instance, return_full_text=pipeline_return_full_text)\n",
        "        print(f\"LLM pipeline initialized for {model_id}.\"); return hf_pipeline_instance, hf_tokenizer_instance\n",
        "    except Exception as e: print(f\"CRITICAL: LLM pipeline init failed: {e}\"); import traceback; traceback.print_exc(); return None, None\n",
        "\n",
        "def build_llm_prompt_for_motifs(text_block_for_prompt: str, max_motifs_to_extract: int = MAX_MOTIFS_PER_CHUNK) -> str:\n",
        "    \"\"\"Uses the simpler prompt structure for better label format adherence.\"\"\"\n",
        "    if len(text_block_for_prompt) > MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK:\n",
        "        text_block_for_prompt = text_block_for_prompt[:MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK]\n",
        "\n",
        "    prompt = f\"\"\"You will receive a set of comments from different people answering the same question.\n",
        "\n",
        "Your task is to identify up to {max_motifs_to_extract} key recurring themes.\n",
        "\n",
        "For each theme, provide:\n",
        "- A short label like [DATA_PRIVACY]\n",
        "- A 1-sentence description of the theme\n",
        "- 2–3 short phrases that often appear in the text (surface forms)\n",
        "\n",
        "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
        "Example of one object in the list:\n",
        "{{\n",
        "  \"label\": \"[EXAMPLE_LABEL]\",\n",
        "  \"description\": \"A concise description of the example theme.\",\n",
        "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
        "}}\n",
        "If no clear motifs are found, output an empty JSON list: `[]`.\n",
        "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
        "\n",
        "Set of comments to analyze:\n",
        "\\\"\\\"\\\"\n",
        "{text_block_for_prompt}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def call_local_llm_for_raw_response(\n",
        "    prompt_content_for_user_turn: str,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str,\n",
        "    chunk_idx_for_log: int\n",
        "    ) -> str:\n",
        "    \"\"\"Makes the LLM call and returns raw text string. Assumes pipeline return_full_text=False.\"\"\"\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance:\n",
        "        print(f\"    ERROR (call_local_llm): LLM pipeline/tokenizer not initialized for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return \"\"\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": prompt_content_for_user_turn}]\n",
        "    try:\n",
        "        prompt_formatted_for_llm = hf_tokenizer_instance.apply_chat_template(\n",
        "            messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception as e_template:\n",
        "        print(f\"    ERROR (call_local_llm): Applying chat template for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_template}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm) ---\\nERROR APPLYING CHAT TEMPLATE: {e_template}\\nPrompt content (first 300): {prompt_content_for_user_turn[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION,\n",
        "        \"do_sample\": False,\n",
        "        \"pad_token_id\": hf_tokenizer_instance.pad_token_id\n",
        "    }\n",
        "    try:\n",
        "        outputs = hf_pipeline_instance(prompt_formatted_for_llm, **generation_args)\n",
        "        if outputs and isinstance(outputs, list) and len(outputs) > 0 and \\\n",
        "           outputs[0] and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
        "            return outputs[0]['generated_text'].strip()\n",
        "        else:\n",
        "            print(f\"    WARN (call_local_llm): LLM pipeline unexpected structure for QID {qid_for_log}, Chunk {chunk_idx_for_log}. Output: {outputs}\")\n",
        "            return \"\"\n",
        "    except Exception as e_pipeline:\n",
        "        print(f\"    ERROR (call_local_llm): Exception during hf_pipeline call for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_pipeline}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm) ---\\nERROR PIPELINE CALL: {e_pipeline}\\nFormatted prompt (first 300): {prompt_formatted_for_llm[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"Cell 3: LLM Interaction Utilities loaded (using simpler prompt).\")"
      ],
      "metadata": {
        "id": "0fMDkMRSAbJh"
      },
      "id": "0fMDkMRSAbJh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4: Motif Processing and Validation\n",
        "\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict\n",
        "# from collections import Counter # Not directly used here, but in Cell 2\n",
        "\n",
        "# Assume constants from Cell 1 are in global scope\n",
        "# Assume text_utils functions (preprocess_corpus_for_motif_extraction, count_sf_occurrences) from Cell 2 are in scope\n",
        "# Assume LLM interaction functions (build_llm_prompt_for_motifs, call_local_llm_for_raw_response) from Cell 3 are in scope\n",
        "\n",
        "try:\n",
        "    LLM_DEBUG_LOG_FILE; LLM_RETRY_ATTEMPTS; MIN_SF_FREQUENCY_IN_FULL_CORPUS # Check some constants\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 4): Key config constants not found. Using fallbacks.\")\n",
        "    LLM_DEBUG_LOG_FILE = \"temp_llm_debug_log_cell4.txt\"; LLM_RETRY_ATTEMPTS = 2; MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "\n",
        "def parse_and_validate_llm_json_response(\n",
        "    llm_raw_response_text: str,\n",
        "    qid_for_log:str,\n",
        "    chunk_idx_for_log:int,\n",
        "    prompt_sent_to_llm:str\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"Parses LLM JSON, attempts label fixing, validates schema.\"\"\"\n",
        "    json_str_candidate = llm_raw_response_text.strip()\n",
        "    if json_str_candidate.startswith(\"```json\"): json_str_candidate = json_str_candidate[len(\"```json\"):].strip()\n",
        "    if json_str_candidate.startswith(\"```\"): json_str_candidate = json_str_candidate[len(\"```\"):].strip()\n",
        "    if json_str_candidate.endswith(\"```\"): json_str_candidate = json_str_candidate[:-len(\"```\")].strip()\n",
        "\n",
        "    if not json_str_candidate or json_str_candidate.lower() == \"[]\" or \\\n",
        "       \"no_themes_found\" in json_str_candidate.lower() or \\\n",
        "       \"no clear motifs\" in json_str_candidate.lower():\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        parsed_data = json.loads(json_str_candidate)\n",
        "        if isinstance(parsed_data, dict): parsed_data = [parsed_data]\n",
        "        if not isinstance(parsed_data, list):\n",
        "            raise ValueError(\"Parsed JSON is not a list or single object.\")\n",
        "\n",
        "        valid_motifs_from_json = []\n",
        "        for item_idx, item in enumerate(parsed_data):\n",
        "            if not isinstance(item, dict) or not item: continue\n",
        "\n",
        "            label_str_original = item.get('label', \"\")\n",
        "            label_str_processed = \"\"\n",
        "            if isinstance(label_str_original, str) and label_str_original.strip():\n",
        "                temp_label = label_str_original.strip()\n",
        "                match = re.search(r\"(\\[[A-Z0-9_]+\\])\", temp_label)\n",
        "                if match and match.group(1) == temp_label: label_str_processed = temp_label\n",
        "                elif match: label_str_processed = match.group(1)\n",
        "                elif not (temp_label.startswith('[') and temp_label.endswith(']')):\n",
        "                    sanitized_content = re.sub(r'\\s+', '_', temp_label)\n",
        "                    sanitized_content = re.sub(r'[^a-zA-Z0-9_]', '', sanitized_content).upper()\n",
        "                    sanitized_content = \"_\".join(sanitized_content.split('_')[:3])\n",
        "                    if sanitized_content: label_str_processed = f\"[{sanitized_content}]\"\n",
        "\n",
        "            # Update item with potentially fixed label for consistent validation and use\n",
        "            # If label_str_processed is empty (e.g. original was empty or sanitization failed),\n",
        "            # it will fail the bool(label_str_for_validation) check later.\n",
        "            item['label'] = label_str_processed\n",
        "\n",
        "            label_to_validate = item.get('label',\"\") # Already processed\n",
        "            desc_str = item.get('description',\"\")\n",
        "            sf_list = item.get('surface_forms', [])\n",
        "\n",
        "            has_all_keys = all(k in item for k in [\"label\", \"description\", \"surface_forms\"])\n",
        "            label_is_valid = isinstance(label_to_validate, str) and bool(label_to_validate) and \\\n",
        "                             label_to_validate.startswith('[') and label_to_validate.endswith(']') and \\\n",
        "                             re.fullmatch(r\"\\[[A-Z0-9_]+\\]\", label_to_validate)\n",
        "            desc_is_valid = isinstance(desc_str, str)\n",
        "            sfs_list_is_valid = isinstance(sf_list, list) and \\\n",
        "                                all(isinstance(sf_item, str) for sf_item in sf_list)\n",
        "\n",
        "            if has_all_keys and label_is_valid and desc_is_valid and sfs_list_is_valid:\n",
        "                valid_motifs_from_json.append({\n",
        "                    \"label\": label_to_validate,\n",
        "                    \"description\": desc_str.strip(),\n",
        "                    \"surface_forms\": [s.strip() for s in sf_list if isinstance(s, str) and s.strip()]\n",
        "                })\n",
        "            else:\n",
        "                print(f\"    [WARN] Invalid motif object schema for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}. Skipping.\")\n",
        "                with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} --- ITEM_SCHEMA_FAILURE (Item {item_idx+1}) ---\\n\")\n",
        "                    f.write(f\"Original Label: '{label_str_original}', Processed Label for Validation: '{label_to_validate}'\\n\")\n",
        "                    f.write(f\"Item Content: {json.dumps(item, indent=2)}\\n\")\n",
        "                    f.write(f\"Validation: keys={has_all_keys}, label={label_is_valid}, desc={desc_is_valid}, sfs_list={sfs_list_is_valid}\\n\")\n",
        "        return valid_motifs_from_json\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        print(f\"    [WARN] Motif JSON parsing/core structure issue for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} --- JSON_PARSE_VALUE_ERROR ---\\n\")\n",
        "            prompt_parts = prompt_sent_to_llm.split('Set of comments to analyze:')\n",
        "            user_content_for_log = prompt_parts[1][:500] if len(prompt_parts) > 1 else (prompt_sent_to_llm[:500] if prompt_sent_to_llm else \"PROMPT_EMPTY\")\n",
        "            f.write(f\"PROMPT USER CONTENT (approx first 500 chars):\\n{user_content_for_log}...\\n\")\n",
        "            f.write(f\"RAW LLM RESPONSE (Error: {type(e).__name__}):\\n{llm_raw_response_text}\\n\")\n",
        "            f.write(f\"EXTRACTED JSON CANDIDATE (Error: {type(e).__name__}):\\n{json_str_candidate}\\n\")\n",
        "        return []\n",
        "\n",
        "def get_motifs_for_qid_batched(\n",
        "    list_of_individual_response_texts: List[str],\n",
        "    responses_per_batch: int,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str\n",
        "    ) -> List[Dict]:\n",
        "    all_raw_motifs_from_chunks = []\n",
        "    batched_text_chunks_for_llm = []\n",
        "    for i in range(0, len(list_of_individual_response_texts), responses_per_batch):\n",
        "        batch_responses = list_of_individual_response_texts[i:i + responses_per_batch]\n",
        "        chunk_text_for_llm = preprocess_corpus_for_motif_extraction(\"\\n\\n<RSP_SEP>\\n\\n\".join(batch_responses))\n",
        "        batched_text_chunks_for_llm.append(chunk_text_for_llm)\n",
        "\n",
        "    print(f\"  QID {qid_for_log}: Processing {len(list_of_individual_response_texts)} responses in {len(batched_text_chunks_for_llm)} preprocessed chunks (batch size: {responses_per_batch} responses).\")\n",
        "\n",
        "    for chunk_idx, text_chunk_to_analyze_processed in enumerate(batched_text_chunks_for_llm):\n",
        "        print(f\"    Analyzing chunk {chunk_idx + 1}/{len(batched_text_chunks_for_llm)} for QID {qid_for_log} (processed chunk len: {len(text_chunk_to_analyze_processed)} chars)...\")\n",
        "        if len(text_chunk_to_analyze_processed.strip()) < 50:\n",
        "            print(f\"      Chunk {chunk_idx+1} (QID {qid_for_log}) too short after preprocessing, skipping.\")\n",
        "            continue\n",
        "\n",
        "        # build_llm_prompt_for_motifs is from Cell 3\n",
        "        prompt_for_llm = build_llm_prompt_for_motifs(text_chunk_to_analyze_processed)\n",
        "\n",
        "        motifs_from_this_chunk = []\n",
        "        for attempt in range(LLM_RETRY_ATTEMPTS):\n",
        "            # call_local_llm_for_raw_response is from Cell 3\n",
        "            raw_llm_response = call_local_llm_for_raw_response(\n",
        "                prompt_for_llm, hf_pipeline_instance, hf_tokenizer_instance, qid_for_log, chunk_idx + 1\n",
        "            )\n",
        "            if not raw_llm_response:\n",
        "                print(f\"      LLM call attempt {attempt + 1} for chunk {chunk_idx+1} (QID {qid_for_log}) returned empty. Retrying if possible...\")\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "                continue\n",
        "\n",
        "            # parse_and_validate_llm_json_response is defined in this cell\n",
        "            parsed_and_validated_motifs = parse_and_validate_llm_json_response(\n",
        "                raw_llm_response, qid_for_log, chunk_idx+1, prompt_for_llm\n",
        "            )\n",
        "            if parsed_and_validated_motifs:\n",
        "                motifs_from_this_chunk = parsed_and_validated_motifs\n",
        "                break\n",
        "            else:\n",
        "                print(f\"      Motif parsing/validation attempt {attempt + 1} yielded no structured motifs for chunk {chunk_idx+1}. Retrying if possible...\")\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "\n",
        "        if motifs_from_this_chunk:\n",
        "            print(f\"      Extracted {len(motifs_from_this_chunk)} structured motif objects from chunk {chunk_idx+1} (QID {qid_for_log}).\")\n",
        "            all_raw_motifs_from_chunks.extend(motifs_from_this_chunk)\n",
        "        else:\n",
        "            print(f\"      No valid structured motifs extracted from chunk {chunk_idx+1} (QID {qid_for_log}) after {LLM_RETRY_ATTEMPTS} attempts.\")\n",
        "\n",
        "    return all_raw_motifs_from_chunks\n",
        "\n",
        "def consolidate_raw_motifs(list_of_all_raw_motifs: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Consolidates motifs by label, merging surface forms (lowercased, unique, sorted).\"\"\"\n",
        "    if not list_of_all_raw_motifs: return []\n",
        "    consolidated_motifs_map = {}\n",
        "    for motif_obj in list_of_all_raw_motifs:\n",
        "        label = motif_obj.get(\"label\",\"\").strip() # Assumes label is already correctly formatted\n",
        "        description = motif_obj.get(\"description\",\"\").strip()\n",
        "        surface_forms = motif_obj.get(\"surface_forms\", [])\n",
        "        if not (label and isinstance(surface_forms, list)): continue\n",
        "\n",
        "        current_sfs_set = set(sf.lower().strip() for sf in surface_forms if isinstance(sf, str) and sf.strip())\n",
        "        if label not in consolidated_motifs_map:\n",
        "            consolidated_motifs_map[label] = {\n",
        "                \"label\": label,\n",
        "                \"description\": description,\n",
        "                \"surface_forms\": sorted(list(current_sfs_set))\n",
        "            }\n",
        "        else:\n",
        "            existing_sfs_set = set(consolidated_motifs_map[label].get(\"surface_forms\", []))\n",
        "            consolidated_motifs_map[label][\"surface_forms\"] = sorted(list(existing_sfs_set.union(current_sfs_set)))\n",
        "    return list(consolidated_motifs_map.values())\n",
        "\n",
        "def filter_surface_forms_by_global_frequency(\n",
        "    consolidated_motifs_list: List[Dict],\n",
        "    full_qid_corpus_text: str,\n",
        "    min_global_freq: int = MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"Filters SFs in consolidated motifs based on frequency in the full QID corpus.\"\"\"\n",
        "    if not consolidated_motifs_list: return []\n",
        "    final_globally_filtered_motifs = []\n",
        "    for motif_obj in consolidated_motifs_list:\n",
        "        globally_frequent_sfs_for_this_motif = []\n",
        "        # Surface forms from consolidation are already lowercased\n",
        "        original_sfs_for_this_motif = motif_obj.get(\"surface_forms\", [])\n",
        "        for sf_str_lower in original_sfs_for_this_motif:\n",
        "            # count_sf_occurrences (Cell 2) also lowercases corpus and sf for matching\n",
        "            count = count_sf_occurrences(full_qid_corpus_text, sf_str_lower)\n",
        "            if count >= min_global_freq:\n",
        "                globally_frequent_sfs_for_this_motif.append(sf_str_lower)\n",
        "        if globally_frequent_sfs_for_this_motif:\n",
        "            filtered_motif_entry = motif_obj.copy()\n",
        "            # SFs are already unique and lowercased, just sort\n",
        "            filtered_motif_entry[\"surface_forms\"] = sorted(globally_frequent_sfs_for_this_motif)\n",
        "            final_globally_filtered_motifs.append(filtered_motif_entry)\n",
        "    return final_globally_filtered_motifs\n",
        "\n",
        "print(\"Cell 4: Motif Processing and Validation Utilities loaded (with label fixing).\")"
      ],
      "metadata": {
        "id": "k26uJi3dAjHR"
      },
      "id": "k26uJi3dAjHR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5: MDL Calculations\n",
        "\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# Assume constants from Cell 1 are in global scope\n",
        "# Assume tokenize_phrase from Cell 2 is in global scope\n",
        "try:\n",
        "    MATRIX_SIZE_GLOBAL; MAX_TEXT_FOR_BDM_HASH; MOTIF_SYMBOLIC_LABEL_COST\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 5): Key config constants not found. Using fallbacks or expecting errors.\")\n",
        "    MATRIX_SIZE_GLOBAL = (8, 8); MAX_TEXT_FOR_BDM_HASH = 2000\n",
        "    MOTIF_SYMBOLIC_LABEL_COST = 0.5; MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "    MOTIF_DESCRIPTION_TOKEN_COST = 0.1; MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.0 # Matching Cell 1 for this experiment\n",
        "    MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.0 # Matching Cell 1 for this experiment\n",
        "\n",
        "\n",
        "def initialize_bdm_instance():\n",
        "    \"\"\"Initializes and returns a BDM instance.\"\"\"\n",
        "    print(\"Initializing BDM instance...\")\n",
        "    try:\n",
        "        bdm_instance = BDM(ndim=2)\n",
        "        print(\"BDM instance initialized successfully (ndim=2, default CTM-based).\")\n",
        "        return bdm_instance\n",
        "    except Exception as e_bdm_init:\n",
        "        print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\")\n",
        "        if \"CTM data files\" in str(e_bdm_init).lower() or \"dataset\" in str(e_bdm_init).lower():\n",
        "            print(\"  BDM Error Hint: This might be related to missing/corrupted CTM data files for PyBDM.\")\n",
        "            print(\"  Ensure PyBDM is installed correctly and can access/download its data.\")\n",
        "            print(\"  You might need to run once: from pybdm import get_ctm_dataset; get_ctm_dataset()\")\n",
        "        return None\n",
        "\n",
        "def text_to_binary_matrix(text_input: str, size: tuple = MATRIX_SIZE_GLOBAL) -> np.ndarray:\n",
        "    \"\"\"Converts a text string to a binary matrix using its SHA256 hash.\"\"\"\n",
        "    if not isinstance(text_input, str) or not text_input.strip():\n",
        "        return np.zeros(size, dtype=int)\n",
        "    hash_obj = hashlib.sha256(text_input.encode('utf-8', 'ignore'))\n",
        "    hash_digest = hash_obj.hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string_from_hash = bin(int(hash_digest, 16))[2:].zfill(256)\n",
        "    binary_string_for_matrix = binary_string_from_hash[:required_bits] if required_bits <= 256 else binary_string_from_hash.ljust(required_bits, '0')\n",
        "    bits_for_matrix = [int(b) for b in binary_string_for_matrix]\n",
        "    return np.array(bits_for_matrix).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input: str, bdm_instance: BDM, matrix_s: tuple = MATRIX_SIZE_GLOBAL) -> float:\n",
        "    \"\"\"Computes BDM for a given text string using a prefix for hashing.\"\"\"\n",
        "    if not isinstance(text_input, str) or not text_input.strip() :\n",
        "        return 0.0\n",
        "    text_for_hash = text_input[:MAX_TEXT_FOR_BDM_HASH] if len(text_input) > MAX_TEXT_FOR_BDM_HASH else text_input\n",
        "    if not text_for_hash.strip():\n",
        "        return 0.0\n",
        "    binary_matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        bdm_value = bdm_instance.bdm(binary_matrix)\n",
        "        return bdm_value\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0\n",
        "\n",
        "def calculate_L_H_token_based_structured(structured_motifs_list: List[Dict]) -> float:\n",
        "    \"\"\"Calculates L(H) - the cost of defining the list of structured motifs.\"\"\"\n",
        "    # This function uses MOTIF_* constants from Cell 1.\n",
        "    # The change to make SFs \"free\" in L(H) is done by setting those constants to 0 in Cell 1.\n",
        "    if not structured_motifs_list: return 0.0\n",
        "    total_lh_cost = 0.0\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        current_motif_lh = 0.0\n",
        "        label_str = motif_obj.get('label', \"\")\n",
        "        if isinstance(label_str, str) and label_str.strip():\n",
        "            current_motif_lh += MOTIF_SYMBOLIC_LABEL_COST\n",
        "        description_str = motif_obj.get('description', \"\")\n",
        "        if isinstance(description_str, str) and description_str.strip():\n",
        "            current_motif_lh += MOTIF_DESCRIPTION_TEXT_BASE_COST\n",
        "            current_motif_lh += len(tokenize_phrase(description_str)) * MOTIF_DESCRIPTION_TOKEN_COST # tokenize_phrase from Cell 2\n",
        "        surface_forms_list = motif_obj.get('surface_forms', [])\n",
        "        if isinstance(surface_forms_list, list) and surface_forms_list:\n",
        "            valid_sfs_for_lh = [sf for sf in surface_forms_list if isinstance(sf, str) and sf.strip()]\n",
        "            if valid_sfs_for_lh:\n",
        "                current_motif_lh += MOTIF_SURFACE_FORMS_LIST_BASE_COST # Will be 0 if constant is 0\n",
        "                for sf_str in valid_sfs_for_lh:\n",
        "                    current_motif_lh += len(tokenize_phrase(sf_str)) * MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH # Will be 0 if constant is 0\n",
        "        total_lh_cost += current_motif_lh\n",
        "    return total_lh_cost\n",
        "\n",
        "def llm_compress_text_structured(text_to_compress: str, structured_motifs_list: List[Dict]) -> str:\n",
        "    \"\"\"Compresses text by replacing occurrences of motif surface forms with their symbolic labels.\"\"\"\n",
        "    if not isinstance(text_to_compress, str): return \"\"\n",
        "    if not structured_motifs_list: return text_to_compress.lower()\n",
        "    compressed_text = text_to_compress.lower()\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        label = motif_obj.get('label', None)\n",
        "        surface_forms = motif_obj.get('surface_forms', [])\n",
        "        if not (isinstance(label, str) and label.strip()) or \\\n",
        "           not (isinstance(surface_forms, list) and surface_forms):\n",
        "            continue\n",
        "        placeholder = label\n",
        "        sorted_sfs_for_this_motif = sorted(\n",
        "            [sf for sf in surface_forms if isinstance(sf, str) and sf.strip()], key=len, reverse=True\n",
        "        )\n",
        "        for sf_str_lower in sorted_sfs_for_this_motif: # Assumes SFs are already lowercased from consolidation/filtering\n",
        "            try:\n",
        "                compressed_text = re.sub(r'\\b' + re.escape(sf_str_lower) + r'\\b', placeholder, compressed_text)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for SF '{sf_str_lower}' of motif '{label}': {re_e}. Skipping.\")\n",
        "                continue\n",
        "    return compressed_text\n",
        "\n",
        "def compute_mdl_cost_for_text_block(\n",
        "    full_qid_corpus_str: str,\n",
        "    final_motifs_to_evaluate: List[Dict],\n",
        "    bdm_instance: BDM,\n",
        "    matrix_s: tuple = MATRIX_SIZE_GLOBAL\n",
        "    ) -> tuple[float, float, float]:\n",
        "    \"\"\"Computes L(H), L(D|H), and Total MDL for a text block given a final set of motifs.\"\"\"\n",
        "    if not isinstance(full_qid_corpus_str, str): full_qid_corpus_str = \"\"\n",
        "    l_h = calculate_L_H_token_based_structured(final_motifs_to_evaluate)\n",
        "    compressed_text_block = llm_compress_text_structured(full_qid_corpus_str, final_motifs_to_evaluate)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "    if l_d_h < 0: return l_h, -1.0, -1.0\n",
        "    total_mdl_cost = l_h + l_d_h\n",
        "    return l_h, l_d_h, total_mdl_cost\n",
        "\n",
        "print(\"Cell 5: MDL Calculation Utilities loaded.\")"
      ],
      "metadata": {
        "id": "hda4nJw3Ap1x"
      },
      "id": "hda4nJw3Ap1x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6: Main Pipeline Orchestration\n",
        "\n",
        "# Assume functions from previous cells are defined\n",
        "\n",
        "def main():\n",
        "    script_version_name = \"Refactored MWP v6 (Simpler Prompt, Label Fix, L(H) SF Cost Zero)\"\n",
        "    print(f\"--- {script_version_name} ---\")\n",
        "    print(f\"Timestamp: {time.asctime()}\")\n",
        "    print(\"\\n--- Configuration Summary ---\")\n",
        "    print(f\"LLM Model: {LOCAL_LLM_MODEL_ID}, Quantization: {USE_QUANTIZATION_FOR_LOCAL_LLM}\")\n",
        "    print(f\"LLM Batch Size (Responses): {LLM_BATCH_SIZE_RESPONSES}, Retries: {LLM_RETRY_ATTEMPTS}\")\n",
        "    print(f\"Max Text Chars per LLM Prompt Chunk: {MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK}\")\n",
        "    print(f\"Max New Tokens for LLM Motif Extraction: {LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION}\")\n",
        "    print(f\"Max Motifs to Request per Chunk: {MAX_MOTIFS_PER_CHUNK}\")\n",
        "    print(f\"L(H) Costs: Label={MOTIF_SYMBOLIC_LABEL_COST}, DescBase={MOTIF_DESCRIPTION_TEXT_BASE_COST}, DescToken={MOTIF_DESCRIPTION_TOKEN_COST}, SFListBase={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, SFTokenInLH={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "    print(f\"Global SF Filtering Min Freq: {MIN_SF_FREQUENCY_IN_FULL_CORPUS}\")\n",
        "    print(f\"BDM Hash Prefix Length: {MAX_TEXT_FOR_BDM_HASH}, BDM Matrix: {MATRIX_SIZE_GLOBAL}\")\n",
        "    print(f\"Debug Log File: {LLM_DEBUG_LOG_FILE}\")\n",
        "    print(\"--- End Configuration Summary ---\\n\")\n",
        "\n",
        "    try:\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"LLM Motif Debug Log - Run Started: {time.asctime()}\\n\")\n",
        "            f.write(f\"Script Version: {script_version_name}\\n\")\n",
        "            f.write(f\"Model ID: {LOCAL_LLM_MODEL_ID}\\n\")\n",
        "            f.write(f\"Pipeline Config: return_full_text=False\\n\") # From initialize_llm_pipeline default\n",
        "            f.write(f\"L(H) SF Costs: Base={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, Token={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\\n---\\n\")\n",
        "    except Exception as e_log: print(f\"WARN: Could not initialize debug log file {LLM_DEBUG_LOG_FILE}: {e_log}\")\n",
        "\n",
        "    hf_pipeline_instance, hf_tokenizer_instance = initialize_llm_pipeline() # From Cell 3\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance: print(\"CRITICAL: Exiting due to LLM pipeline init failure.\"); return\n",
        "\n",
        "    bdm_instance_main = initialize_bdm_instance() # From Cell 5\n",
        "    if not bdm_instance_main: print(\"CRITICAL: Exiting due to BDM init failure.\"); return\n",
        "\n",
        "    if not os.path.exists(P2_COLLATED_FILE): print(f\"ERROR: Phase 2 file {P2_COLLATED_FILE} not found.\"); return\n",
        "    print(f\"Loading Phase 2 data from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f: phase2_data_content = json.load(f)\n",
        "    except Exception as e_load: print(f\"Error loading {P2_COLLATED_FILE}: {e_load}\"); return\n",
        "\n",
        "    all_qid_mdl_results_list = []\n",
        "    aggregated_content_by_qid_from_file = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "    if not aggregated_content_by_qid_from_file: print(f\"No 'aggregated_pdf_content_by_qid' in {P2_COLLATED_FILE}.\"); return\n",
        "\n",
        "    qids_to_process_this_run = [] # From Cell 1\n",
        "    if P3_QIDS_TO_PROCESS_THEMATICALLY and isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and P3_QIDS_TO_PROCESS_THEMATICALLY:\n",
        "        qids_to_process_this_run = [qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY if qid in aggregated_content_by_qid_from_file]\n",
        "        if not qids_to_process_this_run: print(f\"Warning: QIDs {P3_QIDS_TO_PROCESS_THEMATICALLY} not found. Exiting.\"); return\n",
        "    else:\n",
        "        qids_to_process_limit_fallback = 1\n",
        "        print(f\"P3_QIDS_TO_PROCESS_THEMATICALLY not set/empty. Processing up to {qids_to_process_limit_fallback} QID(s) as fallback.\")\n",
        "        qids_to_process_this_run = list(aggregated_content_by_qid_from_file.keys())[:qids_to_process_limit_fallback]\n",
        "        if not qids_to_process_this_run: print(\"No QIDs in data for fallback. Exiting.\"); return\n",
        "    if not qids_to_process_this_run: print(\"No QIDs selected. Exiting.\"); return\n",
        "    print(f\"\\nMDL analysis will run for QIDs: {qids_to_process_this_run}\\n\")\n",
        "\n",
        "    for qid_identifier_str in qids_to_process_this_run:\n",
        "        print(f\"--- Analyzing Data for QID: {qid_identifier_str} ---\")\n",
        "        list_of_individual_response_structs = aggregated_content_by_qid_from_file.get(qid_identifier_str, [])\n",
        "        actual_response_texts_for_qid = [item.get(\"text\", \"\") for item in list_of_individual_response_structs if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()]\n",
        "        if not actual_response_texts_for_qid: print(f\"  No valid text for QID {qid_identifier_str}. Skipping.\"); print(\"-\" * 50); continue\n",
        "        full_corpus_text_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(actual_response_texts_for_qid)\n",
        "        if len(full_corpus_text_for_qid.strip()) < 100: print(f\"  Skipping QID {qid_identifier_str}: text too short.\"); print(\"-\" * 50); continue\n",
        "        num_total_responses_for_qid = len(actual_response_texts_for_qid)\n",
        "        print(f\"  Corpus for QID {qid_identifier_str}: {len(full_corpus_text_for_qid)} chars, {num_total_responses_for_qid} responses.\")\n",
        "\n",
        "        baseline_bdm_original_corpus = compute_bdm_for_text(full_corpus_text_for_qid, bdm_instance_main) # Removed matrix_s, uses default\n",
        "        if baseline_bdm_original_corpus < 0: print(f\"  Error computing baseline BDM for QID {qid_identifier_str}. Skipping.\"); print(\"-\" * 50); continue\n",
        "        current_qid_baseline_mdl_cost = baseline_bdm_original_corpus\n",
        "        print(f\"  Baseline MDL for QID {qid_identifier_str} (L(D_orig)): {current_qid_baseline_mdl_cost:.4f}\")\n",
        "\n",
        "        # get_motifs_for_qid_batched is from Cell 4\n",
        "        raw_motifs_from_chunks = get_motifs_for_qid_batched(\n",
        "            actual_response_texts_for_qid, LLM_BATCH_SIZE_RESPONSES,\n",
        "            hf_pipeline_instance, hf_tokenizer_instance, qid_identifier_str\n",
        "        )\n",
        "        current_qid_result_entry = {\n",
        "            \"qid\": qid_identifier_str, \"corpus_len_chars\": len(full_corpus_text_for_qid), \"num_responses\": num_total_responses_for_qid,\n",
        "            \"baseline_mdl\": current_qid_baseline_mdl_cost, \"final_refined_motifs\": [], \"l_h_final_motifs\": 0.0,\n",
        "            \"l_d_h_final_motifs\": current_qid_baseline_mdl_cost, \"total_mdl_with_final_motifs\": current_qid_baseline_mdl_cost,\n",
        "            \"compression_achieved\": 0.0, \"num_raw_motifs_extracted\": len(raw_motifs_from_chunks),\n",
        "            \"num_consolidated_motifs\": 0, \"num_globally_refined_motifs\": 0\n",
        "        }\n",
        "        if not raw_motifs_from_chunks: print(f\"  No raw motifs by LLM for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "        print(f\"  Extracted {len(raw_motifs_from_chunks)} raw motif objects for QID {qid_identifier_str}.\")\n",
        "\n",
        "        consolidated_motifs_list = consolidate_raw_motifs(raw_motifs_from_chunks) # From Cell 4\n",
        "        current_qid_result_entry[\"num_consolidated_motifs\"] = len(consolidated_motifs_list)\n",
        "        print(f\"  Consolidated into {len(consolidated_motifs_list)} unique motifs for QID {qid_identifier_str}.\")\n",
        "        if not consolidated_motifs_list: print(f\"  No unique motifs after consolidation for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        # print(f\"  Consolidated Motifs (BEFORE Global SF refinement):\")\n",
        "        # for idx, mo_con in enumerate(consolidated_motifs_list): print(f\"    Cons. Motif {idx+1}: L='{mo_con.get('label')}', D='{mo_con.get('description','N/A')[:30]}...', SFs({len(mo_con.get('surface_forms',[]))})='{mo_con.get('surface_forms',[])[:2]}...'\")\n",
        "\n",
        "        # filter_surface_forms_by_global_frequency from Cell 4\n",
        "        globally_refined_motifs = filter_surface_forms_by_global_frequency(\n",
        "            consolidated_motifs_list, full_corpus_text_for_qid, MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "        )\n",
        "        current_qid_result_entry[\"num_globally_refined_motifs\"] = len(globally_refined_motifs)\n",
        "        print(f\"  Globally refined into {len(globally_refined_motifs)} motifs for QID {qid_identifier_str}.\")\n",
        "        if not globally_refined_motifs: print(f\"  No motifs left after GLOBAL SF refinement for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  Final Globally Refined Motifs for QID {qid_identifier_str}:\")\n",
        "        for idx, mo_final in enumerate(globally_refined_motifs): print(f\"    Refined Motif {idx+1}: L='{mo_final.get('label')}', D='{mo_final.get('description','N/A')[:60]}...', SFs({len(mo_final.get('surface_forms',[]))})='{mo_final.get('surface_forms',[])}'\")\n",
        "\n",
        "        # compute_mdl_cost_for_text_block from Cell 5\n",
        "        l_h_final, l_d_h_final, total_mdl_final = compute_mdl_cost_for_text_block(\n",
        "            full_corpus_text_for_qid, globally_refined_motifs, bdm_instance_main # Removed matrix_s, uses default\n",
        "        )\n",
        "        current_qid_result_entry.update({\n",
        "            \"final_refined_motifs\": globally_refined_motifs, \"l_h_final_motifs\": l_h_final,\n",
        "            \"l_d_h_final_motifs\": l_d_h_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"total_mdl_with_final_motifs\": total_mdl_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"compression_achieved\": \"BDM_ERROR\" if l_d_h_final < 0 else (current_qid_baseline_mdl_cost - total_mdl_final)\n",
        "        })\n",
        "        if l_d_h_final < 0: print(f\"  Error computing MDL cost (BDM error) for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  L(H) final motifs: {l_h_final:.4f} (SFs definition cost in L(H) is ZEROED for this run)\")\n",
        "        print(f\"  L(D|H) compressed full corpus: {l_d_h_final:.4f}\")\n",
        "        print(f\"  Total MDL cost with final motifs: {total_mdl_final:.4f}\")\n",
        "        compression_val = current_qid_result_entry[\"compression_achieved\"]\n",
        "        result_status_str = f\"SUCCESS: Comp: {compression_val:.4f}\" if isinstance(compression_val, float) and compression_val > 0.0001 else f\"NOTE: No sig. comp. Diff: {compression_val if isinstance(compression_val, str) else compression_val:.4f}\"\n",
        "        print(f\"  {result_status_str}\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50)\n",
        "\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary ---\")\n",
        "    if not all_qid_mdl_results_list: print(\"No QIDs processed.\")\n",
        "    else:\n",
        "        valid_results = [r for r in all_qid_mdl_results_list if isinstance(r.get('compression_achieved'), float) and r.get('l_h_final_motifs', -1.0) >= 0]\n",
        "        num_qids_ok = len(valid_results); num_comp = sum(1 for r in valid_results if r['compression_achieved'] > 0.0001)\n",
        "        print(f\"Targeted QIDs: {len(qids_to_process_this_run)}, Results logged: {len(all_qid_mdl_results_list)}, Valid MDL: {num_qids_ok}, QIDs compressed: {num_comp}\")\n",
        "        if num_comp > 0:\n",
        "            comp_vals = [r['compression_achieved'] for r in valid_results if r['compression_achieved'] > 0.0001]\n",
        "            print(f\"  Avg compression: {np.mean(comp_vals):.4f}, Max compression: {np.max(comp_vals):.4f}\")\n",
        "        else: print(\"  No compression achieved.\")\n",
        "\n",
        "        output_filename = os.path.join(BASE_PROJECT_DIR, \"mdl_analysis_refactored_v6_LHSFzero.json\") # MODIFIED FILENAME\n",
        "        try:\n",
        "            with open(output_filename, \"w\", encoding=\"utf-8\") as f_out: json.dump(all_qid_mdl_results_list, f_out, indent=2, ensure_ascii=False)\n",
        "            print(f\"Detailed results saved to {output_filename}\")\n",
        "        except Exception as e_s: print(f\"Error saving results: {e_s}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Executing main MDL pipeline (Refactored v6 - L(H) SF Cost Zero) at {time.asctime()}...\")\n",
        "    main()\n",
        "    print(f\"Main MDL pipeline execution finished at {time.asctime()}.\")"
      ],
      "metadata": {
        "id": "AC5PWCRQAvH4"
      },
      "id": "AC5PWCRQAvH4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2nd June"
      ],
      "metadata": {
        "id": "3hL7BgGCSeTX"
      },
      "id": "3hL7BgGCSeTX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continue from yesterday's refactored solution"
      ],
      "metadata": {
        "id": "EHzjRt9QSqBW"
      },
      "id": "EHzjRt9QSqBW"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3: LLM Interaction (MODIFIED create_enhanced_motif_prompt)\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import time\n",
        "from typing import List, Dict\n",
        "import os # Added for LLM_DEBUG_LOG_FILE\n",
        "\n",
        "# Assume constants like LOCAL_LLM_MODEL_ID, MAX_MOTIFS_PER_CHUNK, etc., are loaded from Cell 1\n",
        "# For standalone testing of this cell, you might need to define them here or use try-except.\n",
        "try:\n",
        "    LOCAL_LLM_MODEL_ID\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 3): Config constants not found from Cell 1. Using fallbacks.\")\n",
        "    LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'; USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "    MAX_MOTIFS_PER_CHUNK = 5; MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "    LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 700; LLM_DEBUG_LOG_FILE = \"temp_llm_debug_log_cell3.txt\"\n",
        "\n",
        "\n",
        "def initialize_llm_pipeline(\n",
        "    model_id: str = LOCAL_LLM_MODEL_ID,\n",
        "    use_quantization: bool = USE_QUANTIZATION_FOR_LOCAL_LLM,\n",
        "    pipeline_return_full_text: bool = False\n",
        "    ):\n",
        "    # ... (keep this function exactly as in your last working version) ...\n",
        "    # (It correctly sets pad_token, quantization, etc.)\n",
        "    print(f\"--- Initializing LLM Pipeline (model: {model_id}, quantization: {use_quantization}, return_full_text: {pipeline_return_full_text}) ---\")\n",
        "    hf_pipeline_instance = None; hf_tokenizer_instance = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {model_id}...\"); hf_tokenizer_instance = AutoTokenizer.from_pretrained(model_id)\n",
        "        if hf_tokenizer_instance.pad_token is None:\n",
        "            if hf_tokenizer_instance.eos_token is not None: print(\"Tokenizer setting pad_token = eos_token.\"); hf_tokenizer_instance.pad_token = hf_tokenizer_instance.eos_token\n",
        "            else: print(\"WARN (initialize_llm): Tokenizer has no pad_token and no eos_token.\")\n",
        "        bnb_config = None; quant_active = False\n",
        "        if use_quantization and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if (device.type == 'cuda' and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n",
        "                quant_active = True; print(f\"BNB config created, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb: print(f\"WARN: Failed BitsAndBytesConfig: {e_bnb}. Quantization may be disabled.\"); quant_active = False\n",
        "        print(f\"Loading model {model_id} (Quantization: {quant_active})...\"); model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "        if quant_active and bnb_config: model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        elif device.type == 'cuda': model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "        hf_model_instance = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "        if hf_tokenizer_instance.pad_token_id is not None:\n",
        "            if hf_model_instance.config.pad_token_id is None or hf_model_instance.config.pad_token_id != hf_tokenizer_instance.pad_token_id:\n",
        "                hf_model_instance.config.pad_token_id = hf_tokenizer_instance.pad_token_id\n",
        "                print(f\"Model config pad_token_id set to: {hf_model_instance.config.pad_token_id}\")\n",
        "        hf_pipeline_instance = pipeline(\"text-generation\", model=hf_model_instance, tokenizer=hf_tokenizer_instance, return_full_text=pipeline_return_full_text)\n",
        "        print(f\"LLM pipeline initialized for {model_id}.\"); return hf_pipeline_instance, hf_tokenizer_instance\n",
        "    except Exception as e: print(f\"CRITICAL: LLM pipeline init failed: {e}\"); import traceback; traceback.print_exc(); return None, None\n",
        "\n",
        "\n",
        "def create_enhanced_motif_prompt(text_corpus_chunk: str, max_motifs_to_extract: int = MAX_MOTIFS_PER_CHUNK) -> str: # Renamed for clarity\n",
        "    \"\"\"\n",
        "    Revised prompt for extracting structured motifs, with very strong emphasis on JSON and label format.\n",
        "    \"\"\"\n",
        "    if len(text_corpus_chunk) > MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK:\n",
        "        text_corpus_chunk = text_corpus_chunk[:MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK]\n",
        "\n",
        "    prompt = f\"\"\"You are a highly precise assistant for thematic analysis. Your task is to extract key recurring themes from the provided text.\n",
        "\n",
        "STRICT OUTPUT REQUIREMENTS:\n",
        "1.  Your entire response MUST be a single, valid JSON list.\n",
        "2.  Each element in the list MUST be a JSON object.\n",
        "3.  Each JSON object MUST contain exactly three keys: \"label\", \"description\", and \"surface_forms\".\n",
        "4.  The value for \"label\" MUST be a string, IN ALL_CAPITAL_SNAKE_CASE, AND enclosed in square brackets. Example: \"[DATA_SECURITY_POLICY]\".\n",
        "5.  The value for \"description\" MUST be a single, concise sentence (string).\n",
        "6.  The value for \"surface_forms\" MUST be a JSON list of 2 to 3 short (2-6 words) VERBATIM phrases extracted DIRECTLY from the 'Text to analyze'. These phrases must be strong examples of the theme. If no suitable verbatim phrases are found, provide an empty list `[]`.\n",
        "7.  Identify up to {max_motifs_to_extract} themes. If fewer than {max_motifs_to_extract} themes are clear, provide fewer objects. If no themes are clear, output an empty JSON list: `[]`.\n",
        "8.  Do NOT include any text, explanations, apologies, or markdown (like ```json) before or after the main JSON list.\n",
        "\n",
        "INSTRUCTIONS FOR THEME IDENTIFICATION:\n",
        "- Focus on meaningful recurring concepts directly stated or strongly implied in the 'Text to analyze'.\n",
        "- For 'surface_forms', prioritize phrases that appear to be REPEATED or are highly characteristic of the theme.\n",
        "- Avoid generic labels like [EXAMPLE_THEME] or [GENERAL_TOPIC]. Make labels specific.\n",
        "\n",
        "Text to analyze:\n",
        "\\\"\\\"\\\"\n",
        "{text_corpus_chunk}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Your valid JSON response:\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def call_local_llm_for_raw_response(\n",
        "    prompt_content_for_user_turn: str,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str,\n",
        "    chunk_idx_for_log: int\n",
        "    ) -> str:\n",
        "    # ... (keep this function exactly as in your last working version) ...\n",
        "    # (It uses do_sample=False and should work with the above prompt)\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance:\n",
        "        print(f\"    ERROR (call_local_llm): LLM pipeline/tokenizer not initialized for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return \"\"\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": prompt_content_for_user_turn}]\n",
        "    try:\n",
        "        prompt_formatted_for_llm = hf_tokenizer_instance.apply_chat_template(\n",
        "            messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception as e_template:\n",
        "        print(f\"    ERROR (call_local_llm): Applying chat template failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_template}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm_for_raw_response) ---\\nERROR APPLYING CHAT TEMPLATE: {e_template}\\nUser prompt content (first 300 chars): {prompt_content_for_user_turn[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION,\n",
        "        \"do_sample\": False,\n",
        "        \"pad_token_id\": hf_tokenizer_instance.pad_token_id\n",
        "    }\n",
        "    try:\n",
        "        outputs = hf_pipeline_instance(prompt_formatted_for_llm, **generation_args)\n",
        "        if outputs and isinstance(outputs, list) and len(outputs) > 0 and \\\n",
        "           outputs[0] and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
        "            assistant_response_text = outputs[0]['generated_text'].strip()\n",
        "            return assistant_response_text\n",
        "        else:\n",
        "            print(f\"    WARN (call_local_llm): LLM pipeline returned unexpected structure for QID {qid_for_log}, Chunk {chunk_idx_for_log}. Output: {outputs}\")\n",
        "            return \"\"\n",
        "    except Exception as e_pipeline:\n",
        "        print(f\"    ERROR (call_local_llm): Exception during hf_pipeline call for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_pipeline}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm_for_raw_response) ---\\nERROR DURING PIPELINE CALL: {e_pipeline}\\nFormatted prompt (first 300 chars): {prompt_formatted_for_llm[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"Cell 3: LLM Interaction Utilities loaded (with enhanced prompt).\")"
      ],
      "metadata": {
        "id": "aBFGNvzoShiX"
      },
      "id": "aBFGNvzoShiX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4: Motif Processing and Validation (MODIFIED parse_and_validate_llm_json_response)\n",
        "\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict\n",
        "from collections import Counter\n",
        "\n",
        "# Assume constants like LLM_DEBUG_LOG_FILE, MIN_SF_FREQUENCY_IN_FULL_CORPUS etc. are loaded from Cell 1\n",
        "# Assume text_utils functions like preprocess_corpus_for_motif_extraction, count_sf_occurrences are loaded from Cell 2\n",
        "# Assume LLM interaction functions like create_enhanced_motif_prompt, call_local_llm_for_raw_response are loaded from Cell 3\n",
        "\n",
        "# For standalone testing of this cell:\n",
        "try:\n",
        "    LLM_DEBUG_LOG_FILE\n",
        "except NameError: LLM_DEBUG_LOG_FILE = \"temp_llm_debug_log_cell4.txt\"\n",
        "try:\n",
        "    MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "except NameError: MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "try:\n",
        "    LLM_RETRY_ATTEMPTS\n",
        "except NameError: LLM_RETRY_ATTEMPTS = 2\n",
        "\n",
        "\n",
        "def parse_and_validate_llm_json_response(\n",
        "    llm_raw_response_text: str,\n",
        "    qid_for_log:str,\n",
        "    chunk_idx_for_log:int,\n",
        "    prompt_sent_to_llm:str\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Parses the LLM's raw text response, attempts to fix labels if non-compliant,\n",
        "    and validates schema. Returns a list of valid motif dictionaries.\n",
        "    \"\"\"\n",
        "    json_str_candidate = llm_raw_response_text.strip()\n",
        "    if json_str_candidate.startswith(\"```json\"): json_str_candidate = json_str_candidate[len(\"```json\"):].strip()\n",
        "    if json_str_candidate.startswith(\"```\"): json_str_candidate = json_str_candidate[len(\"```\"):].strip()\n",
        "    if json_str_candidate.endswith(\"```\"): json_str_candidate = json_str_candidate[:-len(\"```\")].strip()\n",
        "\n",
        "    if not json_str_candidate or json_str_candidate.lower() == \"[]\" or \\\n",
        "       \"no_themes_found\" in json_str_candidate.lower() or \\\n",
        "       \"no clear motifs\" in json_str_candidate.lower():\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        parsed_data = json.loads(json_str_candidate)\n",
        "        if isinstance(parsed_data, dict): parsed_data = [parsed_data]\n",
        "        if not isinstance(parsed_data, list):\n",
        "            raise ValueError(\"Parsed JSON from LLM is not a list or a single object.\")\n",
        "\n",
        "        valid_motifs_from_json = []\n",
        "        for item_idx, item in enumerate(parsed_data):\n",
        "            if not isinstance(item, dict) or not item: # Skip non-dicts or empty dicts {}\n",
        "                # print(f\"    DEBUG (parse_validate): Skipping non-dict or empty item for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}: {item}\")\n",
        "                continue\n",
        "\n",
        "            # --- Label Processing and Validation ---\n",
        "            label_str_original = item.get('label', \"\")\n",
        "            label_str_for_validation = \"\" # This will hold the label used for validation\n",
        "\n",
        "            if isinstance(label_str_original, str) and label_str_original.strip():\n",
        "                temp_label_stripped = label_str_original.strip()\n",
        "                # Check if already correctly formatted\n",
        "                if temp_label_stripped.startswith('[') and temp_label_stripped.endswith(']') and \\\n",
        "                   re.fullmatch(r\"\\[[A-Z0-9_]+\\]\", temp_label_stripped): # Strict check for [UPPER_SNAKE_CASE]\n",
        "                    label_str_for_validation = temp_label_stripped\n",
        "                else:\n",
        "                    # Attempt to sanitize/fix if not perfectly formatted\n",
        "                    # Extract potential bracketed part first\n",
        "                    match_bracketed = re.search(r\"(\\[[A-Z0-9_]+\\])\", temp_label_stripped)\n",
        "                    if match_bracketed:\n",
        "                        label_str_for_validation = match_bracketed.group(1)\n",
        "                        # print(f\"    DEBUG (parse_validate): Extracted existing bracketed label '{label_str_for_validation}' from '{label_str_original}'\")\n",
        "                    else: # No bracketed part found, try to create one\n",
        "                        sanitized_content = re.sub(r'\\s+', '_', temp_label_stripped)\n",
        "                        sanitized_content = re.sub(r'[^a-zA-Z0-9_]', '', sanitized_content).upper() # Allow a-z for initial capture then uppercase\n",
        "                        sanitized_content = \"_\".join(sanitized_content.split('_')[:4]) # Limit length\n",
        "                        if sanitized_content:\n",
        "                            label_str_for_validation = f\"[{sanitized_content}]\"\n",
        "                            # print(f\"    DEBUG (parse_validate): Auto-formatted label from '{label_str_original}' to '{label_str_for_validation}'\")\n",
        "                        # If sanitization results in empty, label_str_for_validation remains \"\"\n",
        "\n",
        "            # item['label'] = label_str_for_validation # Update the item's label with the processed one\n",
        "                                                     # This is important if you reuse 'item' later\n",
        "\n",
        "            # --- Schema Validation using processed label ---\n",
        "            desc_str = item.get('description',\"\")\n",
        "            sf_list = item.get('surface_forms', [])\n",
        "\n",
        "            has_all_keys = all(k in item for k in [\"label\", \"description\", \"surface_forms\"])\n",
        "            label_is_valid_format = isinstance(label_str_for_validation, str) and \\\n",
        "                                    bool(label_str_for_validation) and \\\n",
        "                                    label_str_for_validation.startswith('[') and \\\n",
        "                                    label_str_for_validation.endswith(']') and \\\n",
        "                                    re.fullmatch(r\"\\[[A-Z0-9_]+\\]\", label_str_for_validation)\n",
        "\n",
        "            desc_is_valid = isinstance(desc_str, str) # Allow empty description string\n",
        "            sfs_are_valid_list = isinstance(sf_list, list) and \\\n",
        "                                 all(isinstance(sf_item, str) for sf_item in sf_list)\n",
        "\n",
        "            if has_all_keys and label_is_valid_format and desc_is_valid and sfs_are_valid_list:\n",
        "                valid_motifs_from_json.append({\n",
        "                    \"label\": label_str_for_validation,\n",
        "                    \"description\": desc_str.strip(),\n",
        "                    \"surface_forms\": [s.strip() for s in sf_list if isinstance(s, str) and s.strip()]\n",
        "                })\n",
        "            else: # Item failed detailed schema validation\n",
        "                print(f\"    [WARN] Invalid motif object schema after label processing for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}. Skipping.\")\n",
        "                # Log details of failure\n",
        "                with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} --- ITEM_SCHEMA_FAILURE (Item {item_idx+1}) ---\\n\")\n",
        "                    f.write(f\"Original Label: '{label_str_original}', Processed Label for Validation: '{label_str_for_validation}'\\n\")\n",
        "                    f.write(f\"Item Content: {json.dumps(item, indent=2)}\\n\") # Log the item state\n",
        "                    f.write(f\"Validation Details: has_keys={has_all_keys}, label_ok={label_is_valid_format}, desc_ok={desc_is_valid}, sf_list_ok={sfs_are_valid_list}\\n\")\n",
        "        return valid_motifs_from_json\n",
        "\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        print(f\"    [WARN] Motif JSON parsing or core structure issue for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e}\")\n",
        "        # ... (existing logging to file for JSONDecodeError) ...\n",
        "        return []\n",
        "\n",
        "def get_motifs_for_qid_batched( # This function remains structurally similar\n",
        "    list_of_individual_response_texts: List[str],\n",
        "    responses_per_batch: int,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str\n",
        "    ) -> List[Dict]:\n",
        "    # ... (Same as your last correct version, calling create_enhanced_motif_prompt,\n",
        "    #      call_local_llm_for_raw_response, and parse_and_validate_llm_json_response) ...\n",
        "    all_raw_motifs_from_chunks = []\n",
        "    batched_text_chunks_for_llm = []\n",
        "    for i in range(0, len(list_of_individual_response_texts), responses_per_batch):\n",
        "        batch_responses = list_of_individual_response_texts[i:i + responses_per_batch]\n",
        "        chunk_text_for_llm = preprocess_corpus_for_motif_extraction(\"\\n\\n<RSP_SEP>\\n\\n\".join(batch_responses)) # preprocess_corpus from Cell 2\n",
        "        batched_text_chunks_for_llm.append(chunk_text_for_llm)\n",
        "    print(f\"  QID {qid_for_log}: Processing {len(list_of_individual_response_texts)} responses in {len(batched_text_chunks_for_llm)} preprocessed chunks (batch size: {responses_per_batch} responses).\")\n",
        "    for chunk_idx, text_chunk_to_analyze_processed in enumerate(batched_text_chunks_for_llm):\n",
        "        print(f\"    Analyzing chunk {chunk_idx + 1}/{len(batched_text_chunks_for_llm)} for QID {qid_for_log} (processed chunk len: {len(text_chunk_to_analyze_processed)} chars)...\")\n",
        "        if len(text_chunk_to_analyze_processed.strip()) < 50:\n",
        "            print(f\"      Chunk {chunk_idx+1} (QID {qid_for_log}) too short, skipping.\"); continue\n",
        "        prompt_for_llm = create_enhanced_motif_prompt(text_chunk_to_analyze_processed) # create_enhanced_motif_prompt from Cell 3\n",
        "        motifs_from_this_chunk = []\n",
        "        for attempt in range(LLM_RETRY_ATTEMPTS): # LLM_RETRY_ATTEMPTS from config\n",
        "            raw_llm_response = call_local_llm_for_raw_response( # call_local_llm_for_raw_response from Cell 3\n",
        "                prompt_for_llm, hf_pipeline_instance, hf_tokenizer_instance, qid_for_log, chunk_idx + 1\n",
        "            )\n",
        "            if not raw_llm_response:\n",
        "                print(f\"      LLM call attempt {attempt + 1} for chunk {chunk_idx+1} (QID {qid_for_log}) returned empty. Retrying if possible...\");\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1); continue\n",
        "            parsed_motifs_from_this_attempt = parse_and_validate_llm_json_response( # Defined in this cell\n",
        "                raw_llm_response, qid_for_log, chunk_idx+1, prompt_for_llm\n",
        "            )\n",
        "            if parsed_motifs_from_this_attempt:\n",
        "                motifs_from_this_chunk = parsed_motifs_from_this_attempt; break\n",
        "            else:\n",
        "                print(f\"      Motif parsing/validation attempt {attempt + 1} yielded no structured motifs for chunk {chunk_idx+1}. Retrying if possible...\");\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "        if motifs_from_this_chunk:\n",
        "            print(f\"      Extracted {len(motifs_from_this_chunk)} structured motif objects from chunk {chunk_idx+1} (QID {qid_for_log}).\")\n",
        "            all_raw_motifs_from_chunks.extend(motifs_from_this_chunk)\n",
        "        else:\n",
        "            print(f\"      No valid structured motifs from chunk {chunk_idx+1} (QID {qid_for_log}) after {LLM_RETRY_ATTEMPTS} attempts.\")\n",
        "    return all_raw_motifs_from_chunks\n",
        "\n",
        "def consolidate_raw_motifs(list_of_all_raw_motifs: List[Dict]) -> List[Dict]:\n",
        "    # ... (Keep this function exactly as in your last working version) ...\n",
        "    # (It merges by label and combines/deduplicates surface_forms)\n",
        "    if not list_of_all_raw_motifs: return []\n",
        "    consolidated_motifs_map = {}\n",
        "    for motif_obj in list_of_all_raw_motifs:\n",
        "        label = motif_obj.get(\"label\",\"\").strip()\n",
        "        description = motif_obj.get(\"description\",\"\").strip()\n",
        "        surface_forms = motif_obj.get(\"surface_forms\", [])\n",
        "        if not (label and isinstance(surface_forms, list)): continue\n",
        "        current_sfs_set = set(sf.lower().strip() for sf in surface_forms if isinstance(sf, str) and sf.strip())\n",
        "        if label not in consolidated_motifs_map:\n",
        "            consolidated_motifs_map[label] = {\"label\": label, \"description\": description, \"surface_forms\": sorted(list(current_sfs_set))}\n",
        "        else:\n",
        "            existing_sfs_set = set(consolidated_motifs_map[label].get(\"surface_forms\", []))\n",
        "            consolidated_motifs_map[label][\"surface_forms\"] = sorted(list(existing_sfs_set.union(current_sfs_set)))\n",
        "    return list(consolidated_motifs_map.values())\n",
        "\n",
        "\n",
        "def filter_surface_forms_by_global_frequency(\n",
        "    consolidated_motifs_list: List[Dict],\n",
        "    full_qid_corpus_text: str,\n",
        "    min_global_freq: int = MIN_SF_FREQUENCY_IN_FULL_CORPUS # From config\n",
        "    ) -> List[Dict]:\n",
        "    # ... (Keep this function exactly as in your last working version) ...\n",
        "    # (It uses count_sf_occurrences from Cell 2)\n",
        "    if not consolidated_motifs_list: return []\n",
        "    final_globally_filtered_motifs = []\n",
        "    for motif_obj in consolidated_motifs_list:\n",
        "        globally_frequent_sfs_for_this_motif = []\n",
        "        original_sfs_for_this_motif = motif_obj.get(\"surface_forms\", [])\n",
        "        for sf_str in original_sfs_for_this_motif:\n",
        "            count = count_sf_occurrences(full_qid_corpus_text, sf_str) # count_sf_occurrences from Cell 2\n",
        "            if count >= min_global_freq:\n",
        "                globally_frequent_sfs_for_this_motif.append(sf_str)\n",
        "        if globally_frequent_sfs_for_this_motif:\n",
        "            filtered_motif_entry = motif_obj.copy()\n",
        "            filtered_motif_entry[\"surface_forms\"] = sorted(list(set(globally_frequent_sfs_for_this_motif)))\n",
        "            final_globally_filtered_motifs.append(filtered_motif_entry)\n",
        "    return final_globally_filtered_motifs\n",
        "\n",
        "print(\"Cell 4: Motif Processing and Validation Utilities loaded (with enhanced prompt and label fixing).\")"
      ],
      "metadata": {
        "id": "Xznnbv7DS1NA"
      },
      "id": "Xznnbv7DS1NA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1st June"
      ],
      "metadata": {
        "id": "56kuFYejYOE-"
      },
      "id": "56kuFYejYOE-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refactored from New MWP below"
      ],
      "metadata": {
        "id": "1EYXktNkIesv"
      },
      "id": "1EYXktNkIesv"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1: Configuration\n",
        "\n",
        "import os\n",
        "\n",
        "# --- Project Paths ---\n",
        "# !!! IMPORTANT: UPDATE BASE_PROJECT_DIR TO YOUR ACTUAL PATH !!!\n",
        "# Example for Google Colab:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# BASE_PROJECT_DIR = '/content/drive/MyDrive/YourFolder/LegalAnalysis/'\n",
        "# Example for local:\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # Matching your last successful run\n",
        "# Ensure BASE_PROJECT_DIR ends with a slash if it's a directory path\n",
        "\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"]\n",
        "\n",
        "# --- BDM Configuration ---\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "MAX_TEXT_FOR_BDM_HASH = 2000\n",
        "\n",
        "# --- LLM Configuration ---\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "LLM_BATCH_SIZE_RESPONSES = 5\n",
        "LLM_RETRY_ATTEMPTS = 2\n",
        "MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 700\n",
        "MAX_MOTIFS_PER_CHUNK = 5\n",
        "\n",
        "# --- Token-Based L(H) Configuration ---\n",
        "MOTIF_SYMBOLIC_LABEL_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TOKEN_COST = 0.1\n",
        "MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.25\n",
        "MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.1\n",
        "\n",
        "# --- Surface Form Filtering Configuration ---\n",
        "MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "\n",
        "# --- Logging File ---\n",
        "LLM_DEBUG_LOG_FILE = os.path.join(BASE_PROJECT_DIR, \"llm_motif_debug_log_refactored_v1.txt\") # New log file for refactored version\n",
        "\n",
        "print(f\"Cell 1: Configuration loaded. LOCAL_LLM_MODEL_ID set to '{LOCAL_LLM_MODEL_ID}'.\")\n",
        "print(f\"Debug log will be: {LLM_DEBUG_LOG_FILE}\")\n",
        "if not os.path.exists(BASE_PROJECT_DIR):\n",
        "    print(f\"WARNING: BASE_PROJECT_DIR '{BASE_PROJECT_DIR}' does not exist.\")\n",
        "if P2_COLLATED_FILE and not os.path.exists(P2_COLLATED_FILE):\n",
        "     print(f\"WARNING: P2_COLLATED_FILE '{P2_COLLATED_FILE}' does not exist. Data loading may fail.\")"
      ],
      "metadata": {
        "id": "Kz2OG2pSIkD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaadf84f-bae0-4d5d-a2c0-4e2f69ca5699"
      },
      "id": "Kz2OG2pSIkD1",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 1: Configuration loaded. LOCAL_LLM_MODEL_ID set to 'google/gemma-2b-it'.\n",
            "Debug log will be: /content/drive/MyDrive/Colab Notebooks/Legal/llm_motif_debug_log_refactored_v1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2: Text Utilities\n",
        "\n",
        "import re\n",
        "from typing import List, Dict # Added Dict for extract_actual_phrases\n",
        "from collections import Counter # For extract_actual_phrases_from_text\n",
        "\n",
        "# Import constants from Cell 1 (if running as separate cells in a notebook)\n",
        "# If this were a .py file, it would be: from config import MIN_SF_FREQ_IN_CHUNK_VALIDATION\n",
        "# For a notebook, we assume Cell 1's constants are in the global namespace after running it.\n",
        "# However, to make cells more independent if run out of order or for clarity:\n",
        "try:\n",
        "    MIN_SF_FREQ_IN_CHUNK_VALIDATION # Check if defined\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 2): MIN_SF_FREQ_IN_CHUNK_VALIDATION not found from config, using default 2.\")\n",
        "    MIN_SF_FREQ_IN_CHUNK_VALIDATION = 2\n",
        "\n",
        "\n",
        "def tokenize_phrase(phrase_text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Simple tokenizer for phrases, definitions, or surface forms.\n",
        "    Lowercases and splits by space.\n",
        "    \"\"\"\n",
        "    if not isinstance(phrase_text, str) or not phrase_text.strip():\n",
        "        return []\n",
        "    return phrase_text.lower().split()\n",
        "\n",
        "def preprocess_corpus_for_motif_extraction(text_corpus: str) -> str:\n",
        "    \"\"\"\n",
        "    Preprocesses a text corpus (typically a chunk of joined responses)\n",
        "    before sending to LLM or for n-gram extraction.\n",
        "    - Consolidates excessive newlines and spaces.\n",
        "    - Filters out very short lines (potential noise).\n",
        "    \"\"\"\n",
        "    if not isinstance(text_corpus, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text_corpus)\n",
        "    text = re.sub(r' {2,}', ' ', text)\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10 or not line.strip()]\n",
        "\n",
        "    return '\\n'.join(filtered_lines)\n",
        "\n",
        "def count_sf_occurrences(corpus_text: str, surface_form: str) -> int:\n",
        "    \"\"\"\n",
        "    Counts case-insensitive occurrences of a surface_form within the corpus_text.\n",
        "    \"\"\"\n",
        "    if not corpus_text or not surface_form or \\\n",
        "       not isinstance(corpus_text, str) or not isinstance(surface_form, str) or \\\n",
        "       not surface_form.strip(): # Don't count empty surface forms\n",
        "        return 0\n",
        "    try:\n",
        "        # Using word boundaries \\b to ensure we match whole phrases/words\n",
        "        # This might be too restrictive if SFs are substrings, but generally safer\n",
        "        # For simple phrase counting, \\b might not be needed if sf is multi-word\n",
        "        # Let's remove \\b for now to match previous behavior of simple substring count\n",
        "        return len(re.findall(re.escape(surface_form.lower()), corpus_text.lower(), flags=re.IGNORECASE))\n",
        "    except re.error as e:\n",
        "        print(f\"    [WARN] Regex error in count_sf_occurrences for SF '{surface_form}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def extract_actual_phrases_from_text(\n",
        "    text: str,\n",
        "    min_phrase_len: int = 2,\n",
        "    max_phrase_len: int = 6,\n",
        "    min_freq: int = MIN_SF_FREQ_IN_CHUNK_VALIDATION\n",
        "    ) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Extracts n-gram phrases (2 to 6 words by default) and their frequencies from text.\n",
        "    Only returns phrases meeting min_freq.\n",
        "    NOTE: This function is NOT currently used in the main \"reverted prompt\" pipeline flow,\n",
        "          but is kept as a utility for potential future chunk-level SF validation.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return {}\n",
        "\n",
        "    text_cleaned = text.lower()\n",
        "    text_cleaned = re.sub(r'[^\\w\\s\\']', ' ', text_cleaned)\n",
        "    text_cleaned = re.sub(r'\\s+', ' ', text_cleaned).strip()\n",
        "\n",
        "    words = text_cleaned.split()\n",
        "    if not words or len(words) < min_phrase_len:\n",
        "        return {}\n",
        "\n",
        "    phrase_counts = Counter()\n",
        "    for n in range(min_phrase_len, max_phrase_len + 1):\n",
        "        if n > len(words): continue\n",
        "        for i in range(len(words) - n + 1):\n",
        "            phrase_tokens = words[i:i+n]\n",
        "            phrase = ' '.join(phrase_tokens)\n",
        "            if phrase:\n",
        "                phrase_counts[phrase] += 1\n",
        "\n",
        "    recurring_phrases = {phrase: count for phrase, count in phrase_counts.items() if count >= min_freq}\n",
        "    return recurring_phrases\n",
        "\n",
        "print(\"Cell 2: Text Utilities loaded.\")"
      ],
      "metadata": {
        "id": "fisksZquVpnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc23128f-75f0-4a71-b8ee-97fdb579dce5"
      },
      "id": "fisksZquVpnW",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARN (Cell 2): MIN_SF_FREQ_IN_CHUNK_VALIDATION not found from config, using default 2.\n",
            "Cell 2: Text Utilities loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3: LLM Interaction\n",
        "\n",
        "import torch # Should be imported before transformers in some environments\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import time # For potential sleeps if needed, and debug log timestamping\n",
        "from typing import List # Already imported in Cell 1, but good practice for module independence\n",
        "\n",
        "# Import constants from Cell 1 (if running as separate cells in a notebook)\n",
        "# For a .py file, use: from config import LOCAL_LLM_MODEL_ID, ...\n",
        "# For notebooks, assume Cell 1's constants are in global scope.\n",
        "# Add checks or default values if a constant might be missing.\n",
        "try:\n",
        "    LOCAL_LLM_MODEL_ID\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 3): Config constants like LOCAL_LLM_MODEL_ID not found. Using fallback or expecting errors.\")\n",
        "    LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it' # Fallback\n",
        "    USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "    MAX_MOTIFS_PER_CHUNK = 5\n",
        "    MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "    LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 700\n",
        "    LLM_DEBUG_LOG_FILE = \"llm_interaction_debug_temp.txt\"\n",
        "\n",
        "\n",
        "def initialize_llm_pipeline(\n",
        "    model_id: str = LOCAL_LLM_MODEL_ID,\n",
        "    use_quantization: bool = USE_QUANTIZATION_FOR_LOCAL_LLM,\n",
        "    pipeline_return_full_text: bool = False # Defaulting to False as per our findings\n",
        "    ):\n",
        "    \"\"\"Initializes and returns the Hugging Face pipeline and tokenizer.\"\"\"\n",
        "    print(f\"--- Initializing LLM Pipeline (model: {model_id}, quantization: {use_quantization}, return_full_text: {pipeline_return_full_text}) ---\")\n",
        "\n",
        "    hf_pipeline_instance = None\n",
        "    hf_tokenizer_instance = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {model_id}...\")\n",
        "        hf_tokenizer_instance = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "        if hf_tokenizer_instance.pad_token is None:\n",
        "            if hf_tokenizer_instance.eos_token is not None:\n",
        "                print(\"Tokenizer does not have a pad_token; setting pad_token = eos_token.\")\n",
        "                hf_tokenizer_instance.pad_token = hf_tokenizer_instance.eos_token\n",
        "            else:\n",
        "                print(\"WARN (initialize_llm): Tokenizer has no pad_token and no eos_token. This might cause issues.\")\n",
        "                # Add a generic pad token if really needed, but usually models handle this with eos.\n",
        "                # hf_tokenizer_instance.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "        bnb_config = None\n",
        "        quant_active = False\n",
        "        if use_quantization and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if (device.type == 'cuda' and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=compute_dtype,\n",
        "                    bnb_4bit_use_double_quant=True\n",
        "                )\n",
        "                quant_active = True\n",
        "                print(f\"BitsAndBytesConfig created for {model_id}, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb:\n",
        "                print(f\"WARN: Failed to create BitsAndBytesConfig: {e_bnb}. Quantization may be disabled or fall back.\")\n",
        "                quant_active = False\n",
        "\n",
        "        print(f\"Loading local model {model_id} (Quantization active: {quant_active})...\")\n",
        "        model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "\n",
        "        if quant_active and bnb_config:\n",
        "            model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        elif device.type == 'cuda':\n",
        "             model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "        hf_model_instance = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "\n",
        "        # Set model's pad_token_id if tokenizer's was set to eos_token_id (or another valid ID)\n",
        "        if hf_tokenizer_instance.pad_token_id is not None:\n",
        "            if hf_model_instance.config.pad_token_id is None or \\\n",
        "               hf_model_instance.config.pad_token_id != hf_tokenizer_instance.pad_token_id:\n",
        "                hf_model_instance.config.pad_token_id = hf_tokenizer_instance.pad_token_id\n",
        "                print(f\"Model config pad_token_id set to: {hf_model_instance.config.pad_token_id}\")\n",
        "        else: # Should not happen if we set tokenizer.pad_token = tokenizer.eos_token and eos_token exists\n",
        "            print(\"WARN (initialize_llm): Tokenizer pad_token_id is None after attempting to set. Pipeline might use default.\")\n",
        "\n",
        "\n",
        "        hf_pipeline_instance = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=hf_model_instance,\n",
        "            tokenizer=hf_tokenizer_instance,\n",
        "            return_full_text=pipeline_return_full_text\n",
        "        )\n",
        "        print(f\"Local LLM pipeline for {model_id} initialized successfully.\")\n",
        "        return hf_pipeline_instance, hf_tokenizer_instance\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to initialize local LLM pipeline: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "def build_llm_prompt_for_motifs(text_block_for_prompt: str, max_motifs_to_extract: int = MAX_MOTIFS_PER_CHUNK) -> str:\n",
        "    \"\"\"Uses the simpler prompt structure that previously yielded bracketed labels.\"\"\"\n",
        "    if len(text_block_for_prompt) > MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK:\n",
        "        # print(f\"    Note (build_prompt): Text block for LLM prompt analysis truncated from {len(text_block_for_prompt)} to {MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK} chars.\")\n",
        "        text_block_for_prompt = text_block_for_prompt[:MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK]\n",
        "\n",
        "    # This is the prompt from the \"Old Successful Code Cell\"\n",
        "    prompt = f\"\"\"You will receive a set of comments from different people answering the same question.\n",
        "\n",
        "Your task is to identify up to {max_motifs_to_extract} key recurring themes.\n",
        "\n",
        "For each theme, provide:\n",
        "- A short label like [DATA_PRIVACY]\n",
        "- A 1-sentence description of the theme\n",
        "- 2–3 short phrases that often appear in the text (surface forms)\n",
        "\n",
        "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
        "Example of one object in the list:\n",
        "{{\n",
        "  \"label\": \"[EXAMPLE_LABEL]\",\n",
        "  \"description\": \"A concise description of the example theme.\",\n",
        "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
        "}}\n",
        "If no clear motifs are found, output an empty JSON list: `[]`.\n",
        "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
        "\n",
        "Set of comments to analyze:\n",
        "\\\"\\\"\\\"\n",
        "{text_block_for_prompt}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def call_local_llm_for_raw_response(\n",
        "    prompt_content_for_user_turn: str,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str,\n",
        "    chunk_idx_for_log: int\n",
        "    ) -> str:\n",
        "    \"\"\"\n",
        "    Makes the actual call to the local LLM pipeline using a pre-formatted user prompt string.\n",
        "    Returns the raw text string generated by the LLM.\n",
        "    Assumes pipeline is initialized with return_full_text=False.\n",
        "    \"\"\"\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance:\n",
        "        print(f\"    ERROR (call_local_llm): LLM pipeline/tokenizer not initialized for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return \"\"\n",
        "\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": prompt_content_for_user_turn}]\n",
        "\n",
        "    try:\n",
        "        prompt_formatted_for_llm = hf_tokenizer_instance.apply_chat_template(\n",
        "            messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception as e_template:\n",
        "        print(f\"    ERROR (call_local_llm): Applying chat template failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_template}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f: # LLM_DEBUG_LOG_FILE from config\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm_for_raw_response) ---\\n\")\n",
        "            f.write(f\"ERROR APPLYING CHAT TEMPLATE: {e_template}\\n\")\n",
        "            f.write(f\"User prompt content (first 300 chars): {prompt_content_for_user_turn[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION, # From config\n",
        "        \"do_sample\": False, # Key for consistency\n",
        "        \"pad_token_id\": hf_tokenizer_instance.pad_token_id # Ensure this is properly set\n",
        "    }\n",
        "    # print(f\"    DEBUG (call_local_llm): QID {qid_for_log}, Chunk {chunk_idx_for_log}, Prompt len: {len(prompt_formatted_for_llm)}, GenArgs: {generation_args}\")\n",
        "\n",
        "    try:\n",
        "        outputs = hf_pipeline_instance(prompt_formatted_for_llm, **generation_args)\n",
        "\n",
        "        # print(f\"    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{outputs}\\n>>>>>\")\n",
        "\n",
        "        if outputs and isinstance(outputs, list) and len(outputs) > 0 and \\\n",
        "           outputs[0] and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
        "            # Since pipeline is expected to be initialized with return_full_text=False,\n",
        "            # outputs[0]['generated_text'] should only be the new tokens.\n",
        "            assistant_response_text = outputs[0]['generated_text'].strip()\n",
        "            # print(f\"    DEBUG (call_local_llm): 'assistant_response_text' for QID {qid_for_log}, Chunk {chunk_idx_for_log} (len {len(assistant_response_text)}):\\n<<<<<\\n{assistant_response_text[:1000]}...\\n>>>>>\")\n",
        "            return assistant_response_text\n",
        "        else:\n",
        "            print(f\"    WARN (call_local_llm): LLM pipeline returned unexpected or empty structure for QID {qid_for_log}, Chunk {chunk_idx_for_log}. Output: {outputs}\")\n",
        "            return \"\"\n",
        "    except Exception as e_pipeline:\n",
        "        print(f\"    ERROR (call_local_llm): Exception during hf_pipeline call for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_pipeline}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f: # LLM_DEBUG_LOG_FILE from config\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm_for_raw_response) ---\\n\")\n",
        "            f.write(f\"ERROR DURING PIPELINE CALL: {e_pipeline}\\n\")\n",
        "            f.write(f\"Formatted prompt (first 300 chars): {prompt_formatted_for_llm[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"Cell 3: LLM Interaction Utilities loaded.\")"
      ],
      "metadata": {
        "id": "bguFULa0WSkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ed37f60-bc47-4c0d-ab3b-05745bfeb01b"
      },
      "id": "bguFULa0WSkt",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 3: LLM Interaction Utilities loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4: Motif Processing and Validation\n",
        "\n",
        "import json # Already imported in Cell 1, but good for module clarity\n",
        "import re   # Already imported in Cell 1\n",
        "import time # Already imported in Cell 1\n",
        "from typing import List, Dict # Already imported in Cell 1\n",
        "# from collections import Counter # Not directly used here, but extract_actual_phrases was in Cell 2\n",
        "\n",
        "# Import functions/constants from previous cells if needed for standalone cell execution (for .py files)\n",
        "# For notebooks, assume previous cells have run and defined them in global scope.\n",
        "# from text_utils import preprocess_corpus_for_motif_extraction, count_sf_occurrences # If these were in a separate file\n",
        "# from llm_interaction import build_llm_prompt_for_motifs, call_local_llm_for_raw_response # If these were in a separate file\n",
        "# from config import LLM_RETRY_ATTEMPTS, MIN_SF_FREQUENCY_IN_FULL_CORPUS, LLM_DEBUG_LOG_FILE # etc.\n",
        "\n",
        "# Add checks or default values for constants if a cell might be run independently\n",
        "try:\n",
        "    LLM_RETRY_ATTEMPTS\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 4): LLM_RETRY_ATTEMPTS not found from config, using default 2.\")\n",
        "    LLM_RETRY_ATTEMPTS = 2\n",
        "try:\n",
        "    MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 4): MIN_SF_FREQUENCY_IN_FULL_CORPUS not found from config, using default 2.\")\n",
        "    MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "try:\n",
        "    LLM_DEBUG_LOG_FILE\n",
        "except NameError:\n",
        "    LLM_DEBUG_LOG_FILE = \"temp_llm_debug_log_cell4.txt\"\n",
        "\n",
        "\n",
        "def parse_and_validate_llm_json_response(\n",
        "    llm_raw_response_text: str,\n",
        "    qid_for_log:str,\n",
        "    chunk_idx_for_log:int,\n",
        "    prompt_sent_to_llm:str # For logging context if prompt caused error\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Parses the LLM's raw text response, attempts to fix labels, and validates schema.\n",
        "    Returns a list of valid motif dictionaries, or an empty list on failure.\n",
        "    \"\"\"\n",
        "    json_str_candidate = llm_raw_response_text.strip()\n",
        "\n",
        "    # Attempt to remove markdown fences if LLM adds them\n",
        "    if json_str_candidate.startswith(\"```json\"):\n",
        "        json_str_candidate = json_str_candidate[len(\"```json\"):].strip()\n",
        "    if json_str_candidate.startswith(\"```\"): # More generic ```\n",
        "        json_str_candidate = json_str_candidate[len(\"```\"):].strip()\n",
        "    if json_str_candidate.endswith(\"```\"):\n",
        "        json_str_candidate = json_str_candidate[:-len(\"```\")].strip()\n",
        "\n",
        "    # print(f\"    DEBUG (parse_validate): QID {qid_for_log}, Chunk {chunk_idx_for_log}, JSON candidate for parsing:\\n{json_str_candidate[:500]}...\")\n",
        "\n",
        "    # Handle cases where LLM explicitly says no themes or returns empty list string\n",
        "    if not json_str_candidate or json_str_candidate.lower() == \"[]\" or \\\n",
        "       \"no_themes_found\" in json_str_candidate.lower() or \\\n",
        "       \"no clear motifs\" in json_str_candidate.lower():\n",
        "        # print(f\"    DEBUG (parse_validate): LLM indicated no themes or JSON was effectively empty for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        parsed_data = json.loads(json_str_candidate)\n",
        "\n",
        "        # Handle if LLM returns a single JSON object instead of a list\n",
        "        if isinstance(parsed_data, dict):\n",
        "            # print(f\"    DEBUG (parse_validate): LLM returned a single JSON object, wrapping in list for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "            parsed_data = [parsed_data]\n",
        "\n",
        "        if not isinstance(parsed_data, list):\n",
        "            # This error will be caught by the outer except block\n",
        "            raise ValueError(\"Parsed JSON from LLM is not a list (nor a single object that could be wrapped).\")\n",
        "\n",
        "        valid_motifs_from_json = []\n",
        "        for item_idx, item in enumerate(parsed_data):\n",
        "            if not item: # Skip empty dictionary {} items\n",
        "                # print(f\"    DEBUG (parse_validate): Skipping empty item object {{}} for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}\")\n",
        "                continue\n",
        "\n",
        "            # --- Attempt to Fix/Sanitize Label ---\n",
        "            label_str_original = item.get('label', \"\")\n",
        "            label_str_processed = \"\"\n",
        "\n",
        "            if isinstance(label_str_original, str) and label_str_original.strip():\n",
        "                temp_label = label_str_original.strip()\n",
        "                match = re.search(r\"(\\[[A-Z0-9_]+\\])\", temp_label) # Look for [UPPER_SNAKE_CASE]\n",
        "                if match and match.group(1) == temp_label: # Whole string is correctly bracketed\n",
        "                    label_str_processed = temp_label\n",
        "                elif match: # Found a bracketed part within a longer string (e.g., \"[LABEL] some text\")\n",
        "                    label_str_processed = match.group(1)\n",
        "                    # print(f\"    DEBUG (parse_validate): Extracted bracketed label '{label_str_processed}' from '{label_str_original}' for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}\")\n",
        "                elif not (temp_label.startswith('[') and temp_label.endswith(']')):\n",
        "                    # If no brackets, or not solely bracketed, attempt to sanitize and add them\n",
        "                    sanitized_content = re.sub(r'\\s+', '_', temp_label) # Replace spaces with underscores\n",
        "                    sanitized_content = re.sub(r'[^\\w_]', '', sanitized_content).upper() # Keep only alphanum & underscore, then uppercase\n",
        "                    sanitized_content = \"_\".join(sanitized_content.split('_')[:3]) # Limit to first 3 \"words\"\n",
        "                    if sanitized_content: # Ensure not empty after sanitization\n",
        "                        label_str_processed = f\"[{sanitized_content}]\"\n",
        "                        # print(f\"    DEBUG (parse_validate): Auto-formatted label from '{label_str_original}' to '{label_str_processed}' for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}\")\n",
        "            item['label'] = label_str_processed # Update item with processed label for validation\n",
        "            # --- End Label Fix/Sanitize ---\n",
        "\n",
        "            # --- Schema Validation ---\n",
        "            current_item_label_for_validation = item.get('label',\"\") # Already processed and stripped if it was string\n",
        "            desc_str = item.get('description',\"\")\n",
        "            sf_list = item.get('surface_forms', [])\n",
        "\n",
        "            is_dict_val = isinstance(item, dict)\n",
        "            has_all_keys_val = all(k in item for k in [\"label\", \"description\", \"surface_forms\"])\n",
        "            is_label_str_val = isinstance(current_item_label_for_validation, str) and bool(current_item_label_for_validation)\n",
        "            label_starts_bracket_val = current_item_label_for_validation.startswith('[') if is_label_str_val else False\n",
        "            label_ends_bracket_val = current_item_label_for_validation.endswith(']') if is_label_str_val else False\n",
        "            is_desc_str_val = isinstance(desc_str, str)\n",
        "            is_sf_list_val = isinstance(sf_list, list)\n",
        "            sfs_are_strings_val = all(isinstance(sf_item, str) for sf_item in sf_list) if is_sf_list_val else False\n",
        "\n",
        "            if is_dict_val and has_all_keys_val and \\\n",
        "               is_label_str_val and label_starts_bracket_val and label_ends_bracket_val and \\\n",
        "               is_desc_str_val and is_sf_list_val and sfs_are_strings_val:\n",
        "                valid_motifs_from_json.append({\n",
        "                    \"label\": current_item_label_for_validation,\n",
        "                    \"description\": desc_str.strip(),\n",
        "                    \"surface_forms\": [s.strip() for s in sf_list if isinstance(s, str) and s.strip()]\n",
        "                })\n",
        "            else: # Item failed schema validation\n",
        "                print(f\"    [WARN] Invalid motif object structure after label processing for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}. Skipping item.\")\n",
        "                with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} --- ITEM_VALIDATION_FAILURE (Item {item_idx+1}) ---\\n\")\n",
        "                    prompt_parts = prompt_sent_to_llm.split('Set of comments to analyze:')\n",
        "                    user_content_for_log = \"USER_CONTENT_SPLIT_FAILED_OR_PROMPT_EMPTY\"\n",
        "                    if len(prompt_parts) > 1: user_content_for_log = prompt_parts[1][:500]\n",
        "                    elif prompt_sent_to_llm: user_content_for_log = prompt_sent_to_llm[:500]\n",
        "                    f.write(f\"PROMPT USER CONTENT (approx first 500 chars):\\n{user_content_for_log}...\\n\")\n",
        "                    f.write(f\"RAW LLM RESPONSE (where validation failed for item):\\n{llm_raw_response_text}\\n\")\n",
        "                    f.write(f\"PARSED ITEM THAT FAILED SCHEMA (after label processing attempt):\\n{json.dumps(item, indent=2)}\\n\") # Log the item that failed\n",
        "        return valid_motifs_from_json\n",
        "\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        print(f\"    [WARN] Motif JSON parsing or core structure validation failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} --- JSON_PARSE_ERROR_OR_VALUE_ERROR ---\\n\")\n",
        "            prompt_parts = prompt_sent_to_llm.split('Set of comments to analyze:')\n",
        "            user_content_for_log = \"USER_CONTENT_SPLIT_FAILED_OR_PROMPT_EMPTY\"\n",
        "            if len(prompt_parts) > 1: user_content_for_log = prompt_parts[1][:500]\n",
        "            elif prompt_sent_to_llm: user_content_for_log = prompt_sent_to_llm[:500]\n",
        "            f.write(f\"PROMPT USER CONTENT (approx first 500 chars):\\n{user_content_for_log}...\\n\")\n",
        "            f.write(f\"RAW LLM RESPONSE (Error: {type(e).__name__}):\\n{llm_raw_response_text}\\n\")\n",
        "            f.write(f\"EXTRACTED JSON STRING CANDIDATE (Error: {type(e).__name__}):\\n{json_str_candidate}\\n\")\n",
        "        return []\n",
        "\n",
        "def get_motifs_for_qid_batched(\n",
        "    list_of_individual_response_texts: List[str],\n",
        "    responses_per_batch: int,\n",
        "    hf_pipeline_instance,  # From Cell 3\n",
        "    hf_tokenizer_instance, # From Cell 3\n",
        "    qid_for_log: str\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Orchestrates batched LLM calls for a QID to get raw, structurally validated motif candidates.\n",
        "    Does NOT perform chunk-level SF n-gram validation in this version.\n",
        "    \"\"\"\n",
        "    all_raw_motifs_from_chunks = []\n",
        "\n",
        "    # Create text chunks for LLM processing\n",
        "    batched_text_chunks_for_llm = []\n",
        "    for i in range(0, len(list_of_individual_response_texts), responses_per_batch):\n",
        "        batch_responses = list_of_individual_response_texts[i:i + responses_per_batch]\n",
        "        # Preprocess each chunk before building the prompt\n",
        "        chunk_text_for_llm = preprocess_corpus_for_motif_extraction(\"\\n\\n<RSP_SEP>\\n\\n\".join(batch_responses)) # preprocess_corpus_... from Cell 2\n",
        "        batched_text_chunks_for_llm.append(chunk_text_for_llm)\n",
        "\n",
        "    print(f\"  QID {qid_for_log}: Processing {len(list_of_individual_response_texts)} responses in {len(batched_text_chunks_for_llm)} preprocessed chunks (batch size: {responses_per_batch} responses).\")\n",
        "\n",
        "    for chunk_idx, text_chunk_to_analyze_processed in enumerate(batched_text_chunks_for_llm):\n",
        "        print(f\"    Analyzing chunk {chunk_idx + 1}/{len(batched_text_chunks_for_llm)} for QID {qid_for_log} (processed chunk len: {len(text_chunk_to_analyze_processed)} chars)...\")\n",
        "        if len(text_chunk_to_analyze_processed.strip()) < 50: # Arbitrary threshold for very short/empty chunks\n",
        "            print(f\"      Chunk {chunk_idx+1} (QID {qid_for_log}) too short after preprocessing, skipping.\")\n",
        "            continue\n",
        "\n",
        "        # build_llm_prompt_for_motifs is from Cell 3\n",
        "        prompt_for_llm = build_llm_prompt_for_motifs(text_chunk_to_analyze_processed)\n",
        "\n",
        "        motifs_from_this_chunk = [] # Store successfully parsed motifs for this chunk\n",
        "        for attempt in range(LLM_RETRY_ATTEMPTS): # LLM_RETRY_ATTEMPTS from config\n",
        "            # call_local_llm_for_raw_response is from Cell 3\n",
        "            raw_llm_response = call_local_llm_for_raw_response(\n",
        "                prompt_for_llm,\n",
        "                hf_pipeline_instance,\n",
        "                hf_tokenizer_instance,\n",
        "                qid_for_log,\n",
        "                chunk_idx + 1\n",
        "            )\n",
        "\n",
        "            if not raw_llm_response: # LLM call failed or returned empty\n",
        "                print(f\"      LLM call attempt {attempt + 1} for chunk {chunk_idx+1} (QID {qid_for_log}) returned empty string. Retrying if possible...\")\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1) # Small delay before retry\n",
        "                continue # Go to next attempt\n",
        "\n",
        "            # Parse and validate the structure of the JSON response from LLM\n",
        "            parsed_and_validated_motifs = parse_and_validate_llm_json_response(\n",
        "                raw_llm_response,\n",
        "                qid_for_log,\n",
        "                chunk_idx+1,\n",
        "                prompt_for_llm # Pass the prompt for logging context on error\n",
        "            )\n",
        "\n",
        "            if parsed_and_validated_motifs: # If parsing and basic validation were successful\n",
        "                motifs_from_this_chunk = parsed_and_validated_motifs\n",
        "                break # Success for this chunk, exit retry loop\n",
        "            else:\n",
        "                # This means JSON parsing failed, or it parsed to an empty list, or all items failed schema validation\n",
        "                print(f\"      Motif parsing/validation attempt {attempt + 1} yielded no structured motifs for chunk {chunk_idx+1} (QID {qid_for_log}). Retrying if possible...\")\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "\n",
        "        if motifs_from_this_chunk:\n",
        "            print(f\"      Extracted {len(motifs_from_this_chunk)} structured motif objects from chunk {chunk_idx+1} (QID {qid_for_log}).\")\n",
        "            all_raw_motifs_from_chunks.extend(motifs_from_this_chunk)\n",
        "        else:\n",
        "            print(f\"      No valid structured motifs extracted from chunk {chunk_idx+1} (QID {qid_for_log}) after {LLM_RETRY_ATTEMPTS} attempts.\")\n",
        "\n",
        "    return all_raw_motifs_from_chunks\n",
        "\n",
        "def consolidate_raw_motifs(list_of_all_raw_motifs: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Consolidates motifs extracted from all chunks, primarily by label, merging surface forms.\"\"\"\n",
        "    if not list_of_all_raw_motifs:\n",
        "        return []\n",
        "\n",
        "    consolidated_motifs_map = {}\n",
        "    for motif_obj in list_of_all_raw_motifs:\n",
        "        # Ensure motif_obj is a dict and has a label (already validated by parse_and_validate)\n",
        "        if not isinstance(motif_obj, dict) or not motif_obj.get(\"label\"):\n",
        "            continue\n",
        "\n",
        "        label = motif_obj[\"label\"] # Known to exist and be stripped from parse_and_validate\n",
        "        description = motif_obj.get(\"description\",\"\").strip() # Also stripped\n",
        "        surface_forms = motif_obj.get(\"surface_forms\", []) # Already list of stripped strings\n",
        "\n",
        "        # Normalize SFs for merging (lowercase, strip, unique)\n",
        "        current_sfs_set = set(sf.lower() for sf in surface_forms if sf) # Assumes SFs are already stripped\n",
        "\n",
        "        if label not in consolidated_motifs_map:\n",
        "            consolidated_motifs_map[label] = {\n",
        "                \"label\": label,\n",
        "                \"description\": description, # Takes description from first encounter of this label\n",
        "                \"surface_forms\": sorted(list(current_sfs_set))\n",
        "            }\n",
        "        else: # Label exists, merge surface forms\n",
        "            existing_sfs_set = set(consolidated_motifs_map[label].get(\"surface_forms\", [])) # These are already lowercased\n",
        "            consolidated_motifs_map[label][\"surface_forms\"] = sorted(list(existing_sfs_set.union(current_sfs_set)))\n",
        "            # Optionally, could append descriptions or choose the longest, etc. For now, keeps first.\n",
        "\n",
        "    return list(consolidated_motifs_map.values())\n",
        "\n",
        "def filter_surface_forms_by_global_frequency(\n",
        "    consolidated_motifs_list: List[Dict],\n",
        "    full_qid_corpus_text: str, # The entire original text for the QID\n",
        "    min_global_freq: int = MIN_SF_FREQUENCY_IN_FULL_CORPUS # Uses config constant\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Filters surface forms in consolidated motifs based on their frequency\n",
        "    in the full QID corpus text.\n",
        "    Motifs are discarded if they have no SFs left after filtering.\n",
        "    \"\"\"\n",
        "    if not consolidated_motifs_list:\n",
        "        return []\n",
        "\n",
        "    final_globally_filtered_motifs = []\n",
        "    # print(f\"  Filtering SFs from {len(consolidated_motifs_list)} consolidated motifs (min global freq: {min_global_freq})...\")\n",
        "\n",
        "    for motif_obj in consolidated_motifs_list:\n",
        "        globally_frequent_sfs_for_this_motif = []\n",
        "        # Surface forms in consolidated_motifs_list are already lowercased and unique\n",
        "        original_sfs_for_this_motif = motif_obj.get(\"surface_forms\", [])\n",
        "\n",
        "        for sf_str_lower in original_sfs_for_this_motif:\n",
        "            # count_sf_occurrences performs its own lowercasing of corpus and sf\n",
        "            count = count_sf_occurrences(full_qid_corpus_text, sf_str_lower)\n",
        "\n",
        "            if count >= min_global_freq:\n",
        "                globally_frequent_sfs_for_this_motif.append(sf_str_lower) # Keep the lowercased SF\n",
        "                # print(f\"    SF '{sf_str_lower}' (label '{motif_obj.get('label')}') kept, global freq: {count}\")\n",
        "            # else:\n",
        "                # print(f\"    SF '{sf_str_lower}' (label '{motif_obj.get('label')}') filtered out, global freq: {count} (min_req: {min_global_freq})\")\n",
        "\n",
        "        if globally_frequent_sfs_for_this_motif: # Only keep motif if it has at least one globally frequent SF\n",
        "            filtered_motif_entry = motif_obj.copy() # Make a copy to modify\n",
        "            # SFs are already lowercased and unique from consolidation, just sort them\n",
        "            filtered_motif_entry[\"surface_forms\"] = sorted(list(set(globally_frequent_sfs_for_this_motif)))\n",
        "            final_globally_filtered_motifs.append(filtered_motif_entry)\n",
        "        # else:\n",
        "            # print(f\"    Motif '{motif_obj.get('label')}' discarded (no globally frequent SFs after filtering).\")\n",
        "\n",
        "    return final_globally_filtered_motifs\n",
        "\n",
        "print(\"Cell 4: Motif Processing and Validation Utilities loaded.\")"
      ],
      "metadata": {
        "id": "ZmxkVj9sW6OV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4571aa21-25bc-4444-b4fb-c6ec25930d48"
      },
      "id": "ZmxkVj9sW6OV",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 4: Motif Processing and Validation Utilities loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5: MDL Calculations\n",
        "\n",
        "import hashlib # Already imported in Cell 1\n",
        "import numpy as np # Already imported in Cell 1\n",
        "from pybdm import BDM # Already imported in Cell 1\n",
        "import re # Already imported in Cell 1\n",
        "from typing import List, Dict # Already imported in Cell 1\n",
        "\n",
        "# Import functions/constants from previous cells if needed (for .py files)\n",
        "# For notebooks, assume previous cells have run and defined them in global scope.\n",
        "# from config import MATRIX_SIZE_GLOBAL, MAX_TEXT_FOR_BDM_HASH, MOTIF_SYMBOLIC_LABEL_COST, ...\n",
        "# from text_utils import tokenize_phrase\n",
        "\n",
        "# Add checks or default values for constants if a cell might be run independently\n",
        "try:\n",
        "    MATRIX_SIZE_GLOBAL; MAX_TEXT_FOR_BDM_HASH; MOTIF_SYMBOLIC_LABEL_COST # Check a few\n",
        "except NameError:\n",
        "    print(\"WARN (Cell 5): Key config constants not found. Using fallback or expecting errors.\")\n",
        "    MATRIX_SIZE_GLOBAL = (8, 8); MAX_TEXT_FOR_BDM_HASH = 2000\n",
        "    MOTIF_SYMBOLIC_LABEL_COST = 0.5; MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "    MOTIF_DESCRIPTION_TOKEN_COST = 0.1; MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.25\n",
        "    MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.1\n",
        "\n",
        "\n",
        "def initialize_bdm_instance():\n",
        "    \"\"\"Initializes and returns a BDM instance.\"\"\"\n",
        "    print(\"Initializing BDM instance...\")\n",
        "    try:\n",
        "        bdm_instance = BDM(ndim=2) # Corrected: Use default CTM-based NKS\n",
        "        print(\"BDM instance initialized successfully (ndim=2, default CTM-based).\")\n",
        "        return bdm_instance\n",
        "    except Exception as e_bdm_init:\n",
        "        print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\")\n",
        "        if \"CTM data files\" in str(e_bdm_init).lower() or \"dataset\" in str(e_bdm_init).lower():\n",
        "            print(\"  BDM Error Hint: This might be related to missing/corrupted CTM data files for PyBDM.\")\n",
        "            print(\"  Ensure PyBDM is installed correctly and can access/download its data.\")\n",
        "            print(\"  You might need to run once: from pybdm import get_ctm_dataset; get_ctm_dataset()\")\n",
        "        return None\n",
        "\n",
        "def text_to_binary_matrix(text_input: str, size: tuple = MATRIX_SIZE_GLOBAL) -> np.ndarray:\n",
        "    \"\"\"Converts a text string to a binary matrix using its SHA256 hash.\"\"\"\n",
        "    if not isinstance(text_input, str) or not text_input.strip():\n",
        "        return np.zeros(size, dtype=int)\n",
        "\n",
        "    hash_obj = hashlib.sha256(text_input.encode('utf-8', 'ignore'))\n",
        "    hash_digest = hash_obj.hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string_from_hash = bin(int(hash_digest, 16))[2:].zfill(256)\n",
        "\n",
        "    binary_string_for_matrix = binary_string_from_hash[:required_bits] if required_bits <= 256 else binary_string_from_hash.ljust(required_bits, '0')\n",
        "\n",
        "    bits_for_matrix = [int(b) for b in binary_string_for_matrix]\n",
        "    return np.array(bits_for_matrix).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input: str, bdm_instance: BDM, matrix_s: tuple = MATRIX_SIZE_GLOBAL) -> float:\n",
        "    \"\"\"Computes BDM for a given text string using a prefix for hashing.\"\"\"\n",
        "    if not isinstance(text_input, str) or not text_input.strip() :\n",
        "        return 0.0\n",
        "\n",
        "    text_for_hash = text_input[:MAX_TEXT_FOR_BDM_HASH] if len(text_input) > MAX_TEXT_FOR_BDM_HASH else text_input\n",
        "    if not text_for_hash.strip():\n",
        "        return 0.0\n",
        "\n",
        "    binary_matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        bdm_value = bdm_instance.bdm(binary_matrix)\n",
        "        return bdm_value\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (full len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0 # Indicate error\n",
        "\n",
        "def calculate_L_H_token_based_structured(structured_motifs_list: List[Dict]) -> float:\n",
        "    \"\"\"Calculates L(H) - the cost of defining the list of structured motifs.\"\"\"\n",
        "    if not structured_motifs_list:\n",
        "        return 0.0\n",
        "\n",
        "    total_lh_cost = 0.0\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict):\n",
        "            continue\n",
        "\n",
        "        current_motif_lh = 0.0\n",
        "\n",
        "        label_str = motif_obj.get('label', \"\")\n",
        "        if isinstance(label_str, str) and label_str.strip(): # Label must exist\n",
        "            current_motif_lh += MOTIF_SYMBOLIC_LABEL_COST\n",
        "\n",
        "        description_str = motif_obj.get('description', \"\")\n",
        "        if isinstance(description_str, str) and description_str.strip(): # Description can be empty but contributes if present\n",
        "            current_motif_lh += MOTIF_DESCRIPTION_TEXT_BASE_COST\n",
        "            current_motif_lh += len(tokenize_phrase(description_str)) * MOTIF_DESCRIPTION_TOKEN_COST # tokenize_phrase from Cell 2\n",
        "\n",
        "        surface_forms_list = motif_obj.get('surface_forms', [])\n",
        "        if isinstance(surface_forms_list, list) and surface_forms_list:\n",
        "            valid_sfs_for_lh = [sf for sf in surface_forms_list if isinstance(sf, str) and sf.strip()]\n",
        "            if valid_sfs_for_lh: # Only add base cost if there are actual SFs\n",
        "                current_motif_lh += MOTIF_SURFACE_FORMS_LIST_BASE_COST\n",
        "                for sf_str in valid_sfs_for_lh:\n",
        "                    current_motif_lh += len(tokenize_phrase(sf_str)) * MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH\n",
        "\n",
        "        total_lh_cost += current_motif_lh\n",
        "    return total_lh_cost\n",
        "\n",
        "def llm_compress_text_structured(text_to_compress: str, structured_motifs_list: List[Dict]) -> str:\n",
        "    \"\"\"Compresses text by replacing occurrences of motif surface forms with their symbolic labels.\"\"\"\n",
        "    if not isinstance(text_to_compress, str):\n",
        "        return \"\" # Return empty if input is not string\n",
        "    if not structured_motifs_list:\n",
        "        return text_to_compress.lower()\n",
        "\n",
        "    compressed_text = text_to_compress.lower()\n",
        "\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict):\n",
        "            continue\n",
        "\n",
        "        label = motif_obj.get('label', None)\n",
        "        surface_forms = motif_obj.get('surface_forms', []) # SFs should be already lowercased by consolidation/filtering\n",
        "\n",
        "        if not (isinstance(label, str) and label.strip()) or \\\n",
        "           not (isinstance(surface_forms, list) and surface_forms):\n",
        "            continue # Skip motif if label is bad or no surface forms\n",
        "\n",
        "        placeholder = label\n",
        "\n",
        "        # Surface forms should be already lowercased and stripped from previous processing steps\n",
        "        # Sort this motif's own surface forms by length (descending) for greedy matching\n",
        "        sorted_sfs_for_this_motif = sorted(\n",
        "            [sf for sf in surface_forms if isinstance(sf, str) and sf.strip()], # Ensure SFs are strings\n",
        "            key=len,\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        for sf_str_lower in sorted_sfs_for_this_motif: # Assumes sf_str is already lowercase\n",
        "            try:\n",
        "                # Use word boundaries for more precise replacement\n",
        "                compressed_text = re.sub(r'\\b' + re.escape(sf_str_lower) + r'\\b', placeholder, compressed_text)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for SF '{sf_str_lower}' of motif '{label}': {re_e}. Skipping this SF.\")\n",
        "                continue\n",
        "    return compressed_text\n",
        "\n",
        "def compute_mdl_cost_for_text_block(\n",
        "    full_qid_corpus_str: str,\n",
        "    final_motifs_to_evaluate: List[Dict],\n",
        "    bdm_instance: BDM,\n",
        "    matrix_s: tuple = MATRIX_SIZE_GLOBAL\n",
        "    ) -> tuple[float, float, float]:\n",
        "    \"\"\"Computes L(H), L(D|H), and Total MDL for a text block given a final set of motifs.\"\"\"\n",
        "    if not isinstance(full_qid_corpus_str, str):\n",
        "        full_qid_corpus_str = \"\"\n",
        "\n",
        "    l_h = calculate_L_H_token_based_structured(final_motifs_to_evaluate)\n",
        "\n",
        "    compressed_text_block = llm_compress_text_structured(full_qid_corpus_str, final_motifs_to_evaluate)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "\n",
        "    if l_d_h < 0: # Indicates a BDM computation error\n",
        "        return l_h, -1.0, -1.0\n",
        "\n",
        "    total_mdl_cost = l_h + l_d_h\n",
        "    return l_h, l_d_h, total_mdl_cost\n",
        "\n",
        "print(\"Cell 5: MDL Calculation Utilities loaded.\")"
      ],
      "metadata": {
        "id": "G9aO-04zbOGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8157e289-ee31-4422-b1ca-6819d750c487"
      },
      "id": "G9aO-04zbOGA",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 5: MDL Calculation Utilities loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6: Main Pipeline Orchestration\n",
        "\n",
        "# Assumes functions from previous cells (Cells 1-5 content) are defined or imported:\n",
        "# From Cell 1: All configuration constants (BASE_PROJECT_DIR, LOCAL_LLM_MODEL_ID, etc.)\n",
        "# From Cell 2: preprocess_corpus_for_motif_extraction, count_sf_occurrences\n",
        "# From Cell 3: initialize_llm_pipeline (this should be defined here if not imported)\n",
        "# From Cell 4: get_motifs_for_qid_batched, consolidate_raw_motifs, filter_surface_forms_by_global_frequency\n",
        "# From Cell 5: initialize_bdm_instance, compute_bdm_for_text, compute_mdl_cost_for_text_block\n",
        "\n",
        "# For clarity in a notebook, if you are truly running these as separate cells,\n",
        "# you might add specific imports at the top of this cell from the \"modules\" like:\n",
        "# from config import *\n",
        "# from text_utils import preprocess_corpus_for_motif_extraction, count_sf_occurrences\n",
        "# from llm_interaction import initialize_llm_pipeline # (and other llm functions if get_motifs_for_qid_batched was also there)\n",
        "# from motif_processing import get_motifs_for_qid_batched, consolidate_raw_motifs, filter_surface_forms_by_global_frequency\n",
        "# from mdl_calculations import initialize_bdm_instance, compute_bdm_for_text, compute_mdl_cost_for_text_block\n",
        "\n",
        "# However, if all previous cells were run in order, their functions/constants are in scope.\n",
        "\n",
        "def main():\n",
        "    # --- Initial Setup and Welcome Message ---\n",
        "    script_version_name = \"Single Cell MWP (Reverted Simpler Prompt v5, Label Fix, Global SF Filter)\"\n",
        "    print(f\"--- {script_version_name} ---\")\n",
        "    print(f\"Timestamp: {time.asctime()}\")\n",
        "    print(\"\\n--- Configuration Summary ---\")\n",
        "    print(f\"LLM Model: {LOCAL_LLM_MODEL_ID}, Quantization: {USE_QUANTIZATION_FOR_LOCAL_LLM}\")\n",
        "    print(f\"LLM Batch Size (Responses): {LLM_BATCH_SIZE_RESPONSES}, Retries: {LLM_RETRY_ATTEMPTS}\")\n",
        "    print(f\"Max Text Chars per LLM Prompt Chunk: {MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK}\")\n",
        "    print(f\"Max New Tokens for LLM Motif Extraction: {LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION}\")\n",
        "    print(f\"Max Motifs to Request per Chunk: {MAX_MOTIFS_PER_CHUNK}\")\n",
        "    print(f\"L(H) Costs: Label={MOTIF_SYMBOLIC_LABEL_COST}, DescBase={MOTIF_DESCRIPTION_TEXT_BASE_COST}, DescToken={MOTIF_DESCRIPTION_TOKEN_COST}, SFListBase={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, SFTokenInLH={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "    print(f\"Global SF Filtering Min Freq: {MIN_SF_FREQUENCY_IN_FULL_CORPUS}\")\n",
        "    print(f\"BDM Hash Prefix Length: {MAX_TEXT_FOR_BDM_HASH}, BDM Matrix: {MATRIX_SIZE_GLOBAL}\")\n",
        "    print(f\"Debug Log File: {LLM_DEBUG_LOG_FILE}\")\n",
        "    print(\"--- End Configuration Summary ---\\n\")\n",
        "\n",
        "    # --- Initialize Debug Log File ---\n",
        "    try:\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"w\", encoding=\"utf-8\") as f: # Overwrite for new run\n",
        "            f.write(f\"LLM Motif Debug Log - Run Started: {time.asctime()}\\n\")\n",
        "            f.write(f\"Script Version: {script_version_name}\\n\")\n",
        "            f.write(f\"Model ID: {LOCAL_LLM_MODEL_ID}\\n\")\n",
        "            f.write(f\"Pipeline Config: return_full_text=False (Implicit in current setup)\\n\")\n",
        "            f.write(f\"Prompt Strategy: Reverted Simpler Prompt with Label Fix Attempt Active\\n---\\n\")\n",
        "    except Exception as e_log:\n",
        "        print(f\"WARN: Could not initialize debug log file {LLM_DEBUG_LOG_FILE}: {e_log}\")\n",
        "\n",
        "    # --- Initialize LLM and BDM ---\n",
        "    # These functions are expected to be defined (e.g., from Cell 3 and Cell 5 content)\n",
        "    hf_pipeline_instance, hf_tokenizer_instance = initialize_llm_pipeline()\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance:\n",
        "        print(\"CRITICAL: Exiting due to LLM pipeline initialization failure.\")\n",
        "        return\n",
        "\n",
        "    bdm_instance_main = initialize_bdm_instance()\n",
        "    if not bdm_instance_main:\n",
        "        print(\"CRITICAL: Exiting due to BDM initialization failure.\")\n",
        "        return\n",
        "\n",
        "    # --- Load Phase 2 Collated Data ---\n",
        "    if not os.path.exists(P2_COLLATED_FILE):\n",
        "        print(f\"ERROR: Phase 2 output file not found: {P2_COLLATED_FILE}\")\n",
        "        return\n",
        "    print(f\"Loading Phase 2 output from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f:\n",
        "            phase2_data_content = json.load(f)\n",
        "    except Exception as e_load:\n",
        "        print(f\"Error loading or parsing {P2_COLLATED_FILE}: {e_load}\")\n",
        "        return\n",
        "\n",
        "    all_qid_mdl_results_list = []\n",
        "\n",
        "    # --- Determine QIDs to Process ---\n",
        "    aggregated_content_by_qid_from_file = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "    if not aggregated_content_by_qid_from_file:\n",
        "        print(f\"No 'aggregated_pdf_content_by_qid' key found or data is empty in {P2_COLLATED_FILE}. Exiting.\")\n",
        "        return\n",
        "\n",
        "    qids_to_process_this_run = []\n",
        "    if P3_QIDS_TO_PROCESS_THEMATICALLY and \\\n",
        "       isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and \\\n",
        "       P3_QIDS_TO_PROCESS_THEMATICALLY: # Check if the list is not empty\n",
        "\n",
        "        qids_to_process_this_run = [\n",
        "            qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY\n",
        "            if qid in aggregated_content_by_qid_from_file\n",
        "        ]\n",
        "        if not qids_to_process_this_run:\n",
        "            print(f\"Warning: None of the specified QIDs {P3_QIDS_TO_PROCESS_THEMATICALLY} were found in the loaded data's QIDs. Exiting.\")\n",
        "            return\n",
        "    else:\n",
        "        qids_to_process_limit_fallback = 1\n",
        "        print(f\"P3_QIDS_TO_PROCESS_THEMATICALLY not set or empty. Processing up to {qids_to_process_limit_fallback} QID(s) from data as a fallback.\")\n",
        "        qids_to_process_this_run = list(aggregated_content_by_qid_from_file.keys())[:qids_to_process_limit_fallback]\n",
        "        if not qids_to_process_this_run:\n",
        "            print(\"No QIDs found in data to process based on the fallback limit. Exiting.\")\n",
        "            return\n",
        "\n",
        "    if not qids_to_process_this_run: # Final safeguard\n",
        "        print(\"No QIDs selected for processing after all checks. Exiting.\")\n",
        "        return\n",
        "    print(f\"\\nMDL analysis will run for QIDs: {qids_to_process_this_run}\\n\")\n",
        "\n",
        "    # --- Main QID Processing Loop ---\n",
        "    for qid_identifier_str in qids_to_process_this_run:\n",
        "        print(f\"--- Analyzing Data for QID: {qid_identifier_str} ---\")\n",
        "\n",
        "        list_of_individual_response_structs = aggregated_content_by_qid_from_file.get(qid_identifier_str, [])\n",
        "        actual_response_texts_for_qid = [\n",
        "            item.get(\"text\", \"\") for item in list_of_individual_response_structs\n",
        "            if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()\n",
        "        ]\n",
        "        if not actual_response_texts_for_qid:\n",
        "            print(f\"  No valid text strings extracted from responses for QID {qid_identifier_str}. Skipping.\");\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        full_corpus_text_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(actual_response_texts_for_qid)\n",
        "\n",
        "        if len(full_corpus_text_for_qid.strip()) < 100:\n",
        "            print(f\"  Skipping QID {qid_identifier_str}: combined text too short ({len(full_corpus_text_for_qid)} chars).\")\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        num_total_responses_for_qid = len(actual_response_texts_for_qid)\n",
        "        print(f\"  Corpus for QID {qid_identifier_str}: {len(full_corpus_text_for_qid)} chars, {num_total_responses_for_qid} responses.\")\n",
        "\n",
        "        baseline_bdm_original_corpus = compute_bdm_for_text(full_corpus_text_for_qid, bdm_instance_main, MATRIX_SIZE_GLOBAL)\n",
        "        if baseline_bdm_original_corpus < 0:\n",
        "            print(f\"  Error computing baseline BDM for QID {qid_identifier_str}. Skipping this QID.\")\n",
        "            # Log an error entry\n",
        "            error_entry = {\"qid\": qid_identifier_str, \"status\": \"ERROR_BASELINE_BDM\", \"baseline_mdl\": -1.0}\n",
        "            all_qid_mdl_results_list.append(error_entry)\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        current_qid_baseline_mdl_cost = baseline_bdm_original_corpus\n",
        "        print(f\"  Baseline MDL for QID {qid_identifier_str} (L(D_orig)): {current_qid_baseline_mdl_cost:.4f}\")\n",
        "\n",
        "        # Get raw motifs (these have passed initial JSON parsing and schema validation from parse_and_validate_llm_json_response)\n",
        "        # get_motifs_for_qid_batched is from Cell 4 equivalent\n",
        "        raw_motifs_from_chunks = get_motifs_for_qid_batched(\n",
        "            actual_response_texts_for_qid,\n",
        "            LLM_BATCH_SIZE_RESPONSES,\n",
        "            hf_pipeline_instance,\n",
        "            hf_tokenizer_instance,\n",
        "            qid_identifier_str\n",
        "        )\n",
        "\n",
        "        current_qid_result_entry = { # Initialize result dict for this QID\n",
        "            \"qid\": qid_identifier_str,\n",
        "            \"corpus_len_chars\": len(full_corpus_text_for_qid),\n",
        "            \"num_responses\": num_total_responses_for_qid,\n",
        "            \"baseline_mdl\": current_qid_baseline_mdl_cost,\n",
        "            \"final_refined_motifs\": [],\n",
        "            \"l_h_final_motifs\": 0.0,\n",
        "            \"l_d_h_final_motifs\": current_qid_baseline_mdl_cost,\n",
        "            \"total_mdl_with_final_motifs\": current_qid_baseline_mdl_cost,\n",
        "            \"compression_achieved\": 0.0,\n",
        "            \"num_raw_motifs_extracted\": len(raw_motifs_from_chunks),\n",
        "            \"num_consolidated_motifs\": 0,\n",
        "            \"num_globally_refined_motifs\": 0\n",
        "        }\n",
        "\n",
        "        if not raw_motifs_from_chunks:\n",
        "            print(f\"  No raw motifs extracted by LLM for QID {qid_identifier_str} from any chunk.\")\n",
        "            all_qid_mdl_results_list.append(current_qid_result_entry)\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  Extracted {len(raw_motifs_from_chunks)} raw motif objects from LLM for QID {qid_identifier_str} (across all chunks).\")\n",
        "\n",
        "        # Consolidate motifs (from Cell 4 equivalent)\n",
        "        consolidated_motifs_list = consolidate_raw_motifs(raw_motifs_from_chunks)\n",
        "        current_qid_result_entry[\"num_consolidated_motifs\"] = len(consolidated_motifs_list)\n",
        "        print(f\"  Consolidated into {len(consolidated_motifs_list)} unique motifs (by label) for QID {qid_identifier_str}.\")\n",
        "\n",
        "        if not consolidated_motifs_list:\n",
        "            print(f\"  No unique motifs left after consolidation for QID {qid_identifier_str}.\")\n",
        "            all_qid_mdl_results_list.append(current_qid_result_entry)\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        # print(f\"  Consolidated Motifs for QID {qid_identifier_str} (BEFORE Global SF refinement):\")\n",
        "        # for idx, mo_con in enumerate(consolidated_motifs_list):\n",
        "        #     print(f\"    Cons. Motif {idx+1}: L='{mo_con.get('label')}', D='{mo_con.get('description','N/A')[:30]}...', SFs({len(mo_con.get('surface_forms',[]))})='{mo_con.get('surface_forms',[])[:2]}...'\")\n",
        "\n",
        "        # Globally filter surface forms (from Cell 4 equivalent)\n",
        "        globally_refined_motifs = filter_surface_forms_by_global_frequency(\n",
        "            consolidated_motifs_list,\n",
        "            full_corpus_text_for_qid, # Filter against the full original corpus for this QID\n",
        "            min_global_freq=MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "        )\n",
        "        current_qid_result_entry[\"num_globally_refined_motifs\"] = len(globally_refined_motifs)\n",
        "        print(f\"  Globally refined into {len(globally_refined_motifs)} motifs for QID {qid_identifier_str}.\")\n",
        "\n",
        "        if not globally_refined_motifs:\n",
        "            print(f\"  No motifs left after GLOBAL surface form frequency refinement for QID {qid_identifier_str}.\")\n",
        "            all_qid_mdl_results_list.append(current_qid_result_entry)\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  Final Globally Refined Motifs for QID {qid_identifier_str} (for MDL eval):\")\n",
        "        for idx, mo_final in enumerate(globally_refined_motifs):\n",
        "            print(f\"    Refined Motif {idx+1}: L='{mo_final.get('label')}', D='{mo_final.get('description','N/A')[:60]}...', SFs({len(mo_final.get('surface_forms',[]))})='{mo_final.get('surface_forms',[])}'\")\n",
        "\n",
        "        # Final MDL Calculation (from Cell 5 equivalent)\n",
        "        l_h_final_val, l_d_h_final_val, total_mdl_final_val = compute_mdl_cost_for_text_block(\n",
        "            full_corpus_text_for_qid,\n",
        "            globally_refined_motifs,\n",
        "            bdm_instance_main,\n",
        "            MATRIX_SIZE_GLOBAL\n",
        "        )\n",
        "\n",
        "        current_qid_result_entry[\"final_refined_motifs\"] = globally_refined_motifs\n",
        "        current_qid_result_entry[\"l_h_final_motifs\"] = l_h_final_val\n",
        "\n",
        "        if l_d_h_final_val < 0: # BDM error\n",
        "            print(f\"  Error computing MDL cost with final refined motifs for QID {qid_identifier_str} (BDM error in L(D|H)).\")\n",
        "            current_qid_result_entry.update({\"l_d_h_final_motifs\": -1.0, \"total_mdl_with_final_motifs\": -1.0, \"compression_achieved\": \"BDM_ERROR\"})\n",
        "        else:\n",
        "            current_qid_result_entry[\"l_d_h_final_motifs\"] = l_d_h_final_val\n",
        "            current_qid_result_entry[\"total_mdl_with_final_motifs\"] = total_mdl_final_val\n",
        "            compression_final_val = current_qid_baseline_mdl_cost - total_mdl_final_val\n",
        "            current_qid_result_entry[\"compression_achieved\"] = compression_final_val\n",
        "\n",
        "        all_qid_mdl_results_list.append(current_qid_result_entry) # Append result for this QID\n",
        "\n",
        "        # Print QID-specific MDL outcome\n",
        "        if l_d_h_final_val >=0 :\n",
        "            print(f\"  L(H) final motifs: {l_h_final_val:.4f}\")\n",
        "            print(f\"  L(D|H) compressed full corpus: {l_d_h_final_val:.4f}\")\n",
        "            print(f\"  Total MDL cost with final motifs: {total_mdl_final_val:.4f}\")\n",
        "            compression_val = current_qid_result_entry[\"compression_achieved\"]\n",
        "            result_status_str = f\"SUCCESS: Comp: {compression_val:.4f}\" if isinstance(compression_val, float) and compression_val > 0.0001 else f\"NOTE: No sig. comp. Diff: {compression_val if isinstance(compression_val, str) else compression_val:.4f}\"\n",
        "            print(f\"  {result_status_str}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # --- Summary Printing and Saving Results ---\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary ---\")\n",
        "    if not all_qid_mdl_results_list:\n",
        "        print(\"No QIDs were processed or no valid results were generated to summarize.\")\n",
        "    else:\n",
        "        valid_results_for_stats = [\n",
        "            r for r in all_qid_mdl_results_list\n",
        "            if isinstance(r.get('compression_achieved'), float) and r.get('l_h_final_motifs', -1.0) >= 0\n",
        "        ]\n",
        "        num_qids_processed_total = len(all_qid_mdl_results_list)\n",
        "        num_qids_with_valid_mdl_calc = len(valid_results_for_stats)\n",
        "        num_qids_achieving_compression = sum(1 for r in valid_results_for_stats if r['compression_achieved'] > 0.0001)\n",
        "\n",
        "        print(f\"Total QIDs targeted for analysis: {len(qids_to_process_this_run)}\")\n",
        "        print(f\"Total QID result entries logged: {num_qids_processed_total}\")\n",
        "        print(f\"Number of QIDs with valid MDL calculations: {num_qids_with_valid_mdl_calc}\")\n",
        "        print(f\"Number of QIDs where compression was achieved: {num_qids_achieving_compression}\")\n",
        "\n",
        "        if num_qids_achieving_compression > 0:\n",
        "            successful_compressions_values = [r['compression_achieved'] for r in valid_results_for_stats if r['compression_achieved'] > 0.0001]\n",
        "            avg_compression_val = np.mean(successful_compressions_values)\n",
        "            max_compression_val = np.max(successful_compressions_values)\n",
        "            print(f\"  Average compression (for successful cases): {avg_compression_val:.4f}\")\n",
        "            print(f\"  Maximum compression achieved across QIDs: {max_compression_val:.4f}\")\n",
        "        else:\n",
        "            print(\"  No compression achieved for any QID in this run.\")\n",
        "\n",
        "        output_filename_final = os.path.join(BASE_PROJECT_DIR, \"mdl_analysis_single_cell_reverted_prompt_v5_final.json\")\n",
        "        try:\n",
        "            with open(output_filename_final, \"w\", encoding=\"utf-8\") as f_out:\n",
        "                json.dump(all_qid_mdl_results_list, f_out, indent=2, ensure_ascii=False)\n",
        "            print(f\"Detailed QID-based results saved to {output_filename_final}\")\n",
        "        except Exception as e_save:\n",
        "            print(f\"Error saving QID-based results to {output_filename_final}: {e_save}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Executing main MDL pipeline (Single Cell Reverted Prompt V5 with Label Fix & UnboundLocalError fix) at {time.asctime()}...\")\n",
        "    main()\n",
        "    print(f\"Main MDL pipeline execution finished at {time.asctime()}.\")"
      ],
      "metadata": {
        "id": "KmsliWhJb1o4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "436be3a820ee41d7917db35e531aeec7",
            "70ef2bef7cb84061995aa2b375097498",
            "dfebb7d8deb247a687a8368601a56e75",
            "ec4aa8c5a24841c3baa423fa0e565b0e",
            "95f5669ca7644575b0dff80a333f6b2d",
            "1fb727e27d5449bcbd5576edd186fbb8",
            "91ddc9b9a2da41bc83d7cbf4cca2af61",
            "6691dc6e4070444cbac061d2412b0226",
            "ba9aa456c8ae4570a2e3b94d6192786d",
            "f0b67f68494c4cb69800e7ef71dbbd82",
            "30d20d9ab9944eeba96b79ed3661bb33",
            "e4d50fb712ad4a108758a337be04c800",
            "58d0a15690f14939a9aa200f210c071a",
            "e76f32d175eb48fa9343a27e0ba563da",
            "f14cc66d9bfa4e5f8257104c033c3e95",
            "77e0489e441148ae9bad26dfdbd6f023",
            "a279255add454a2d8c96691ef795780a",
            "a5220ec979364099a7307e58fb525a60",
            "5050d4b91ed44f1a8634b6f399402b31",
            "3d9e7ba141e84544be2c513c8aebba3c",
            "ed7cf64cbe6c476a82c4625f2665cb65",
            "61882815d5bb4babb6ed15430eb7af3e",
            "c8fc69bd699044a4be2a97d3c924c5e6",
            "bb24380be0c24b108ca3d2c7d2d49c17",
            "57658a00135e4f889c37d785e329f17f",
            "01d173e1f9c8476d94c50f7ac11c805a",
            "c1978eae54e742f9b9cb83c82177dbe3",
            "50ea366caae24e68b54afc808e4208e6",
            "cfc0f9eda7a149768f7f48ed9a172b3b",
            "f430f4a437344632a78f578fd72e9afd",
            "642d02a84c7a4c19807fe7c49c8c114c",
            "9a3dd531bf6b40cdb0fcf2559d496f42",
            "ca23ae35eaaf4d8f8102592c08c460c0",
            "5e0d11c75fcc4cffaac044a4491c4ae9",
            "27f2c89ed84e4c869159e33899d3b1c3",
            "b92820eadfd84260bb0ce7f58838bd5a",
            "d8f3e876552d434db658d046dbd897e0",
            "1e2211e0f4bc4cd589a9d30abbf7150d",
            "887449d18f6746a28f2bbc1cc9c95dd3",
            "728cef1185fe4d098582995d38ba5300",
            "ff134eb536264e3cac7f44e328bda13f",
            "bc0289462a294805b86648872333527d",
            "0693816d9ce540a5a661b23e2047166f",
            "5e290d8b966744d9a832fa19812c815c",
            "4adfb6aa76db4c17bca719257b1293e5",
            "fc738f2780a444cebeb0a4d9c9a00af8",
            "983c5d8aa4d341e4afbe8229b01d5497",
            "de57e431dd964b30b772f971963f5c25",
            "d22b5956d6424487bdc0734e7b883f99",
            "313d1f98c5614ff9ad0c4544fe94d8f4",
            "d95220c2d94d45d7aa0de84fbac4e80e",
            "e999dc9a536142dab72525fc00734ad7",
            "99a99b6a2dc6422ebb147b0881d7ee21",
            "0f94bcd239ac455ab5221634569e3f09",
            "3cb7be72fb5c491c827f09db3e4ee4a0",
            "c91ad6f26af5420ca9914fdf839da990",
            "fd9bf18bb53d40aaab5e82b4c470fc64",
            "ba2db9fee3f1480e95e426984eaddc13",
            "e76971a26e8b4170a4d9ad0bb2adb7e0",
            "20e5a6cfc27447a8b5e0b2172baf244a",
            "29b13661d0d844e29352fa90e97be065",
            "3469b089b64c4b779b06ba0f78998187",
            "d7031a51f7594bff970f9b3747c339f5",
            "f5489ce067d34f699225f7d59934af0d",
            "ef255162da4f4a708a26af8597a5cec2",
            "0aaf086c2cb64af1bd0f7a2d90094b0b",
            "c79bebe889e64e6f9c15192b38beb59f",
            "30b2c494d6484be8a7095620f44e7e43",
            "f5cea509a8474089a3936e8127aeab79",
            "c501f0f14dbb4404ac74f2886fda8406",
            "3cec2e396ae9444485ffaa86b1aa2617",
            "c2b576e39d804f5ebbb67c41780ef835",
            "aaa7c482291f42478ac5e04dcc8fd80c",
            "dad2b441f74e42eea8a6db829d0f085d",
            "bef56f67a98a4cdbacadd54dfedaf4a6",
            "d0c3a75245f547a8bcd385516c79c701",
            "0c9077b6a3bf471d9c8360eb30b7d283",
            "35abf6a5315e4da18b6847852e45ad5d",
            "42bdba9290a34b44b0bc1db642239427",
            "47aef0f1957f4299a9e66a142f413140",
            "c8c57c0155aa466fb532c18d4cb1983e",
            "144bcbcb35e24bd6b8bd6b0fbd84d65b",
            "725a0e91e22d4f27b2832cb6b3c4843c",
            "527c956557694f2982d9d89c77b96a8e",
            "9aae6059b1a44e86a5c837f9945069c9",
            "8472d26dd60a4628991db0e65ff29384",
            "439cb11e4ab64dbe9615bd14b40f81df",
            "fc2124df06694090b88760c48f1c8ad2",
            "f841a353b9614ed0b05500a044aec6d7",
            "90e797c6937c443eaf27083feb65f498",
            "b22daccf74444ac4841aac59fbb32398",
            "e0bd0566e05345c1b770e71cffa71443",
            "8ed15193e4364c659958c0f525d74f39",
            "3e0c25469d914073a32a8fd5d2e058d6",
            "dbad72520ba34ec5be6cf280d9907313",
            "09aad63dba4b45fb86f18b09df34c272",
            "9f3e615b968d4789831b5a1f9b90b2f4",
            "190fc74176e14cf180538f95dce6db3d",
            "4050330efc5c4095885cd075736ec438",
            "87b9ee3e02784b6cbdddf63ac0466eec",
            "285bae89558b470c94e4f4048e4f322a",
            "06410c692523465caba78cc307528a28",
            "70e7d9983d664ea0ada6b024e7877b03",
            "68742bbd120c43ad8b1f8384ff2f4c3a",
            "00db6340e7274dfeaeff34bbc9e6d835",
            "d2cfa20574ea4939a9feda1995a8cb2a",
            "6aad2df267e544ba90e24de1e80acb63",
            "31525f2a87c544dc8ed101aeff5cde36",
            "5dc39fa015c243fe96a30ef6a42aa918",
            "53b5270bc85040b18fe22fa6aa418aca",
            "ddc87dbbcfca45fe8fe133f2c1540b85",
            "e09de585c75144cea4b79355051f9103",
            "ed1b7fdc685b4a95873930dc7527b3b9",
            "7d24b8a406214418af03a151788f273b",
            "a41725045ffa424c9597d89d5844aad8",
            "d7eb6ae9803e4ec18c61a90398793b8e",
            "4781eccbf14a4f3e8ab4cedd42e9d2d5",
            "b25bd2ae3dea4cc892af7a09579901ec",
            "0ed5bf7f00b1486e9009e2021fe87fe6",
            "2cf9249ee5464aaf88e2faa31589b969",
            "abff27c5aa8a4ccab45757b1ff8eae3f"
          ]
        },
        "outputId": "cfb86f52-1e9c-4888-da0a-2e7868187690"
      },
      "id": "KmsliWhJb1o4",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing main MDL pipeline (Single Cell Reverted Prompt V5 with Label Fix & UnboundLocalError fix) at Sun Jun  1 12:34:21 2025...\n",
            "--- Single Cell MWP (Reverted Simpler Prompt v5, Label Fix, Global SF Filter) ---\n",
            "Timestamp: Sun Jun  1 12:34:21 2025\n",
            "\n",
            "--- Configuration Summary ---\n",
            "LLM Model: google/gemma-2b-it, Quantization: True\n",
            "LLM Batch Size (Responses): 5, Retries: 2\n",
            "Max Text Chars per LLM Prompt Chunk: 7000\n",
            "Max New Tokens for LLM Motif Extraction: 700\n",
            "Max Motifs to Request per Chunk: 5\n",
            "L(H) Costs: Label=0.5, DescBase=0.5, DescToken=0.1, SFListBase=0.25, SFTokenInLH=0.1\n",
            "Global SF Filtering Min Freq: 2\n",
            "BDM Hash Prefix Length: 2000, BDM Matrix: (8, 8)\n",
            "Debug Log File: /content/drive/MyDrive/Colab Notebooks/Legal/llm_motif_debug_log_refactored_v1.txt\n",
            "--- End Configuration Summary ---\n",
            "\n",
            "--- Initializing LLM Pipeline (model: google/gemma-2b-it, quantization: True, return_full_text: False) ---\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-2b-it...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "436be3a820ee41d7917db35e531aeec7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4d50fb712ad4a108758a337be04c800"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8fc69bd699044a4be2a97d3c924c5e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e0d11c75fcc4cffaac044a4491c4ae9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BitsAndBytesConfig created for google/gemma-2b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-2b-it (Quantization active: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4adfb6aa76db4c17bca719257b1293e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c91ad6f26af5420ca9914fdf839da990"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c79bebe889e64e6f9c15192b38beb59f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35abf6a5315e4da18b6847852e45ad5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f841a353b9614ed0b05500a044aec6d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87b9ee3e02784b6cbdddf63ac0466eec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddc87dbbcfca45fe8fe133f2c1540b85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-2b-it initialized successfully.\n",
            "Initializing BDM instance...\n",
            "BDM instance initialized successfully (ndim=2, default CTM-based).\n",
            "Loading Phase 2 output from: /content/drive/MyDrive/Colab Notebooks/Legal/Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MDL analysis will run for QIDs: ['Q4']\n",
            "\n",
            "--- Analyzing Data for QID: Q4 ---\n",
            "  Corpus for QID Q4: 129501 chars, 209 responses.\n",
            "  Baseline MDL for QID Q4 (L(D_orig)): 121.3693\n",
            "  QID Q4: Processing 209 responses in 42 preprocessed chunks (batch size: 5 responses).\n",
            "    Analyzing chunk 1/42 for QID Q4 (processed chunk len: 3158 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 1 (QID Q4).\n",
            "    Analyzing chunk 2/42 for QID Q4 (processed chunk len: 3262 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 2 (QID Q4).\n",
            "    Analyzing chunk 3/42 for QID Q4 (processed chunk len: 3219 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 3 (QID Q4).\n",
            "    Analyzing chunk 4/42 for QID Q4 (processed chunk len: 3134 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 4 (QID Q4).\n",
            "    Analyzing chunk 5/42 for QID Q4 (processed chunk len: 3142 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 5 (QID Q4).\n",
            "    Analyzing chunk 6/42 for QID Q4 (processed chunk len: 2924 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 6 (QID Q4).\n",
            "    Analyzing chunk 7/42 for QID Q4 (processed chunk len: 3325 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [WARN] Invalid motif object structure after label processing for QID Q4, Chunk 7, Item 3. Skipping item.\n",
            "      Extracted 3 structured motif objects from chunk 7 (QID Q4).\n",
            "    Analyzing chunk 8/42 for QID Q4 (processed chunk len: 2977 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 8 (QID Q4).\n",
            "    Analyzing chunk 9/42 for QID Q4 (processed chunk len: 3084 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 9 (QID Q4).\n",
            "    Analyzing chunk 10/42 for QID Q4 (processed chunk len: 3218 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 10 (QID Q4).\n",
            "    Analyzing chunk 11/42 for QID Q4 (processed chunk len: 2930 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 11 (QID Q4).\n",
            "    Analyzing chunk 12/42 for QID Q4 (processed chunk len: 3101 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 12 (QID Q4).\n",
            "    Analyzing chunk 13/42 for QID Q4 (processed chunk len: 3277 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 13 (QID Q4).\n",
            "    Analyzing chunk 14/42 for QID Q4 (processed chunk len: 3023 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 14 (QID Q4).\n",
            "    Analyzing chunk 15/42 for QID Q4 (processed chunk len: 3354 chars)...\n",
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 15: Expecting ',' delimiter: line 16 column 3 (char 1787)\n",
            "      Motif parsing/validation attempt 1 yielded no structured motifs for chunk 15 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 15: Expecting ',' delimiter: line 16 column 3 (char 1787)\n",
            "      Motif parsing/validation attempt 2 yielded no structured motifs for chunk 15 (QID Q4). Retrying if possible...\n",
            "      No valid structured motifs extracted from chunk 15 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 16/42 for QID Q4 (processed chunk len: 3108 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 16 (QID Q4).\n",
            "    Analyzing chunk 17/42 for QID Q4 (processed chunk len: 3043 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 17 (QID Q4).\n",
            "    Analyzing chunk 18/42 for QID Q4 (processed chunk len: 3019 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 18 (QID Q4).\n",
            "    Analyzing chunk 19/42 for QID Q4 (processed chunk len: 2844 chars)...\n",
            "      Motif parsing/validation attempt 1 yielded no structured motifs for chunk 19 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Motif parsing/validation attempt 2 yielded no structured motifs for chunk 19 (QID Q4). Retrying if possible...\n",
            "      No valid structured motifs extracted from chunk 19 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 20/42 for QID Q4 (processed chunk len: 2528 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 20 (QID Q4).\n",
            "    Analyzing chunk 21/42 for QID Q4 (processed chunk len: 2857 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 21 (QID Q4).\n",
            "    Analyzing chunk 22/42 for QID Q4 (processed chunk len: 3198 chars)...\n",
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 22: Expecting value: line 6 column 3 (char 371)\n",
            "      Motif parsing/validation attempt 1 yielded no structured motifs for chunk 22 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 22: Expecting value: line 6 column 3 (char 371)\n",
            "      Motif parsing/validation attempt 2 yielded no structured motifs for chunk 22 (QID Q4). Retrying if possible...\n",
            "      No valid structured motifs extracted from chunk 22 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 23/42 for QID Q4 (processed chunk len: 3311 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 23 (QID Q4).\n",
            "    Analyzing chunk 24/42 for QID Q4 (processed chunk len: 3323 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 24 (QID Q4).\n",
            "    Analyzing chunk 25/42 for QID Q4 (processed chunk len: 2882 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 25 (QID Q4).\n",
            "    Analyzing chunk 26/42 for QID Q4 (processed chunk len: 2450 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 26 (QID Q4).\n",
            "    Analyzing chunk 27/42 for QID Q4 (processed chunk len: 2412 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 27 (QID Q4).\n",
            "    Analyzing chunk 28/42 for QID Q4 (processed chunk len: 2441 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 28 (QID Q4).\n",
            "    Analyzing chunk 29/42 for QID Q4 (processed chunk len: 2999 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 29 (QID Q4).\n",
            "    Analyzing chunk 30/42 for QID Q4 (processed chunk len: 3353 chars)...\n",
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 30: Expecting ',' delimiter: line 24 column 71 (char 1483)\n",
            "      Motif parsing/validation attempt 1 yielded no structured motifs for chunk 30 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 30: Expecting ',' delimiter: line 24 column 71 (char 1483)\n",
            "      Motif parsing/validation attempt 2 yielded no structured motifs for chunk 30 (QID Q4). Retrying if possible...\n",
            "      No valid structured motifs extracted from chunk 30 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 31/42 for QID Q4 (processed chunk len: 3095 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 31 (QID Q4).\n",
            "    Analyzing chunk 32/42 for QID Q4 (processed chunk len: 3258 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 32 (QID Q4).\n",
            "    Analyzing chunk 33/42 for QID Q4 (processed chunk len: 2497 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 33 (QID Q4).\n",
            "    Analyzing chunk 34/42 for QID Q4 (processed chunk len: 2522 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 34 (QID Q4).\n",
            "    Analyzing chunk 35/42 for QID Q4 (processed chunk len: 3148 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 35 (QID Q4).\n",
            "    Analyzing chunk 36/42 for QID Q4 (processed chunk len: 3052 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 36 (QID Q4).\n",
            "    Analyzing chunk 37/42 for QID Q4 (processed chunk len: 3303 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 37 (QID Q4).\n",
            "    Analyzing chunk 38/42 for QID Q4 (processed chunk len: 3141 chars)...\n",
            "      Motif parsing/validation attempt 1 yielded no structured motifs for chunk 38 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Motif parsing/validation attempt 2 yielded no structured motifs for chunk 38 (QID Q4). Retrying if possible...\n",
            "      No valid structured motifs extracted from chunk 38 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 39/42 for QID Q4 (processed chunk len: 3279 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 39 (QID Q4).\n",
            "    Analyzing chunk 40/42 for QID Q4 (processed chunk len: 3326 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 40 (QID Q4).\n",
            "    Analyzing chunk 41/42 for QID Q4 (processed chunk len: 3164 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 41 (QID Q4).\n",
            "    Analyzing chunk 42/42 for QID Q4 (processed chunk len: 2617 chars)...\n",
            "      Extracted 5 structured motif objects from chunk 42 (QID Q4).\n",
            "  Extracted 161 raw motif objects from LLM for QID Q4 (across all chunks).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'current_s_set' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6fdb70ca6d9d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Executing main MDL pipeline (Single Cell Reverted Prompt V5 with Label Fix & UnboundLocalError fix) at {time.asctime()}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Main MDL pipeline execution finished at {time.asctime()}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6fdb70ca6d9d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Consolidate motifs (from Cell 4 equivalent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mconsolidated_motifs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsolidate_raw_motifs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_motifs_from_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mcurrent_qid_result_entry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_consolidated_motifs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsolidated_motifs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Consolidated into {len(consolidated_motifs_list)} unique motifs (by label) for QID {qid_identifier_str}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-93d208751cab>\u001b[0m in \u001b[0;36mconsolidate_raw_motifs\u001b[0;34m(list_of_all_raw_motifs)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Label exists, merge surface forms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mexisting_sfs_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsolidated_motifs_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"surface_forms\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# These are already lowercased\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mconsolidated_motifs_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"surface_forms\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexisting_sfs_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_s_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0;31m# Optionally, could append descriptions or choose the longest, etc. For now, keeps first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'current_s_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old to New MWP"
      ],
      "metadata": {
        "id": "oY6WtaUvNTgZ"
      },
      "id": "oY6WtaUvNTgZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Enhanced Prompt Single-Cell MWP\n",
        "# --- Imports ---\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict, Set\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "\n",
        "# --- Configuration ---\n",
        "# !!! IMPORTANT: UPDATE BASE_PROJECT_DIR TO YOUR ACTUAL PATH !!!\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n",
        "# BASE_PROJECT_DIR = './' # For local testing if files are relative\n",
        "\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"]\n",
        "\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "LLM_BATCH_SIZE_RESPONSES = 5\n",
        "LLM_RETRY_ATTEMPTS = 2\n",
        "MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 700\n",
        "MAX_MOTIFS_PER_CHUNK = 5 # Max motifs to ask LLM per chunk\n",
        "MAX_TEXT_FOR_BDM_HASH = 2000 # Max characters from text to use for BDM hash input\n",
        "\n",
        "\n",
        "MOTIF_SYMBOLIC_LABEL_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TOKEN_COST = 0.1\n",
        "MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.25\n",
        "MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.1\n",
        "\n",
        "MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "\n",
        "LLM_DEBUG_LOG_FILE = os.path.join(BASE_PROJECT_DIR, \"llm_motif_debug_log_single_cell_v5_label_fix.txt\")\n",
        "\n",
        "# --- Helper Function Definitions ---\n",
        "\n",
        "def tokenize_phrase(phrase_text: str) -> List[str]:\n",
        "    if not isinstance(phrase_text, str) or not phrase_text.strip(): return []\n",
        "    return phrase_text.lower().split()\n",
        "\n",
        "def calculate_L_H_token_based_structured(structured_motifs_list: List[Dict]) -> float:\n",
        "    # (Same as before - calculates L(H) based on config constants and motif structure)\n",
        "    if not structured_motifs_list: return 0.0\n",
        "    total_lh_cost = 0.0\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        current_motif_lh = 0.0\n",
        "        label_str = motif_obj.get('label', \"\")\n",
        "        if isinstance(label_str, str) and label_str.strip():\n",
        "            current_motif_lh += MOTIF_SYMBOLIC_LABEL_COST\n",
        "        description_str = motif_obj.get('description', \"\")\n",
        "        if isinstance(description_str, str) and description_str.strip():\n",
        "            current_motif_lh += MOTIF_DESCRIPTION_TEXT_BASE_COST\n",
        "            current_motif_lh += len(tokenize_phrase(description_str)) * MOTIF_DESCRIPTION_TOKEN_COST\n",
        "        surface_forms_list = motif_obj.get('surface_forms', [])\n",
        "        if isinstance(surface_forms_list, list) and surface_forms_list:\n",
        "            valid_sfs_for_lh = [sf for sf in surface_forms_list if isinstance(sf, str) and sf.strip()]\n",
        "            if valid_sfs_for_lh:\n",
        "                current_motif_lh += MOTIF_SURFACE_FORMS_LIST_BASE_COST\n",
        "                for sf_str in valid_sfs_for_lh:\n",
        "                    current_motif_lh += len(tokenize_phrase(sf_str)) * MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH\n",
        "        total_lh_cost += current_motif_lh\n",
        "    return total_lh_cost\n",
        "\n",
        "def llm_compress_text_structured(text_to_compress: str, structured_motifs_list: List[Dict]) -> str:\n",
        "    # (Same as before - compresses text using motif labels for surface forms)\n",
        "    if not isinstance(text_to_compress, str): return \"\"\n",
        "    if not structured_motifs_list: return text_to_compress.lower()\n",
        "    compressed_text = text_to_compress.lower()\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        label = motif_obj.get('label', None)\n",
        "        surface_forms = motif_obj.get('surface_forms', [])\n",
        "        if not (isinstance(label, str) and label.strip()) or \\\n",
        "           not (isinstance(surface_forms, list) and surface_forms):\n",
        "            continue\n",
        "        placeholder = label\n",
        "        sorted_sfs_for_this_motif = sorted(\n",
        "            [sf for sf in surface_forms if isinstance(sf, str) and sf.strip()], key=len, reverse=True\n",
        "        )\n",
        "        for sf_str in sorted_sfs_for_this_motif:\n",
        "            sf_lower = sf_str.lower()\n",
        "            try:\n",
        "                compressed_text = re.sub(r'\\b' + re.escape(sf_lower) + r'\\b', placeholder, compressed_text)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for SF '{sf_str}' of motif '{label}': {re_e}. Skipping.\")\n",
        "                continue\n",
        "    return compressed_text\n",
        "\n",
        "def text_to_binary_matrix(text_input: str, size: tuple = MATRIX_SIZE_GLOBAL) -> np.ndarray:\n",
        "    # (Same as before)\n",
        "    if not isinstance(text_input, str) or not text_input.strip(): return np.zeros(size, dtype=int)\n",
        "    hash_obj = hashlib.sha256(text_input.encode('utf-8', 'ignore'))\n",
        "    hash_digest = hash_obj.hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string_from_hash = bin(int(hash_digest, 16))[2:].zfill(256)\n",
        "    binary_string_for_matrix = binary_string_from_hash[:required_bits] if required_bits <= 256 else binary_string_from_hash.ljust(required_bits, '0')\n",
        "    bits_for_matrix = [int(b) for b in binary_string_for_matrix]\n",
        "    return np.array(bits_for_matrix).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input: str, bdm_instance: BDM, matrix_s: tuple = MATRIX_SIZE_GLOBAL) -> float:\n",
        "    # (Same as before)\n",
        "    if not isinstance(text_input, str) or not text_input.strip() : return 0.0\n",
        "    text_for_hash = text_input[:MAX_TEXT_FOR_BDM_HASH] if len(text_input) > MAX_TEXT_FOR_BDM_HASH else text_input\n",
        "    if not text_for_hash.strip(): return 0.0\n",
        "    binary_matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        return bdm_instance.bdm(binary_matrix)\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0\n",
        "\n",
        "def compute_mdl_cost_for_text_block(full_qid_corpus_str: str,\n",
        "                                    final_motifs_to_evaluate: List[Dict],\n",
        "                                    bdm_instance: BDM,\n",
        "                                    matrix_s: tuple = MATRIX_SIZE_GLOBAL) -> tuple[float, float, float]:\n",
        "    # (Same as before)\n",
        "    if not isinstance(full_qid_corpus_str, str) : full_qid_corpus_str = \"\"\n",
        "    l_h = calculate_L_H_token_based_structured(final_motifs_to_evaluate)\n",
        "    compressed_text_block = llm_compress_text_structured(full_qid_corpus_str, final_motifs_to_evaluate)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "    if l_d_h < 0: return l_h, -1.0, -1.0\n",
        "    return l_h, l_d_h, l_h + l_d_h\n",
        "\n",
        "def preprocess_corpus_for_motif_extraction(text_corpus: str) -> str:\n",
        "    # (Same as before)\n",
        "    if not isinstance(text_corpus, str): return \"\"\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text_corpus)\n",
        "    text = re.sub(r' {2,}', ' ', text)\n",
        "    lines = text.split('\\n')\n",
        "    filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10 or not line.strip()]\n",
        "    return '\\n'.join(filtered_lines)\n",
        "\n",
        "def count_sf_occurrences(corpus_text: str, surface_form: str) -> int:\n",
        "    # (Same as before)\n",
        "    if not corpus_text or not surface_form or not isinstance(corpus_text, str) or not isinstance(surface_form, str):\n",
        "        return 0\n",
        "    try:\n",
        "        return len(re.findall(re.escape(surface_form.lower()), corpus_text.lower(), flags=re.IGNORECASE))\n",
        "    except re.error as e:\n",
        "        print(f\"    [WARN] Regex error in count_sf_occurrences for SF '{surface_form}': {e}\")\n",
        "        return 0\n",
        "\n",
        "# --- LLM Interaction Functions ---\n",
        "def build_llm_prompt_for_motifs(text_block_for_prompt: str, max_motifs_to_extract: int = MAX_MOTIFS_PER_CHUNK) -> str:\n",
        "    \"\"\"Reverted to the simpler prompt structure, hoping for better label format adherence.\"\"\"\n",
        "    if len(text_block_for_prompt) > MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK:\n",
        "        text_block_for_prompt = text_block_for_prompt[:MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK]\n",
        "\n",
        "    # This is the prompt from the \"Old Successful Code Cell\" that seemed to get bracketed labels\n",
        "    prompt = f\"\"\"You will receive a set of comments from different people answering the same question.\n",
        "\n",
        "Your task is to identify up to {max_motifs_to_extract} key recurring themes.\n",
        "\n",
        "For each theme, provide:\n",
        "- A short label like [DATA_PRIVACY]\n",
        "- A 1-sentence description of the theme\n",
        "- 2–3 short phrases that often appear in the text (surface forms)\n",
        "\n",
        "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
        "Example of one object in the list:\n",
        "{{\n",
        "  \"label\": \"[EXAMPLE_LABEL]\",\n",
        "  \"description\": \"A concise description of the example theme.\",\n",
        "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
        "}}\n",
        "If no clear motifs are found, output an empty JSON list: `[]`.\n",
        "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
        "\n",
        "Set of comments to analyze:\n",
        "\\\"\\\"\\\"\n",
        "{text_block_for_prompt}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def call_local_llm_for_raw_response(\n",
        "    prompt_content_for_user_turn: str,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str,\n",
        "    chunk_idx_for_log: int\n",
        "    ) -> str:\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance:\n",
        "        print(f\"    ERROR (call_local_llm): LLM pipeline/tokenizer not initialized for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return \"\"\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": prompt_content_for_user_turn}]\n",
        "    try:\n",
        "        prompt_formatted_for_llm = hf_tokenizer_instance.apply_chat_template(\n",
        "            messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception as e_template:\n",
        "        print(f\"    ERROR (call_local_llm): Applying chat template failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_template}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm_for_raw_response) ---\\nERROR APPLYING CHAT TEMPLATE: {e_template}\\nUser prompt content (first 300 chars): {prompt_content_for_user_turn[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "    generation_args = { # Using args for deterministic output\n",
        "        \"max_new_tokens\": LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION,\n",
        "        \"do_sample\": False,\n",
        "        \"pad_token_id\": hf_tokenizer_instance.pad_token_id\n",
        "    }\n",
        "    # print(f\"    DEBUG (call_local_llm): QID {qid_for_log}, Chunk {chunk_idx_for_log}, Prompt len: {len(prompt_formatted_for_llm)}, GenArgs: {generation_args}\")\n",
        "\n",
        "    try:\n",
        "        outputs = hf_pipeline_instance(prompt_formatted_for_llm, **generation_args)\n",
        "        if outputs and isinstance(outputs, list) and len(outputs) > 0 and \\\n",
        "           outputs[0] and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
        "            assistant_response_text = outputs[0]['generated_text'].strip()\n",
        "            # print(f\"    DEBUG (call_local_llm): QID {qid_for_log}, Chunk {chunk_idx_for_log}, Raw LLM Output:\\n{assistant_response_text[:500]}...\")\n",
        "            return assistant_response_text\n",
        "        else:\n",
        "            print(f\"    WARN (call_local_llm): LLM pipeline returned unexpected structure for QID {qid_for_log}, Chunk {chunk_idx_for_log}. Output: {outputs}\")\n",
        "            return \"\"\n",
        "    except Exception as e_pipeline:\n",
        "        print(f\"    ERROR (call_local_llm): Exception during hf_pipeline call for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_pipeline}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm_for_raw_response) ---\\nERROR DURING PIPELINE CALL: {e_pipeline}\\nFormatted prompt (first 300 chars): {prompt_formatted_for_llm[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "def parse_and_validate_llm_json_response(\n",
        "    llm_raw_response_text: str,\n",
        "    qid_for_log:str,\n",
        "    chunk_idx_for_log:int,\n",
        "    prompt_sent_to_llm:str\n",
        "    ) -> List[Dict]:\n",
        "    json_str_candidate = llm_raw_response_text.strip()\n",
        "    if json_str_candidate.startswith(\"```json\"): json_str_candidate = json_str_candidate[len(\"```json\"):].strip()\n",
        "    if json_str_candidate.startswith(\"```\"): json_str_candidate = json_str_candidate[len(\"```\"):].strip()\n",
        "    if json_str_candidate.endswith(\"```\"): json_str_candidate = json_str_candidate[:-len(\"```\")].strip()\n",
        "\n",
        "    # print(f\"    DEBUG (parse_validate): QID {qid_for_log}, Chunk {chunk_idx_for_log}, JSON candidate for parsing:\\n{json_str_candidate[:500]}...\")\n",
        "\n",
        "    if not json_str_candidate or json_str_candidate.lower() == \"[]\" or \"no_themes_found\" in json_str_candidate.lower() or \"no clear motifs\" in json_str_candidate.lower():\n",
        "        return []\n",
        "    try:\n",
        "        parsed_data = json.loads(json_str_candidate)\n",
        "        if isinstance(parsed_data, dict): parsed_data = [parsed_data]\n",
        "        if not isinstance(parsed_data, list): raise ValueError(\"Parsed JSON is not a list or single object.\")\n",
        "\n",
        "        valid_motifs_from_json = []\n",
        "        for item_idx, item in enumerate(parsed_data):\n",
        "            # --- Start: Added logic to handle empty item dict {} ---\n",
        "            if not item: # If item is an empty dictionary\n",
        "                # print(f\"    DEBUG (parse_validate): Skipping empty item object {{}} for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}\")\n",
        "                continue\n",
        "            # --- End: Added logic to handle empty item dict {} ---\n",
        "\n",
        "            label_str_original = item.get('label', \"\") # Get original label\n",
        "            label_str_processed = \"\" # This will store the potentially fixed label\n",
        "\n",
        "            if isinstance(label_str_original, str) and label_str_original.strip():\n",
        "                temp_label = label_str_original.strip()\n",
        "                # Try to extract a bracketed part if it exists and there's other text\n",
        "                match = re.search(r\"(\\[[A-Z0-9_]+\\])\", temp_label)\n",
        "                if match and match.group(1) == temp_label: # The whole string is correctly bracketed\n",
        "                    label_str_processed = temp_label\n",
        "                elif match: # Found a bracketed part within a longer string\n",
        "                    label_str_processed = match.group(1)\n",
        "                    # print(f\"    DEBUG (parse_validate): Extracted bracketed label '{label_str_processed}' from '{label_str_original}' for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}\")\n",
        "                elif not (temp_label.startswith('[') and temp_label.endswith(']')):\n",
        "                    # If no brackets or not solely bracketed, attempt to sanitize and add them\n",
        "                    sanitized_content = re.sub(r'\\s+', '_', temp_label)\n",
        "                    sanitized_content = re.sub(r'[^\\w_]', '', sanitized_content).upper()\n",
        "                    # Take first 3 \"words\" (snake_case parts) for label if too long\n",
        "                    sanitized_content = \"_\".join(sanitized_content.split('_')[:3])\n",
        "                    if sanitized_content:\n",
        "                        label_str_processed = f\"[{sanitized_content}]\"\n",
        "                        # print(f\"    DEBUG (parse_validate): Auto-formatted label from '{label_str_original}' to '{label_str_processed}' for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}\")\n",
        "                    # else label_str_processed remains \"\"\n",
        "\n",
        "            # Use the processed label for validation\n",
        "            current_item_label_for_validation = label_str_processed\n",
        "            item['label'] = current_item_label_for_validation # Update item with processed label for consistency if it passes\n",
        "\n",
        "            desc_str = item.get('description',\"\")\n",
        "            sf_list = item.get('surface_forms', [])\n",
        "\n",
        "            is_dict_val = isinstance(item, dict)\n",
        "            has_all_keys_val = all(k in item for k in [\"label\", \"description\", \"surface_forms\"])\n",
        "\n",
        "            is_label_str_val = isinstance(current_item_label_for_validation, str) and bool(current_item_label_for_validation)\n",
        "            label_starts_bracket_val = current_item_label_for_validation.startswith('[') if is_label_str_val else False\n",
        "            label_ends_bracket_val = current_item_label_for_validation.endswith(']') if is_label_str_val else False\n",
        "\n",
        "            is_desc_str_val = isinstance(desc_str, str) # Allow empty description string\n",
        "            is_sf_list_val = isinstance(sf_list, list)\n",
        "            sfs_are_strings_val = all(isinstance(sf_item, str) for sf_item in sf_list) if is_sf_list_val else False\n",
        "\n",
        "            # --- UNCOMMENT FOR DETAILED VALIDATION DEBUG ---\n",
        "            # print(f\"      DEBUG (parse_validate): Validating item {item_idx+1} for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\")\n",
        "            # print(f\"        ITEM_CONTENT_AFTER_LABEL_PROCESSING: {json.dumps(item, indent=2)}\")\n",
        "            # print(f\"        is_dict: {is_dict_val}\")\n",
        "            # print(f\"        has_all_keys: {has_all_keys_val}\")\n",
        "            # print(f\"        is_label_str: {is_label_str_val}, label_content_for_validation: '{current_item_label_for_validation}'\")\n",
        "            # print(f\"        label_starts_bracket: {label_starts_bracket_val}\")\n",
        "            # print(f\"        label_ends_bracket: {label_ends_bracket_val}\")\n",
        "            # print(f\"        is_desc_str: {is_desc_str_val}\")\n",
        "            # print(f\"        is_sf_list: {is_sf_list_val}\")\n",
        "            # print(f\"        sfs_are_strings: {sfs_are_strings_val}\")\n",
        "            # --- END DETAILED VALIDATION DEBUG ---\n",
        "\n",
        "            if is_dict_val and has_all_keys_val and \\\n",
        "               is_label_str_val and label_starts_bracket_val and label_ends_bracket_val and \\\n",
        "               is_desc_str_val and is_sf_list_val and sfs_are_strings_val:\n",
        "                valid_motifs_from_json.append({\n",
        "                    \"label\": current_item_label_for_validation,\n",
        "                    \"description\": desc_str.strip(),\n",
        "                    \"surface_forms\": [s.strip() for s in sf_list if isinstance(s, str) and s.strip()]\n",
        "                })\n",
        "                # print(\"        RESULT: Item PASSED validation.\")\n",
        "            else:\n",
        "                print(f\"    [WARN] Invalid motif object structure in LLM JSON for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}. Skipping item.\")\n",
        "                # ... (logging to file) ...\n",
        "        return valid_motifs_from_json\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        print(f\"    [WARN] Motif JSON parsing or core structure validation failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e}\")\n",
        "        # ... (logging to file) ...\n",
        "        return []\n",
        "\n",
        "def get_motifs_for_qid_batched(\n",
        "    list_of_individual_response_texts: List[str],\n",
        "    responses_per_batch: int,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str\n",
        "    ) -> List[Dict]:\n",
        "    # (This function remains largely the same as your last correct version, calling the above helpers)\n",
        "    all_raw_motifs_from_chunks = []\n",
        "    batched_text_chunks_for_llm = []\n",
        "    for i in range(0, len(list_of_individual_response_texts), responses_per_batch):\n",
        "        batch_responses = list_of_individual_response_texts[i:i + responses_per_batch]\n",
        "        chunk_text_for_llm = preprocess_corpus_for_motif_extraction(\"\\n\\n<RSP_SEP>\\n\\n\".join(batch_responses))\n",
        "        batched_text_chunks_for_llm.append(chunk_text_for_llm)\n",
        "\n",
        "    print(f\"  QID {qid_for_log}: Processing {len(list_of_individual_response_texts)} responses in {len(batched_text_chunks_for_llm)} preprocessed chunks (batch size: {responses_per_batch} responses).\")\n",
        "\n",
        "    for chunk_idx, text_chunk_to_analyze_processed in enumerate(batched_text_chunks_for_llm):\n",
        "        print(f\"    Analyzing chunk {chunk_idx + 1}/{len(batched_text_chunks_for_llm)} for QID {qid_for_log} (processed chunk len: {len(text_chunk_to_analyze_processed)} chars)...\")\n",
        "        if len(text_chunk_to_analyze_processed.strip()) < 50:\n",
        "            print(f\"      Chunk {chunk_idx+1} (QID {qid_for_log}) too short after preprocessing, skipping.\")\n",
        "            continue\n",
        "        prompt_for_llm = build_llm_prompt_for_motifs(text_chunk_to_analyze_processed)\n",
        "        motifs_from_this_chunk = []\n",
        "        for attempt in range(LLM_RETRY_ATTEMPTS):\n",
        "            raw_llm_response = call_local_llm_for_raw_response(\n",
        "                prompt_for_llm, hf_pipeline_instance, hf_tokenizer_instance, qid_for_log, chunk_idx + 1\n",
        "            )\n",
        "            if not raw_llm_response:\n",
        "                print(f\"      LLM call attempt {attempt + 1} for chunk {chunk_idx+1} (QID {qid_for_log}) returned empty string. Retrying if possible...\")\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "                continue\n",
        "            parsed_motifs_from_this_attempt = parse_and_validate_llm_json_response(\n",
        "                raw_llm_response, qid_for_log, chunk_idx+1, prompt_for_llm\n",
        "            )\n",
        "            if parsed_motifs_from_this_attempt:\n",
        "                motifs_from_this_chunk = parsed_motifs_from_this_attempt\n",
        "                break\n",
        "            else:\n",
        "                print(f\"      Motif parsing/validation attempt {attempt + 1} yielded no structured motifs for chunk {chunk_idx+1} (QID {qid_for_log}). Retrying if possible...\")\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "        if motifs_from_this_chunk:\n",
        "            print(f\"      Extracted {len(motifs_from_this_chunk)} structured motif objects from chunk {chunk_idx+1} (QID {qid_for_log}).\")\n",
        "            all_raw_motifs_from_chunks.extend(motifs_from_this_chunk)\n",
        "        else:\n",
        "            print(f\"      No valid structured motifs extracted from chunk {chunk_idx+1} (QID {qid_for_log}) after {LLM_RETRY_ATTEMPTS} attempts.\")\n",
        "    return all_raw_motifs_from_chunks\n",
        "\n",
        "\n",
        "def consolidate_raw_motifs(list_of_all_raw_motifs: List[Dict]) -> List[Dict]:\n",
        "    # (Same as before)\n",
        "    if not list_of_all_raw_motifs: return []\n",
        "    consolidated_motifs_map = {}\n",
        "    for motif_obj in list_of_all_raw_motifs:\n",
        "        label = motif_obj.get(\"label\",\"\").strip()\n",
        "        description = motif_obj.get(\"description\",\"\").strip()\n",
        "        surface_forms = motif_obj.get(\"surface_forms\", [])\n",
        "        if not (label and isinstance(surface_forms, list)): continue # Description can be empty string but label must exist\n",
        "        current_sfs_set = set(sf.lower().strip() for sf in surface_forms if isinstance(sf, str) and sf.strip())\n",
        "        if label not in consolidated_motifs_map:\n",
        "            consolidated_motifs_map[label] = {\n",
        "                \"label\": label, \"description\": description, \"surface_forms\": sorted(list(current_sfs_set))\n",
        "            }\n",
        "        else:\n",
        "            existing_sfs_set = set(consolidated_motifs_map[label].get(\"surface_forms\", []))\n",
        "            consolidated_motifs_map[label][\"surface_forms\"] = sorted(list(existing_sfs_set.union(current_sfs_set)))\n",
        "    return list(consolidated_motifs_map.values())\n",
        "\n",
        "def filter_surface_forms_by_global_frequency(\n",
        "    consolidated_motifs_list: List[Dict],\n",
        "    full_qid_corpus_text: str,\n",
        "    min_global_freq: int = MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "    ) -> List[Dict]:\n",
        "    # (Same as before)\n",
        "    if not consolidated_motifs_list: return []\n",
        "    final_globally_filtered_motifs = []\n",
        "    for motif_obj in consolidated_motifs_list:\n",
        "        globally_frequent_sfs_for_this_motif = []\n",
        "        original_sfs_for_this_motif = motif_obj.get(\"surface_forms\", [])\n",
        "        for sf_str in original_sfs_for_this_motif:\n",
        "            count = count_sf_occurrences(full_qid_corpus_text, sf_str)\n",
        "            if count >= min_global_freq:\n",
        "                globally_frequent_sfs_for_this_motif.append(sf_str)\n",
        "        if globally_frequent_sfs_for_this_motif:\n",
        "            filtered_motif_entry = motif_obj.copy()\n",
        "            filtered_motif_entry[\"surface_forms\"] = sorted(list(set(globally_frequent_sfs_for_this_motif)))\n",
        "            final_globally_filtered_motifs.append(filtered_motif_entry)\n",
        "    return final_globally_filtered_motifs\n",
        "\n",
        "# This function should be defined BEFORE your main() function\n",
        "\n",
        "def initialize_llm_pipeline(\n",
        "    model_id: str = LOCAL_LLM_MODEL_ID,\n",
        "    use_quantization: bool = USE_QUANTIZATION_FOR_LOCAL_LLM,\n",
        "    pipeline_return_full_text: bool = False # Defaulting to False\n",
        "    ):\n",
        "    \"\"\"Initializes and returns the Hugging Face pipeline and tokenizer.\"\"\"\n",
        "    print(f\"--- Initializing LLM Pipeline (model: {model_id}, quantization: {use_quantization}, return_full_text: {pipeline_return_full_text}) ---\")\n",
        "\n",
        "    hf_pipeline_instance = None\n",
        "    hf_tokenizer_instance = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {model_id}...\")\n",
        "        hf_tokenizer_instance = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "        if hf_tokenizer_instance.pad_token is None:\n",
        "            print(\"Tokenizer does not have a pad_token; setting pad_token = eos_token.\")\n",
        "            hf_tokenizer_instance.pad_token = hf_tokenizer_instance.eos_token\n",
        "            # Note: model.config.pad_token_id will be set below after model loading\n",
        "\n",
        "        bnb_config = None\n",
        "        quant_active = False\n",
        "        if use_quantization and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if (device.type == 'cuda' and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=compute_dtype,\n",
        "                    bnb_4bit_use_double_quant=True\n",
        "                )\n",
        "                quant_active = True\n",
        "                print(f\"BitsAndBytesConfig created for {model_id}, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb:\n",
        "                print(f\"WARN: Failed to create BitsAndBytesConfig: {e_bnb}. Quantization may be disabled or fall back.\")\n",
        "                quant_active = False\n",
        "\n",
        "        print(f\"Loading local model {model_id} (Quantization active: {quant_active})...\")\n",
        "        model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "\n",
        "        if quant_active and bnb_config:\n",
        "            model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        elif device.type == 'cuda':\n",
        "             model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "        hf_model_instance = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "\n",
        "        if hf_tokenizer_instance.pad_token_id == hf_tokenizer_instance.eos_token_id:\n",
        "             hf_model_instance.config.pad_token_id = hf_model_instance.config.eos_token_id\n",
        "\n",
        "        hf_pipeline_instance = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=hf_model_instance,\n",
        "            tokenizer=hf_tokenizer_instance,\n",
        "            return_full_text=pipeline_return_full_text\n",
        "        )\n",
        "        print(f\"Local LLM pipeline for {model_id} initialized successfully.\")\n",
        "        return hf_pipeline_instance, hf_tokenizer_instance\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to initialize local LLM pipeline: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "# --- BDM Initialization (should also be defined before main) ---\n",
        "def initialize_bdm_instance():\n",
        "    \"\"\"Initializes and returns a BDM instance.\"\"\"\n",
        "    print(\"Initializing BDM instance...\")\n",
        "    try:\n",
        "        bdm_instance = BDM(ndim=2)\n",
        "        print(\"BDM instance initialized successfully (ndim=2, default CTM-based).\")\n",
        "        return bdm_instance\n",
        "    except Exception as e_bdm_init:\n",
        "        print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\")\n",
        "        if \"CTM data files\" in str(e_bdm_init).lower() or \"dataset\" in str(e_bdm_init).lower():\n",
        "            print(\"  BDM Error Hint: This might be related to missing/corrupted CTM data files for PyBDM.\")\n",
        "            print(\"  Ensure PyBDM is installed correctly and can access/download its data.\")\n",
        "            print(\"  You might need to run the following once in your environment:\")\n",
        "            print(\"  from pybdm import get_ctm_dataset; get_ctm_dataset(force=False)\")\n",
        "        return None\n",
        "\n",
        "# Now your main() function can be defined and called\n",
        "# def main():\n",
        "#   ...\n",
        "#   hf_pipeline_instance, hf_tokenizer_instance = initialize_llm_pipeline()\n",
        "#   bdm_instance_main = initialize_bdm_instance()\n",
        "#   ...\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#   main()\n",
        "\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "def main():\n",
        "    print(\"--- MWP Single Cell (Reverted Simpler Prompt v4, Label Fix Attempt, Global SF Filter, Batched LLM, Token-L(H), BDM L(D|H)) ---\")\n",
        "    # ... (Print config params - same as before) ...\n",
        "    # ... (Initialize debug log file - same as before) ...\n",
        "    # ... (Initialize LLM Pipeline - same as before, ensure return_full_text=False) ...\n",
        "    # ... (Initialize BDM - same as before, ensure BDM(ndim=2)) ...\n",
        "    # ... (Load Phase 2 Data - same as before) ...\n",
        "    # ... (QID selection logic with UnboundLocalError fix - same as before) ...\n",
        "    # --- Full main() logic from previous complete cell goes here ---\n",
        "    # --- It will call the updated functions defined above ---\n",
        "    print(f\"Timestamp: {time.asctime()}\")\n",
        "    print(\"\\n--- Configuration Summary ---\")\n",
        "    print(f\"LLM Model: {LOCAL_LLM_MODEL_ID}, Quantization: {USE_QUANTIZATION_FOR_LOCAL_LLM}\")\n",
        "    print(f\"LLM Batch Size (Responses): {LLM_BATCH_SIZE_RESPONSES}, Retries: {LLM_RETRY_ATTEMPTS}\")\n",
        "    print(f\"Max Text Chars per LLM Prompt Chunk: {MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK}\")\n",
        "    print(f\"Max New Tokens for LLM Motif Extraction: {LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION}\")\n",
        "    print(f\"Max Motifs to Request per Chunk: {MAX_MOTIFS_PER_CHUNK}\")\n",
        "    print(f\"L(H) Costs: Label={MOTIF_SYMBOLIC_LABEL_COST}, DescBase={MOTIF_DESCRIPTION_TEXT_BASE_COST}, DescToken={MOTIF_DESCRIPTION_TOKEN_COST}, SFListBase={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, SFTokenInLH={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "    print(f\"Global SF Filtering Min Freq: {MIN_SF_FREQUENCY_IN_FULL_CORPUS}\")\n",
        "    print(f\"BDM Hash Prefix Length: {MAX_TEXT_FOR_BDM_HASH}, BDM Matrix: {MATRIX_SIZE_GLOBAL}\")\n",
        "    print(f\"Debug Log File: {LLM_DEBUG_LOG_FILE}\")\n",
        "    print(\"--- End Configuration Summary ---\\n\")\n",
        "\n",
        "    try:\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"LLM Motif Debug Log - Run Started: {time.asctime()}\\nModel ID: {LOCAL_LLM_MODEL_ID}\\nPipeline Config: return_full_text=False\\nReverted Simpler Prompt with Label Fix Attempt Active\\n---\\n\")\n",
        "    except Exception as e_log: print(f\"WARN: Could not initialize debug log file {LLM_DEBUG_LOG_FILE}: {e_log}\")\n",
        "\n",
        "    try:\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f: # Use \"a\" to append to existing header\n",
        "            f.write(f\"\\n--- Test Log Entry from main() at {time.asctime()} --m`, `llm_raw_response_text`, `item`) might contain some character or be of a type that causes `f.write()` to fail silently or throw an error that's caught broadly elsewhere without specific indication. (Less-\\n\")\n",
        "            f.flush()\n",
        "        print(f\"DEBUG: Successfully wrote test entry to {LLM_DEBUG_LOG_FILE}\")\n",
        "    except Exception as e_test_log:\n",
        "        print(f\"CRITICAL DEBUG: FAILED TO WRITE TEST ENTRY TO LOG FILE {LLM_DEBUG_LOG_FILE}: {e_test_log}\")\n",
        "\n",
        "    hf_pipeline_instance, hf_tokenizer_instance = initialize_llm_pipeline()\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance: print(\"CRITICAL: Exiting due to LLM pipeline initialization failure.\"); return\n",
        "\n",
        "    bdm_instance_main = initialize_bdm_instance()\n",
        "    if not bdm_instance_main: print(\"CRITICAL: Exiting due to BDM initialization failure.\"); return\n",
        "\n",
        "    if not os.path.exists(P2_COLLATED_FILE): print(f\"ERROR: Phase 2 file not found: {P2_COLLATED_FILE}\"); return\n",
        "    print(f\"Loading Phase 2 data from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f: phase2_data_content = json.load(f)\n",
        "    except Exception as e_load: print(f\"Error loading {P2_COLLATED_FILE}: {e_load}\"); return\n",
        "\n",
        "    all_qid_mdl_results_list = []\n",
        "    aggregated_content_by_qid_from_file = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "    if not aggregated_content_by_qid_from_file: print(f\"No 'aggregated_pdf_content_by_qid' in {P2_COLLATED_FILE}.\"); return\n",
        "\n",
        "    qids_to_process_this_run = []\n",
        "    if P3_QIDS_TO_PROCESS_THEMATICALLY and isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and P3_QIDS_TO_PROCESS_THEMATICALLY:\n",
        "        qids_to_process_this_run = [qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY if qid in aggregated_content_by_qid_from_file]\n",
        "        if not qids_to_process_this_run:\n",
        "            print(f\"Warning: None of specified QIDs {P3_QIDS_TO_PROCESS_THEMATICALLY} found in loaded data. Exiting.\")\n",
        "            return\n",
        "    else:\n",
        "        qids_to_process_limit_fallback = 1\n",
        "        print(f\"P3_QIDS_TO_PROCESS_THEMATICALLY not set or empty. Processing up to {qids_to_process_limit_fallback} QID(s) from data as fallback.\")\n",
        "        qids_to_process_this_run = list(aggregated_content_by_qid_from_file.keys())[:qids_to_process_limit_fallback]\n",
        "        if not qids_to_process_this_run:\n",
        "            print(\"No QIDs available in data to process based on fallback. Exiting.\")\n",
        "            return\n",
        "\n",
        "    if not qids_to_process_this_run:\n",
        "        print(\"No QIDs selected for processing. Exiting.\")\n",
        "        return\n",
        "    print(f\"\\nMDL analysis will run for QIDs: {qids_to_process_this_run}\\n\")\n",
        "\n",
        "    for qid_identifier_str in qids_to_process_this_run:\n",
        "        print(f\"--- Analyzing Data for QID: {qid_identifier_str} ---\")\n",
        "        list_of_individual_response_structs = aggregated_content_by_qid_from_file.get(qid_identifier_str, [])\n",
        "        actual_response_texts_for_qid = [item.get(\"text\", \"\") for item in list_of_individual_response_structs if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()]\n",
        "        if not actual_response_texts_for_qid: print(f\"  No valid text for QID {qid_identifier_str}. Skipping.\"); print(\"-\" * 50); continue\n",
        "\n",
        "        full_corpus_text_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(actual_response_texts_for_qid)\n",
        "        if len(full_corpus_text_for_qid.strip()) < 100: print(f\"  Skipping QID {qid_identifier_str}: text too short.\"); print(\"-\" * 50); continue\n",
        "\n",
        "        num_total_responses_for_qid = len(actual_response_texts_for_qid)\n",
        "        print(f\"  Corpus for QID {qid_identifier_str}: {len(full_corpus_text_for_qid)} chars, {num_total_responses_for_qid} responses.\")\n",
        "\n",
        "        baseline_bdm_original_corpus = compute_bdm_for_text(full_corpus_text_for_qid, bdm_instance_main, MATRIX_SIZE_GLOBAL)\n",
        "        if baseline_bdm_original_corpus < 0: print(f\"  Error computing baseline BDM for QID {qid_identifier_str}. Skipping.\"); print(\"-\" * 50); continue\n",
        "        current_qid_baseline_mdl_cost = baseline_bdm_original_corpus\n",
        "        print(f\"  Baseline MDL for QID {qid_identifier_str} (L(D_orig)): {current_qid_baseline_mdl_cost:.4f}\")\n",
        "\n",
        "        raw_motifs_from_chunks = get_motifs_for_qid_batched(\n",
        "            actual_response_texts_for_qid, LLM_BATCH_SIZE_RESPONSES,\n",
        "            hf_pipeline_instance, hf_tokenizer_instance, qid_identifier_str\n",
        "        )\n",
        "        current_qid_result_entry = {\n",
        "            \"qid\": qid_identifier_str, \"corpus_len_chars\": len(full_corpus_text_for_qid), \"num_responses\": num_total_responses_for_qid,\n",
        "            \"baseline_mdl\": current_qid_baseline_mdl_cost, \"final_refined_motifs\": [], \"l_h_final_motifs\": 0.0,\n",
        "            \"l_d_h_final_motifs\": current_qid_baseline_mdl_cost, \"total_mdl_with_final_motifs\": current_qid_baseline_mdl_cost,\n",
        "            \"compression_achieved\": 0.0, \"num_raw_motifs_extracted\": len(raw_motifs_from_chunks),\n",
        "            \"num_consolidated_motifs\": 0, \"num_globally_refined_motifs\": 0\n",
        "        }\n",
        "        if not raw_motifs_from_chunks: print(f\"  No raw motifs by LLM for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "        print(f\"  Extracted {len(raw_motifs_from_chunks)} raw motif objects for QID {qid_identifier_str}.\")\n",
        "\n",
        "        consolidated_motifs_list = consolidate_raw_motifs(raw_motifs_from_chunks)\n",
        "        current_qid_result_entry[\"num_consolidated_motifs\"] = len(consolidated_motifs_list)\n",
        "        print(f\"  Consolidated into {len(consolidated_motifs_list)} unique motifs for QID {qid_identifier_str}.\")\n",
        "        if not consolidated_motifs_list: print(f\"  No unique motifs after consolidation for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  Consolidated Motifs for QID {qid_identifier_str} (BEFORE Global SF refinement):\")\n",
        "        for idx, mo_con in enumerate(consolidated_motifs_list): print(f\"    Cons. Motif {idx+1}: L='{mo_con.get('label')}', D='{mo_con.get('description','N/A')[:30]}...', SFs({len(mo_con.get('surface_forms',[]))})='{mo_con.get('surface_forms',[])[:2]}...'\")\n",
        "\n",
        "        globally_refined_motifs = filter_surface_forms_by_global_frequency(\n",
        "            consolidated_motifs_list, full_corpus_text_for_qid, MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "        )\n",
        "        current_qid_result_entry[\"num_globally_refined_motifs\"] = len(globally_refined_motifs)\n",
        "        print(f\"  Globally refined into {len(globally_refined_motifs)} motifs for QID {qid_identifier_str}.\")\n",
        "        if not globally_refined_motifs: print(f\"  No motifs left after GLOBAL SF refinement for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  Final Globally Refined Motifs for QID {qid_identifier_str}:\")\n",
        "        for idx, mo_final in enumerate(globally_refined_motifs): print(f\"    Refined Motif {idx+1}: L='{mo_final.get('label')}', D='{mo_final.get('description','N/A')[:60]}...', SFs({len(mo_final.get('surface_forms',[]))})='{mo_final.get('surface_forms',[])}'\")\n",
        "\n",
        "        l_h_final, l_d_h_final, total_mdl_final = compute_mdl_cost_for_text_block(\n",
        "            full_corpus_text_for_qid, globally_refined_motifs, bdm_instance_main, MATRIX_SIZE_GLOBAL\n",
        "        )\n",
        "        current_qid_result_entry.update({\n",
        "            \"final_refined_motifs\": globally_refined_motifs, \"l_h_final_motifs\": l_h_final,\n",
        "            \"l_d_h_final_motifs\": l_d_h_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"total_mdl_with_final_motifs\": total_mdl_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"compression_achieved\": \"BDM_ERROR\" if l_d_h_final < 0 else (current_qid_baseline_mdl_cost - total_mdl_final)\n",
        "        })\n",
        "        if l_d_h_final < 0: print(f\"  Error computing MDL cost (BDM error) for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  L(H) final motifs: {l_h_final:.4f}, L(D|H) compressed: {l_d_h_final:.4f}, Total MDL: {total_mdl_final:.4f}\")\n",
        "        compression_val = current_qid_result_entry[\"compression_achieved\"]\n",
        "        result_status_str = f\"SUCCESS: Comp: {compression_val:.4f}\" if isinstance(compression_val, float) and compression_val > 0.0001 else f\"NOTE: No sig. comp. Diff: {compression_val if isinstance(compression_val, str) else compression_val:.4f}\"\n",
        "        print(f\"  {result_status_str}\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50)\n",
        "\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary ---\")\n",
        "    if not all_qid_mdl_results_list: print(\"No QIDs processed.\")\n",
        "    else:\n",
        "        valid_results = [r for r in all_qid_mdl_results_list if isinstance(r.get('compression_achieved'), float) and r.get('l_h_final_motifs', -1.0) >= 0]\n",
        "        num_qids_ok = len(valid_results); num_comp = sum(1 for r in valid_results if r['compression_achieved'] > 0.0001)\n",
        "        print(f\"Targeted QIDs: {len(qids_to_process_this_run)}, Results logged: {len(all_qid_mdl_results_list)}, Valid MDL: {num_qids_ok}, QIDs compressed: {num_comp}\")\n",
        "        if num_comp > 0:\n",
        "            comp_vals = [r['compression_achieved'] for r in valid_results if r['compression_achieved'] > 0.0001]\n",
        "            print(f\"  Avg compression: {np.mean(comp_vals):.4f}, Max compression: {np.max(comp_vals):.4f}\")\n",
        "        else: print(\"  No compression achieved.\")\n",
        "\n",
        "        output_filename = os.path.join(BASE_PROJECT_DIR, \"mdl_analysis_single_cell_reverted_prompt_v4_labelfix.json\")\n",
        "        try:\n",
        "            with open(output_filename, \"w\", encoding=\"utf-8\") as f_out: json.dump(all_qid_mdl_results_list, f_out, indent=2, ensure_ascii=False)\n",
        "            print(f\"Detailed results saved to {output_filename}\")\n",
        "        except Exception as e_s: print(f\"Error saving results: {e_s}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Executing main MDL pipeline (Single Cell Reverted Prompt V4 with Label Fix Attempt) at {time.asctime()}...\")\n",
        "    main()\n",
        "    print(f\"Main MDL pipeline execution finished at {time.asctime()}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "544b149ee9b84729a55d6a455bac9c90",
            "a1e8a0ec78214c5b99e7eb0f813c0906",
            "838b815131e44d2dbbdba57ecb103208",
            "f6b3cf07405549bbabe7f906bed98a65",
            "ab8c4bd7599f4ed5a1f5204905cca6e9",
            "d431f912399f4ce8a47c27ab803350f0",
            "29032a12246d4526a764e77a8207b11a",
            "df418e4f62b94f18a290f5043fd87ac4",
            "60aa7572f23a451ba601c7336dcf997e",
            "83fe78a19cee47a5b2538f0eec11c150",
            "c0c036733d58472f98f18e9444e052b5",
            "e027f0310bde4079aedf69540abbf080",
            "f4a69a214ac14f63a66704063a06bdb0",
            "6f9f41fee36e465b87e78564c3bfc87d",
            "811c614ae93e4c798705941379b27581",
            "c4921b98f5c5435dad071cb78c7b6264",
            "faec9076954b4065b325e10da878d9e3",
            "8001c3d1c3eb4052ac41cf515948d44d",
            "a9207e813ed24c73918fdaceb3e560cb",
            "8e09f19832ef4940bc0a27d35cb19d4f",
            "8bc7c8bf36a5409e8d9c7dc4254e03bf",
            "2100ab3c1d2e4983a19720058f2cd4e6",
            "f1ecc31a0a3944b0b8c538fae95477b7",
            "03550f00a9dd4b30993d30e7aa284085",
            "cbbc7d0133da4cfd96fc714177923ade",
            "40ea27218c4d420fb396dfddc2a93e95",
            "c1601eaf9d8c469cbaf3dfd029848add",
            "fe83604d163d494dbd9fd63a56855c9b",
            "b3dda36901cb41c7bc17afc78255c20a",
            "af4a463d5c67432998ce063b4a911861",
            "6817babbc89e44a19ddbcba27d1a1e8c",
            "a91bb0bdab1e497b9961149a14b5fa43",
            "deffbfbcebf54d25945e364b67ca51d8",
            "4c5c170836e54bc9996cbd66c7b885ba",
            "d611e8a6ba74429196a534dd4bbc4174",
            "8e2b0a3c9f31434f9f9909a1b60c0ec6",
            "3761b8f77c9d4f2aa27187f1b3971775",
            "b5e5badcbc6f4e5eb27965f1c7c3adbe",
            "a8e4da327c9945c9ace894be067615b1",
            "b00f58216a5e4c1ea5deb180e948d705",
            "f29ab6b845f648ea99765a6a5f3ff9f8",
            "b2ca733d2047402aa7d240966e9e1e1f",
            "4a5be45728414aa0b2c0a2da72fde0ad",
            "45cbcd4a1a364eb6836a171f0fdd8a73",
            "3cb96c1713f04077a66e80efc3bcd0a2",
            "674d9d9dc3c84b2985516202ac73b193",
            "4647bf04a5734b4b8a03d831e27d71ea",
            "73f43d4a00cf4225b1f0695480de9121",
            "4566e5044db245e3a859ace1f92a37b7",
            "c3edfcaf8a6c4ea78cb8bbded7ada100",
            "0dc2469a30cd49cc89ff95b43465cce4",
            "2e964980f8ab477f939cfe88163ab13f",
            "1af4509d8b934606a8e849cb3acf46bc",
            "dea710f63bca4e2380716a814414e6b9",
            "86de0c21e1d44771a008deb1f9dfbab7",
            "58852eab7f804c1f960a9dd2f8af254c",
            "ff36e206409e461db9a9d48e843db499",
            "801f5fb175a6475aad9c48c2a797587a",
            "1f96711539df4f048a7f5ae06af77f41",
            "05daee421ab0412c965f9e87d6edcf49",
            "0d5296ae7bf84f1a91a0c798422436a5",
            "ed9067e2a2f2434482f7ca34dd6006a3",
            "dff8e3bcddbb4fb19d9291b7e99b1bc5",
            "f5b2b276571648ceab5c3827422b2ab5",
            "a4e148c54f104204af69b21ebfb583e4",
            "97a086416fa14f56ad74c7a118e66888",
            "3a8f0ed61ef34a80a03771fc3ecbae99",
            "26332e5d98594c75ad7e9dedb5f7977d",
            "beeb087ff3a54fc7ad3e6e4299ec0406",
            "1a2beae78a2c492bb62f6afb1ebe4264",
            "06c990e1edb94353888eb005b4a82288",
            "58a69f8304064ce588b20163ce7906b5",
            "72b88ab3cd5e4380998bff7874f89155",
            "1361d9b262a14baeaf9f9429ba72b73c",
            "b0575b7e6f6744c1aeef931c7130861e",
            "f6163977591b4165a715c4f30dce2a2e",
            "1272689efb714b04b244891fb4618d6f",
            "c717b35804754afca3e04a55b5d63a67",
            "c918bb59e3794beca4809a56a3214c63",
            "7d0cb3fe712341058189a55645d45c02",
            "95ddbae01df2472183e966de0a8d34ee",
            "9c34ae0ff78447eb965021109ce276a9",
            "f9649b9d582d451c9079e7358e854cbf",
            "7dc46cb490d3453e857755d0bb6c814d",
            "954617cf030c4982afafdcc463c97ca2",
            "4f2cdf0e06324b30ae1b90fd48d64c3a",
            "6715ac484f764fe58eed1e21827f7c80",
            "5a7f12aa5873413dbd0116f21c1da02b",
            "e69dfcb15a8a40b79ba6c0206728b7fa",
            "1b1fb2a9280b4a98852b65302129f74d",
            "2a7f91f7003b49d289d1ce1863083b3c",
            "0cc168a176b446f5a4aceca19573b9ba",
            "330652077f3445f8965a5ac39919ce27",
            "b9dc7edb1ef242c58fce49ce6eb575a7",
            "f56bace57e88409f866e1c3c258a4600",
            "e62144c209654e1eb5b57a7da630c9b1",
            "94fa7f0ca1a9457bbc6bfbe49bf813b7",
            "8f9f59f27ec94374acaf7660be552681",
            "38c785f41a7b478e862cdeb07c1fcc99",
            "5b9d0eb98a904bb6b05c8155b726c417",
            "837c5ac1a2e5467cacd148c840a87bc2",
            "f4a8cceeb58b4bb4ac314d7edfaa2754",
            "6f2c14a138664eebb9dc75c2849a4396",
            "f4bcdf8ed29c4dfab23ddd02dfc31bfa",
            "64da5c42a33f4c90bf4b70abc8393742",
            "610fdabe80704e95bea9628075c9cfc2",
            "ed596c3733804b96bbb3df6b6bf2fea5",
            "1cd33871c73a4d108cd6fc1ad0a45fba",
            "37ee374d4b35449588155870a424a0af",
            "3fc0668356914f748d611fb1bb95522a",
            "b19d8269d4e14c74a921ecb216a0dfe5",
            "58f7a3b11d7a499482f8145879782a20",
            "49c797ab9e1c480f9a9d7682db941aaf",
            "809819349c364d91bef83386a5a3dcaa",
            "5fdfc2d3558e46b4834a1419bc9cd2a2",
            "e8cff635f52643b0a02f0307b9b2c3e6",
            "e089aebba59b455a8299149b80af992f",
            "ecda6762438f49f5995bb3d61851d62c",
            "0f56bd992f94425eb9c5acdb6c7d4fed",
            "3238d3d52e534d488b129a544ced2b60",
            "bf0737078bb44b51b837a49f524dd5a3"
          ]
        },
        "id": "Sz3VF1_hb2tb",
        "outputId": "2f878d9b-29ea-4b6d-8db2-6741a221a69a"
      },
      "id": "Sz3VF1_hb2tb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing main MDL pipeline (Single Cell Reverted Prompt V4 with Label Fix Attempt) at Sun Jun  1 10:22:04 2025...\n",
            "--- MWP Single Cell (Reverted Simpler Prompt v4, Label Fix Attempt, Global SF Filter, Batched LLM, Token-L(H), BDM L(D|H)) ---\n",
            "Timestamp: Sun Jun  1 10:22:04 2025\n",
            "\n",
            "--- Configuration Summary ---\n",
            "LLM Model: google/gemma-2b-it, Quantization: True\n",
            "LLM Batch Size (Responses): 5, Retries: 2\n",
            "Max Text Chars per LLM Prompt Chunk: 7000\n",
            "Max New Tokens for LLM Motif Extraction: 700\n",
            "Max Motifs to Request per Chunk: 5\n",
            "L(H) Costs: Label=0.5, DescBase=0.5, DescToken=0.1, SFListBase=0.25, SFTokenInLH=0.1\n",
            "Global SF Filtering Min Freq: 2\n",
            "BDM Hash Prefix Length: 2000, BDM Matrix: (8, 8)\n",
            "Debug Log File: /content/drive/MyDrive/Colab Notebooks/Legal/llm_motif_debug_log_single_cell_v5_label_fix.txt\n",
            "--- End Configuration Summary ---\n",
            "\n",
            "DEBUG: Successfully wrote test entry to /content/drive/MyDrive/Colab Notebooks/Legal/llm_motif_debug_log_single_cell_v5_label_fix.txt\n",
            "--- Initializing LLM Pipeline (model: google/gemma-2b-it, quantization: True, return_full_text: False) ---\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-2b-it...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "544b149ee9b84729a55d6a455bac9c90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e027f0310bde4079aedf69540abbf080"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1ecc31a0a3944b0b8c538fae95477b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c5c170836e54bc9996cbd66c7b885ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BitsAndBytesConfig created for google/gemma-2b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-2b-it (Quantization active: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cb96c1713f04077a66e80efc3bcd0a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58852eab7f804c1f960a9dd2f8af254c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a8f0ed61ef34a80a03771fc3ecbae99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c717b35804754afca3e04a55b5d63a67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e69dfcb15a8a40b79ba6c0206728b7fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b9d0eb98a904bb6b05c8155b726c417"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b19d8269d4e14c74a921ecb216a0dfe5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-2b-it initialized successfully.\n",
            "Initializing BDM instance...\n",
            "BDM instance initialized successfully (ndim=2, default CTM-based).\n",
            "Loading Phase 2 data from: /content/drive/MyDrive/Colab Notebooks/Legal/Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MDL analysis will run for QIDs: ['Q4']\n",
            "\n",
            "--- Analyzing Data for QID: Q4 ---\n",
            "  Corpus for QID Q4: 129501 chars, 209 responses.\n",
            "  Baseline MDL for QID Q4 (L(D_orig)): 121.3693\n",
            "  QID Q4: Processing 209 responses in 42 preprocessed chunks (batch size: 5 responses).\n",
            "    Analyzing chunk 1/42 for QID Q4 (processed chunk len: 3158 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 1 (QID Q4).\n",
            "    Analyzing chunk 2/42 for QID Q4 (processed chunk len: 3262 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 2 (QID Q4).\n",
            "    Analyzing chunk 3/42 for QID Q4 (processed chunk len: 3219 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 3 (QID Q4).\n",
            "    Analyzing chunk 4/42 for QID Q4 (processed chunk len: 3134 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 4 (QID Q4).\n",
            "    Analyzing chunk 5/42 for QID Q4 (processed chunk len: 3142 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 5 (QID Q4).\n",
            "    Analyzing chunk 6/42 for QID Q4 (processed chunk len: 2924 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 6 (QID Q4).\n",
            "    Analyzing chunk 7/42 for QID Q4 (processed chunk len: 3325 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [WARN] Invalid motif object structure in LLM JSON for QID Q4, Chunk 7, Item 3. Skipping item.\n",
            "      Extracted 3 structured motif objects from chunk 7 (QID Q4).\n",
            "    Analyzing chunk 8/42 for QID Q4 (processed chunk len: 2977 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 8 (QID Q4).\n",
            "    Analyzing chunk 9/42 for QID Q4 (processed chunk len: 3084 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 9 (QID Q4).\n",
            "    Analyzing chunk 10/42 for QID Q4 (processed chunk len: 3218 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 10 (QID Q4).\n",
            "    Analyzing chunk 11/42 for QID Q4 (processed chunk len: 2930 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 11 (QID Q4).\n",
            "    Analyzing chunk 12/42 for QID Q4 (processed chunk len: 3101 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 12 (QID Q4).\n",
            "    Analyzing chunk 13/42 for QID Q4 (processed chunk len: 3277 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 13 (QID Q4).\n",
            "    Analyzing chunk 14/42 for QID Q4 (processed chunk len: 3023 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 14 (QID Q4).\n",
            "    Analyzing chunk 15/42 for QID Q4 (processed chunk len: 3354 chars)...\n",
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 15: Expecting ',' delimiter: line 16 column 3 (char 1787)\n",
            "      Motif parsing/validation attempt 1 yielded no structured motifs for chunk 15 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 15: Expecting ',' delimiter: line 16 column 3 (char 1787)\n",
            "      Motif parsing/validation attempt 2 yielded no structured motifs for chunk 15 (QID Q4). Retrying if possible...\n",
            "      No valid structured motifs extracted from chunk 15 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 16/42 for QID Q4 (processed chunk len: 3108 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 16 (QID Q4).\n",
            "    Analyzing chunk 17/42 for QID Q4 (processed chunk len: 3043 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 17 (QID Q4).\n",
            "    Analyzing chunk 18/42 for QID Q4 (processed chunk len: 3019 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 18 (QID Q4).\n",
            "    Analyzing chunk 19/42 for QID Q4 (processed chunk len: 2844 chars)...\n",
            "      Motif parsing/validation attempt 1 yielded no structured motifs for chunk 19 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Motif parsing/validation attempt 2 yielded no structured motifs for chunk 19 (QID Q4). Retrying if possible...\n",
            "      No valid structured motifs extracted from chunk 19 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 20/42 for QID Q4 (processed chunk len: 2528 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 20 (QID Q4).\n",
            "    Analyzing chunk 21/42 for QID Q4 (processed chunk len: 2857 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 21 (QID Q4).\n",
            "    Analyzing chunk 22/42 for QID Q4 (processed chunk len: 3198 chars)...\n",
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 22: Expecting value: line 6 column 3 (char 371)\n",
            "      Motif parsing/validation attempt 1 yielded no structured motifs for chunk 22 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 22: Expecting value: line 6 column 3 (char 371)\n",
            "      Motif parsing/validation attempt 2 yielded no structured motifs for chunk 22 (QID Q4). Retrying if possible...\n",
            "      No valid structured motifs extracted from chunk 22 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 23/42 for QID Q4 (processed chunk len: 3311 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 23 (QID Q4).\n",
            "    Analyzing chunk 24/42 for QID Q4 (processed chunk len: 3323 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 24 (QID Q4).\n",
            "    Analyzing chunk 25/42 for QID Q4 (processed chunk len: 2882 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 25 (QID Q4).\n",
            "    Analyzing chunk 26/42 for QID Q4 (processed chunk len: 2450 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 26 (QID Q4).\n",
            "    Analyzing chunk 27/42 for QID Q4 (processed chunk len: 2412 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 27 (QID Q4).\n",
            "    Analyzing chunk 28/42 for QID Q4 (processed chunk len: 2441 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 28 (QID Q4).\n",
            "    Analyzing chunk 29/42 for QID Q4 (processed chunk len: 2999 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 29 (QID Q4).\n",
            "    Analyzing chunk 30/42 for QID Q4 (processed chunk len: 3353 chars)...\n",
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 30: Expecting ',' delimiter: line 24 column 71 (char 1483)\n",
            "      Motif parsing/validation attempt 1 yielded no structured motifs for chunk 30 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [WARN] Motif JSON parsing or core structure validation failed for QID Q4, Chunk 30: Expecting ',' delimiter: line 24 column 71 (char 1483)\n",
            "      Motif parsing/validation attempt 2 yielded no structured motifs for chunk 30 (QID Q4). Retrying if possible...\n",
            "      No valid structured motifs extracted from chunk 30 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 31/42 for QID Q4 (processed chunk len: 3095 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 31 (QID Q4).\n",
            "    Analyzing chunk 32/42 for QID Q4 (processed chunk len: 3258 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 32 (QID Q4).\n",
            "    Analyzing chunk 33/42 for QID Q4 (processed chunk len: 2497 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 33 (QID Q4).\n",
            "    Analyzing chunk 34/42 for QID Q4 (processed chunk len: 2522 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 34 (QID Q4).\n",
            "    Analyzing chunk 35/42 for QID Q4 (processed chunk len: 3148 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 35 (QID Q4).\n",
            "    Analyzing chunk 36/42 for QID Q4 (processed chunk len: 3052 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 structured motif objects from chunk 36 (QID Q4).\n",
            "    Analyzing chunk 37/42 for QID Q4 (processed chunk len: 3303 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 37 (QID Q4).\n",
            "    Analyzing chunk 38/42 for QID Q4 (processed chunk len: 3141 chars)...\n",
            "      Motif parsing/validation attempt 1 yielded no structured motifs for chunk 38 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Motif parsing/validation attempt 2 yielded no structured motifs for chunk 38 (QID Q4). Retrying if possible...\n",
            "      No valid structured motifs extracted from chunk 38 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 39/42 for QID Q4 (processed chunk len: 3279 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 39 (QID Q4).\n",
            "    Analyzing chunk 40/42 for QID Q4 (processed chunk len: 3326 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 40 (QID Q4).\n",
            "    Analyzing chunk 41/42 for QID Q4 (processed chunk len: 3164 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 structured motif objects from chunk 41 (QID Q4).\n",
            "    Analyzing chunk 42/42 for QID Q4 (processed chunk len: 2617 chars)...\n",
            "      Extracted 5 structured motif objects from chunk 42 (QID Q4).\n",
            "  Extracted 161 raw motif objects for QID Q4.\n",
            "  Consolidated into 17 unique motifs for QID Q4.\n",
            "  Consolidated Motifs for QID Q4 (BEFORE Global SF refinement):\n",
            "    Cons. Motif 1: L='[DATA_PRIVACY]', D='A concern regarding the potent...', SFs(140)='[\"a 3-sentence summary of the excerpt's main points, addressing the question about exceptions in the employment context\", 'a brief summary of the topic']...'\n",
            "    Cons. Motif 2: L='[EXAMPLE_LABEL]', D='A concise summary of the excer...', SFs(23)='['a concise description of the example theme.', 'a focus on consumer expectations and current privacy laws']...'\n",
            "    Cons. Motif 3: L='[GDPR_BENCHMARK]', D='The GDPR provides a valuable b...', SFs(1)='['focus on robust privacy controls and careful consideration of exceptions']...'\n",
            "    Cons. Motif 4: L='[MISSING_LABEL]', D='The need for stronger privacy ...', SFs(2)='['importance of fair treatment and meaningful consent', 'potential application beyond consumer contexts']...'\n",
            "    Cons. Motif 5: L='[DATA_CONTROL_RESPONSIBILITY]', D='Data controllers, not data pro...', SFs(1)='['compliance with individual data rights should be voluntary and informed']...'\n",
            "    Cons. Motif 6: L='[NEW_SOUTH_WALES]', D='An exception for public sector...', SFs(1)='['exception for legitimate interest in employment']...'\n",
            "    Cons. Motif 7: L='[CONSENT_REQUIREMENT_FOR]', D='The employee records exemption...', SFs(1)='['collection of sensitive information without consent']...'\n",
            "    Cons. Motif 8: L='[PROTECTING_CHILDRENS_PRIVACY]', D='Emphasis on clear and accessib...', SFs(1)='['child-focused language, international recognition']...'\n",
            "    Cons. Motif 9: L='[BALANCING_INDIVIDUAL_RIGHTS]', D='Concerns about potential confl...', SFs(3)='['a discussion of the balance between rights and interests', 'a rationale for prioritizing clarity and practicality']...'\n",
            "    Cons. Motif 10: L='[LACK_OF_CLARITY]', D='The excerpt does not provide s...', SFs(0)='[]...'\n",
            "    Cons. Motif 11: L='[NEED_FOR_FURTHER]', D='The excerpt suggests the gover...', SFs(2)='['a call for consultation and engagement', 'a suggestion for an exposure draft to detail proposed exceptions']...'\n",
            "    Cons. Motif 12: L='[EXCEPTIONS_TO_INDIVIDUAL]', D='The excerpt discusses exceptio...', SFs(1)='['specific rights that are partially supported by the law council']...'\n",
            "    Cons. Motif 13: L='[INTERNATIONAL_STANDARDS]', D='The excerpt emphasizes the imp...', SFs(1)='['mention of prescribed countries and certification schemes']...'\n",
            "    Cons. Motif 14: L='[ENFORCEMENT_AND_CLARITY]', D='The excerpt highlights the nee...', SFs(1)='['recommendations for strengthening enforcement powers and clarifying roles']...'\n",
            "    Cons. Motif 15: L='[EXCEPTIONS_TO_RIGHTS]', D='The text supports removing the...', SFs(1)='[\"concerns that consent shouldn't fully override privacy protections\"]...'\n",
            "    Cons. Motif 16: L='[FLEXIBILITY_REGARDING_EMPLOYEE]', D='The text highlights the need f...', SFs(1)='['legal precedents and statutory provisions']...'\n",
            "    Cons. Motif 17: L='[CONCERNS_REGARDING_OVERLY]', D='The MFA argues that overly str...', SFs(1)='['unintended negative consequences for the digital ecosystem']...'\n",
            "  Globally refined into 2 motifs for QID Q4.\n",
            "  Final Globally Refined Motifs for QID Q4:\n",
            "    Refined Motif 1: L='[DATA_PRIVACY]', D='A concern regarding the potential erosion of privacy rights ...', SFs(15)='['code', 'commonwealth records', 'competing public interests', 'compliance burden', 'existing australian privacy principles', 'expanding individual rights to access, object, erase, correct, and de-index search results', 'international human rights standards', 'jurisdictional limitation to australia', 'media organizations', 'privacy act', 'privacy concerns', 'privacy principles', 'privacy protection', 'specific exceptions to these rights within the employment context', 'the text emphasizes the fundamental importance of privacy for democratic freedoms and other rights']'\n",
            "    Refined Motif 2: L='[EXAMPLE_LABEL]', D='A concise summary of the excerpt's main points regarding exc...', SFs(3)='['competing public interests', 'privacy act review report', 'relationships with a legal character']'\n",
            "  L(H) final motifs: 13.6000, L(D|H) compressed: 116.3387, Total MDL: 129.9387\n",
            "  NOTE: No sig. comp. Diff: -8.5694\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Overall QID-based MDL Analysis Summary ---\n",
            "Targeted QIDs: 1, Results logged: 1, Valid MDL: 1, QIDs compressed: 0\n",
            "  No compression achieved.\n",
            "Detailed results saved to /content/drive/MyDrive/Colab Notebooks/Legal/mdl_analysis_single_cell_reverted_prompt_v4_labelfix.json\n",
            "Main MDL pipeline execution finished at Sun Jun  1 10:37:13 2025.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Enhanced Prompt Single-Cell MWP\n",
        "# --- Imports ---\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict, Set\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/'\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"]\n",
        "\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "LLM_BATCH_SIZE_RESPONSES = 5\n",
        "LLM_RETRY_ATTEMPTS = 2\n",
        "MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000\n",
        "LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 700 # Back to 700 as per old successful script\n",
        "MAX_MOTIFS_PER_CHUNK = 5 # Back to 5 as per old successful script's prompt\n",
        "\n",
        "MOTIF_SYMBOLIC_LABEL_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TOKEN_COST = 0.1\n",
        "MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.25\n",
        "MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.1\n",
        "\n",
        "MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2\n",
        "\n",
        "LLM_DEBUG_LOG_FILE = os.path.join(BASE_PROJECT_DIR, \"llm_motif_debug_log_single_cell_v2_revert_prompt.txt\")\n",
        "\n",
        "# --- Helper Function Definitions ---\n",
        "\n",
        "def tokenize_phrase(phrase_text: str) -> List[str]:\n",
        "    if not isinstance(phrase_text, str) or not phrase_text.strip(): return []\n",
        "    return phrase_text.lower().split()\n",
        "\n",
        "def calculate_L_H_token_based_structured(structured_motifs_list: List[Dict]) -> float:\n",
        "    if not structured_motifs_list: return 0.0\n",
        "    total_lh_cost = 0.0\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        current_motif_lh = 0.0\n",
        "        label_str = motif_obj.get('label', \"\")\n",
        "        if isinstance(label_str, str) and label_str.strip():\n",
        "            current_motif_lh += MOTIF_SYMBOLIC_LABEL_COST\n",
        "        description_str = motif_obj.get('description', \"\")\n",
        "        if isinstance(description_str, str) and description_str.strip():\n",
        "            current_motif_lh += MOTIF_DESCRIPTION_TEXT_BASE_COST\n",
        "            current_motif_lh += len(tokenize_phrase(description_str)) * MOTIF_DESCRIPTION_TOKEN_COST\n",
        "        surface_forms_list = motif_obj.get('surface_forms', [])\n",
        "        if isinstance(surface_forms_list, list) and surface_forms_list:\n",
        "            valid_sfs_for_lh = [sf for sf in surface_forms_list if isinstance(sf, str) and sf.strip()]\n",
        "            if valid_sfs_for_lh:\n",
        "                current_motif_lh += MOTIF_SURFACE_FORMS_LIST_BASE_COST\n",
        "                for sf_str in valid_sfs_for_lh:\n",
        "                    current_motif_lh += len(tokenize_phrase(sf_str)) * MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH\n",
        "        total_lh_cost += current_motif_lh\n",
        "    return total_lh_cost\n",
        "\n",
        "def llm_compress_text_structured(text_to_compress: str, structured_motifs_list: List[Dict]) -> str:\n",
        "    if not isinstance(text_to_compress, str): return \"\"\n",
        "    if not structured_motifs_list: return text_to_compress.lower()\n",
        "    compressed_text = text_to_compress.lower()\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        label = motif_obj.get('label', None)\n",
        "        surface_forms = motif_obj.get('surface_forms', [])\n",
        "        if not (isinstance(label, str) and label.strip()) or \\\n",
        "           not (isinstance(surface_forms, list) and surface_forms):\n",
        "            continue\n",
        "        placeholder = label\n",
        "        sorted_sfs_for_this_motif = sorted(\n",
        "            [sf for sf in surface_forms if isinstance(sf, str) and sf.strip()], key=len, reverse=True\n",
        "        )\n",
        "        for sf_str in sorted_sfs_for_this_motif:\n",
        "            sf_lower = sf_str.lower()\n",
        "            try:\n",
        "                compressed_text = re.sub(r'\\b' + re.escape(sf_lower) + r'\\b', placeholder, compressed_text)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for SF '{sf_str}' of motif '{label}': {re_e}. Skipping.\")\n",
        "                continue\n",
        "    return compressed_text\n",
        "\n",
        "def text_to_binary_matrix(text_input: str, size: tuple = MATRIX_SIZE_GLOBAL) -> np.ndarray:\n",
        "    if not isinstance(text_input, str) or not text_input.strip(): return np.zeros(size, dtype=int)\n",
        "    hash_obj = hashlib.sha256(text_input.encode('utf-8', 'ignore'))\n",
        "    hash_digest = hash_obj.hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string_from_hash = bin(int(hash_digest, 16))[2:].zfill(256)\n",
        "    binary_string_for_matrix = binary_string_from_hash[:required_bits] if required_bits <= 256 else binary_string_from_hash.ljust(required_bits, '0')\n",
        "    bits_for_matrix = [int(b) for b in binary_string_for_matrix]\n",
        "    return np.array(bits_for_matrix).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input: str, bdm_instance: BDM, matrix_s: tuple = MATRIX_SIZE_GLOBAL) -> float:\n",
        "    if not isinstance(text_input, str) or not text_input.strip() : return 0.0\n",
        "    text_for_hash = text_input[:MAX_TEXT_FOR_BDM_HASH] if len(text_input) > MAX_TEXT_FOR_BDM_HASH else text_input\n",
        "    if not text_for_hash.strip(): return 0.0\n",
        "    binary_matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        return bdm_instance.bdm(binary_matrix)\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0\n",
        "\n",
        "def compute_mdl_cost_for_text_block(full_qid_corpus_str: str,\n",
        "                                    final_motifs_to_evaluate: List[Dict],\n",
        "                                    bdm_instance: BDM,\n",
        "                                    matrix_s: tuple = MATRIX_SIZE_GLOBAL) -> tuple[float, float, float]:\n",
        "    if not isinstance(full_qid_corpus_str, str) : full_qid_corpus_str = \"\"\n",
        "    l_h = calculate_L_H_token_based_structured(final_motifs_to_evaluate)\n",
        "    compressed_text_block = llm_compress_text_structured(full_qid_corpus_str, final_motifs_to_evaluate)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "    if l_d_h < 0: return l_h, -1.0, -1.0\n",
        "    return l_h, l_d_h, l_h + l_d_h\n",
        "\n",
        "def preprocess_corpus_for_motif_extraction(text_corpus: str) -> str:\n",
        "    if not isinstance(text_corpus, str): return \"\"\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text_corpus)\n",
        "    text = re.sub(r' {2,}', ' ', text)\n",
        "    lines = text.split('\\n')\n",
        "    filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10 or not line.strip()]\n",
        "    return '\\n'.join(filtered_lines)\n",
        "\n",
        "def count_sf_occurrences(corpus_text: str, surface_form: str) -> int:\n",
        "    if not corpus_text or not surface_form or not isinstance(corpus_text, str) or not isinstance(surface_form, str):\n",
        "        return 0\n",
        "    try:\n",
        "        return len(re.findall(re.escape(surface_form.lower()), corpus_text.lower(), flags=re.IGNORECASE))\n",
        "    except re.error as e:\n",
        "        print(f\"    [WARN] Regex error in count_sf_occurrences for SF '{surface_form}': {e}\")\n",
        "        return 0\n",
        "\n",
        "# --- LLM Interaction Functions ---\n",
        "def build_llm_prompt_for_motifs(text_block_for_prompt: str, max_motifs_to_extract: int = MAX_MOTIFS_PER_CHUNK) -> str:\n",
        "    \"\"\"Reverted to the simpler prompt that previously yielded bracketed labels.\"\"\"\n",
        "    if len(text_block_for_prompt) > MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK:\n",
        "        text_block_for_prompt = text_block_for_prompt[:MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK]\n",
        "\n",
        "    # This is the prompt from the \"Old Successful Code Cell\"\n",
        "    prompt = f\"\"\"You will receive a set of comments from different people answering the same question.\n",
        "\n",
        "Your task is to identify up to {max_motifs_to_extract} key recurring themes.\n",
        "\n",
        "For each theme, provide:\n",
        "- A short label like [DATA_PRIVACY]\n",
        "- A 1-sentence description of the theme\n",
        "- 2–3 short phrases that often appear in the text (surface forms)\n",
        "\n",
        "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
        "Example of one object in the list:\n",
        "{{\n",
        "  \"label\": \"[EXAMPLE_LABEL]\",\n",
        "  \"description\": \"A concise description of the example theme.\",\n",
        "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
        "}}\n",
        "If no clear motifs are found, output an empty JSON list: `[]`.\n",
        "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
        "\n",
        "Set of comments to analyze:\n",
        "\\\"\\\"\\\"\n",
        "{text_block_for_prompt}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def call_local_llm_for_raw_response(\n",
        "    prompt_content_for_user_turn: str,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str,\n",
        "    chunk_idx_for_log: int\n",
        "    ) -> str:\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance:\n",
        "        print(f\"    ERROR (call_local_llm): LLM pipeline/tokenizer not initialized for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return \"\"\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": prompt_content_for_user_turn}]\n",
        "    try:\n",
        "        prompt_formatted_for_llm = hf_tokenizer_instance.apply_chat_template(\n",
        "            messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception as e_template:\n",
        "        print(f\"    ERROR (call_local_llm): Applying chat template failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_template}\")\n",
        "        # ... (logging to file) ...\n",
        "        return \"\"\n",
        "\n",
        "    # Using generation_args consistent with \"Old Successful Code\"\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION,\n",
        "        \"do_sample\": False,\n",
        "        \"pad_token_id\": hf_tokenizer_instance.pad_token_id\n",
        "    }\n",
        "    # print(f\"    DEBUG (call_local_llm): QID {qid_for_log}, Chunk {chunk_idx_for_log}, Prompt len: {len(prompt_formatted_for_llm)}, GenArgs: {generation_args}\")\n",
        "\n",
        "    try:\n",
        "        outputs = hf_pipeline_instance(prompt_formatted_for_llm, **generation_args)\n",
        "        if outputs and isinstance(outputs, list) and len(outputs) > 0 and \\\n",
        "           outputs[0] and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
        "            assistant_response_text = outputs[0]['generated_text'].strip()\n",
        "            # print(f\"    DEBUG (call_local_llm): QID {qid_for_log}, Chunk {chunk_idx_for_log}, Raw LLM Output:\\n{assistant_response_text[:500]}...\")\n",
        "            return assistant_response_text\n",
        "        else: # ... (warn and return \"\") ...\n",
        "            print(f\"    WARN (call_local_llm): LLM pipeline returned unexpected structure for QID {qid_for_log}, Chunk {chunk_idx_for_log}. Output: {outputs}\")\n",
        "            return \"\"\n",
        "    except Exception as e_pipeline: # ... (error handling and logging) ...\n",
        "        print(f\"    ERROR (call_local_llm): Exception during hf_pipeline call for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_pipeline}\")\n",
        "        # ... (logging to file) ...\n",
        "        return \"\"\n",
        "\n",
        "def parse_and_validate_llm_json_response(\n",
        "    llm_raw_response_text: str,\n",
        "    qid_for_log:str,\n",
        "    chunk_idx_for_log:int,\n",
        "    prompt_sent_to_llm:str\n",
        "    ) -> List[Dict]:\n",
        "    json_str_candidate = llm_raw_response_text.strip()\n",
        "    if json_str_candidate.startswith(\"```json\"): json_str_candidate = json_str_candidate[len(\"```json\"):].strip()\n",
        "    if json_str_candidate.startswith(\"```\"): json_str_candidate = json_str_candidate[len(\"```\"):].strip()\n",
        "    if json_str_candidate.endswith(\"```\"): json_str_candidate = json_str_candidate[:-len(\"```\")].strip()\n",
        "\n",
        "    # print(f\"    DEBUG (parse_validate): QID {qid_for_log}, Chunk {chunk_idx_for_log}, JSON candidate:\\n{json_str_candidate[:500]}...\")\n",
        "\n",
        "    if not json_str_candidate or json_str_candidate.lower() == \"[]\" or \"no_themes_found\" in json_str_candidate.lower() or \"no clear motifs\" in json_str_candidate.lower():\n",
        "        return []\n",
        "    try:\n",
        "        parsed_data = json.loads(json_str_candidate)\n",
        "        if isinstance(parsed_data, dict): parsed_data = [parsed_data]\n",
        "        if not isinstance(parsed_data, list): raise ValueError(\"Parsed JSON is not a list or single object.\")\n",
        "\n",
        "        valid_motifs_from_json = []\n",
        "        for item_idx, item in enumerate(parsed_data):\n",
        "            # print(f\"    DEBUG (parse_validate): Validating item {item_idx+1} for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\")\n",
        "            # print(f\"      ITEM_RAW_CONTENT: {item}\")\n",
        "\n",
        "            label_str = item.get('label',\"\").strip()\n",
        "            desc_str = item.get('description',\"\").strip()\n",
        "            sf_list = item.get('surface_forms', [])\n",
        "\n",
        "            is_dict_val = isinstance(item, dict)\n",
        "            has_all_keys_val = all(k in item for k in [\"label\", \"description\", \"surface_forms\"])\n",
        "            is_label_str_val = isinstance(label_str, str) and bool(label_str) # Ensure label is not empty\n",
        "            label_starts_bracket_val = label_str.startswith('[')\n",
        "            label_ends_bracket_val = label_str.endswith(']')\n",
        "            is_desc_str_val = isinstance(desc_str, str) # Allow empty description\n",
        "            is_sf_list_val = isinstance(sf_list, list)\n",
        "            sfs_are_strings_val = all(isinstance(sf_item, str) for sf_item in sf_list)\n",
        "\n",
        "            # print(f\"      VALIDATION CHECKS: is_dict={is_dict_val}, has_all_keys={has_all_keys_val}, is_label_str={is_label_str_val}, label_content='{label_str}', starts_bracket={label_starts_bracket_val}, ends_bracket={label_ends_bracket_val}, is_desc_str={is_desc_str_val}, is_sf_list={is_sf_list_val}, sfs_are_strings={sfs_are_strings_val}\")\n",
        "\n",
        "            if is_dict_val and has_all_keys_val and \\\n",
        "               is_label_str_val and label_starts_bracket_val and label_ends_bracket_val and \\\n",
        "               is_desc_str_val and is_sf_list_val and sfs_are_strings_val:\n",
        "                valid_motifs_from_json.append({\n",
        "                    \"label\": label_str,\n",
        "                    \"description\": desc_str,\n",
        "                    \"surface_forms\": [s.strip() for s in sf_list if s.strip()]\n",
        "                })\n",
        "                # print(\"        RESULT: Item PASSED validation.\")\n",
        "            else:\n",
        "                print(f\"    [WARN] Invalid motif object structure in LLM JSON for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}. Skipping item.\")\n",
        "                # print(f\"      ITEM CONTENT: {item}\") # Keep this for deeper debug if needed\n",
        "                # ... (logging to file for failed item structure) ...\n",
        "        return valid_motifs_from_json\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        print(f\"    [WARN] Motif JSON parsing or core structure validation failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e}\")\n",
        "        # ... (logging to file for JSONDecodeError) ...\n",
        "        return []\n",
        "\n",
        "def get_motifs_for_qid_batched(\n",
        "    list_of_individual_response_texts: List[str],\n",
        "    responses_per_batch: int,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str\n",
        "    ) -> List[Dict]:\n",
        "    all_raw_motifs_from_chunks = []\n",
        "    batched_text_chunks_for_llm = []\n",
        "    for i in range(0, len(list_of_individual_response_texts), responses_per_batch):\n",
        "        batch_responses = list_of_individual_response_texts[i:i + responses_per_batch]\n",
        "        chunk_text_for_llm = preprocess_corpus_for_motif_extraction(\"\\n\\n<RSP_SEP>\\n\\n\".join(batch_responses))\n",
        "        batched_text_chunks_for_llm.append(chunk_text_for_llm)\n",
        "\n",
        "    print(f\"  QID {qid_for_log}: Processing {len(list_of_individual_response_texts)} responses in {len(batched_text_chunks_for_llm)} preprocessed chunks (batch size: {responses_per_batch} responses).\")\n",
        "\n",
        "    for chunk_idx, text_chunk_to_analyze_processed in enumerate(batched_text_chunks_for_llm):\n",
        "        print(f\"    Analyzing chunk {chunk_idx + 1}/{len(batched_text_chunks_for_llm)} for QID {qid_for_log} (processed chunk len: {len(text_chunk_to_analyze_processed)} chars)...\")\n",
        "        if len(text_chunk_to_analyze_processed.strip()) < 50:\n",
        "            print(f\"      Chunk {chunk_idx+1} (QID {qid_for_log}) too short after preprocessing, skipping.\")\n",
        "            continue\n",
        "        prompt_for_llm = build_llm_prompt_for_motifs(text_chunk_to_analyze_processed)\n",
        "        motifs_from_this_chunk = []\n",
        "        for attempt in range(LLM_RETRY_ATTEMPTS):\n",
        "            raw_llm_response = call_local_llm_for_raw_response(\n",
        "                prompt_for_llm, hf_pipeline_instance, hf_tokenizer_instance, qid_for_log, chunk_idx + 1\n",
        "            )\n",
        "            if not raw_llm_response:\n",
        "                print(f\"      LLM call attempt {attempt + 1} for chunk {chunk_idx+1} (QID {qid_for_log}) returned empty string. Retrying if possible...\")\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "                continue\n",
        "            parsed_motifs_from_this_attempt = parse_and_validate_llm_json_response(\n",
        "                raw_llm_response, qid_for_log, chunk_idx+1, prompt_for_llm\n",
        "            )\n",
        "            if parsed_motifs_from_this_attempt:\n",
        "                motifs_from_this_chunk = parsed_motifs_from_this_attempt\n",
        "                break\n",
        "            else:\n",
        "                print(f\"      Motif parsing/validation attempt {attempt + 1} yielded no structured motifs for chunk {chunk_idx+1} (QID {qid_for_log}). Retrying if possible...\")\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "        if motifs_from_this_chunk:\n",
        "            print(f\"      Extracted {len(motifs_from_this_chunk)} structured motif objects from chunk {chunk_idx+1} (QID {qid_for_log}).\")\n",
        "            all_raw_motifs_from_chunks.extend(motifs_from_this_chunk)\n",
        "        else:\n",
        "            print(f\"      No valid structured motifs extracted from chunk {chunk_idx+1} (QID {qid_for_log}) after {LLM_RETRY_ATTEMPTS} attempts.\")\n",
        "    return all_raw_motifs_from_chunks\n",
        "\n",
        "def consolidate_raw_motifs(list_of_all_raw_motifs: List[Dict]) -> List[Dict]:\n",
        "    if not list_of_all_raw_motifs: return []\n",
        "    consolidated_motifs_map = {}\n",
        "    for motif_obj in list_of_all_raw_motifs:\n",
        "        label = motif_obj.get(\"label\",\"\").strip()\n",
        "        description = motif_obj.get(\"description\",\"\").strip()\n",
        "        surface_forms = motif_obj.get(\"surface_forms\", [])\n",
        "        if not (label and isinstance(surface_forms, list)): continue # Description can be empty\n",
        "        current_sfs_set = set(sf.lower().strip() for sf in surface_forms if isinstance(sf, str) and sf.strip())\n",
        "        if label not in consolidated_motifs_map:\n",
        "            consolidated_motifs_map[label] = {\n",
        "                \"label\": label, \"description\": description, \"surface_forms\": sorted(list(current_sfs_set))\n",
        "            }\n",
        "        else:\n",
        "            existing_sfs_set = set(consolidated_motifs_map[label].get(\"surface_forms\", []))\n",
        "            consolidated_motifs_map[label][\"surface_forms\"] = sorted(list(existing_sfs_set.union(current_sfs_set)))\n",
        "    return list(consolidated_motifs_map.values())\n",
        "\n",
        "def filter_surface_forms_by_global_frequency(\n",
        "    consolidated_motifs_list: List[Dict],\n",
        "    full_qid_corpus_text: str,\n",
        "    min_global_freq: int = MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "    ) -> List[Dict]:\n",
        "    if not consolidated_motifs_list: return []\n",
        "    final_globally_filtered_motifs = []\n",
        "    for motif_obj in consolidated_motifs_list:\n",
        "        globally_frequent_sfs_for_this_motif = []\n",
        "        original_sfs_for_this_motif = motif_obj.get(\"surface_forms\", [])\n",
        "        for sf_str in original_sfs_for_this_motif:\n",
        "            count = count_sf_occurrences(full_qid_corpus_text, sf_str)\n",
        "            if count >= min_global_freq:\n",
        "                globally_frequent_sfs_for_this_motif.append(sf_str)\n",
        "        if globally_frequent_sfs_for_this_motif:\n",
        "            filtered_motif_entry = motif_obj.copy()\n",
        "            filtered_motif_entry[\"surface_forms\"] = sorted(list(set(globally_frequent_sfs_for_this_motif)))\n",
        "            final_globally_filtered_motifs.append(filtered_motif_entry)\n",
        "    return final_globally_filtered_motifs\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "def main():\n",
        "    print(\"--- MWP Single Cell (Reverted Prompt, Global SF Filter, Batched LLM, Token-L(H), BDM L(D|H)) ---\") # Updated title\n",
        "    # ... (Print config params - same as before) ...\n",
        "    # ... (Initialize debug log file - same as before) ...\n",
        "    # ... (Initialize LLM Pipeline - same as before, ensure return_full_text=False) ...\n",
        "    # ... (Initialize BDM - same as before, ensure BDM(ndim=2)) ...\n",
        "    # ... (Load Phase 2 Data - same as before) ...\n",
        "    # ... (Determine qids_to_process_this_run - same as before) ...\n",
        "    # --- Main Execution (Copied from previous complete cell, with GDB lines removed for brevity here) ---\n",
        "    # (The long main loop from your previous \"complete revised single cell MWP now\" post would go here,\n",
        "    # ensuring it calls the updated build_llm_prompt_for_motifs and updated parse_and_validate_llm_json_response)\n",
        "    # ... it will look very similar to the last main() I provided, just ensure it uses the functions defined in *this* cell.\n",
        "\n",
        "    # For brevity, I'll sketch the main loop parts that changed or are key:\n",
        "    # --- Initialize LLM and BDM ---\n",
        "    hf_pipeline_instance, hf_tokenizer_instance = initialize_llm_pipeline()\n",
        "    if not hf_pipeline_instance: return\n",
        "    bdm_instance_main = initialize_bdm_instance() # Using new init function name\n",
        "    if not bdm_instance_main: return\n",
        "    # --- Load Data --- (as before)\n",
        "    # --- Determine QIDs --- (as before)\n",
        "    # --- Loop QIDs ---\n",
        "    for qid_identifier_str in qids_to_process_this_run:\n",
        "        # --- Get QID text and baseline BDM --- (as before)\n",
        "        # ...\n",
        "        # --- Call get_motifs_for_qid_batched (which uses the new prompt and parsing) ---\n",
        "        raw_motifs_from_chunks = get_motifs_for_qid_batched(\n",
        "            actual_response_texts_for_qid,\n",
        "            LLM_BATCH_SIZE_RESPONSES, # Use correct constant\n",
        "            hf_pipeline_instance,\n",
        "            hf_tokenizer_instance,\n",
        "            qid_identifier_str\n",
        "        )\n",
        "        # --- Initialize qid_result_entry --- (as before)\n",
        "        # --- Handle no raw motifs --- (as before)\n",
        "        # --- Consolidate motifs --- (as before, uses consolidate_raw_motifs)\n",
        "        # --- Handle no consolidated motifs --- (as before)\n",
        "        # --- Print consolidated motifs (optional detailed) --- (as before)\n",
        "        # --- Global SF Frequency Filtering --- (as before, uses filter_surface_forms_by_global_frequency)\n",
        "        # --- Handle no refined motifs --- (as before)\n",
        "        # --- Print refined motifs --- (as before)\n",
        "        # --- Final MDL Calculation --- (as before, uses compute_mdl_cost_for_text_block)\n",
        "        # --- Log results for QID --- (as before)\n",
        "    # --- Final Summary and Save --- (as before)\n",
        "\n",
        "    # Placeholder for the actual main loop structure from the previous full cell\n",
        "    # This is just to indicate that the main logic flow remains, but it now\n",
        "    # calls the functions defined within *this specific cell*.\n",
        "    # You would copy the full main() from the previous complete cell I provided,\n",
        "    # ensuring it uses the `build_llm_prompt_for_motifs` and\n",
        "    # `parse_and_validate_llm_json_response` defined *here*.\n",
        "\n",
        "    # --- THIS IS THE FULL MAIN() from the previous \"complete revised code cell\" ---\n",
        "    # --- It should work with the functions now defined above in *this* cell ---\n",
        "    print(f\"Timestamp: {time.asctime()}\")\n",
        "    print(\"\\n--- Configuration Summary ---\") # ... (print all config constants)\n",
        "    with open(LLM_DEBUG_LOG_FILE, \"w\", encoding=\"utf-8\") as f: f.write(f\"LLM Log - {time.asctime()}\\nModel: {LOCAL_LLM_MODEL_ID}\\n\")\n",
        "\n",
        "    hf_pipeline_instance, hf_tokenizer_instance = initialize_llm_pipeline()\n",
        "    if not hf_pipeline_instance: return\n",
        "    bdm_instance_main = initialize_bdm_instance()\n",
        "    if not bdm_instance_main: return\n",
        "\n",
        "    if not os.path.exists(P2_COLLATED_FILE): print(f\"ERROR: File {P2_COLLATED_FILE} not found.\"); return\n",
        "    print(f\"Loading data from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f: phase2_data_content = json.load(f)\n",
        "    except Exception as e: print(f\"Error loading {P2_COLLATED_FILE}: {e}\"); return\n",
        "\n",
        "    all_qid_mdl_results_list = []\n",
        "    aggregated_content_by_qid_from_file = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "    if not aggregated_content_by_qid_from_file: print(f\"No 'aggregated_pdf_content_by_qid' in {P2_COLLATED_FILE}.\"); return\n",
        "\n",
        "    qids_to_process_this_run = [qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY if qid in aggregated_content_by_qid_from_file] if (P3_QIDS_TO_PROCESS_THEMATICALLY and isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and P3_QIDS_TO_PROCESS_THEMATICALLY) else list(aggregated_content_by_qid_from_file.keys())[:1]\n",
        "    if not qids_to_process_this_run: print(f\"No QIDs to process. Exiting.\"); return\n",
        "    print(f\"\\nMDL analysis for QIDs: {qids_to_process_this_run}\\n\")\n",
        "\n",
        "    for qid_identifier_str in qids_to_process_this_run:\n",
        "        print(f\"--- Analyzing QID: {qid_identifier_str} ---\")\n",
        "        list_of_individual_response_structs = aggregated_content_by_qid_from_file.get(qid_identifier_str, [])\n",
        "        actual_response_texts_for_qid = [item.get(\"text\", \"\") for item in list_of_individual_response_structs if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()]\n",
        "        if not actual_response_texts_for_qid: print(f\"  No valid text for QID {qid_identifier_str}. Skipping.\"); print(\"-\" * 50); continue\n",
        "\n",
        "        full_corpus_text_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(actual_response_texts_for_qid)\n",
        "        if len(full_corpus_text_for_qid.strip()) < 100: print(f\"  Skipping QID {qid_identifier_str}: text too short.\"); print(\"-\" * 50); continue\n",
        "\n",
        "        num_total_responses_for_qid = len(actual_response_texts_for_qid)\n",
        "        print(f\"  Corpus for QID {qid_identifier_str}: {len(full_corpus_text_for_qid)} chars, {num_total_responses_for_qid} responses.\")\n",
        "\n",
        "        baseline_bdm_original_corpus = compute_bdm_for_text(full_corpus_text_for_qid, bdm_instance_main, MATRIX_SIZE_GLOBAL)\n",
        "        if baseline_bdm_original_corpus < 0: print(f\"  Error computing baseline BDM for QID {qid_identifier_str}. Skipping.\"); print(\"-\" * 50); continue\n",
        "        current_qid_baseline_mdl_cost = baseline_bdm_original_corpus\n",
        "        print(f\"  Baseline MDL for QID {qid_identifier_str} (L(D_orig)): {current_qid_baseline_mdl_cost:.4f}\")\n",
        "\n",
        "        raw_motifs_from_chunks = get_motifs_for_qid_batched(\n",
        "            actual_response_texts_for_qid, LLM_BATCH_SIZE_RESPONSES,\n",
        "            hf_pipeline_instance, hf_tokenizer_instance, qid_identifier_str\n",
        "        )\n",
        "        current_qid_result_entry = {\n",
        "            \"qid\": qid_identifier_str, \"corpus_len_chars\": len(full_corpus_text_for_qid), \"num_responses\": num_total_responses_for_qid,\n",
        "            \"baseline_mdl\": current_qid_baseline_mdl_cost, \"final_refined_motifs\": [], \"l_h_final_motifs\": 0.0,\n",
        "            \"l_d_h_final_motifs\": current_qid_baseline_mdl_cost, \"total_mdl_with_final_motifs\": current_qid_baseline_mdl_cost,\n",
        "            \"compression_achieved\": 0.0, \"num_raw_motifs_extracted\": len(raw_motifs_from_chunks),\n",
        "            \"num_consolidated_motifs\": 0, \"num_globally_refined_motifs\": 0\n",
        "        }\n",
        "        if not raw_motifs_from_chunks: print(f\"  No raw motifs by LLM for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "        print(f\"  Extracted {len(raw_motifs_from_chunks)} raw motif objects for QID {qid_identifier_str}.\")\n",
        "\n",
        "        consolidated_motifs_list = consolidate_raw_motifs(raw_motifs_from_chunks)\n",
        "        current_qid_result_entry[\"num_consolidated_motifs\"] = len(consolidated_motifs_list)\n",
        "        print(f\"  Consolidated into {len(consolidated_motifs_list)} unique motifs for QID {qid_identifier_str}.\")\n",
        "        if not consolidated_motifs_list: print(f\"  No unique motifs after consolidation for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        # print(f\"  Consolidated Motifs (BEFORE Global SF refinement):\") # Optional detailed print\n",
        "        # for idx, mo_con in enumerate(consolidated_motifs_list): print(f\"    Cons. Motif {idx+1}: L='{mo_con.get('label')}', D='{mo_con.get('description','N/A')[:30]}...', SFs({len(mo_con.get('surface_forms',[]))})='{mo_con.get('surface_forms',[])[:2]}...'\")\n",
        "\n",
        "        globally_refined_motifs = filter_surface_forms_by_global_frequency(\n",
        "            consolidated_motifs_list, full_corpus_text_for_qid, MIN_SF_FREQUENCY_IN_FULL_CORPUS\n",
        "        )\n",
        "        current_qid_result_entry[\"num_globally_refined_motifs\"] = len(globally_refined_motifs)\n",
        "        print(f\"  Globally refined into {len(globally_refined_motifs)} motifs for QID {qid_identifier_str}.\")\n",
        "        if not globally_refined_motifs: print(f\"  No motifs left after GLOBAL SF refinement for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  Final Globally Refined Motifs for QID {qid_identifier_str}:\")\n",
        "        for idx, mo_final in enumerate(globally_refined_motifs): print(f\"    Refined Motif {idx+1}: L='{mo_final.get('label')}', D='{mo_final.get('description','N/A')[:60]}...', SFs({len(mo_final.get('surface_forms',[]))})='{mo_final.get('surface_forms',[])}'\")\n",
        "\n",
        "        l_h_final, l_d_h_final, total_mdl_final = compute_mdl_cost_for_text_block(\n",
        "            full_corpus_text_for_qid, globally_refined_motifs, bdm_instance_main, MATRIX_SIZE_GLOBAL\n",
        "        )\n",
        "        current_qid_result_entry.update({\n",
        "            \"final_refined_motifs\": globally_refined_motifs, \"l_h_final_motifs\": l_h_final,\n",
        "            \"l_d_h_final_motifs\": l_d_h_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"total_mdl_with_final_motifs\": total_mdl_final if l_d_h_final >=0 else \"BDM_ERROR\",\n",
        "            \"compression_achieved\": \"BDM_ERROR\" if l_d_h_final < 0 else (current_qid_baseline_mdl_cost - total_mdl_final)\n",
        "        })\n",
        "        if l_d_h_final < 0: print(f\"  Error computing MDL cost (BDM error) for QID {qid_identifier_str}.\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  L(H) final motifs: {l_h_final:.4f}, L(D|H) compressed: {l_d_h_final:.4f}, Total MDL: {total_mdl_final:.4f}\")\n",
        "        compression_val = current_qid_result_entry[\"compression_achieved\"]\n",
        "        result_status_str = f\"SUCCESS: Comp: {compression_val:.4f}\" if isinstance(compression_val, float) and compression_val > 0.0001 else f\"NOTE: No sig. comp. Diff: {compression_val if isinstance(compression_val, str) else compression_val:.4f}\"\n",
        "        print(f\"  {result_status_str}\"); all_qid_mdl_results_list.append(current_qid_result_entry); print(\"-\" * 50)\n",
        "\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary ---\")\n",
        "    if not all_qid_mdl_results_list: print(\"No QIDs processed.\")\n",
        "    else:\n",
        "        valid_results = [r for r in all_qid_mdl_results_list if isinstance(r.get('compression_achieved'), float) and r.get('l_h_final_motifs', -1.0) >= 0]\n",
        "        num_qids_ok = len(valid_results); num_comp = sum(1 for r in valid_results if r['compression_achieved'] > 0.0001)\n",
        "        print(f\"Targeted QIDs: {len(qids_to_process_this_run)}, Results logged: {len(all_qid_mdl_results_list)}, Valid MDL: {num_qids_ok}, QIDs compressed: {num_comp}\")\n",
        "        if num_comp > 0:\n",
        "            comp_vals = [r['compression_achieved'] for r in valid_results if r['compression_achieved'] > 0.0001]\n",
        "            print(f\"  Avg compression: {np.mean(comp_vals):.4f}, Max compression: {np.max(comp_vals):.4f}\")\n",
        "        else: print(\"  No compression achieved.\")\n",
        "\n",
        "        output_filename = os.path.join(BASE_PROJECT_DIR, \"mdl_analysis_single_cell_reverted_prompt_v1.json\")\n",
        "        try:\n",
        "            with open(output_filename, \"w\", encoding=\"utf-8\") as f_out: json.dump(all_qid_mdl_results_list, f_out, indent=2, ensure_ascii=False)\n",
        "            print(f\"Detailed results saved to {output_filename}\")\n",
        "        except Exception as e_s: print(f\"Error saving results: {e_s}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Executing main MDL pipeline (Single Cell Reverted Prompt Version) at {time.asctime()}...\")\n",
        "    main()\n",
        "    print(f\"Main MDL pipeline execution finished at {time.asctime()}.\")"
      ],
      "metadata": {
        "id": "JUk12gb4R0qH",
        "outputId": "8260b75e-83a7-4fdc-8a4e-d4c115f2191f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609,
          "referenced_widgets": [
            "dbae32a5ed2942649c8b2b2dd81c9598",
            "9dc17b55cf6a4e8d9d316ddd5759ad39",
            "1a56b46a59d7447cab5b50b84f8164ab",
            "2584ea08f90c4a6a90dde60576f3ceb1",
            "4d5c08d8bef24b358d77b094bacdb460",
            "b297ed3417bc4a79891b13355e900ab5",
            "f1dc021730bb402a97ad30eb9c8340b8",
            "a8371dab2ab641e7adf5e444fa4c6910",
            "d99f7b4468324a898287b3c4f6ae3554",
            "414d7e0acf72421aa61e4e5302c90962",
            "a2acc6f54c204c10beaeed31995454bb"
          ]
        }
      },
      "id": "JUk12gb4R0qH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing main MDL pipeline (Single Cell Reverted Prompt Version) at Sun Jun  1 06:57:27 2025...\n",
            "--- MWP Single Cell (Reverted Prompt, Global SF Filter, Batched LLM, Token-L(H), BDM L(D|H)) ---\n",
            "--- Initializing LLM Pipeline (model: google/gemma-2b-it, quantization: True, return_full_text: False) ---\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-2b-it...\n",
            "BitsAndBytesConfig created for google/gemma-2b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-2b-it (Quantization active: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbae32a5ed2942649c8b2b2dd81c9598"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-2b-it initialized successfully.\n",
            "Initializing BDM instance...\n",
            "BDM instance initialized successfully (ndim=2, default CTM-based).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'qids_to_process_this_run' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e36fe483e609>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Executing main MDL pipeline (Single Cell Reverted Prompt Version) at {time.asctime()}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Main MDL pipeline execution finished at {time.asctime()}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-e36fe483e609>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# --- Determine QIDs --- (as before)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# --- Loop QIDs ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mqid_identifier_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqids_to_process_this_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0;31m# --- Get QID text and baseline BDM --- (as before)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m# ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'qids_to_process_this_run' where it is not associated with a value"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nSUoKo6yPfok"
      },
      "id": "nSUoKo6yPfok"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revised after Claude's inputs"
      ],
      "metadata": {
        "id": "oHcmGQK6xNZY"
      },
      "id": "oHcmGQK6xNZY"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Configuration\n",
        "# Purpose: Centralize all global constants and configuration parameters.\n",
        "# Cell 1: Imports and Configuration\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict, Set # For type hinting\n",
        "from collections import Counter\n",
        "\n",
        "# --- Third-party Library Imports ---\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import torch # Ensure torch is imported before transformers for some setups\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "\n",
        "# --- Configuration Constants ---\n",
        "\n",
        "# --- Project Paths ---\n",
        "# !!! IMPORTANT: UPDATE BASE_PROJECT_DIR TO YOUR ACTUAL PATH !!!\n",
        "# Example for Google Colab:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# BASE_PROJECT_DIR = '/content/drive/MyDrive/YourFolder/LegalAnalysis/'\n",
        "# Example for local:\n",
        "BASE_PROJECT_DIR = './' # Assumes files are in current or subdirectories\n",
        "# Ensure BASE_PROJECT_DIR ends with a slash if it's a directory path\n",
        "\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json') # Example filename\n",
        "\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"] # QIDs to process, e.g., [\"Q4\", \"Q12\"] or None to process fallback\n",
        "\n",
        "# --- BDM Configuration ---\n",
        "MATRIX_SIZE_GLOBAL = (8, 8) # For BDM text_to_binary_matrix representation\n",
        "MAX_TEXT_FOR_BDM_HASH = 2000 # Max characters from text to use for BDM hash input\n",
        "\n",
        "# --- LLM Configuration ---\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True # Set to False if no GPU or issues\n",
        "LLM_BATCH_SIZE_RESPONSES = 5 # Number of individual responses to batch together for one LLM call\n",
        "LLM_RETRY_ATTEMPTS = 2\n",
        "MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK = 7000 # Max chars for the actual text block within the LLM prompt\n",
        "LLM_MAX_NEW_TOKENS_ENHANCED_MOTIF = 800 # Max new tokens LLM should generate for motif extraction output\n",
        "MAX_MOTIFS_PER_CHUNK = 3 # Ask LLM for up to this many motifs per processed chunk\n",
        "\n",
        "# --- Token-Based L(H) Configuration ---\n",
        "MOTIF_SYMBOLIC_LABEL_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TOKEN_COST = 0.1\n",
        "MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.25\n",
        "MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.1\n",
        "\n",
        "# --- Surface Form Filtering Configuration ---\n",
        "# For validate_and_refine_surface_forms_in_chunk (applied to SFs from one LLM call on one chunk)\n",
        "MIN_SF_FREQ_IN_CHUNK_VALIDATION = 2\n",
        "# For filter_surface_forms_by_global_frequency (applied to consolidated motifs, against full QID corpus)\n",
        "MIN_SF_FREQ_FOR_FINAL_MOTIFS = 2\n",
        "\n",
        "# --- Logging ---\n",
        "LLM_DEBUG_LOG_FILE = os.path.join(BASE_PROJECT_DIR, \"llm_motif_debug_log_mwp_enhanced.txt\")\n",
        "\n",
        "# --- Derived Constants (Optional, for clarity if paths get complex) ---\n",
        "# (None needed for now)\n",
        "\n",
        "print(\"Cell 1: Imports and Configuration loaded.\")\n",
        "print(f\"Base project directory set to: {BASE_PROJECT_DIR}\")\n",
        "print(f\"LLM Debug Log will be at: {LLM_DEBUG_LOG_FILE}\")\n",
        "if not os.path.exists(BASE_PROJECT_DIR):\n",
        "    print(f\"WARNING: BASE_PROJECT_DIR '{BASE_PROJECT_DIR}' does not exist. Please create it or update the path.\")\n",
        "if not os.path.exists(PHASE2_OUTPUT_DIR):\n",
        "    print(f\"WARNING: PHASE2_OUTPUT_DIR '{PHASE2_OUTPUT_DIR}' does not exist. Will attempt to create if needed by data loading.\")\n"
      ],
      "metadata": {
        "id": "ilyBIGaUxtOc",
        "outputId": "dbe73c89-f11e-4b30-ef6b-aa3a509cd873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ilyBIGaUxtOc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 1: Imports and Configuration loaded.\n",
            "Base project directory set to: ./\n",
            "LLM Debug Log will be at: ./llm_motif_debug_log_mwp_enhanced.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Text Utilities\n",
        "# Purpose: General text processing and utility functions not specific to LLMs or MDL.\n",
        "# Cell 2: Text Utilities\n",
        "\n",
        "def tokenize_phrase(phrase_text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Simple tokenizer for phrases, definitions, or surface forms.\n",
        "    Lowercases and splits by space.\n",
        "    \"\"\"\n",
        "    if not isinstance(phrase_text, str) or not phrase_text.strip():\n",
        "        return []\n",
        "    # More advanced tokenization could be used here if needed (e.g., handling punctuation differently)\n",
        "    # For now, simple split and lowercase is consistent with other parts.\n",
        "    return phrase_text.lower().split()\n",
        "\n",
        "def preprocess_corpus_for_motif_extraction(text_corpus: str) -> str:\n",
        "    \"\"\"\n",
        "    Preprocesses a text corpus (typically a chunk of joined responses)\n",
        "    before sending to LLM or for n-gram extraction.\n",
        "    - Consolidates excessive newlines and spaces.\n",
        "    - Filters out very short lines (potential noise).\n",
        "    \"\"\"\n",
        "    if not isinstance(text_corpus, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Consolidate multiple newlines to a maximum of two (to preserve paragraph-like breaks)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text_corpus)\n",
        "    # Consolidate multiple spaces into a single space\n",
        "    text = re.sub(r' {2,}', ' ', text)\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    # Filter out lines that are too short to likely contain meaningful themes,\n",
        "    # but keep intentionally blank lines (which result from '\\n\\n').\n",
        "    # This threshold is heuristic.\n",
        "    filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10 or not line.strip()]\n",
        "\n",
        "    return '\\n'.join(filtered_lines)\n",
        "\n",
        "def count_sf_occurrences(corpus_text: str, surface_form: str) -> int:\n",
        "    \"\"\"\n",
        "    Counts case-insensitive occurrences of a surface_form within the corpus_text.\n",
        "    \"\"\"\n",
        "    if not corpus_text or not surface_form or not isinstance(corpus_text, str) or not isinstance(surface_form, str):\n",
        "        return 0\n",
        "    # Ensure search is case-insensitive and handles regex special characters in the surface_form\n",
        "    try:\n",
        "        return len(re.findall(re.escape(surface_form.lower()), corpus_text.lower(), flags=re.IGNORECASE))\n",
        "    except re.error as e:\n",
        "        print(f\"    [WARN] Regex error in count_sf_occurrences for SF '{surface_form}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def extract_actual_phrases_from_text(\n",
        "    text: str,\n",
        "    min_phrase_len: int = 2,\n",
        "    max_phrase_len: int = 6,\n",
        "    min_freq: int = MIN_SF_FREQ_IN_CHUNK_VALIDATION # Uses config constant\n",
        "    ) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Extracts n-gram phrases (2 to 6 words by default) and their frequencies from text.\n",
        "    Only returns phrases meeting min_freq.\n",
        "    Designed for validating LLM-generated surface forms against actual recurring content in a text CHUNK.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return {}\n",
        "\n",
        "    # Basic cleaning: lowercase, remove punctuation (except intra-word apostrophes if any), split into words\n",
        "    # This cleaning should be somewhat consistent with how surface forms are expected/matched.\n",
        "    text_cleaned = text.lower()\n",
        "    text_cleaned = re.sub(r'[^\\w\\s\\']', ' ', text_cleaned) # Keep apostrophes, replace other non-alphanum with space\n",
        "    text_cleaned = re.sub(r'\\s+', ' ', text_cleaned).strip() # Consolidate whitespace\n",
        "\n",
        "    words = text_cleaned.split()\n",
        "    if not words or len(words) < min_phrase_len:\n",
        "        return {}\n",
        "\n",
        "    phrase_counts = Counter()\n",
        "    for n in range(min_phrase_len, max_phrase_len + 1):\n",
        "        if n > len(words): continue # Cannot form n-gram if n > num words\n",
        "        for i in range(len(words) - n + 1):\n",
        "            phrase_tokens = words[i:i+n]\n",
        "            # Optional: filter out phrases composed only of stopwords, or starting/ending with them\n",
        "            # For now, keeping it simple.\n",
        "            phrase = ' '.join(phrase_tokens)\n",
        "            if phrase: # Ensure not empty after join (e.g. if words list had empty strings)\n",
        "                phrase_counts[phrase] += 1\n",
        "\n",
        "    recurring_phrases = {phrase: count for phrase, count in phrase_counts.items() if count >= min_freq}\n",
        "    return recurring_phrases\n",
        "\n",
        "print(\"Cell 2: Text Utilities loaded.\")"
      ],
      "metadata": {
        "id": "ZSw08S4VxTFW",
        "outputId": "a1ea431d-ebf5-4f63-8f4f-6a9dce334993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZSw08S4VxTFW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 2: Text Utilities loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. LLM Interaction\n",
        "# Purpose: Functions directly related to interacting with the LLM (prompt building, LLM calls, basic response parsing).\n",
        "# Cell 3: LLM Interaction\n",
        "\n",
        "def initialize_llm_pipeline(\n",
        "    model_id: str = LOCAL_LLM_MODEL_ID,\n",
        "    use_quantization: bool = USE_QUANTIZATION_FOR_LOCAL_LLM,\n",
        "    pipeline_return_full_text: bool = False # Defaulting to False as per our findings\n",
        "    ):\n",
        "    \"\"\"Initializes and returns the Hugging Face pipeline and tokenizer.\"\"\"\n",
        "    print(f\"--- Initializing LLM Pipeline (model: {model_id}, quantization: {use_quantization}, return_full_text: {pipeline_return_full_text}) ---\")\n",
        "\n",
        "    hf_pipeline_instance = None\n",
        "    hf_tokenizer_instance = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {model_id}...\")\n",
        "        hf_tokenizer_instance = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "        if hf_tokenizer_instance.pad_token is None:\n",
        "            print(\"Tokenizer does not have a pad_token; setting pad_token = eos_token.\")\n",
        "            hf_tokenizer_instance.pad_token = hf_tokenizer_instance.eos_token\n",
        "            # Note: model.config.pad_token_id will be set below after model loading\n",
        "\n",
        "        bnb_config = None\n",
        "        quant_active = False\n",
        "        if use_quantization and torch.cuda.is_available():\n",
        "            try:\n",
        "                # Ensure bfloat16 is available if trying to use it\n",
        "                compute_dtype = torch.bfloat16 if (device.type == 'cuda' and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=compute_dtype,\n",
        "                    bnb_4bit_use_double_quant=True\n",
        "                )\n",
        "                quant_active = True\n",
        "                print(f\"BitsAndBytesConfig created for {model_id}, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb:\n",
        "                print(f\"WARN: Failed to create BitsAndBytesConfig: {e_bnb}. Quantization may be disabled or fall back.\")\n",
        "                quant_active = False # Ensure it's False if config fails\n",
        "\n",
        "        print(f\"Loading local model {model_id} (Quantization active: {quant_active})...\")\n",
        "        model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True} # trust_remote_code for some models\n",
        "\n",
        "        if quant_active and bnb_config:\n",
        "            model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        elif device.type == 'cuda': # If not quantizing but on GPU, use appropriate dtype\n",
        "             model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "        hf_model_instance = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "\n",
        "        # Set model's pad_token_id if tokenizer's was set to eos_token_id\n",
        "        if hf_tokenizer_instance.pad_token_id == hf_tokenizer_instance.eos_token_id:\n",
        "             hf_model_instance.config.pad_token_id = hf_model_instance.config.eos_token_id\n",
        "\n",
        "        hf_pipeline_instance = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=hf_model_instance,\n",
        "            tokenizer=hf_tokenizer_instance,\n",
        "            return_full_text=pipeline_return_full_text\n",
        "        )\n",
        "        print(f\"Local LLM pipeline for {model_id} initialized successfully.\")\n",
        "        return hf_pipeline_instance, hf_tokenizer_instance\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to initialize local LLM pipeline: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "def create_enhanced_motif_prompt(text_corpus_chunk: str, max_motifs_to_extract: int = MAX_MOTIFS_PER_CHUNK) -> str:\n",
        "    \"\"\"\n",
        "    Creates the enhanced prompt for the LLM to extract structured motifs.\n",
        "    Truncates text_corpus_chunk if it exceeds MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK.\n",
        "    \"\"\"\n",
        "    # Truncate the input text chunk if it's too long for the prompt context\n",
        "    # This truncation happens *before* it's embedded in the larger prompt template\n",
        "    if len(text_corpus_chunk) > MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK:\n",
        "        # print(f\"    Note: Text block for LLM prompt analysis truncated from {len(text_corpus_chunk)} to {MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK} chars.\")\n",
        "        text_corpus_chunk = text_corpus_chunk[:MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK]\n",
        "\n",
        "    prompt = f\"\"\"You are analyzing text to find recurring themes and the specific phrases that signal these themes.\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "1. Surface forms MUST be SHORT PHRASES (2-6 words) that appear to recur or are representative of how the theme is actually stated in the provided text.\n",
        "2. Do NOT create generic descriptions or full sentences as surface forms - find phrases that people actually used.\n",
        "3. Focus on meaningful recurring concepts, not obvious topics that span the entire text.\n",
        "4. Labels should be specific and descriptive, in [UPPER_SNAKE_CASE_WITH_BRACKETS] (e.g., \"[REGULATORY_COMPLIANCE]\", \"[EMPLOYEE_MONITORING]\"). Avoid generic labels like [EXAMPLE_LABEL] or [THEME_A].\n",
        "\n",
        "Your task: Identify up to {max_motifs_to_extract} recurring themes from the \"Text to analyze\" below.\n",
        "\n",
        "For each theme, provide:\n",
        "- label: A specific, descriptive label.\n",
        "- description: One concise sentence explaining what this theme represents in the context of the analyzed text.\n",
        "- surface_forms: A JSON list of 2 to 4 short (2-6 words) phrases that are strong examples of how this theme is expressed in the analyzed text. These should ideally be phrases you see or infer as recurring from the provided text.\n",
        "\n",
        "IMPORTANT: Surface forms should be actual phrases indicative of the theme, not summaries.\n",
        "\n",
        "Examples of GOOD surface forms (if they were recurring in analyzed text):\n",
        "- \"data protection measures\"\n",
        "- \"employee privacy rights\"\n",
        "- \"compliance requirements\"\n",
        "\n",
        "Examples of BAD surface forms (AVOID these):\n",
        "- \"A comprehensive approach to privacy\" (too long, a description)\n",
        "- \"The need for better policies\" (generic, not a specific phrase from text)\n",
        "- \"Various stakeholders expressed concerns about data handling and privacy\" (a full sentence summary)\n",
        "\n",
        "Text to analyze:\n",
        "\\\"\\\"\\\"\n",
        "{text_corpus_chunk}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Output MUST be a valid JSON list of objects. Each object must have \"label\", \"description\", and \"surface_forms\" keys.\n",
        "If no clear themes with appropriate surface forms are found, output an empty JSON list: `[]`.\n",
        "Ensure the entire output is ONLY the JSON list, with no other surrounding text or markdown.\n",
        "\n",
        "Valid JSON Output:\"\"\"\n",
        "# Removed the explicit JSON example from the end of the prompt to prevent copying,\n",
        "# relying on the description and \"Output MUST be a valid JSON list...\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def call_local_llm_for_raw_response( # Renamed for clarity: gets RAW response\n",
        "    prompt_content_for_user_turn: str,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str, # For logging context\n",
        "    chunk_idx_for_log: int # For logging context\n",
        "    ) -> str:\n",
        "    \"\"\"\n",
        "    Makes the actual call to the local LLM pipeline using a pre-formatted user prompt string.\n",
        "    Returns the raw text string generated by the LLM.\n",
        "    Assumes pipeline is initialized with return_full_text=False.\n",
        "    \"\"\"\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance:\n",
        "        print(f\"    ERROR (call_local_llm): LLM pipeline/tokenizer not initialized for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return \"\"\n",
        "\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": prompt_content_for_user_turn}]\n",
        "\n",
        "    try:\n",
        "        prompt_formatted_for_llm = hf_tokenizer_instance.apply_chat_template(\n",
        "            messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    except Exception as e_template:\n",
        "        print(f\"    ERROR (call_local_llm): Applying chat template failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_template}\")\n",
        "        # Log this error too\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm_for_raw_response) ---\\n\")\n",
        "            f.write(f\"ERROR APPLYING CHAT TEMPLATE: {e_template}\\n\")\n",
        "            f.write(f\"User prompt content (first 300 chars): {prompt_content_for_user_turn[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": LLM_MAX_NEW_TOKENS_ENHANCED_MOTIF, # From config\n",
        "        \"do_sample\": False, # True,\n",
        "        \"temperature\": 0.3,\n",
        "        \"top_p\": 0.9,\n",
        "        \"repetition_penalty\": 1.1,\n",
        "        \"pad_token_id\": hf_tokenizer_instance.pad_token_id\n",
        "    }\n",
        "    # print(f\"    DEBUG (call_local_llm): Sending prompt to LLM for QID {qid_for_log}, Chunk {chunk_idx_for_log}. Prompt length for pipeline: {len(prompt_formatted_for_llm)}\")\n",
        "\n",
        "    try:\n",
        "        outputs = hf_pipeline_instance(prompt_formatted_for_llm, **generation_args)\n",
        "\n",
        "        # print(f\"    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{outputs}\\n>>>>>\")\n",
        "\n",
        "        if outputs and isinstance(outputs, list) and len(outputs) > 0 and \\\n",
        "           outputs[0] and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
        "            # Since pipeline is expected to be initialized with return_full_text=False,\n",
        "            # outputs[0]['generated_text'] is only the new tokens.\n",
        "            assistant_response_text = outputs[0]['generated_text'].strip()\n",
        "            # print(f\"    DEBUG (call_local_llm): 'assistant_response_text' (return_full_text=False) for QID {qid_for_log}, Chunk {chunk_idx_for_log} (len {len(assistant_response_text)}):\\n<<<<<\\n{assistant_response_text[:1000]}...\\n>>>>>\")\n",
        "            return assistant_response_text\n",
        "        else:\n",
        "            print(f\"    WARN (call_local_llm): LLM pipeline returned unexpected or empty structure for QID {qid_for_log}, Chunk {chunk_idx_for_log}. Output: {outputs}\")\n",
        "            return \"\"\n",
        "    except Exception as e_pipeline:\n",
        "        print(f\"    ERROR (call_local_llm): Exception during hf_pipeline call for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e_pipeline}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (call_local_llm_for_raw_response) ---\\n\")\n",
        "            f.write(f\"ERROR DURING PIPELINE CALL: {e_pipeline}\\n\")\n",
        "            f.write(f\"Formatted prompt (first 300 chars): {prompt_formatted_for_llm[:300]}...\\n\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"Cell 3: LLM Interaction Utilities loaded.\")"
      ],
      "metadata": {
        "id": "3-oT1IAVyJbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ed6014-8a8b-4f90-ef82-3d2cf3f5bca3"
      },
      "id": "3-oT1IAVyJbY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 3: LLM Interaction Utilities loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Motif Processing & Validation\n",
        "# Purpose: Functions related to processing, validating, and refining motifs after they've been initially extracted by the LLM.\n",
        "# Cell 4: Motif Processing and Validation\n",
        "\n",
        "def parse_and_validate_llm_json_response(\n",
        "    llm_raw_response_text: str,\n",
        "    qid_for_log: str,\n",
        "    chunk_idx_for_log: int,\n",
        "    prompt_sent_to_llm: str # For logging context if error\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Parses the LLM's raw text response (expected to be JSON or contain JSON),\n",
        "    validates the basic structure of extracted motif objects.\n",
        "    Returns a list of valid motif dictionaries, or an empty list on failure.\n",
        "    \"\"\"\n",
        "    # print(f\"    DEBUG (parse_validate): Raw LLM response for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{llm_raw_response_text}\\n>>>>>\")\n",
        "\n",
        "    json_str_candidate = llm_raw_response_text.strip()\n",
        "\n",
        "    # Attempt to remove markdown fences if LLM adds them\n",
        "    if json_str_candidate.startswith(\"```json\"):\n",
        "        json_str_candidate = json_str_candidate[len(\"```json\"):].strip()\n",
        "    if json_str_candidate.startswith(\"```\"):\n",
        "        json_str_candidate = json_str_candidate[len(\"```\"):].strip()\n",
        "    if json_str_candidate.endswith(\"```\"):\n",
        "        json_str_candidate = json_str_candidate[:-len(\"```\")].strip()\n",
        "\n",
        "    # print(f\"    DEBUG (parse_validate): Final json_str candidate for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{json_str_candidate}\\n>>>>>\")\n",
        "\n",
        "    if not json_str_candidate or json_str_candidate.lower() == \"[]\":\n",
        "        # print(f\"    LLM indicated no themes or JSON was effectively empty for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return []\n",
        "    # Check for explicit \"no themes found\" type messages that are not JSON '[]'\n",
        "    if \"no_themes_found\" in json_str_candidate.lower() or \"no clear motifs\" in json_str_candidate.lower():\n",
        "        # print(f\"    LLM explicitly stated no themes found for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        parsed_data = json.loads(json_str_candidate)\n",
        "\n",
        "        # Handle if LLM returns a single JSON object instead of a list (as seen in tests)\n",
        "        if isinstance(parsed_data, dict):\n",
        "            # print(f\"    DEBUG (parse_validate): LLM returned a single JSON object, wrapping in list for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "            parsed_data = [parsed_data]\n",
        "\n",
        "        if not isinstance(parsed_data, list):\n",
        "            raise ValueError(\"Parsed JSON from LLM is not a list (nor a single object that could be wrapped).\")\n",
        "\n",
        "        # Validate structure of each item in the parsed list\n",
        "        valid_motifs_from_json = []\n",
        "        for item_idx, item in enumerate(parsed_data):\n",
        "            if isinstance(item, dict) and \\\n",
        "               all(k in item for k in [\"label\", \"description\", \"surface_forms\"]) and \\\n",
        "               isinstance(item['label'], str) and item['label'].strip().startswith('[') and item['label'].strip().endswith(']') and \\\n",
        "               isinstance(item['description'], str) and \\\n",
        "               isinstance(item['surface_forms'], list) and \\\n",
        "               all(isinstance(sf_item, str) for sf_item in item['surface_forms']):\n",
        "\n",
        "                valid_motifs_from_json.append({\n",
        "                    \"label\": item['label'].strip(),\n",
        "                    \"description\": item['description'].strip(),\n",
        "                    \"surface_forms\": [s.strip() for s in item['surface_forms'] if s.strip()] # Clean SFs\n",
        "                })\n",
        "            else:\n",
        "                print(f\"    [WARN] Invalid motif object structure in LLM JSON for QID {qid_for_log}, Chunk {chunk_idx_for_log}, Item {item_idx+1}. Skipping item: {str(item)[:200]}...\")\n",
        "                with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (parse_and_validate_llm_json_response) ---\\n\")\n",
        "                    f.write(f\"INVALID MOTIF ITEM STRUCTURE (Item {item_idx+1}):\\n{item}\\n\")\n",
        "                    f.write(f\"FULL PROBLEMATIC JSON CANDIDATE:\\n{json_str_candidate}\\n\")\n",
        "\n",
        "\n",
        "        return valid_motifs_from_json\n",
        "\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        print(f\"    [WARN] Motif JSON parsing or structure validation failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} (parse_and_validate_llm_json_response) ---\\n\")\n",
        "            f.write(f\"PROMPT SENT (first 500 chars of user content):\\n{prompt_sent_to_llm.split('Text to analyze:')[1][:500]}...\\n\") # Log relevant part of prompt\n",
        "            f.write(f\"RAW LLM RESPONSE (Error: {type(e).__name__}):\\n{llm_raw_response_text}\\n\")\n",
        "            f.write(f\"EXTRACTED JSON STRING CANDIDATE (Error: {type(e).__name__}):\\n{json_str_candidate}\\n\")\n",
        "        return []\n",
        "\n",
        "def validate_and_refine_surface_forms_in_chunk(\n",
        "    llm_parsed_motifs: List[Dict],\n",
        "    text_chunk_analyzed_by_llm: str # The specific text chunk LLM saw\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Validates LLM-generated SFs against actual recurring phrases in the *specific text chunk*.\n",
        "    Uses extract_actual_phrases_from_text (from text_utils cell/module).\n",
        "    \"\"\"\n",
        "    if not llm_parsed_motifs:\n",
        "        return []\n",
        "\n",
        "    # Get recurring phrases (n-grams) from the *specific text chunk* the LLM analyzed\n",
        "    # These n-grams are already lowercased by extract_actual_phrases_from_text\n",
        "    actual_recurring_phrases_in_chunk_map = extract_actual_phrases_from_text(\n",
        "        text_chunk_analyzed_by_llm,\n",
        "        min_freq=MIN_SF_FREQ_IN_CHUNK_VALIDATION # Uses config constant\n",
        "    )\n",
        "\n",
        "    if not actual_recurring_phrases_in_chunk_map:\n",
        "        # print(f\"    No recurring phrases (min_freq={MIN_SF_FREQ_IN_CHUNK_VALIDATION}) found in chunk for SF validation.\")\n",
        "        # If no actual recurring phrases, LLM SFs cannot be validated against them.\n",
        "        # We might choose to return motifs with their original SFs or discard them.\n",
        "        # For now, let's require SFs to be found in recurring phrases.\n",
        "        return [] # Or return llm_parsed_motifs if you want to skip this validation if no n-grams found\n",
        "\n",
        "    refined_motifs_for_this_chunk = []\n",
        "    for motif_dict in llm_parsed_motifs:\n",
        "        llm_proposed_sfs_list = motif_dict.get('surface_forms', [])\n",
        "        if not isinstance(llm_proposed_sfs_list, list): llm_proposed_sfs_list = []\n",
        "\n",
        "        validated_sfs_for_this_motif = set()\n",
        "\n",
        "        for sf_candidate_from_llm in llm_proposed_sfs_list:\n",
        "            if not isinstance(sf_candidate_from_llm, str) or not sf_candidate_from_llm.strip():\n",
        "                continue\n",
        "            sf_lower_candidate = sf_candidate_from_llm.lower().strip()\n",
        "\n",
        "            # Check for exact match (case-insensitive) in the recurring n-grams from this chunk\n",
        "            if sf_lower_candidate in actual_recurring_phrases_in_chunk_map:\n",
        "                validated_sfs_for_this_motif.add(sf_lower_candidate)\n",
        "            # else:\n",
        "                # Optional: Implement fuzzy matching here if desired\n",
        "                # print(f\"      SF '{sf_lower_candidate}' from LLM not found in chunk's recurring phrases.\")\n",
        "\n",
        "        if validated_sfs_for_this_motif: # Only keep motif if it has at least one SF validated against chunk's n-grams\n",
        "            refined_motif_entry = motif_dict.copy()\n",
        "            refined_motif_entry['surface_forms'] = sorted(list(validated_sfs_for_this_motif))\n",
        "            refined_motifs_for_this_chunk.append(refined_motif_entry)\n",
        "        # else:\n",
        "            # print(f\"    Motif '{motif_dict.get('label')}' discarded for chunk (no SFs validated against recurring n-grams in chunk).\")\n",
        "\n",
        "    return refined_motifs_for_this_chunk\n",
        "\n",
        "def enhanced_motif_extraction_per_chunk(\n",
        "    text_chunk_to_analyze: str,\n",
        "    hf_pipeline_instance,\n",
        "    hf_tokenizer_instance,\n",
        "    qid_for_log: str,\n",
        "    chunk_idx_for_log: int\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Orchestrates LLM call for motif extraction from a single text chunk,\n",
        "    parses the JSON response, and performs chunk-level SF validation.\n",
        "    \"\"\"\n",
        "    # 1. Build the prompt using the text_chunk_to_analyze\n",
        "    prompt_str_for_llm = create_enhanced_motif_prompt(text_chunk_to_analyze, MAX_MOTIFS_PER_CHUNK)\n",
        "\n",
        "    raw_llm_response_text = \"\" # Initialize for logging/return\n",
        "    parsed_and_validated_motifs_from_chunk = []\n",
        "\n",
        "    for attempt in range(LLM_RETRY_ATTEMPTS):\n",
        "        # print(f\"      LLM Call attempt {attempt + 1} for chunk {chunk_idx_for_log} (QID {qid_for_log})...\")\n",
        "        raw_llm_response_text = call_local_llm_for_raw_response( # From llm_interaction cell\n",
        "            prompt_str_for_llm,\n",
        "            hf_pipeline_instance,\n",
        "            hf_tokenizer_instance,\n",
        "            qid_for_log,\n",
        "            chunk_idx_for_log\n",
        "        )\n",
        "\n",
        "        if not raw_llm_response_text:\n",
        "            print(f\"      LLM call attempt {attempt + 1} for chunk {chunk_idx_for_log} (QID {qid_for_log}) returned empty string. Retrying if possible...\")\n",
        "            if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1) # Small delay before retry\n",
        "            continue # Go to next attempt\n",
        "\n",
        "        # 2. Parse LLM's raw response into list of motif dicts (basic structure validation)\n",
        "        llm_parsed_motifs = parse_and_validate_llm_json_response(\n",
        "            raw_llm_response_text,\n",
        "            qid_for_log,\n",
        "            chunk_idx_for_log,\n",
        "            prompt_str_for_llm # Pass full prompt for logging context on error\n",
        "        )\n",
        "\n",
        "        if llm_parsed_motifs: # If parsing was successful and returned some motif objects\n",
        "            # 3. Validate the SFs of these parsed motifs against the current text_chunk_to_analyze\n",
        "            # print(f\"      Validating SFs for {len(llm_parsed_motifs)} LLM-parsed motifs from chunk {chunk_idx_for_log}...\")\n",
        "            motifs_validated_in_chunk = validate_and_refine_surface_forms_in_chunk(\n",
        "                llm_parsed_motifs,\n",
        "                text_chunk_to_analyze # Validate against the text chunk LLM saw\n",
        "            )\n",
        "\n",
        "            if motifs_validated_in_chunk:\n",
        "                # print(f\"      Found {len(motifs_validated_in_chunk)} motifs with SFs validated against current chunk's n-grams.\")\n",
        "                parsed_and_validated_motifs_from_chunk = motifs_validated_in_chunk\n",
        "                break # Successful extraction and validation for this chunk, exit retry loop\n",
        "            else:\n",
        "                print(f\"      Chunk {chunk_idx_for_log} (QID {qid_for_log}): LLM provided motifs, but none had SFs validated against chunk's recurring n-grams (Attempt {attempt+1}). Retrying if possible...\")\n",
        "                if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "        else: # Parsing itself failed or returned empty list (e.g., LLM output `[]`)\n",
        "            print(f\"      Motif JSON parsing/validation attempt {attempt + 1} yielded no structured motifs for chunk {chunk_idx_for_log} (QID {qid_for_log}). Retrying if possible...\")\n",
        "            if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "\n",
        "    if not parsed_and_validated_motifs_from_chunk:\n",
        "        print(f\"      No valid & validated motifs extracted from chunk {chunk_idx_for_log} (QID {qid_for_log}) after {LLM_RETRY_ATTEMPTS} attempts.\")\n",
        "\n",
        "    return parsed_and_validated_motifs_from_chunk\n",
        "\n",
        "\n",
        "def consolidate_raw_motifs(list_of_all_raw_motifs: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Consolidates motifs extracted from all chunks, primarily by label, merging surface forms.\"\"\"\n",
        "    if not list_of_all_raw_motifs:\n",
        "        return []\n",
        "\n",
        "    consolidated_motifs_map = {}\n",
        "    for motif_obj in list_of_all_raw_motifs:\n",
        "        label = motif_obj.get(\"label\")\n",
        "        description = motif_obj.get(\"description\") # Assuming description from first encounter is fine\n",
        "        surface_forms = motif_obj.get(\"surface_forms\", [])\n",
        "\n",
        "        if not isinstance(label, str) or not label.strip() or \\\n",
        "           not isinstance(description, str) or \\\n",
        "           not isinstance(surface_forms, list):\n",
        "            # print(f\"    [WARN] Skipping malformed raw motif during consolidation: {motif_obj}\")\n",
        "            continue # Skip malformed motif objects\n",
        "\n",
        "        if label not in consolidated_motifs_map:\n",
        "            consolidated_motifs_map[label] = {\n",
        "                \"label\": label,\n",
        "                \"description\": description,\n",
        "                \"surface_forms\": sorted(list(set(sf.lower().strip() for sf in surface_forms if sf.strip()))) # Store unique, lowercased SFs\n",
        "            }\n",
        "        else: # Label exists, merge surface forms\n",
        "            existing_sfs_set = set(consolidated_motifs_map[label].get(\"surface_forms\", []))\n",
        "            new_sfs_set = set(sf.lower().strip() for sf in surface_forms if sf.strip())\n",
        "            consolidated_motifs_map[label][\"surface_forms\"] = sorted(list(existing_sfs_set.union(new_sfs_set)))\n",
        "            # Could also choose to update/average description, but simplest is to keep the first one.\n",
        "\n",
        "    return list(consolidated_motifs_map.values())\n",
        "\n",
        "def filter_surface_forms_by_global_frequency(\n",
        "    consolidated_motifs_list: List[Dict],\n",
        "    full_qid_corpus_text: str, # The entire original text for the QID\n",
        "    min_global_freq: int = MIN_SF_FREQ_FOR_FINAL_MOTIFS # Uses config\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Filters surface forms in consolidated motifs based on their frequency\n",
        "    in the full QID corpus text.\n",
        "    \"\"\"\n",
        "    if not consolidated_motifs_list:\n",
        "        return []\n",
        "\n",
        "    final_globally_filtered_motifs = []\n",
        "    # print(f\"  Filtering SFs from {len(consolidated_motifs_list)} consolidated motifs (min global freq: {min_global_freq})...\")\n",
        "\n",
        "    # Pre-calculate frequencies of all potential SFs for efficiency if many motifs share SFs\n",
        "    # For now, count per SF as it appears.\n",
        "\n",
        "    for motif_obj in consolidated_motifs_list:\n",
        "        globally_frequent_sfs_for_this_motif = []\n",
        "        original_sfs_for_this_motif = motif_obj.get(\"surface_forms\", [])\n",
        "\n",
        "        for sf_str in original_sfs_for_this_motif:\n",
        "            # Count occurrences in the *full* QID corpus text\n",
        "            # count_sf_occurrences handles case-insensitivity and re.escape\n",
        "            count = count_sf_occurrences(full_qid_corpus_text, sf_str)\n",
        "\n",
        "            if count >= min_global_freq:\n",
        "                globally_frequent_sfs_for_this_motif.append(sf_str) # Keep original casing from consolidated list\n",
        "                # print(f\"    SF '{sf_str}' (label '{motif_obj.get('label')}') kept, global freq: {count}\")\n",
        "            # else:\n",
        "                # print(f\"    SF '{sf_str}' (label '{motif_obj.get('label')}') filtered out, global freq: {count} (min_req: {min_global_freq})\")\n",
        "\n",
        "        if globally_frequent_sfs_for_this_motif: # Only keep motif if it has at least one globally frequent SF\n",
        "            filtered_motif_entry = motif_obj.copy() # Make a copy to modify\n",
        "            filtered_motif_entry[\"surface_forms\"] = sorted(list(set(globally_frequent_sfs_for_this_motif))) # Ensure unique and sorted\n",
        "            final_globally_filtered_motifs.append(filtered_motif_entry)\n",
        "        # else:\n",
        "            # print(f\"    Motif '{motif_obj.get('label')}' discarded (no globally frequent SFs after filtering).\")\n",
        "\n",
        "    return final_globally_filtered_motifs\n",
        "\n",
        "print(\"Cell 4: Motif Processing and Validation Utilities loaded.\")"
      ],
      "metadata": {
        "id": "eBoeS9YbyYKX",
        "outputId": "abdaf122-0d17-4808-d6b2-fa4129e5ecb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eBoeS9YbyYKX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 4: Motif Processing and Validation Utilities loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. MDL Calculations\n",
        "# Purpose: All functions related to MDL cost calculations (L(H), L(D|H)) and BDM.\n",
        "# Cell 5: MDL Calculations\n",
        "\n",
        "def initialize_bdm_instance():\n",
        "    \"\"\"Initializes and returns a BDM instance.\"\"\"\n",
        "    print(\"Initializing BDM instance...\")\n",
        "    try:\n",
        "        # Using default CTM-based NKS for 2D data\n",
        "        bdm_instance = BDM(ndim=2)\n",
        "        print(\"BDM instance initialized successfully (ndim=2, default CTM-based).\")\n",
        "        return bdm_instance\n",
        "    except Exception as e_bdm_init:\n",
        "        print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\")\n",
        "        if \"CTM data files\" in str(e_bdm_init).lower() or \"dataset\" in str(e_bdm_init).lower():\n",
        "            print(\"  BDM Error Hint: This might be related to missing/corrupted CTM data files for PyBDM.\")\n",
        "            print(\"  Ensure PyBDM is installed correctly and can access/download its data.\")\n",
        "            print(\"  You might need to run the following once in your environment:\")\n",
        "            print(\"  from pybdm import get_ctm_dataset; get_ctm_dataset(force=False)\") # Add force=True if re-download needed\n",
        "        return None\n",
        "\n",
        "def text_to_binary_matrix(text_input: str, size: tuple = MATRIX_SIZE_GLOBAL) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Converts a text string to a binary matrix using its SHA256 hash.\n",
        "    The BDM calculation is sensitive to this representation.\n",
        "    \"\"\"\n",
        "    if not isinstance(text_input, str) or not text_input.strip():\n",
        "        # Return a zero matrix for empty or invalid input to ensure BDM gets a matrix\n",
        "        # BDM of a zero matrix will be low, reflecting low complexity.\n",
        "        return np.zeros(size, dtype=int)\n",
        "\n",
        "    # Using SHA256 hash of the text\n",
        "    hash_obj = hashlib.sha256(text_input.encode('utf-8', 'ignore')) # Ignore encoding errors for robustness\n",
        "    hash_digest = hash_obj.hexdigest() # 64 hex characters = 256 bits\n",
        "\n",
        "    required_bits = size[0] * size[1]\n",
        "\n",
        "    # Convert hex digest to a binary string\n",
        "    binary_string_from_hash = bin(int(hash_digest, 16))[2:].zfill(256) # Ensure it's 256 bits long\n",
        "\n",
        "    # Handle matrix size vs. hash size\n",
        "    if required_bits <= 256:\n",
        "        # Take the first 'required_bits' from the hash's binary string\n",
        "        binary_string_for_matrix = binary_string_from_hash[:required_bits]\n",
        "    else:\n",
        "        # If matrix is larger than hash bits, pad with zeros (or could repeat hash, but zero-padding is simpler)\n",
        "        # print(f\"    WARN (text_to_binary_matrix): Matrix size {size} requires {required_bits} bits, SHA256 provides 256. Padding with zeros.\")\n",
        "        binary_string_for_matrix = binary_string_from_hash.ljust(required_bits, '0')\n",
        "\n",
        "    bits_for_matrix = [int(b) for b in binary_string_for_matrix]\n",
        "    return np.array(bits_for_matrix).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input: str, bdm_instance: BDM, matrix_s: tuple = MATRIX_SIZE_GLOBAL) -> float:\n",
        "    \"\"\"\n",
        "    Computes BDM for a given text string.\n",
        "    Uses a truncated prefix of the text for hashing if text is too long,\n",
        "    as BDM on very large matrices derived from full text can be slow and BDM\n",
        "    is often used on fixed-size representations.\n",
        "    \"\"\"\n",
        "    if not isinstance(text_input, str) or not text_input.strip():\n",
        "        return 0.0 # BDM of effectively nothing is low complexity\n",
        "\n",
        "    # Using a prefix of the text for BDM calculation to keep it manageable\n",
        "    # and to compare changes in a consistent part of the data.\n",
        "    text_for_hash = text_input[:MAX_TEXT_FOR_BDM_HASH] if len(text_input) > MAX_TEXT_FOR_BDM_HASH else text_input\n",
        "\n",
        "    if not text_for_hash.strip(): # If the prefix is also empty/whitespace\n",
        "        return 0.0\n",
        "\n",
        "    binary_matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        bdm_value = bdm_instance.bdm(binary_matrix)\n",
        "        return bdm_value\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (full len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0 # Indicate error\n",
        "\n",
        "def calculate_L_H_token_based_structured(structured_motifs_list: List[Dict]) -> float:\n",
        "    \"\"\"\n",
        "    Calculates L(H) - the cost of defining the list of structured motifs.\n",
        "    Uses token-based costs defined in the configuration.\n",
        "    \"\"\"\n",
        "    if not structured_motifs_list:\n",
        "        return 0.0\n",
        "\n",
        "    total_lh_cost = 0.0\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict):\n",
        "            continue # Skip malformed entries\n",
        "\n",
        "        current_motif_lh = 0.0\n",
        "\n",
        "        # Cost for the symbolic label itself\n",
        "        label_str = motif_obj.get('label', \"\")\n",
        "        if isinstance(label_str, str) and label_str.strip():\n",
        "            current_motif_lh += MOTIF_SYMBOLIC_LABEL_COST\n",
        "\n",
        "        # Cost for the textual description\n",
        "        description_str = motif_obj.get('description', \"\")\n",
        "        if isinstance(description_str, str) and description_str.strip():\n",
        "            current_motif_lh += MOTIF_DESCRIPTION_TEXT_BASE_COST\n",
        "            current_motif_lh += len(tokenize_phrase(description_str)) * MOTIF_DESCRIPTION_TOKEN_COST\n",
        "\n",
        "        # Cost for listing the surface forms in the L(H) definition\n",
        "        surface_forms_list = motif_obj.get('surface_forms', [])\n",
        "        if isinstance(surface_forms_list, list) and surface_forms_list: # Check if list is not empty\n",
        "            valid_sfs_for_lh = [sf for sf in surface_forms_list if isinstance(sf, str) and sf.strip()]\n",
        "            if valid_sfs_for_lh:\n",
        "                current_motif_lh += MOTIF_SURFACE_FORMS_LIST_BASE_COST\n",
        "                for sf_str in valid_sfs_for_lh:\n",
        "                    current_motif_lh += len(tokenize_phrase(sf_str)) * MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH\n",
        "\n",
        "        total_lh_cost += current_motif_lh\n",
        "    return total_lh_cost\n",
        "\n",
        "def llm_compress_text_structured(text_to_compress: str, structured_motifs_list: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Compresses text by replacing occurrences of motif surface forms with their symbolic labels.\n",
        "    Text and surface forms are lowercased for matching.\n",
        "    \"\"\"\n",
        "    if not isinstance(text_to_compress, str):\n",
        "        return \"\"\n",
        "    if not structured_motifs_list:\n",
        "        return text_to_compress.lower() # Return lowercased original if no motifs\n",
        "\n",
        "    # Start with the lowercased version of the text to compress\n",
        "    compressed_text = text_to_compress.lower()\n",
        "\n",
        "    # It can be beneficial to process motifs that have longer surface forms first,\n",
        "    # or motifs that are more specific. For now, we process in given order.\n",
        "    # A more advanced strategy might sort structured_motifs_list here.\n",
        "\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict):\n",
        "            continue\n",
        "\n",
        "        label = motif_obj.get('label', None)\n",
        "        surface_forms = motif_obj.get('surface_forms', [])\n",
        "\n",
        "        if not (isinstance(label, str) and label.strip()) or \\\n",
        "           not (isinstance(surface_forms, list) and surface_forms): # Ensure label is valid and SF list is not empty\n",
        "            continue # Skip motif if label is bad or no surface forms\n",
        "\n",
        "        placeholder = label # Use the actual symbolic label as the placeholder\n",
        "\n",
        "        # Sort this motif's own surface forms by length (descending)\n",
        "        # to ensure longer matches are prioritized over shorter sub-matches.\n",
        "        sorted_sfs_for_this_motif = sorted(\n",
        "            [sf for sf in surface_forms if isinstance(sf, str) and sf.strip()],\n",
        "            key=len,\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        for sf_str in sorted_sfs_for_this_motif:\n",
        "            sf_lower = sf_str.lower() # Surface forms are already lowercased during consolidation & filtering in this pipeline\n",
        "            try:\n",
        "                # Replace all occurrences of this surface form (case-insensitive due to prior lowercasing)\n",
        "                # re.escape handles any special regex characters in sf_lower\n",
        "                compressed_text = re.sub(r'\\b' + re.escape(sf_lower) + r'\\b', placeholder, compressed_text)\n",
        "                # Using word boundaries \\b to avoid partial word matches, e.g. 'cat' in 'caterpillar'\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for SF '{sf_str}' of motif '{label}': {re_e}. Skipping this SF.\")\n",
        "                continue # Skip this surface form, try next\n",
        "    return compressed_text\n",
        "\n",
        "def compute_mdl_cost_for_text_block(\n",
        "    full_qid_corpus_str: str,\n",
        "    final_motifs_to_evaluate: List[Dict], # Renamed for clarity\n",
        "    bdm_instance: BDM,\n",
        "    matrix_s: tuple = MATRIX_SIZE_GLOBAL\n",
        "    ) -> tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Computes L(H), L(D|H), and Total MDL for a text block given a final set of motifs.\n",
        "    \"\"\"\n",
        "    if not isinstance(full_qid_corpus_str, str):\n",
        "        full_qid_corpus_str = \"\" # Ensure it's a string\n",
        "\n",
        "    # Calculate L(H): Cost of defining the final_motifs_to_evaluate\n",
        "    l_h = calculate_L_H_token_based_structured(final_motifs_to_evaluate)\n",
        "\n",
        "    # Calculate L(D|H): Compress the full corpus and then compute BDM\n",
        "    compressed_text_block = llm_compress_text_structured(full_qid_corpus_str, final_motifs_to_evaluate)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "\n",
        "    if l_d_h < 0: # Indicates a BDM computation error\n",
        "        # print(f\"    WARN (compute_mdl_cost): BDM error for L(D|H). L(H) was {l_h:.4f}\")\n",
        "        return l_h, -1.0, -1.0 # Propagate error for L(D|H) and total\n",
        "\n",
        "    total_mdl_cost = l_h + l_d_h\n",
        "    return l_h, l_d_h, total_mdl_cost\n",
        "\n",
        "print(\"Cell 5: MDL Calculation Utilities loaded.\")"
      ],
      "metadata": {
        "id": "Lgcgja8xyfID",
        "outputId": "da038b09-4d24-441a-dd2d-4f6405e6fe30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Lgcgja8xyfID",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 5: MDL Calculation Utilities loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Main Orchestration\n",
        "# Purpose: The top-level script that orchestrates the entire workflow, importing functions from other modules. This would contain your main() function.\n",
        "# Cell 6: Main Pipeline Orchestration\n",
        "\n",
        "def main():\n",
        "    # --- Initial Setup and Welcome Message ---\n",
        "    print(\"--- MWP Enhanced: Batched LLM, Validated SFs, Structured Motifs, Token-L(H), BDM L(D|H) ---\")\n",
        "    print(f\"Timestamp: {time.asctime()}\")\n",
        "    print(\"\\n--- Configuration Summary ---\")\n",
        "    print(f\"LLM Model: {LOCAL_LLM_MODEL_ID}, Quantization: {USE_QUANTIZATION_FOR_LOCAL_LLM}\")\n",
        "    print(f\"LLM Batch Size (Responses): {LLM_BATCH_SIZE_RESPONSES}, Retries: {LLM_RETRY_ATTEMPTS}\")\n",
        "    print(f\"Max Text Chars per LLM Prompt Chunk: {MAX_TEXT_CHARS_PER_LLM_PROMPT_CHUNK}\")\n",
        "    print(f\"Max New Tokens for LLM Motif Extraction: {LLM_MAX_NEW_TOKENS_ENHANCED_MOTIF}\")\n",
        "    print(f\"Max Motifs to Request per Chunk: {MAX_MOTIFS_PER_CHUNK}\")\n",
        "    print(f\"L(H) Costs: Label={MOTIF_SYMBOLIC_LABEL_COST}, DescBase={MOTIF_DESCRIPTION_TEXT_BASE_COST}, DescToken={MOTIF_DESCRIPTION_TOKEN_COST}, SFListBase={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, SFTokenInLH={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "    print(f\"SF Validation (Chunk N-grams) Min Freq: {MIN_SF_FREQ_IN_CHUNK_VALIDATION}\")\n",
        "    print(f\"SF Filtering (Global Corpus) Min Freq: {MIN_SF_FREQ_FOR_FINAL_MOTIFS}\")\n",
        "    print(f\"BDM Hash Prefix Length: {MAX_TEXT_FOR_BDM_HASH}, BDM Matrix: {MATRIX_SIZE_GLOBAL}\")\n",
        "    print(f\"Debug Log File: {LLM_DEBUG_LOG_FILE}\")\n",
        "    print(\"--- End Configuration Summary ---\\n\")\n",
        "\n",
        "    # --- Initialize Debug Log File ---\n",
        "    try:\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"w\", encoding=\"utf-8\") as f: # Overwrite for new run\n",
        "            f.write(f\"LLM Motif Debug Log - Run Started: {time.asctime()}\\n\")\n",
        "            f.write(f\"Model ID: {LOCAL_LLM_MODEL_ID}\\n\")\n",
        "            f.write(f\"Pipeline Config: return_full_text=False (Implicit in enhanced_motif_extraction_per_chunk)\\n\")\n",
        "            f.write(\"Enhanced Prompting, Chunk-level SF Validation, and Global SF Filtering Active\\n---\\n\")\n",
        "    except Exception as e_log:\n",
        "        print(f\"WARN: Could not initialize debug log file {LLM_DEBUG_LOG_FILE}: {e_log}\")\n",
        "\n",
        "    # --- Initialize LLM and BDM ---\n",
        "    # Functions are expected to be defined in previous cells:\n",
        "    # initialize_llm_pipeline (from llm_interaction.py / Cell 3)\n",
        "    # initialize_bdm_instance (from mdl_calculations.py / Cell 5)\n",
        "\n",
        "    hf_pipeline_instance, hf_tokenizer_instance = initialize_llm_pipeline() # Uses defaults from config\n",
        "    if not hf_pipeline_instance or not hf_tokenizer_instance:\n",
        "        print(\"CRITICAL: Exiting due to LLM pipeline initialization failure.\")\n",
        "        return\n",
        "\n",
        "    bdm_instance_main = initialize_bdm_instance()\n",
        "    if not bdm_instance_main:\n",
        "        print(\"CRITICAL: Exiting due to BDM initialization failure.\")\n",
        "        return\n",
        "\n",
        "    # --- Load Phase 2 Collated Data ---\n",
        "    if not os.path.exists(P2_COLLATED_FILE):\n",
        "        print(f\"ERROR: Phase 2 output file not found: {P2_COLLATED_FILE}\")\n",
        "        return\n",
        "    print(f\"Loading Phase 2 output from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f:\n",
        "            phase2_data_content = json.load(f)\n",
        "    except Exception as e_load:\n",
        "        print(f\"Error loading or parsing {P2_COLLATED_FILE}: {e_load}\")\n",
        "        return\n",
        "\n",
        "    all_qid_mdl_results_list = [] # Stores result dict for each QID\n",
        "\n",
        "    # --- Determine QIDs to Process ---\n",
        "    aggregated_content_by_qid_from_file = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "    if not aggregated_content_by_qid_from_file:\n",
        "        print(f\"No 'aggregated_pdf_content_by_qid' key found or data is empty in {P2_COLLATED_FILE}.\")\n",
        "        return\n",
        "\n",
        "    qids_to_process_this_run = []\n",
        "    if P3_QIDS_TO_PROCESS_THEMATICALLY and isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and P3_QIDS_TO_PROCESS_THEMATICALLY:\n",
        "        qids_to_process_this_run = [qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY if qid in aggregated_content_by_qid_from_file]\n",
        "        if not qids_to_process_this_run:\n",
        "            print(f\"Warning: None of specified QIDs {P3_QIDS_TO_PROCESS_THEMATICALLY} found in loaded data. Exiting.\")\n",
        "            return\n",
        "    else: # Fallback if P3_QIDS_TO_PROCESS_THEMATICALLY is not set or empty\n",
        "        qids_to_process_limit_fallback = 1 # Process only the first QID found in data as a fallback\n",
        "        print(f\"P3_QIDS_TO_PROCESS_THEMATICALLY not set or empty. Processing up to {qids_to_process_limit_fallback} QID(s) from data as fallback.\")\n",
        "        qids_to_process_this_run = list(aggregated_content_by_qid_from_file.keys())[:qids_to_process_limit_fallback]\n",
        "        if not qids_to_process_this_run:\n",
        "            print(\"No QIDs available in data to process based on fallback. Exiting.\")\n",
        "            return\n",
        "    print(f\"\\nMDL analysis will run for these QIDs: {qids_to_process_this_run}\\n\")\n",
        "\n",
        "    # --- Main QID Processing Loop ---\n",
        "    for qid_identifier_str in qids_to_process_this_run:\n",
        "        print(f\"--- Analyzing Data for QID: {qid_identifier_str} ---\")\n",
        "\n",
        "        list_of_individual_response_structs = aggregated_content_by_qid_from_file.get(qid_identifier_str, [])\n",
        "        # Extract actual text strings from the list of response dicts/structs\n",
        "        actual_response_texts_for_qid = [\n",
        "            item.get(\"text\", \"\") for item in list_of_individual_response_structs\n",
        "            if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()\n",
        "        ]\n",
        "        if not actual_response_texts_for_qid:\n",
        "            print(f\"  No valid text strings extracted from responses for QID {qid_identifier_str}. Skipping.\")\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        # Create the full corpus for this QID (used for baseline BDM and final L(D|H))\n",
        "        full_corpus_text_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(actual_response_texts_for_qid)\n",
        "        # Optional: Preprocess the full corpus if SF counting and compression should operate on preprocessed text\n",
        "        # For now, SF counting and compression will use full_corpus_text_for_qid.lower()\n",
        "\n",
        "        if len(full_corpus_text_for_qid.strip()) < 100: # Arbitrary threshold for meaningful analysis\n",
        "            print(f\"  Skipping QID {qid_identifier_str}: combined text too short ({len(full_corpus_text_for_qid)} chars).\")\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        num_total_responses_for_qid = len(actual_response_texts_for_qid)\n",
        "        print(f\"  Combined corpus for QID {qid_identifier_str} has {len(full_corpus_text_for_qid)} chars from {num_total_responses_for_qid} individual responses.\")\n",
        "\n",
        "        # Calculate baseline L(D) for the entire QID's text\n",
        "        baseline_bdm_original_corpus = compute_bdm_for_text(full_corpus_text_for_qid, bdm_instance_main, MATRIX_SIZE_GLOBAL)\n",
        "        if baseline_bdm_original_corpus < 0: # BDM error\n",
        "            print(f\"  Error computing baseline BDM for QID {qid_identifier_str}. Skipping this QID.\")\n",
        "            # Log an error entry for this QID\n",
        "            error_entry = {\"qid\": qid_identifier_str, \"status\": \"ERROR_BASELINE_BDM\", \"baseline_mdl\": -1.0}\n",
        "            all_qid_mdl_results_list.append(error_entry)\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        current_qid_baseline_mdl_cost = baseline_bdm_original_corpus # L(H) is 0 for baseline\n",
        "        print(f\"  Baseline MDL for QID {qid_identifier_str} (L(D_orig)): {current_qid_baseline_mdl_cost:.4f}\")\n",
        "\n",
        "        # --- Batched Enhanced Motif Extraction ---\n",
        "        # Create text chunks where each chunk is a string of joined individual responses\n",
        "        batched_text_chunks_for_llm_input = []\n",
        "        for i in range(0, len(actual_response_texts_for_qid), LLM_BATCH_SIZE_RESPONSES):\n",
        "            batch_of_responses = actual_response_texts_for_qid[i:i + LLM_BATCH_SIZE_RESPONSES]\n",
        "            # Preprocess the joined text of the chunk before sending to LLM\n",
        "            chunk_text_for_llm = preprocess_corpus_for_motif_extraction(\"\\n\\n<RSP_SEP>\\n\\n\".join(batch_of_responses))\n",
        "            batched_text_chunks_for_llm_input.append(chunk_text_for_llm)\n",
        "\n",
        "        print(f\"  QID {qid_identifier_str}: Processing {num_total_responses_for_qid} responses in {len(batched_text_chunks_for_llm_input)} preprocessed chunks (batch size: {LLM_BATCH_SIZE_RESPONSES} responses).\")\n",
        "\n",
        "        # Accumulate motifs from all chunks (these have passed chunk-level SF validation)\n",
        "        raw_motifs_chunk_validated = []\n",
        "        for chunk_idx, text_chunk_being_analyzed in enumerate(batched_text_chunks_for_llm_input):\n",
        "            print(f\"    Analyzing chunk {chunk_idx + 1}/{len(batched_text_chunks_for_llm_input)} for QID {qid_identifier_str} (processed chunk len: {len(text_chunk_being_analyzed)} chars)...\")\n",
        "            if len(text_chunk_being_analyzed.strip()) < 50: # Skip very small/empty chunks after preprocessing\n",
        "                print(f\"      Chunk {chunk_idx+1} (QID {qid_identifier_str}) too short after preprocessing, skipping.\")\n",
        "                continue\n",
        "\n",
        "            # enhanced_motif_extraction_per_chunk is from motif_processing.py / Cell 4\n",
        "            motifs_from_this_chunk_list = enhanced_motif_extraction_per_chunk(\n",
        "                text_chunk_being_analyzed,\n",
        "                hf_pipeline_instance,\n",
        "                hf_tokenizer_instance,\n",
        "                qid_identifier_str,\n",
        "                chunk_idx + 1\n",
        "            )\n",
        "            if motifs_from_this_chunk_list:\n",
        "                print(f\"      Extracted {len(motifs_from_this_chunk_list)} chunk-validated motif objects from chunk {chunk_idx+1} (QID {qid_identifier_str}).\")\n",
        "                raw_motifs_chunk_validated.extend(motifs_from_this_chunk_list)\n",
        "            else:\n",
        "                print(f\"      No valid & chunk-validated motifs extracted from chunk {chunk_idx+1} (QID {qid_identifier_str}).\")\n",
        "\n",
        "        # --- Initialize result entry for this QID ---\n",
        "        current_qid_result_entry = {\n",
        "            \"qid\": qid_identifier_str,\n",
        "            \"corpus_len_chars\": len(full_corpus_text_for_qid),\n",
        "            \"num_responses\": num_total_responses_for_qid,\n",
        "            \"baseline_mdl\": current_qid_baseline_mdl_cost,\n",
        "            \"final_refined_motifs\": [], # Will hold globally refined motifs\n",
        "            \"l_h_final_motifs\": 0.0,\n",
        "            \"l_d_h_final_motifs\": current_qid_baseline_mdl_cost, # Default to baseline if no motifs\n",
        "            \"total_mdl_with_final_motifs\": current_qid_baseline_mdl_cost, # Default to baseline\n",
        "            \"compression_achieved\": 0.0,\n",
        "            \"num_raw_motifs_chunk_validated\": len(raw_motifs_chunk_validated),\n",
        "            \"num_consolidated_motifs\": 0,\n",
        "            \"num_globally_refined_motifs\": 0\n",
        "        }\n",
        "\n",
        "        if not raw_motifs_chunk_validated:\n",
        "            print(f\"  No raw & chunk-validated motifs extracted by LLM for QID {qid_identifier_str} from any chunk.\")\n",
        "            all_qid_mdl_results_list.append(current_qid_result_entry)\n",
        "            print(\"-\" * 50); continue # Go to next QID\n",
        "\n",
        "        print(f\"  Total {len(raw_motifs_chunk_validated)} raw (chunk-validated) motifs extracted from LLM for QID {qid_identifier_str}.\")\n",
        "\n",
        "        # --- Motif Consolidation Step ---\n",
        "        # consolidate_raw_motifs is from motif_processing.py / Cell 4\n",
        "        consolidated_motifs_list = consolidate_raw_motifs(raw_motifs_chunk_validated)\n",
        "        current_qid_result_entry[\"num_consolidated_motifs\"] = len(consolidated_motifs_list)\n",
        "        print(f\"  Consolidated into {len(consolidated_motifs_list)} unique motifs (by label) for QID {qid_identifier_str}.\")\n",
        "\n",
        "        if not consolidated_motifs_list:\n",
        "            print(f\"  No unique motifs left after consolidation for QID {qid_identifier_str}.\")\n",
        "            all_qid_mdl_results_list.append(current_qid_result_entry)\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        # --- Global Surface Form Frequency Filtering ---\n",
        "        print(f\"  Refining {len(consolidated_motifs_list)} consolidated motifs by GLOBAL SF frequency (min freq: {MIN_SF_FREQ_FOR_FINAL_MOTIFS})...\")\n",
        "        # filter_surface_forms_by_global_frequency is from motif_processing.py / Cell 4\n",
        "        globally_refined_motifs_for_mdl = filter_surface_forms_by_global_frequency(\n",
        "            consolidated_motifs_list,\n",
        "            full_corpus_text_for_qid, # Filter against the full original corpus for this QID\n",
        "            min_global_freq=MIN_SF_FREQ_FOR_FINAL_MOTIFS\n",
        "        )\n",
        "        current_qid_result_entry[\"num_globally_refined_motifs\"] = len(globally_refined_motifs_for_mdl)\n",
        "        print(f\"  Globally refined into {len(globally_refined_motifs_for_mdl)} motifs for QID {qid_identifier_str}.\")\n",
        "\n",
        "        if not globally_refined_motifs_for_mdl:\n",
        "            print(f\"  No motifs left after GLOBAL surface form frequency refinement for QID {qid_identifier_str}.\")\n",
        "            all_qid_mdl_results_list.append(current_qid_result_entry)\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        print(f\"  Final Globally Refined Motifs for QID {qid_identifier_str} (for MDL eval):\")\n",
        "        for idx, mo_final in enumerate(globally_refined_motifs_for_mdl): # Iterate through globally_refined_motifs_for_mdl\n",
        "            print(f\"    --- Refined Motif {idx+1} ---\")\n",
        "            print(f\"      Label: {mo_final.get('label', 'N/A')}, Desc: {mo_final.get('description','N/A')[:70]}..., SFs ({len(mo_final.get('surface_forms',[]))}): {mo_final.get('surface_forms',[])}\")\n",
        "\n",
        "        # --- Final MDL Calculation ---\n",
        "        # compute_mdl_cost_for_text_block is from mdl_calculations.py / Cell 5\n",
        "        l_h_final_val, l_d_h_final_val, total_mdl_with_final_motifs_val = compute_mdl_cost_for_text_block(\n",
        "            full_corpus_text_for_qid,\n",
        "            globally_refined_motifs_for_mdl, # Use globally refined motifs\n",
        "            bdm_instance_main,\n",
        "            MATRIX_SIZE_GLOBAL\n",
        "        )\n",
        "\n",
        "        current_qid_result_entry[\"final_refined_motifs\"] = globally_refined_motifs_for_mdl # Store the motifs used\n",
        "        current_qid_result_entry[\"l_h_final_motifs\"] = l_h_final_val\n",
        "\n",
        "        if l_d_h_final_val < 0: # BDM error during L(D|H) calculation\n",
        "            print(f\"  Error computing MDL cost with final refined motifs for QID {qid_identifier_str} (BDM error in L(D|H)).\")\n",
        "            current_qid_result_entry.update({\"l_d_h_final_motifs\": -1.0, \"total_mdl_with_final_motifs\": -1.0, \"compression_achieved\": \"BDM_ERROR\"})\n",
        "            all_qid_mdl_results_list.append(current_qid_result_entry)\n",
        "            print(\"-\" * 50); continue\n",
        "\n",
        "        current_qid_result_entry[\"l_d_h_final_motifs\"] = l_d_h_final_val\n",
        "        current_qid_result_entry[\"total_mdl_with_final_motifs\"] = total_mdl_with_final_motifs_val\n",
        "        compression_final_val = current_qid_baseline_mdl_cost - total_mdl_with_final_motifs_val\n",
        "        current_qid_result_entry[\"compression_achieved\"] = compression_final_val\n",
        "\n",
        "        print(f\"  L(H) for final motifs: {l_h_final_val:.4f}\")\n",
        "        print(f\"  L(D|H) compressed full corpus: {l_d_h_final_val:.4f}\")\n",
        "        print(f\"  Total MDL cost with final motifs: {total_mdl_with_final_motifs_val:.4f}\")\n",
        "\n",
        "        result_status_str = f\"SUCCESS: Compression: {compression_final_val:.4f}\" if compression_final_val > 0.0001 else f\"NOTE: No sig. comp. Diff: {compression_final_val:.4f}\"\n",
        "        print(f\"  {result_status_str}\")\n",
        "        all_qid_mdl_results_list.append(current_qid_result_entry)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # --- Summary Printing and Saving Results ---\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary (Enhanced Pipeline) ---\")\n",
        "    if not all_qid_mdl_results_list:\n",
        "        print(\"No QIDs were processed or no valid results generated.\")\n",
        "    else:\n",
        "        valid_results_for_stats = [r for r in all_qid_mdl_results_list if isinstance(r.get('compression_achieved'), float) and r.get('l_h_final_motifs', -1.0) >= 0]\n",
        "        num_qids_processed = len(all_qid_mdl_results_list)\n",
        "        num_qids_with_valid_mdl = len(valid_results_for_stats)\n",
        "        num_compressed_qids = sum(1 for r in valid_results_for_stats if r['compression_achieved'] > 0.0001)\n",
        "\n",
        "        successful_compressions_values = [r['compression_achieved'] for r in valid_results_for_stats if r['compression_achieved'] > 0.0001]\n",
        "        avg_compression_val = np.mean(successful_compressions_values) if successful_compressions_values else 0\n",
        "        max_compression_val = np.max(successful_compressions_values) if successful_compressions_values else 0\n",
        "\n",
        "        print(f\"Total QIDs targeted for analysis: {len(qids_to_process_this_run)}\")\n",
        "        print(f\"Total QID result entries logged: {num_qids_processed}\")\n",
        "        print(f\"Number of QIDs with valid MDL calculations: {num_qids_with_valid_mdl}\")\n",
        "        print(f\"Number of QIDs where compression was achieved: {num_compressed_qids}\")\n",
        "        if num_compressed_qids > 0:\n",
        "            print(f\"  Average compression (for successful cases): {avg_compression_val:.4f}\")\n",
        "            print(f\"  Maximum compression achieved across QIDs: {max_compression_val:.4f}\")\n",
        "        else:\n",
        "            print(\"  No compression achieved for any QID in this run.\")\n",
        "\n",
        "        output_filename_qids_final = os.path.join(BASE_PROJECT_DIR, \"mdl_analysis_per_qid_enhanced_pipeline_vLatest.json\")\n",
        "        try:\n",
        "            with open(output_filename_qids_final, \"w\", encoding=\"utf-8\") as f_out:\n",
        "                json.dump(all_qid_mdl_results_list, f_out, indent=2, ensure_ascii=False) # ensure_ascii=False for non-latin chars\n",
        "            print(f\"Detailed QID-based results saved to {output_filename_qids_final}\")\n",
        "        except Exception as e_save:\n",
        "            print(f\"Error saving QID-based results to {output_filename_qids_final}: {e_save}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure this script is run after cells 1-5 have defined their constants and functions\n",
        "    print(\"Executing main MDL pipeline...\")\n",
        "    main()\n",
        "    print(\"Main MDL pipeline execution finished.\")"
      ],
      "metadata": {
        "id": "jdti5_AKyoLZ",
        "outputId": "3de1760d-cfe4-430f-f579-502170519ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7e30a632dfc043b487fe015d2f4425a2",
            "11d336596bf74eeb81a5853d30f0688a",
            "eb867fd7ce9145918709fb4485b94afa",
            "150690c97f654b0595c19cf33ea07a69",
            "98dbe79659444665b0f36907e0d90ae0",
            "429b10f41894411ba6e442a42d64b3f7",
            "c400181857da49fcb75675be19a98769",
            "6164bab89c1746d2a6e4fdd4e8663d89",
            "802d1a122825460f8664263b7ba78f3a",
            "ad0595bc26584579b6913ab42bea72d3",
            "d72b5964eea7460fb1e5a1365da7050b",
            "23633088a11e4f7d8e06f0b424a55b5a",
            "51ab088fea0046fab6f7bfc9606d895f",
            "bd2446f7dae14fbb82c6b820c943281b",
            "fa1f3158a230476485f7fae7642b3d17",
            "9dd679e6ca1e4130827ecd25cb72515f",
            "a3ccd59cf05a42f0912b20289bc507d0",
            "46381cfbfea341a3bf19d6766a0cea1e",
            "d1424b6efe4f4bfe9fa2502297985935",
            "7b800f4c979e4d0fa3fb26b10ef2d247",
            "5b300830b69d4d40b661e4e2d2d482c5",
            "dd73de3dbe38471085314549e4967679",
            "58391969e96542f79f28ced1640ecd53",
            "6c2c145685ab47bab2a496c073b60311",
            "980d1069d2d54a74ac1037ab1c283874",
            "2f1fd9497fb84f349c20232ae3a31333",
            "02a5b50eba404656bb06f38e9c6bb5b0",
            "62332dff83cb4a2db8a4d6da5e953f86",
            "231937a3002e40a5b0ba08b07ee08680",
            "d2183c5e31474c168d913389546b46b0",
            "c0a8db49b3c94c6b89b95ea132cd5aa3",
            "054df9bb60ed425487de64b055ec5476",
            "9b607c0a24664ab0b7ef011b45bff427",
            "a3f5b78c50144db397adaf6058d3053b",
            "c06c94fb370947a186f02d369d4e971c",
            "08e189579df34388831ab9f5b511698b",
            "34f66c76a0924f99b248f99b3cdc8acc",
            "66d2cb6a0d204c7295af214273248623",
            "8282c9fd37424a05a84725e2abf878a7",
            "6a38c2a48fb1479d830651ff4d4c0ae7",
            "0bbdeaa3b19f40598296ff4d5ccf1e74",
            "b4e9a7ad2af341328a18f2cd2894dfe6",
            "c8481a589ef9458c8bd6cb83e9352ec9",
            "7af9fd85c71847a1abb60fe3aeb8a4ce",
            "6bda2c7a76a740c1820855ab0d1a6d27",
            "f9814970fd654f698009b90ae316fbb8",
            "7ef73aefdf394f7686c6f89cbeed8629",
            "bc288460a8074588a0e986b152a3249c",
            "8afaca580d4d485aa45fe43fe49960cf",
            "6d60d2bc0ce14b00a4b46e5351505a99",
            "e4d84d8f132c444e854922e66e35c7b0",
            "d20bd65f24b241bcabc0e7ebe18554d1",
            "4f356750cd9c497296ac26860c89f74f",
            "81fa0101b7ee4235a7a2d733835c0ff1",
            "e6febb1102434988a2aed3cbecc217a1",
            "aa2c347c796a423eb0add6a6fff533ce",
            "c6e8fad762a24a65844dfd16afc756c5",
            "bb82065867c6474aa1e991aa06601248",
            "8a0dced1f1e14f20aa9234737b1f0292",
            "1ef8bdaa51e24daf83e0aeddfec9dddb",
            "fd828f950be44f1ebebdf91ffbd32af6",
            "aa5c215e1c5b41e795b3a1d730540d54",
            "82805cc6ac6b499aad51ac738c865c48",
            "e8aa1b2b2e7341d383b24825d497d623",
            "0d6d544ed2274115a3117276622c0c31",
            "d223cbccd6e342159b55280b7261baaf",
            "e8ca2f6eaf624a9594b9c0222ad64bbc",
            "28dc1a3b996f49ce99a43ba7ea1bef0d",
            "61836ee9f7e54c6095f4d4f1a711f108",
            "6213ae0a5754471b97be7ca3bfc4bc2c",
            "0f6221cb4a33421db2112b48ca80ee81",
            "cd174bc0905e4431b7e355605fc1929e",
            "f10441d25c4c480e8eb61fba1696672e",
            "db426c13be804bbbb1ac6e58a0e2aa9c",
            "e223c79986834125ac52c6ebf2877688",
            "c4762661c1d54e88be80c5cf47065a10",
            "6dd37035888346338f3f506f5dc05bad",
            "9f5470f3874f4239a902ea0feee05a1a",
            "21c6a659253a4e8cac66035c45123285",
            "88216c2df9174004a2563134574469ee",
            "295fd1f6e3294d8098d6220c051e6f22",
            "2ee247a4585742e9a0774cf34eeabcbe",
            "5fd595dc6c354516bd824a6ef6890223",
            "ead590be321a47caa9bef1851d981948",
            "807199c5604a48b2bb602b7ec8679efb",
            "c3704506e5914de58dde941767a58dbc",
            "c376075f6a6f41e08cb334bb17e65aa0",
            "465fcc4654014c43a63edb58d00b212b",
            "ce5fab931b674271ba6a3011737ab667",
            "6a82b6acd0b34df19101b0bebd5a07e6",
            "975e7fe97a494dfcb5d420f878e6d75c",
            "ee81089d9d274a98805f254efb397f81",
            "7cea138fd2f04b42b7ec67e503e50252",
            "b71815dc35bf49f3a2e44da6d781097c",
            "835841490a194cb8a8e4005de641f1b5",
            "9b6bec8b6b7049f4b07beaca37a6b9ee",
            "a853735973604ce6af12b6665241eea0",
            "8e92c26c989d4082a3c8f0c0db160632",
            "2c525814f2ef42ec984ec835b55a6646",
            "990346f345b346cbaa728a38a219a5cf",
            "7b95c9b9c21e4030980f9c817a9ee883",
            "073cfed82b934b559ba9fc8bcae57ee4",
            "b0627e4e7fe34f218abc9a0aa14c022f",
            "cce558ef0430433f9bb7ea4568adb316",
            "4dfe2fef684c4a098ec767efca9d48eb",
            "ce75883d900e4f7bbda730290c08898c",
            "0cebe12e6f99453b9bc61d5c9a1f144f",
            "eb0686a6d07e4d6d864ad7b1defea582",
            "de776b70ec1d4b9899ab1ea8ca2cb01d",
            "b4581b4f55de4e33bb3d67dbd3aa5826",
            "ca90efc14b2d4ad5af56661882a015e9",
            "967bb44a7373442487b09e04972e6204",
            "19cce7a1a0f94b4aaf40632e65d59c64",
            "08d3eadea6804bba8447ff09cc86887b",
            "4aea8c9d69a04fe780ab79f1d466b662",
            "2a570924c34d42bea4153f233ee60e09",
            "16d3c74426154373977e8ec04e0544db",
            "a2297eca392f467caa486ddda8dc6ee1",
            "ec0ed16ba20b42f0a8f799d135c7f01c",
            "cba9395f483f4a68ba037a7eb61f1ce5",
            "b8229e8ca52941028f3441fb5022cf69"
          ]
        }
      },
      "id": "jdti5_AKyoLZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing main MDL pipeline...\n",
            "--- MWP Enhanced: Batched LLM, Validated SFs, Structured Motifs, Token-L(H), BDM L(D|H) ---\n",
            "Timestamp: Sun Jun  1 06:25:13 2025\n",
            "\n",
            "--- Configuration Summary ---\n",
            "LLM Model: google/gemma-2b-it, Quantization: True\n",
            "LLM Batch Size (Responses): 5, Retries: 2\n",
            "Max Text Chars per LLM Prompt Chunk: 7000\n",
            "Max New Tokens for LLM Motif Extraction: 800\n",
            "Max Motifs to Request per Chunk: 3\n",
            "L(H) Costs: Label=0.5, DescBase=0.5, DescToken=0.1, SFListBase=0.25, SFTokenInLH=0.1\n",
            "SF Validation (Chunk N-grams) Min Freq: 2\n",
            "SF Filtering (Global Corpus) Min Freq: 2\n",
            "BDM Hash Prefix Length: 2000, BDM Matrix: (8, 8)\n",
            "Debug Log File: ./llm_motif_debug_log_mwp_enhanced.txt\n",
            "--- End Configuration Summary ---\n",
            "\n",
            "--- Initializing LLM Pipeline (model: google/gemma-2b-it, quantization: True, return_full_text: False) ---\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-2b-it...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e30a632dfc043b487fe015d2f4425a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23633088a11e4f7d8e06f0b424a55b5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58391969e96542f79f28ced1640ecd53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3f5b78c50144db397adaf6058d3053b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BitsAndBytesConfig created for google/gemma-2b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-2b-it (Quantization active: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bda2c7a76a740c1820855ab0d1a6d27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa2c347c796a423eb0add6a6fff533ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8ca2f6eaf624a9594b9c0222ad64bbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f5470f3874f4239a902ea0feee05a1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce5fab931b674271ba6a3011737ab667"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "990346f345b346cbaa728a38a219a5cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca90efc14b2d4ad5af56661882a015e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-2b-it initialized successfully.\n",
            "Initializing BDM instance...\n",
            "BDM instance initialized successfully (ndim=2, default CTM-based).\n",
            "Loading Phase 2 output from: ./Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MDL analysis will run for these QIDs: ['Q4']\n",
            "\n",
            "--- Analyzing Data for QID: Q4 ---\n",
            "  Combined corpus for QID Q4 has 129501 chars from 209 individual responses.\n",
            "  Baseline MDL for QID Q4 (L(D_orig)): 121.3693\n",
            "  QID Q4: Processing 209 responses in 42 preprocessed chunks (batch size: 5 responses).\n",
            "    Analyzing chunk 1/42 for QID Q4 (processed chunk len: 3158 chars)...\n",
            "    [WARN] Invalid motif object structure in LLM JSON for QID Q4, Chunk 1, Item 1. Skipping item: {'label': 'Exceptions to Individual Rights', 'description': 'The text discusses the need for exceptions to certain individual rights in the employment context due to competing public interests.', 'sur...\n",
            "      Motif JSON parsing/validation attempt 1 yielded no structured motifs for chunk 1 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [WARN] Invalid motif object structure in LLM JSON for QID Q4, Chunk 1, Item 1. Skipping item: {'label': 'Exceptions to Individual Rights', 'description': 'The text discusses the need for exceptions to certain individual rights in the employment context due to competing public interests.', 'sur...\n",
            "      Motif JSON parsing/validation attempt 2 yielded no structured motifs for chunk 1 (QID Q4). Retrying if possible...\n",
            "      No valid & validated motifs extracted from chunk 1 (QID Q4) after 2 attempts.\n",
            "      No valid & chunk-validated motifs extracted from chunk 1 (QID Q4).\n",
            "    Analyzing chunk 2/42 for QID Q4 (processed chunk len: 3262 chars)...\n",
            "    [WARN] Invalid motif object structure in LLM JSON for QID Q4, Chunk 2, Item 1. Skipping item: {'label': 'Exceptions to Data Rights in Employment Context', 'description': 'The excerpt argues for stronger data rights for Australians, highlighting their current limitations compared to internation...\n",
            "    [WARN] Invalid motif object structure in LLM JSON for QID Q4, Chunk 2, Item 2. Skipping item: {'label': 'Exceptions for Employee Rights', 'description': 'The excerpt emphasizes the need for careful consideration when granting specific exceptions to individual rights in the employment context, ...\n",
            "    [WARN] Invalid motif object structure in LLM JSON for QID Q4, Chunk 2, Item 3. Skipping item: {}...\n",
            "      Motif JSON parsing/validation attempt 1 yielded no structured motifs for chunk 2 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [WARN] Invalid motif object structure in LLM JSON for QID Q4, Chunk 2, Item 1. Skipping item: {'label': 'Exceptions to Data Rights in Employment Context', 'description': 'The excerpt argues for stronger data rights for Australians, highlighting their current limitations compared to internation...\n",
            "    [WARN] Invalid motif object structure in LLM JSON for QID Q4, Chunk 2, Item 2. Skipping item: {'label': 'Exceptions for Employee Rights', 'description': 'The excerpt emphasizes the need for careful consideration when granting specific exceptions to individual rights in the employment context, ...\n",
            "    [WARN] Invalid motif object structure in LLM JSON for QID Q4, Chunk 2, Item 3. Skipping item: {}...\n",
            "      Motif JSON parsing/validation attempt 2 yielded no structured motifs for chunk 2 (QID Q4). Retrying if possible...\n",
            "      No valid & validated motifs extracted from chunk 2 (QID Q4) after 2 attempts.\n",
            "      No valid & chunk-validated motifs extracted from chunk 2 (QID Q4).\n",
            "    Analyzing chunk 3/42 for QID Q4 (processed chunk len: 3219 chars)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a809f3fa8e59>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;31m# Ensure this script is run after cells 1-5 have defined their constants and functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Executing main MDL pipeline...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Main MDL pipeline execution finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a809f3fa8e59>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# enhanced_motif_extraction_per_chunk is from motif_processing.py / Cell 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             motifs_from_this_chunk_list = enhanced_motif_extraction_per_chunk(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mtext_chunk_being_analyzed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mhf_pipeline_instance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-597ce4c4cd1a>\u001b[0m in \u001b[0;36menhanced_motif_extraction_per_chunk\u001b[0;34m(text_chunk_to_analyze, hf_pipeline_instance, hf_tokenizer_instance, qid_for_log, chunk_idx_for_log)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLM_RETRY_ATTEMPTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# print(f\"      LLM Call attempt {attempt + 1} for chunk {chunk_idx_for_log} (QID {qid_for_log})...\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         raw_llm_response_text = call_local_llm_for_raw_response( # From llm_interaction cell\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mprompt_str_for_llm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mhf_pipeline_instance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-7b6f95801df0>\u001b[0m in \u001b[0;36mcall_local_llm_for_raw_response\u001b[0;34m(prompt_content_for_user_turn, hf_pipeline_instance, hf_tokenizer_instance, qid_for_log, chunk_idx_for_log)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_pipeline_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_formatted_for_llm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgeneration_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# print(f\"    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{outputs}\\n>>>>>\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m             )\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1336\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2598\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3558\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3560\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3562\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    684\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    450\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# Fully Connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     \u001b[0;31m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SEPARATOR"
      ],
      "metadata": {
        "id": "TGYH_8Gf552w"
      },
      "id": "TGYH_8Gf552w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "NMBsJUhh53Ao"
      },
      "id": "NMBsJUhh53Ao"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Revised MWP\n",
        "# --- Imports ---\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict # For type hinting\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # !!! EXAMPLE - UPDATE THIS PATH !!!\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"] # Process only Q4 for this example\n",
        "\n",
        "# --- BDM and LLM Model Configuration ---\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it' # Or your verified 'gemma-3n-e4b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "LLM_BATCH_SIZE = 5\n",
        "LLM_RETRY_ATTEMPTS = 2\n",
        "MAX_TEXT_PER_LLM_PROMPT_CHUNK = 7000 # Max characters for the text_block to be analyzed within a single LLM prompt\n",
        "LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION = 700 # Max tokens for LLM to generate for motif extraction\n",
        "\n",
        "# --- Token-Based L(H) Configuration ---\n",
        "MOTIF_SYMBOLIC_LABEL_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TOKEN_COST = 0.1\n",
        "MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.25\n",
        "MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.1\n",
        "\n",
        "# --- Logging File ---\n",
        "LLM_DEBUG_LOG_FILE = \"llm_motif_debug_log_mwp.txt\" # New log file for this version\n",
        "\n",
        "# --- Helper Function Definitions (Tokenization, L(H), BDM, Compression) ---\n",
        "\n",
        "def tokenize_phrase(phrase_text: str) -> List[str]:\n",
        "    if not isinstance(phrase_text, str): return []\n",
        "    phrase_text = phrase_text.lower()\n",
        "    tokens = phrase_text.split()\n",
        "    return [t for t in tokens if t]\n",
        "\n",
        "def calculate_L_H_token_based_structured(structured_motifs_list: List[Dict]) -> float:\n",
        "    if not structured_motifs_list: return 0.0\n",
        "    total_lh_cost = 0.0\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        current_motif_lh = 0\n",
        "        label_str = motif_obj.get('label', \"\")\n",
        "        if label_str and isinstance(label_str, str) and label_str.strip():\n",
        "            current_motif_lh += MOTIF_SYMBOLIC_LABEL_COST\n",
        "\n",
        "        description_str = motif_obj.get('description', \"\")\n",
        "        if description_str and isinstance(description_str, str) and description_str.strip():\n",
        "            current_motif_lh += MOTIF_DESCRIPTION_TEXT_BASE_COST\n",
        "            current_motif_lh += len(tokenize_phrase(description_str)) * MOTIF_DESCRIPTION_TOKEN_COST\n",
        "\n",
        "        surface_forms_list = motif_obj.get('surface_forms', [])\n",
        "        if surface_forms_list and isinstance(surface_forms_list, list):\n",
        "            valid_sfs_for_lh = [sf for sf in surface_forms_list if isinstance(sf, str) and sf.strip()]\n",
        "            if valid_sfs_for_lh:\n",
        "                current_motif_lh += MOTIF_SURFACE_FORMS_LIST_BASE_COST\n",
        "                for sf_str in valid_sfs_for_lh:\n",
        "                    current_motif_lh += len(tokenize_phrase(sf_str)) * MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH\n",
        "        total_lh_cost += current_motif_lh\n",
        "    return total_lh_cost\n",
        "\n",
        "def llm_compress_text_structured(text_to_compress: str, structured_motifs_list: List[Dict]) -> str:\n",
        "    if not isinstance(text_to_compress, str): return \"\"\n",
        "    if not structured_motifs_list: return text_to_compress.lower()\n",
        "\n",
        "    compressed_text = text_to_compress.lower()\n",
        "\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "\n",
        "        label = motif_obj.get('label', None)\n",
        "        surface_forms = motif_obj.get('surface_forms', [])\n",
        "\n",
        "        if not label or not surface_forms or not isinstance(surface_forms, list):\n",
        "            continue\n",
        "\n",
        "        placeholder = label\n",
        "\n",
        "        sorted_sfs_for_this_motif = sorted(\n",
        "            [sf for sf in surface_forms if isinstance(sf, str) and sf.strip()],\n",
        "            key=len,\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        for sf_str in sorted_sfs_for_this_motif:\n",
        "            try:\n",
        "                compressed_text = re.sub(re.escape(sf_str.lower()), placeholder, compressed_text, flags=re.IGNORECASE)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for SF '{sf_str}' of motif '{label}': {re_e}. Skipping.\")\n",
        "                continue\n",
        "    return compressed_text\n",
        "\n",
        "def text_to_binary_matrix(text_input: str, size=MATRIX_SIZE_GLOBAL) -> np.ndarray:\n",
        "    if not text_input or not isinstance(text_input, str) or not text_input.strip():\n",
        "        return np.zeros(size, dtype=int)\n",
        "    hash_obj = hashlib.sha256(text_input.encode('utf-8', 'ignore'))\n",
        "    hash_digest = hash_obj.hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string = bin(int(hash_digest, 16))[2:].zfill(256)\n",
        "    binary_string_padded = binary_string.ljust(required_bits, '0')\n",
        "    bits = [int(b) for b in binary_string_padded[:required_bits]]\n",
        "    return np.array(bits).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input: str, bdm_instance: BDM, matrix_s=MATRIX_SIZE_GLOBAL) -> float:\n",
        "    if not text_input or not isinstance(text_input, str) or not text_input.strip() : return 0.0\n",
        "    MAX_TEXT_FOR_BDM_HASH = 2000\n",
        "    text_for_hash = text_input if len(text_input) <= MAX_TEXT_FOR_BDM_HASH else text_input[:MAX_TEXT_FOR_BDM_HASH]\n",
        "    matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        bdm_value = bdm_instance.bdm(matrix)\n",
        "        return bdm_value\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0\n",
        "\n",
        "def compute_mdl_cost_for_text_block(full_qid_corpus_str: str,\n",
        "                                    final_consolidated_motifs: List[Dict],\n",
        "                                    bdm_instance: BDM,\n",
        "                                    matrix_s=MATRIX_SIZE_GLOBAL) -> tuple[float, float, float]:\n",
        "    if not isinstance(full_qid_corpus_str, str) : full_qid_corpus_str = \"\"\n",
        "    l_h = calculate_L_H_token_based_structured(final_consolidated_motifs)\n",
        "    compressed_text_block = llm_compress_text_structured(full_qid_corpus_str, final_consolidated_motifs)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "    if l_d_h < 0: return l_h, -1.0, -1.0\n",
        "    return l_h, l_d_h, l_h + l_d_h\n",
        "\n",
        "# --- LLM Motif Extraction ---\n",
        "\n",
        "def build_llm_prompt_for_motifs(text_block_for_prompt: str) -> str:\n",
        "    if len(text_block_for_prompt) > MAX_TEXT_PER_LLM_PROMPT_CHUNK:\n",
        "        # print(f\"    Note: Text block for LLM prompt truncated from {len(text_block_for_prompt)} to {MAX_TEXT_PER_LLM_PROMPT_CHUNK} chars.\")\n",
        "        text_block_for_prompt = text_block_for_prompt[:MAX_TEXT_PER_LLM_PROMPT_CHUNK]\n",
        "\n",
        "    prompt = f\"\"\"You will receive a set of comments from different people answering the same question.\n",
        "\n",
        "Your task is to identify up to 5 key recurring themes.\n",
        "\n",
        "For each theme, provide:\n",
        "- A short label like [DATA_PRIVACY]\n",
        "- A 1-sentence description of the theme\n",
        "- 2–3 short phrases that often appear in the text (surface forms)\n",
        "\n",
        "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
        "Example of one object in the list:\n",
        "{{\n",
        "  \"label\": \"[EXAMPLE_LABEL]\",\n",
        "  \"description\": \"A concise description of the example theme.\",\n",
        "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
        "}}\n",
        "If no clear motifs are found, output an empty JSON list: `[]`.\n",
        "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
        "\n",
        "Set of comments to analyze:\n",
        "\\\"\\\"\\\"\n",
        "{text_block_for_prompt}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def call_local_llm_for_motifs(prompt_str: str, hf_pipeline, hf_tokenizer, qid_for_log: str, chunk_idx_for_log: int) -> str:\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": prompt_str}]\n",
        "    prompt_formatted_for_llm = hf_tokenizer.apply_chat_template(\n",
        "        messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION,\n",
        "        \"do_sample\": False,\n",
        "        \"pad_token_id\": hf_tokenizer.pad_token_id\n",
        "    }\n",
        "    # print(f\"    DEBUG (call_local_llm): Sending prompt to LLM for QID {qid_for_log}, Chunk {chunk_idx_for_log}. Prompt length for pipeline: {len(prompt_formatted_for_llm)}\")\n",
        "\n",
        "    outputs = hf_pipeline(prompt_formatted_for_llm, **generation_args)\n",
        "\n",
        "    # print(f\"    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{outputs}\\n>>>>>\")\n",
        "\n",
        "    if outputs and isinstance(outputs, list) and len(outputs) > 0 and \\\n",
        "       outputs[0] and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
        "        # Since pipeline initialized with return_full_text=False, generated_text is only new tokens\n",
        "        assistant_response_text = outputs[0]['generated_text'].strip()\n",
        "        # print(f\"    DEBUG (call_local_llm): 'assistant_response_text' (return_full_text=False) for QID {qid_for_log}, Chunk {chunk_idx_for_log} (len {len(assistant_response_text)}):\\n<<<<<\\n{assistant_response_text[:1000]}...\\n>>>>>\")\n",
        "        return assistant_response_text\n",
        "    else:\n",
        "        print(f\"    WARN (call_local_llm): LLM pipeline returned unexpected or empty structure for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_motifs_from_llm_response(llm_response_str: str, qid_for_log:str, chunk_idx_for_log:int, prompt_sent:str) -> List[Dict]:\n",
        "    # print(f\"    DEBUG (extract_motifs): Raw LLM response for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{llm_response_str}\\n>>>>>\")\n",
        "    json_str_candidate = llm_response_str.strip()\n",
        "\n",
        "    if json_str_candidate.startswith(\"```json\"):\n",
        "        json_str_candidate = json_str_candidate[len(\"```json\"):].strip()\n",
        "    if json_str_candidate.startswith(\"```\"):\n",
        "        json_str_candidate = json_str_candidate[len(\"```\"):].strip()\n",
        "    if json_str_candidate.endswith(\"```\"):\n",
        "        json_str_candidate = json_str_candidate[:-len(\"```\")].strip()\n",
        "\n",
        "    # print(f\"    DEBUG (extract_motifs): Final json_str candidate for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{json_str_candidate}\\n>>>>>\")\n",
        "\n",
        "    if not json_str_candidate or json_str_candidate.lower() == \"[]\" or \"no_themes_found\" in json_str_candidate.lower() or \"no clear motifs\" in json_str_candidate.lower():\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        parsed_data = json.loads(json_str_candidate)\n",
        "        if isinstance(parsed_data, dict): # Handle if LLM returns a single object instead of a list\n",
        "            # print(f\"    DEBUG (extract_motifs): LLM returned a single JSON object, wrapping in list for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "            parsed_data = [parsed_data]\n",
        "\n",
        "        if not isinstance(parsed_data, list):\n",
        "            raise ValueError(\"Parsed JSON is not a list (or a single object that could be wrapped).\")\n",
        "\n",
        "        valid_motifs_from_json = []\n",
        "        for item in parsed_data:\n",
        "            if isinstance(item, dict) and \\\n",
        "               'label' in item and isinstance(item['label'], str) and \\\n",
        "               'description' in item and isinstance(item['description'], str) and \\\n",
        "               item['label'].strip().startswith('[') and item['label'].strip().endswith(']'): # Check for brackets after stripping\n",
        "\n",
        "                sf = item.get('surface_forms', [])\n",
        "                if not isinstance(sf, list) or not all(isinstance(s, str) for s in sf):\n",
        "                    sf = []\n",
        "\n",
        "                valid_motifs_from_json.append({\n",
        "                    \"label\": item['label'].strip(),\n",
        "                    \"description\": item['description'].strip(),\n",
        "                    \"surface_forms\": [s.strip() for s in sf if s.strip()]\n",
        "                })\n",
        "        return valid_motifs_from_json\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        print(f\"    [WARN] Motif JSON parsing/validation failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} ---\\n\")\n",
        "            f.write(f\"PROMPT SENT (first 500 chars):\\n{prompt_sent[:500]}...\\n\")\n",
        "            f.write(f\"RAW LLM RESPONSE (Error: {type(e).__name__}):\\n{llm_response_str}\\n\")\n",
        "            f.write(f\"EXTRACTED JSON STRING CANDIDATE (Error: {type(e).__name__}):\\n{json_str_candidate}\\n\")\n",
        "        return []\n",
        "\n",
        "def get_motifs_for_text_chunks(\n",
        "    list_of_response_strings: List[str],\n",
        "    batch_size: int,\n",
        "    hf_pipeline,\n",
        "    hf_tokenizer,\n",
        "    qid_for_log: str\n",
        "    ) -> List[Dict]:\n",
        "    all_extracted_motifs_from_all_chunks = []\n",
        "    batched_text_blocks = []\n",
        "    for i in range(0, len(list_of_response_strings), batch_size):\n",
        "        current_batch_responses = list_of_response_strings[i:i + batch_size]\n",
        "        text_block_for_chunk = \"\\n\\n<RSP_SEP>\\n\\n\".join(current_batch_responses)\n",
        "        batched_text_blocks.append(text_block_for_chunk)\n",
        "\n",
        "    print(f\"  QID {qid_for_log}: Processing {len(list_of_response_strings)} responses in {len(batched_text_blocks)} chunks (batch size: {batch_size} responses).\")\n",
        "\n",
        "    for chunk_idx, text_chunk_to_analyze in enumerate(batched_text_blocks):\n",
        "        print(f\"    Analyzing chunk {chunk_idx + 1}/{len(batched_text_blocks)} for QID {qid_for_log} (len: {len(text_chunk_to_analyze)} chars)...\")\n",
        "        if len(text_chunk_to_analyze.strip()) < 50:\n",
        "            print(f\"      Chunk {chunk_idx+1} for QID {qid_for_log} too short, skipping.\")\n",
        "            continue\n",
        "\n",
        "        prompt_for_llm = build_llm_prompt_for_motifs(text_chunk_to_analyze)\n",
        "        motifs_from_this_chunk = []\n",
        "        for attempt in range(LLM_RETRY_ATTEMPTS):\n",
        "            try:\n",
        "                raw_llm_response_text = call_local_llm_for_motifs(prompt_for_llm, hf_pipeline, hf_tokenizer, qid_for_log, chunk_idx + 1)\n",
        "                if not raw_llm_response_text:\n",
        "                    print(f\"      LLM call attempt {attempt + 1} for chunk {chunk_idx+1} (QID {qid_for_log}) returned empty. Retrying if possible...\")\n",
        "                    if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "                    continue\n",
        "\n",
        "                motifs_attempt_from_chunk = extract_motifs_from_llm_response(raw_llm_response_text, qid_for_log, chunk_idx+1, prompt_for_llm)\n",
        "                if motifs_attempt_from_chunk:\n",
        "                    motifs_from_this_chunk = motifs_attempt_from_chunk\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"      Motif extraction/parsing attempt {attempt + 1} yielded no valid motifs for chunk {chunk_idx+1} (QID {qid_for_log}). Retrying if possible...\")\n",
        "                    if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "            except Exception as e_call:\n",
        "                 print(f\"      Critical error during LLM call/parsing attempt {attempt + 1} for chunk {chunk_idx+1} (QID {qid_for_log}): {e_call}\")\n",
        "                 if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "                 with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f: # Log critical errors too\n",
        "                    f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx+1} --- ATTEMPT: {attempt+1} ---\\n\")\n",
        "                    f.write(f\"PROMPT SENT (first 500 chars):\\n{prompt_for_llm[:500]}...\\n\")\n",
        "                    f.write(f\"CRITICAL LLM CALL/PARSING ERROR:\\n{e_call}\\n\")\n",
        "        if motifs_from_this_chunk:\n",
        "            print(f\"      Extracted {len(motifs_from_this_chunk)} motif objects from chunk {chunk_idx+1} (QID {qid_for_log}).\")\n",
        "            all_extracted_motifs_from_all_chunks.extend(motifs_from_this_chunk)\n",
        "        else:\n",
        "            print(f\"      No valid motifs extracted from chunk {chunk_idx+1} (QID {qid_for_log}) after {LLM_RETRY_ATTEMPTS} attempts.\")\n",
        "    return all_extracted_motifs_from_all_chunks\n",
        "\n",
        "\n",
        "\n",
        "# (Keep all your existing imports, constants, and helper functions as they are)\n",
        "# ... (calculate_L_H_token_based_structured, llm_compress_text_structured, etc.) ...\n",
        "# ... (get_motifs_for_text_chunks, build_llm_prompt_for_motifs, etc.) ...\n",
        "\n",
        "# --- NEW CONSTANT for Surface Form Filtering ---\n",
        "MIN_SF_FREQUENCY_IN_FULL_CORPUS = 2 # Surface form must appear at least this many times in the full QID corpus\n",
        "\n",
        "def count_sf_occurrences(corpus_text: str, surface_form: str) -> int:\n",
        "    \"\"\"Counts case-insensitive occurrences of a surface form in the corpus text.\"\"\"\n",
        "    if not corpus_text or not surface_form:\n",
        "        return 0\n",
        "    return len(re.findall(re.escape(surface_form.lower()), corpus_text.lower(), flags=re.IGNORECASE))\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "def main():\n",
        "    print(\"--- MWP: Batched LLM Motif Extraction, Refined SFs, Structured Motifs, Token-L(H), BDM L(D|H) ---\") # Updated title\n",
        "    print(f\"Using L(H) Cost Params: Label={MOTIF_SYMBOLIC_LABEL_COST}, DescBase={MOTIF_DESCRIPTION_TEXT_BASE_COST}, DescToken={MOTIF_DESCRIPTION_TOKEN_COST}, SFListBase={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, SFTokenInLH={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "    print(f\"LLM: {LOCAL_LLM_MODEL_ID}, Batch Size: {LLM_BATCH_SIZE} responses, Retries: {LLM_RETRY_ATTEMPTS}, Max Text per Prompt: {MAX_TEXT_PER_LLM_PROMPT_CHUNK}, Max New Tokens: {LLM_MAX_NEW_TOKENS_MOTIF_EXTRACTION}\")\n",
        "    print(f\"Min SF Frequency for Refinement: {MIN_SF_FREQUENCY_IN_FULL_CORPUS}\")\n",
        "\n",
        "    with open(LLM_DEBUG_LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"LLM Motif Debug Log - {time.asctime()}\\nModel: {LOCAL_LLM_MODEL_ID}\\nPipeline return_full_text=False\\n\")\n",
        "\n",
        "    # ... (LLM Pipeline Initialization - same as your last complete cell) ...\n",
        "    local_llm_pipeline_instance = None\n",
        "    local_llm_tokenizer_instance = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {LOCAL_LLM_MODEL_ID}...\")\n",
        "        local_llm_tokenizer_instance = AutoTokenizer.from_pretrained(LOCAL_LLM_MODEL_ID)\n",
        "        if local_llm_tokenizer_instance.pad_token is None:\n",
        "            print(\"Tokenizer setting pad_token = eos_token.\")\n",
        "            local_llm_tokenizer_instance.pad_token = local_llm_tokenizer_instance.eos_token\n",
        "\n",
        "        bnb_config = None\n",
        "        quant_active = False\n",
        "        if USE_QUANTIZATION_FOR_LOCAL_LLM and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n",
        "                quant_active = True\n",
        "                print(f\"BNB config created for {LOCAL_LLM_MODEL_ID}, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb:\n",
        "                print(f\"WARN: Failed to create BitsAndBytesConfig: {e_bnb}. Quantization disabled.\")\n",
        "\n",
        "        print(f\"Loading local model {LOCAL_LLM_MODEL_ID} (Quantization: {quant_active})...\")\n",
        "        model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "        if quant_active: model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        else:\n",
        "            if device.type == 'cuda': model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "        local_llm_model_instance = AutoModelForCausalLM.from_pretrained(LOCAL_LLM_MODEL_ID, **model_kwargs)\n",
        "        if local_llm_tokenizer_instance.pad_token_id == local_llm_tokenizer_instance.eos_token_id:\n",
        "             local_llm_model_instance.config.pad_token_id = local_llm_model_instance.config.eos_token_id\n",
        "\n",
        "        local_llm_pipeline_instance = pipeline(\n",
        "            \"text-generation\", model=local_llm_model_instance, tokenizer=local_llm_tokenizer_instance, return_full_text=False\n",
        "        )\n",
        "        print(f\"Local LLM pipeline for {LOCAL_LLM_MODEL_ID} initialized successfully (return_full_text=False).\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to initialize local LLM pipeline: {e}\")\n",
        "        return\n",
        "\n",
        "    # ... (BDM Initialization - same as your last complete cell, ensure it uses BDM(ndim=2)) ...\n",
        "    try:\n",
        "        bdm_instance_main = BDM(ndim=2)\n",
        "        print(\"BDM instance initialized successfully (ndim=2, default CTM-based).\")\n",
        "    except Exception as e_bdm_init:\n",
        "        print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\")\n",
        "        if \"CTM data files\" in str(e_bdm_init):\n",
        "            print(\"  BDM Error Hint: This might be related to missing CTM data files for PyBDM.\")\n",
        "        return\n",
        "\n",
        "    # ... (Loading Phase 2 Data - same as your last complete cell) ...\n",
        "    if not os.path.exists(P2_COLLATED_FILE):\n",
        "        print(f\"ERROR: Phase 2 output file not found: {P2_COLLATED_FILE}\")\n",
        "        return\n",
        "    print(f\"Loading Phase 2 output from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f:\n",
        "            phase2_data_content = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or parsing {P2_COLLATED_FILE}: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- ADD DEBUG PRINT HERE ---\n",
        "    if phase2_data_content:\n",
        "        print(f\"DEBUG: Type of phase2_data_content: {type(phase2_data_content)}\")\n",
        "        if isinstance(phase2_data_content, dict):\n",
        "            print(f\"DEBUG: Keys in phase2_data_content: {list(phase2_data_content.keys())}\")\n",
        "            aggregated_content_map_debug = phase2_data_content.get(\"aggregated_pdf_content_by_qid\")\n",
        "            if aggregated_content_map_debug:\n",
        "                print(f\"DEBUG: 'aggregated_pdf_content_by_qid' found. Type: {type(aggregated_content_map_debug)}\")\n",
        "                print(f\"DEBUG: QIDs available in 'aggregated_pdf_content_by_qid': {list(aggregated_content_map_debug.keys())}\")\n",
        "                if \"Q4\" in aggregated_content_map_debug:\n",
        "                    print(f\"DEBUG: Number of raw response items for Q4 in loaded file: {len(aggregated_content_map_debug['Q4'])}\")\n",
        "                else:\n",
        "                    print(\"DEBUG: Q4 not found in 'aggregated_pdf_content_by_qid' at this stage.\")\n",
        "            else:\n",
        "                print(\"DEBUG: 'aggregated_pdf_content_by_qid' key NOT found in loaded data.\")\n",
        "    else:\n",
        "        print(\"DEBUG: phase2_data_content is None after attempting to load.\")\n",
        "    # --- END DEBUG PRINT ---\n",
        "\n",
        "\n",
        "    all_qid_mdl_results = []\n",
        "    qids_to_process_final = [] # Renamed from qids_to_target\n",
        "    aggregated_content_by_qid_map = {}\n",
        "\n",
        "    if phase2_data_content:\n",
        "        aggregated_content_by_qid_map = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "    if not aggregated_content_by_qid_map:\n",
        "        print(f\"No 'aggregated_pdf_content_by_qid' key found or data is empty in {P2_COLLATED_FILE}.\")\n",
        "        return\n",
        "\n",
        "    if P3_QIDS_TO_PROCESS_THEMATICALLY and isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and P3_QIDS_TO_PROCESS_THEMATICALLY:\n",
        "        qids_to_process_final = [qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY if qid in aggregated_content_by_qid_map]\n",
        "        if not qids_to_process_final:\n",
        "            print(f\"Warning: None of QIDs {P3_QIDS_TO_PROCESS_THEMATICALLY} found in loaded data.\")\n",
        "            return\n",
        "    else:\n",
        "        qids_to_process_limit_fallback = 1\n",
        "        qids_to_process_final = list(aggregated_content_by_qid_map.keys())[:qids_to_process_limit_fallback]\n",
        "        if not qids_to_process_final:\n",
        "            print(\"No QIDs available to process based on fallback.\")\n",
        "            return\n",
        "    print(f\"\\nMDL analysis will run for these QIDs: {qids_to_process_final}\\n\")\n",
        "\n",
        "\n",
        "    for qid_str in qids_to_process_final:\n",
        "        list_of_response_item_dicts = aggregated_content_by_qid_map.get(qid_str, [])\n",
        "        actual_response_text_strings = [\n",
        "            item.get(\"text\", \"\") for item in list_of_response_item_dicts\n",
        "            if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()\n",
        "        ]\n",
        "        if not actual_response_text_strings:\n",
        "            print(f\"No valid text strings extracted from responses for QID {qid_str}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"--- Analyzing Aggregated Text for QID: {qid_str} ---\")\n",
        "        full_corpus_for_qid_str = \"\\n\\n<RSP_SEP>\\n\\n\".join(actual_response_text_strings)\n",
        "        if len(full_corpus_for_qid_str.strip()) < 100:\n",
        "            print(f\"  Skipping QID {qid_str}: combined text too short ({len(full_corpus_for_qid_str)} chars).\")\n",
        "            continue\n",
        "\n",
        "        num_responses_for_qid = len(actual_response_text_strings)\n",
        "        print(f\"  Combined corpus for QID {qid_str} has {len(full_corpus_for_qid_str)} chars from {num_responses_for_qid} individual responses.\")\n",
        "\n",
        "        baseline_l_d_original = compute_bdm_for_text(full_corpus_for_qid_str, bdm_instance_main, MATRIX_SIZE_GLOBAL)\n",
        "        if baseline_l_d_original < 0:\n",
        "            print(f\"  Error computing baseline BDM for QID {qid_str}. Skipping this QID.\")\n",
        "            continue\n",
        "        baseline_total_mdl_cost = baseline_l_d_original\n",
        "        print(f\"  Baseline MDL for QID {qid_str} (L(D_orig)): {baseline_total_mdl_cost:.4f}\")\n",
        "\n",
        "        raw_motifs_from_all_chunks = get_motifs_for_text_chunks(\n",
        "            actual_response_text_strings, LLM_BATCH_SIZE,\n",
        "            local_llm_pipeline_instance, local_llm_tokenizer_instance, qid_str\n",
        "        )\n",
        "\n",
        "        qid_result_entry = { # Initialize with baseline values\n",
        "            \"qid\": qid_str, \"corpus_len_for_qid\": len(full_corpus_for_qid_str),\n",
        "            \"num_responses\": num_responses_for_qid,\n",
        "            \"baseline_mdl\": baseline_total_mdl_cost, \"final_motifs\": [],\n",
        "            \"l_h_motifs\": 0.0, \"l_d_h_motifs\": baseline_total_mdl_cost,\n",
        "            \"total_mdl_motifs\": baseline_total_mdl_cost,\n",
        "            \"compression_achieved\": 0.0,\n",
        "            \"num_raw_motifs_extracted\": len(raw_motifs_from_all_chunks),\n",
        "            \"num_consolidated_motifs\": 0,\n",
        "            \"num_refined_motifs\": 0 # For motifs after SF frequency filtering\n",
        "        }\n",
        "\n",
        "        if not raw_motifs_from_all_chunks:\n",
        "            print(f\"  No raw motifs extracted by LLM for QID {qid_str} from any chunk.\")\n",
        "            all_qid_mdl_results.append(qid_result_entry)\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        print(f\"  Extracted {len(raw_motifs_from_all_chunks)} raw motif objects from LLM for QID {qid_str} (across all chunks).\")\n",
        "\n",
        "        # --- Motif Consolidation Step ---\n",
        "        consolidated_motifs_temp_dict = {}\n",
        "        for motif_obj_raw in raw_motifs_from_all_chunks:\n",
        "            label = motif_obj_raw.get(\"label\")\n",
        "            description = motif_obj_raw.get(\"description\") # Ensure your LLM output uses this key\n",
        "            surface_forms = motif_obj_raw.get(\"surface_forms\")\n",
        "            if label and description and surface_forms is not None:\n",
        "                if label not in consolidated_motifs_temp_dict:\n",
        "                    consolidated_motifs_temp_dict[label] = motif_obj_raw\n",
        "                else:\n",
        "                    existing_sfs_set = set(consolidated_motifs_temp_dict[label].get(\"surface_forms\", []))\n",
        "                    new_sfs_set = set(surface_forms)\n",
        "                    consolidated_motifs_temp_dict[label][\"surface_forms\"] = sorted(list(dict.fromkeys(list(existing_sfs_set.union(new_sfs_set))))) # Deduplicate merged SFs\n",
        "\n",
        "        consolidated_motifs_list = list(consolidated_motifs_temp_dict.values())\n",
        "        qid_result_entry[\"num_consolidated_motifs\"] = len(consolidated_motifs_list)\n",
        "        print(f\"  Consolidated into {len(consolidated_motifs_list)} unique motifs (by label) for QID {qid_str}.\")\n",
        "\n",
        "        if not consolidated_motifs_list:\n",
        "            print(f\"  No unique motifs left after consolidation for QID {qid_str}.\")\n",
        "            all_qid_mdl_results.append(qid_result_entry)\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        print(f\"  Consolidated Motifs for QID {qid_str} (BEFORE SF refinement):\")\n",
        "        for idx, mo_con in enumerate(consolidated_motifs_list):\n",
        "            print(f\"    --- Cons. Motif {idx+1} ---\")\n",
        "            print(f\"      Label: {mo_con.get('label', 'N/A')}\")\n",
        "            print(f\"      Description: {mo_con.get('description','N/A')[:70]}...\") # Truncate for brevity\n",
        "            print(f\"      Surface Forms ({len(mo_con.get('surface_forms',[]))}): {mo_con.get('surface_forms',[])[:3]}...\") # Print first 3 SFs\n",
        "\n",
        "        # --- NEW: Refine Consolidated Motifs by Surface Form Frequency ---\n",
        "        print(f\"  Refining {len(consolidated_motifs_list)} consolidated motifs by SF frequency (min freq: {MIN_SF_FREQUENCY_IN_FULL_CORPUS})...\")\n",
        "        refined_motifs_for_mdl = []\n",
        "        for motif_obj_consolidated in consolidated_motifs_list:\n",
        "            original_sfs = motif_obj_consolidated.get(\"surface_forms\", [])\n",
        "            frequent_sfs = []\n",
        "            for sf_str in original_sfs:\n",
        "                if count_sf_occurrences(full_corpus_for_qid_str, sf_str) >= MIN_SF_FREQUENCY_IN_FULL_CORPUS:\n",
        "                    frequent_sfs.append(sf_str)\n",
        "\n",
        "            if frequent_sfs: # Only keep motif if it has at least one frequent SF\n",
        "                refined_motif = motif_obj_consolidated.copy() # Create a copy to modify\n",
        "                refined_motif[\"surface_forms\"] = frequent_sfs\n",
        "                refined_motifs_for_mdl.append(refined_motif)\n",
        "            # else:\n",
        "                # print(f\"    Motif '{motif_obj_consolidated.get('label')}' discarded after SF refinement (no frequent SFs).\")\n",
        "\n",
        "        qid_result_entry[\"num_refined_motifs\"] = len(refined_motifs_for_mdl)\n",
        "        print(f\"  Refined into {len(refined_motifs_for_mdl)} motifs with frequent surface forms for QID {qid_str}.\")\n",
        "\n",
        "        if not refined_motifs_for_mdl:\n",
        "            print(f\"  No motifs left after surface form frequency refinement for QID {qid_str}.\")\n",
        "            all_qid_mdl_results.append(qid_result_entry) # Save with 0 refined motifs\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        print(f\"  Final Refined Motifs for QID {qid_str} (for MDL eval):\")\n",
        "        for idx, mo_final in enumerate(refined_motifs_for_mdl):\n",
        "            print(f\"    --- Refined Motif {idx+1} ---\")\n",
        "            print(f\"      Label: {mo_final.get('label', 'N/A')}\")\n",
        "            print(f\"      Description: {mo_final.get('description','N/A')[:70]}...\")\n",
        "            print(f\"      Surface Forms ({len(mo_final.get('surface_forms',[]))}): {mo_final.get('surface_forms',[])}\")\n",
        "\n",
        "        # Calculate MDL cost using the full original corpus for this QID and the *refined* motifs\n",
        "        l_h_final, l_d_h_final, total_mdl_with_final_motifs = compute_mdl_cost_for_text_block(\n",
        "            full_corpus_for_qid_str,\n",
        "            refined_motifs_for_mdl, # Use refined motifs\n",
        "            bdm_instance_main,\n",
        "            MATRIX_SIZE_GLOBAL\n",
        "        )\n",
        "\n",
        "        qid_result_entry[\"final_motifs\"] = refined_motifs_for_mdl # Store refined motifs\n",
        "        qid_result_entry[\"l_h_motifs\"] = l_h_final\n",
        "\n",
        "        if l_d_h_final < 0:\n",
        "            print(f\"  Error computing MDL cost with final refined motifs for QID {qid_str} (BDM error in L(D|H)).\")\n",
        "            # ... (update qid_result_entry for error) ...\n",
        "            all_qid_mdl_results.append(qid_result_entry)\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        # ... (update qid_result_entry with final MDL costs and compression_final - same as before) ...\n",
        "        qid_result_entry[\"l_d_h_motifs\"] = l_d_h_final\n",
        "        qid_result_entry[\"total_mdl_motifs\"] = total_mdl_with_final_motifs\n",
        "        compression_final = baseline_total_mdl_cost - total_mdl_with_final_motifs\n",
        "        qid_result_entry[\"compression_achieved\"] = compression_final\n",
        "\n",
        "        print(f\"  L(H) (Token-based Structured) for final refined motifs of QID {qid_str}: {l_h_final:.4f}\")\n",
        "        print(f\"  L(D|H) (BDM-based) compressed full corpus complexity for QID {qid_str}: {l_d_h_final:.4f}\")\n",
        "        print(f\"  Total MDL cost with final refined motifs for QID {qid_str}: {total_mdl_with_final_motifs:.4f}\")\n",
        "\n",
        "        result_status_str = \"\"\n",
        "        if compression_final > 0.0001:\n",
        "            result_status_str = f\"SUCCESS: Compression achieved: {compression_final:.4f}\"\n",
        "        else:\n",
        "            result_status_str = f\"NOTE: No significant compression (or cost increased). Diff: {compression_final:.4f}\"\n",
        "        print(f\"  {result_status_str}\")\n",
        "        all_qid_mdl_results.append(qid_result_entry)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # --- Summary Printing and Saving Results ---\n",
        "    # (This part remains largely the same)\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary (Batched LLM, Refined SFs, Structured Motifs, Token-L(H)) ---\") # Updated title\n",
        "    # ... (rest of summary and saving logic) ...\n",
        "    if not all_qid_mdl_results:\n",
        "        print(\"No QIDs were processed or no valid results generated.\")\n",
        "    else:\n",
        "        # ... (same summary logic as before, ensure keys match qid_result_entry) ...\n",
        "        output_filename_qids_final = \"mdl_analysis_per_qid_batched_llm_refinedSF_v1.json\" # New filename\n",
        "        # ... (save to file) ...\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "xSFwlLs9fUS1",
        "outputId": "cc51f3bf-200e-4faf-b836-3d2223d7ed26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "de092b7514b24d8c88e0c1d39d9f37f3",
            "6a3d80a9d7bb47b9a90e37c14e7ed378",
            "296c113db2c649d7ac0d75b52b8ebc15",
            "4e7bbb8256a547b994f56b5aee857751",
            "dbc3c46c680d441b901f2d9466b66bed",
            "4d4fc5f45d694f9fac1baf6c02d507c1",
            "63890ba768de43a69fd111a2b805c88f",
            "cedc5741214f4541b292c01eb4045ec9",
            "d5688f57e2974e38b4d47431c7ff469c",
            "dd9beb8203ab48649a8b4f97a68f93de",
            "207f40f1be3746af88e5abd4c2dc9900"
          ]
        }
      },
      "id": "xSFwlLs9fUS1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- MWP: Batched LLM Motif Extraction, Refined SFs, Structured Motifs, Token-L(H), BDM L(D|H) ---\n",
            "Using L(H) Cost Params: Label=0.5, DescBase=0.5, DescToken=0.1, SFListBase=0.25, SFTokenInLH=0.1\n",
            "LLM: google/gemma-2b-it, Batch Size: 5 responses, Retries: 2, Max Text per Prompt: 7000, Max New Tokens: 700\n",
            "Min SF Frequency for Refinement: 2\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-2b-it...\n",
            "BNB config created for google/gemma-2b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-2b-it (Quantization: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de092b7514b24d8c88e0c1d39d9f37f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-2b-it initialized successfully (return_full_text=False).\n",
            "BDM instance initialized successfully (ndim=2, default CTM-based).\n",
            "Loading Phase 2 output from: /content/drive/MyDrive/Colab Notebooks/Legal/Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json...\n",
            "DEBUG: Type of phase2_data_content: <class 'dict'>\n",
            "DEBUG: Keys in phase2_data_content: ['metadata', 'aggregated_pdf_content_by_qid']\n",
            "DEBUG: 'aggregated_pdf_content_by_qid' found. Type: <class 'dict'>\n",
            "DEBUG: QIDs available in 'aggregated_pdf_content_by_qid': ['Q4', 'Q5', 'Q6', 'Q7', 'Q10', 'Q13', 'Q19', 'Q28', 'Q31', 'Q1', 'Q2', 'Q3', 'Q8', 'Q9', 'Q11', 'Q12', 'Q14', 'Q16', 'Q17', 'Q21', 'Q22', 'Q23', 'Q24', 'Q27', 'Q29', 'Q30', 'Q33', 'Q15', 'Q18', 'Q20', 'Q25', 'Q26', 'Q34', 'Q36', 'Q32', 'Q35']\n",
            "DEBUG: Number of raw response items for Q4 in loaded file: 209\n",
            "\n",
            "MDL analysis will run for these QIDs: ['Q4']\n",
            "\n",
            "--- Analyzing Aggregated Text for QID: Q4 ---\n",
            "  Combined corpus for QID Q4 has 129501 chars from 209 individual responses.\n",
            "  Baseline MDL for QID Q4 (L(D_orig)): 121.3693\n",
            "  QID Q4: Processing 209 responses in 42 chunks (batch size: 5 responses).\n",
            "    Analyzing chunk 1/42 for QID Q4 (len: 3198 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 1 motif objects from chunk 1 (QID Q4).\n",
            "    Analyzing chunk 2/42 for QID Q4 (len: 3302 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 2 (QID Q4).\n",
            "    Analyzing chunk 3/42 for QID Q4 (len: 3259 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 3 (QID Q4).\n",
            "    Analyzing chunk 4/42 for QID Q4 (len: 3174 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 2 motif objects from chunk 4 (QID Q4).\n",
            "    Analyzing chunk 5/42 for QID Q4 (len: 3182 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 5 (QID Q4).\n",
            "    Analyzing chunk 6/42 for QID Q4 (len: 2964 chars)...\n",
            "      Motif extraction/parsing attempt 1 yielded no valid motifs for chunk 6 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Motif extraction/parsing attempt 2 yielded no valid motifs for chunk 6 (QID Q4). Retrying if possible...\n",
            "      No valid motifs extracted from chunk 6 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 7/42 for QID Q4 (len: 3365 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 7 (QID Q4).\n",
            "    Analyzing chunk 8/42 for QID Q4 (len: 3017 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 8 (QID Q4).\n",
            "    Analyzing chunk 9/42 for QID Q4 (len: 3124 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 2 motif objects from chunk 9 (QID Q4).\n",
            "    Analyzing chunk 10/42 for QID Q4 (len: 3258 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 10 (QID Q4).\n",
            "    Analyzing chunk 11/42 for QID Q4 (len: 2970 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 2 motif objects from chunk 11 (QID Q4).\n",
            "    Analyzing chunk 12/42 for QID Q4 (len: 3141 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 2 motif objects from chunk 12 (QID Q4).\n",
            "    Analyzing chunk 13/42 for QID Q4 (len: 3317 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 13 (QID Q4).\n",
            "    Analyzing chunk 14/42 for QID Q4 (len: 3063 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 14 (QID Q4).\n",
            "    Analyzing chunk 15/42 for QID Q4 (len: 3394 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 1 motif objects from chunk 15 (QID Q4).\n",
            "    Analyzing chunk 16/42 for QID Q4 (len: 3148 chars)...\n",
            "    [WARN] Motif JSON parsing/validation failed for QID Q4, Chunk 16: Expecting value: line 6 column 3 (char 369)\n",
            "      Motif extraction/parsing attempt 1 yielded no valid motifs for chunk 16 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [WARN] Motif JSON parsing/validation failed for QID Q4, Chunk 16: Expecting value: line 6 column 3 (char 369)\n",
            "      Motif extraction/parsing attempt 2 yielded no valid motifs for chunk 16 (QID Q4). Retrying if possible...\n",
            "      No valid motifs extracted from chunk 16 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 17/42 for QID Q4 (len: 3083 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 17 (QID Q4).\n",
            "    Analyzing chunk 18/42 for QID Q4 (len: 3059 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 18 (QID Q4).\n",
            "    Analyzing chunk 19/42 for QID Q4 (len: 2884 chars)...\n",
            "      Motif extraction/parsing attempt 1 yielded no valid motifs for chunk 19 (QID Q4). Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Motif extraction/parsing attempt 2 yielded no valid motifs for chunk 19 (QID Q4). Retrying if possible...\n",
            "      No valid motifs extracted from chunk 19 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 20/42 for QID Q4 (len: 2568 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 20 (QID Q4).\n",
            "    Analyzing chunk 21/42 for QID Q4 (len: 2897 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 21 (QID Q4).\n",
            "    Analyzing chunk 22/42 for QID Q4 (len: 3238 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 22 (QID Q4).\n",
            "    Analyzing chunk 23/42 for QID Q4 (len: 3351 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 23 (QID Q4).\n",
            "    Analyzing chunk 24/42 for QID Q4 (len: 3363 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 2 motif objects from chunk 24 (QID Q4).\n",
            "    Analyzing chunk 25/42 for QID Q4 (len: 2922 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 25 (QID Q4).\n",
            "    Analyzing chunk 26/42 for QID Q4 (len: 2490 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 26 (QID Q4).\n",
            "    Analyzing chunk 27/42 for QID Q4 (len: 2452 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 27 (QID Q4).\n",
            "    Analyzing chunk 28/42 for QID Q4 (len: 2481 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 28 (QID Q4).\n",
            "    Analyzing chunk 29/42 for QID Q4 (len: 3039 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 2 motif objects from chunk 29 (QID Q4).\n",
            "    Analyzing chunk 30/42 for QID Q4 (len: 3393 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 2 motif objects from chunk 30 (QID Q4).\n",
            "    Analyzing chunk 31/42 for QID Q4 (len: 3135 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 31 (QID Q4).\n",
            "    Analyzing chunk 32/42 for QID Q4 (len: 3298 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 32 (QID Q4).\n",
            "    Analyzing chunk 33/42 for QID Q4 (len: 2537 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 1 motif objects from chunk 33 (QID Q4).\n",
            "    Analyzing chunk 34/42 for QID Q4 (len: 2562 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 34 (QID Q4).\n",
            "    Analyzing chunk 35/42 for QID Q4 (len: 3188 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 2 motif objects from chunk 35 (QID Q4).\n",
            "    Analyzing chunk 36/42 for QID Q4 (len: 3092 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 36 (QID Q4).\n",
            "    Analyzing chunk 37/42 for QID Q4 (len: 3343 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 37 (QID Q4).\n",
            "    Analyzing chunk 38/42 for QID Q4 (len: 3181 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 38 (QID Q4).\n",
            "    Analyzing chunk 39/42 for QID Q4 (len: 3319 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 4 motif objects from chunk 39 (QID Q4).\n",
            "    Analyzing chunk 40/42 for QID Q4 (len: 3366 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 40 (QID Q4).\n",
            "    Analyzing chunk 41/42 for QID Q4 (len: 3204 chars)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Extracted 5 motif objects from chunk 41 (QID Q4).\n",
            "    Analyzing chunk 42/42 for QID Q4 (len: 2647 chars)...\n",
            "      Extracted 4 motif objects from chunk 42 (QID Q4).\n",
            "  Extracted 144 raw motif objects from LLM for QID Q4 (across all chunks).\n",
            "  Consolidated into 4 unique motifs (by label) for QID Q4.\n",
            "  Consolidated Motifs for QID Q4 (BEFORE SF refinement):\n",
            "    --- Cons. Motif 1 ---\n",
            "      Label: [DATA_PRIVACY]\n",
            "      Description: A concern regarding the potential erosion of privacy rights due to the...\n",
            "      Surface Forms (96): ['A breach of privacy when displaying images of private residences', 'A concise summary of the excerpt', 'A concise summary of the excerpt, focusing on exceptions to privacy rights in the employment context']...\n",
            "    --- Cons. Motif 2 ---\n",
            "      Label: [EXAMPLE_LABEL]\n",
            "      Description: A concise summary of the excerpt's argument for stronger data rights f...\n",
            "      Surface Forms (43): ['Advocate for a consistent age of 18 for defining a child', 'Australian perspective on data rights', 'Balancing privacy with legitimate business needs']...\n",
            "    --- Cons. Motif 3 ---\n",
            "      Label: [RSP_SEP]\n",
            "      Description: A focus on exceptions for employee rights within the employment contex...\n",
            "      Surface Forms (49): ['Balancing employee privacy with data security', 'Balancing privacy protections with societal benefit', 'Clear and accessible privacy settings']...\n",
            "    --- Cons. Motif 4 ---\n",
            "      Label: [ARTK]\n",
            "      Description: A concise summary of the excerpt's key points regarding exceptions for...\n",
            "      Surface Forms (1): [\"Here’s a 3-sentence summary of the excerpt's key points regarding exceptions for individual rights in the employment context\"]...\n",
            "  Refining 4 consolidated motifs by SF frequency (min freq: 2)...\n",
            "  Refined into 3 motifs with frequent surface forms for QID Q4.\n",
            "  Final Refined Motifs for QID Q4 (for MDL eval):\n",
            "    --- Refined Motif 1 ---\n",
            "      Label: [DATA_PRIVACY]\n",
            "      Description: A concern regarding the potential erosion of privacy rights due to the...\n",
            "      Surface Forms (8): ['A concise summary of the excerpt', \"However, the excerpt doesn't explicitly detail any specific exceptions to these rights within the employment context; it primarily focuses on the broader value of privacy itself.\", \"The excerpt doesn't explicitly detail any specific exceptions to these rights within the employment context; it primarily focuses on the broader value of privacy itself.\", 'competing public interests', 'individual rights', 'specific exceptions to these rights', 'strengthening Australian privacy laws', 'use of personal information for marketing']\n",
            "    --- Refined Motif 2 ---\n",
            "      Label: [EXAMPLE_LABEL]\n",
            "      Description: A concise summary of the excerpt's argument for stronger data rights f...\n",
            "      Surface Forms (8): ['Careful consideration of exceptions', 'Expanding individual rights to access, object, erase, correct, and de-index search results', 'collection and use of personal information', 'competing public interests', 'employee records exemption', 'exceptions to individual rights', 'specific exceptions to individual rights', 'voluntary consent']\n",
            "    --- Refined Motif 3 ---\n",
            "      Label: [RSP_SEP]\n",
            "      Description: A focus on exceptions for employee rights within the employment contex...\n",
            "      Surface Forms (11): ['Harmonized approach to privacy laws', 'Here’s a 3-sentence summary of the excerpt, focusing on the requested points', 'children', 'collection and use of personal information', 'expanding individual rights to access, object, erase, correct, and de-index search results', 'explicit, voluntary consent', 'health and wellbeing', 'marketing practices', 'privacy Act', 'social media platforms', 'specific exceptions to individual rights']\n",
            "  L(H) (Token-based Structured) for final refined motifs of QID Q4: 24.8500\n",
            "  L(D|H) (BDM-based) compressed full corpus complexity for QID Q4: 113.5377\n",
            "  Total MDL cost with final refined motifs for QID Q4: 138.3877\n",
            "  NOTE: No significant compression (or cost increased). Diff: -17.0184\n",
            "----------------------------------------\n",
            "\n",
            "--- Overall QID-based MDL Analysis Summary (Batched LLM, Refined SFs, Structured Motifs, Token-L(H)) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Revised return_full_text=False\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import time\n",
        "\n",
        "# --- Configuration (Adjust as needed) ---\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it' # Or your 'gemma-3n-e4b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "MAX_NEW_TOKENS_TEST = 150 # Default for simple tests\n",
        "\n",
        "# --- LLM Initialization ---\n",
        "def initialize_llm_pipeline_for_test(return_full_text_setting: bool):\n",
        "    print(f\"--- Initializing LLM for Test (return_full_text={return_full_text_setting}) ---\")\n",
        "    local_llm_pipeline_instance = None\n",
        "    local_llm_tokenizer_instance = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {LOCAL_LLM_MODEL_ID}...\")\n",
        "        local_llm_tokenizer_instance = AutoTokenizer.from_pretrained(LOCAL_LLM_MODEL_ID)\n",
        "\n",
        "        if local_llm_tokenizer_instance.pad_token is None:\n",
        "            print(\"Tokenizer does not have a pad_token; setting pad_token = eos_token.\")\n",
        "            local_llm_tokenizer_instance.pad_token = local_llm_tokenizer_instance.eos_token\n",
        "            # Important: The model's config might also need pad_token_id set if it's used during generation\n",
        "            # For pipeline, usually handled if tokenizer has it.\n",
        "\n",
        "        bnb_config = None\n",
        "        quant_active = False\n",
        "        if USE_QUANTIZATION_FOR_LOCAL_LLM and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n",
        "                quant_active = True\n",
        "                print(f\"BNB config created for {LOCAL_LLM_MODEL_ID}, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb:\n",
        "                print(f\"WARN: Failed to create BitsAndBytesConfig: {e_bnb}. Quantization disabled.\")\n",
        "\n",
        "        print(f\"Loading local model {LOCAL_LLM_MODEL_ID} (Quantization: {quant_active})...\")\n",
        "        model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "        if quant_active: model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        else:\n",
        "            if device.type == 'cuda': model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "        local_llm_model_instance = AutoModelForCausalLM.from_pretrained(LOCAL_LLM_MODEL_ID, **model_kwargs)\n",
        "\n",
        "        # Explicitly set pad_token_id in model config if tokenizer's was None\n",
        "        if local_llm_tokenizer_instance.pad_token_id == local_llm_tokenizer_instance.eos_token_id:\n",
        "             local_llm_model_instance.config.pad_token_id = local_llm_model_instance.config.eos_token_id\n",
        "\n",
        "\n",
        "        local_llm_pipeline_instance = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=local_llm_model_instance,\n",
        "            tokenizer=local_llm_tokenizer_instance,\n",
        "            return_full_text=return_full_text_setting\n",
        "        )\n",
        "        print(f\"Local LLM pipeline for {LOCAL_LLM_MODEL_ID} initialized successfully.\")\n",
        "        return local_llm_pipeline_instance, local_llm_tokenizer_instance\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to initialize local LLM pipeline for test: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# --- Test Function ---\n",
        "def run_llm_test(test_name: str,\n",
        "                 user_prompt_content: str,\n",
        "                 hf_pipeline,\n",
        "                 hf_tokenizer,\n",
        "                 is_pipeline_returning_full_text: bool, # Argument to know the pipeline's setting\n",
        "                 max_new_tok=MAX_NEW_TOKENS_TEST):\n",
        "    print(f\"\\n--- Running Test: {test_name} (Pipeline return_full_text={is_pipeline_returning_full_text}) ---\")\n",
        "    if not hf_pipeline or not hf_tokenizer:\n",
        "        print(\"LLM Pipeline not initialized. Skipping test.\")\n",
        "        return\n",
        "\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": user_prompt_content.strip()}]\n",
        "\n",
        "    try:\n",
        "        # For Gemma, apply_chat_template adds the generation prompt for the model turn\n",
        "        prompt_formatted_for_llm = hf_tokenizer.apply_chat_template(\n",
        "            messages_for_template,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "    except Exception as e_template:\n",
        "        print(f\"ERROR applying chat template: {e_template}\")\n",
        "        # This can happen if the tokenizer doesn't have a chat_template defined\n",
        "        # or if the messages format is wrong.\n",
        "        print(\"Ensure your tokenizer has a chat_template (e.g., tokenizer.chat_template).\")\n",
        "        print(\"For Gemma, messages should be like: [{'role': 'user', 'content': '...'}, {'role': 'model', 'content': '...'}]\")\n",
        "        return\n",
        "\n",
        "    print(f\"Formatted Prompt (sent to pipeline):\\n<<<<<\\n{prompt_formatted_for_llm}\\n>>>>>\")\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": max_new_tok,\n",
        "        \"do_sample\": False, # Keep False for predictable output during debugging\n",
        "        \"pad_token_id\": hf_tokenizer.pad_token_id # Use the tokenizer's pad_token_id\n",
        "                                                # which we ensured is set to eos_token_id if originally None\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        outputs = hf_pipeline(prompt_formatted_for_llm, **generation_args)\n",
        "        end_time = time.time()\n",
        "        print(f\"LLM call took {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "        print(f\"Raw 'outputs' from hf_pipeline:\\n<<<<<\\n{outputs}\\n>>>>>\")\n",
        "\n",
        "        if outputs and isinstance(outputs, list) and len(outputs) > 0 and \\\n",
        "           outputs[0] and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
        "\n",
        "            generated_text_full = outputs[0]['generated_text']\n",
        "            print(f\"'generated_text' from LLM (len {len(generated_text_full)}):\\n<<<<<\\n{generated_text_full}\\n>>>>>\")\n",
        "\n",
        "            assistant_response = \"\"\n",
        "            if is_pipeline_returning_full_text:\n",
        "                print(\"Attempting to strip prompt (since is_pipeline_returning_full_text=True)...\")\n",
        "                # Try to strip the exact formatted prompt first\n",
        "                if generated_text_full.startswith(prompt_formatted_for_llm):\n",
        "                    assistant_response = generated_text_full[len(prompt_formatted_for_llm):].strip()\n",
        "                    print(f\"Isolated Assistant Response (exact prompt strip):\\n<<<<<\\n{assistant_response}\\n>>>>>\")\n",
        "                else:\n",
        "                    # Fallback: Look for the model's turn marker if the start doesn't match exactly\n",
        "                    # This is common if the echoed prompt has slight variations or only part is echoed\n",
        "                    model_turn_start_token_default = \"<start_of_turn>model\" # Common for Gemma\n",
        "\n",
        "                    # Try to get the actual model turn start from chat template if possible\n",
        "                    # This is more robust if the tokenizer has a proper template\n",
        "                    try:\n",
        "                        model_turn_template_parts = hf_tokenizer.apply_chat_template(\n",
        "                            [{\"role\": \"assistant\", \"content\": \"\"}], # Empty assistant message\n",
        "                            tokenize=False,\n",
        "                            add_generation_prompt=False # We only want the prefix for assistant\n",
        "                        )\n",
        "                        # If it adds a newline or space after model token, strip it for matching\n",
        "                        model_turn_start_token = model_turn_template_parts.strip()\n",
        "                        if not model_turn_start_token: # Fallback if template gives empty for assistant prefix\n",
        "                            model_turn_start_token = model_turn_start_token_default\n",
        "                    except: # If apply_chat_template fails for assistant role, use default\n",
        "                        model_turn_start_token = model_turn_start_token_default\n",
        "\n",
        "                    idx = generated_text_full.rfind(model_turn_start_token) # Find the last model turn\n",
        "                    if idx != -1:\n",
        "                        potential_response = generated_text_full[idx + len(model_turn_start_token):].strip()\n",
        "                        if potential_response.startswith(\"\\n\"): # Clean leading newline often added by models\n",
        "                            potential_response = potential_response[1:].strip()\n",
        "\n",
        "                        # Heuristic: if the stripped part is significantly shorter than original, it's likely the response\n",
        "                        if len(potential_response) < len(generated_text_full) or not generated_text_full.strip().endswith(prompt_formatted_for_llm.strip()):\n",
        "                             assistant_response = potential_response\n",
        "                             print(f\"Isolated Assistant Response (using model turn token '{model_turn_start_token}'):\\n<<<<<\\n{assistant_response}\\n>>>>>\")\n",
        "                        else:\n",
        "                            print(f\"WARN: Model turn token '{model_turn_start_token}' found, but stripping did not significantly shorten text. This might be an issue.\")\n",
        "                            assistant_response = potential_response # Keep it for inspection\n",
        "                            print(f\"Potentially problematic isolated response:\\n<<<<<\\n{assistant_response}\\n>>>>>\")\n",
        "                    else:\n",
        "                        print(f\"WARN: Could not reliably strip prompt. Model turn token '{model_turn_start_token}' not found as expected in 'generated_text'.\")\n",
        "                        assistant_response = generated_text_full # Fallback to full text if stripping fails\n",
        "            else: # if is_pipeline_returning_full_text=False\n",
        "                assistant_response = generated_text_full.strip() # generated_text is already just the new tokens\n",
        "                print(f\"Assistant Response (since is_pipeline_returning_full_text=False):\\n<<<<<\\n{assistant_response}\\n>>>>>\")\n",
        "\n",
        "            # Final check for empty response after stripping\n",
        "            if not assistant_response:\n",
        "                print(\"WARN: Assistant response is empty after processing.\")\n",
        "\n",
        "        else:\n",
        "            print(\"LLM output structure was not as expected (e.g., no 'generated_text' or empty list).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during LLM test call or processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "# --- Main Test Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure API key is set if your model/SDK needs it (not for local HF transformers usually)\n",
        "    # if 'GEMINI_API_KEY' not in os.environ and 'HF_TOKEN' not in os.environ:\n",
        "    #     print(\"Warning: Relevant API key (e.g., GEMINI_API_KEY or HF_TOKEN for gated models) not found in environment.\")\n",
        "\n",
        "    # --- Test with return_full_text=False first ---\n",
        "    print(\"\\n\" + \"#\"*10 + \" TESTING WITH pipeline(return_full_text=False) \" + \"#\"*10)\n",
        "    test_pipeline_rff_false, test_tokenizer_rff_false = initialize_llm_pipeline_for_test(return_full_text_setting=False)\n",
        "\n",
        "    if test_pipeline_rff_false and test_tokenizer_rff_false:\n",
        "        prompt1 = \"What is 2+2?\"\n",
        "        run_llm_test(\"Test 1 (Simple Q&A)\",\n",
        "                     prompt1, test_pipeline_rff_false, test_tokenizer_rff_false,\n",
        "                     is_pipeline_returning_full_text=False, max_new_tok=20)\n",
        "\n",
        "        prompt2 = \"List three primary colors.\"\n",
        "        run_llm_test(\"Test 2 (Simple Instruction)\",\n",
        "                     prompt2, test_pipeline_rff_false, test_tokenizer_rff_false,\n",
        "                     is_pipeline_returning_full_text=False, max_new_tok=30)\n",
        "\n",
        "        prompt3 = \"\"\"\n",
        "        Provide a JSON object with a \"fruit\" key and a \"color\" key.\n",
        "        Example: {\"fruit\": \"apple\", \"color\": \"red\"}\n",
        "        Your JSON:\n",
        "        \"\"\"\n",
        "        run_llm_test(\"Test 3 (Simple JSON, no context)\",\n",
        "                     prompt3, test_pipeline_rff_false, test_tokenizer_rff_false,\n",
        "                     is_pipeline_returning_full_text=False, max_new_tok=50)\n",
        "\n",
        "        short_context_for_test4 = \"The user expressed concerns about data privacy. Another user mentioned data security. Access control was also discussed as important.\"\n",
        "        motif_prompt_template_for_test4 = f\"\"\"You will receive a set of comments from different people answering the same question.\n",
        "Your task is to identify up to 2 key recurring themes.\n",
        "For each theme, provide:\n",
        "- A short label like [DATA_PRIVACY]\n",
        "- A 1-sentence definition\n",
        "- 1-2 short phrases that often appear in the text (surface forms)\n",
        "\n",
        "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
        "Example of one object in the list:\n",
        "{{\n",
        "  \"label\": \"[EXAMPLE_LABEL]\",\n",
        "  \"description\": \"A concise description of the example theme.\",\n",
        "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
        "}}\n",
        "If no clear motifs are found, output an empty JSON list: `[]`.\n",
        "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
        "\n",
        "Set of comments to analyze:\n",
        "\\\"\\\"\\\"\n",
        "{short_context_for_test4}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):\n",
        "\"\"\"\n",
        "        run_llm_test(\"Test 4 (Motif Prompt, Short Context)\",\n",
        "                     motif_prompt_template_for_test4, test_pipeline_rff_false, test_tokenizer_rff_false,\n",
        "                     is_pipeline_returning_full_text=False, max_new_tok=300)\n",
        "    else:\n",
        "        print(\"Skipping tests for return_full_text=False due to pipeline init failure.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # --- Test with return_full_text=True ---\n",
        "    print(\"\\n\" + \"#\"*10 + \" TESTING WITH pipeline(return_full_text=True) \" + \"#\"*10)\n",
        "    test_pipeline_rff_true, test_tokenizer_rff_true = initialize_llm_pipeline_for_test(return_full_text_setting=True)\n",
        "    if test_pipeline_rff_true and test_tokenizer_rff_true:\n",
        "        # Re-use prompt1 from above for brevity\n",
        "        prompt1_reused = \"What is 2+2?\"\n",
        "        run_llm_test(\"Test 5 (Simple Q&A, RFF=True)\",\n",
        "                     prompt1_reused, test_pipeline_rff_true, test_tokenizer_rff_true,\n",
        "                     is_pipeline_returning_full_text=True, max_new_tok=100) # Increased max_new_tokens slightly to accommodate echoed prompt\n",
        "\n",
        "        # Re-use motif_prompt_template_for_test4\n",
        "        run_llm_test(\"Test 6 (Motif Prompt, Short Context, RFF=True)\",\n",
        "                     motif_prompt_template_for_test4, test_pipeline_rff_true, test_tokenizer_rff_true,\n",
        "                     is_pipeline_returning_full_text=True, max_new_tok=1000) # Increased max_new_tokens significantly\n",
        "    else:\n",
        "        print(\"Skipping tests for return_full_text=True due to pipeline init failure.\")\n",
        "\n",
        "    # Clean up (optional, if GPU memory is an issue between runs)\n",
        "    # print(\"Cleaning up model and tokenizer objects...\")\n",
        "    # del test_pipeline_rff_false, test_tokenizer_rff_false, test_pipeline_rff_true, test_tokenizer_rff_true\n",
        "    # if torch.cuda.is_available():\n",
        "    #    torch.cuda.empty_cache()\n",
        "    # print(\"Cleanup complete.\")"
      ],
      "metadata": {
        "id": "R_RxqMA2bC6o",
        "outputId": "b2ba783e-8d9b-4e6a-abb1-426250b5de69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a21fba4cc298413e8dfddeb442bd1808",
            "7d6cf6cb97524e939ec83feac69a494a",
            "81201a97fa71413ba7ee0743b391368f",
            "4ea4e6357aba4a5f92e192aa8beb9ce0",
            "b555f7f3c57148c0afed1d3a30548488",
            "2a38b994f5ca4a13a4b484012c06b793",
            "1475e693f7f14cafa806a84e7d76c899",
            "20b847c1cbe84e0881237c16f9f81817",
            "557ef3ddcb61472fad04269774283785",
            "c33753344ad245618155cb809574fa9d",
            "a7e7478becde448daf516f44105aad61",
            "9342a1062ca94a2ba4448081e0c76413",
            "77011edf3a09404ea3dc93d68b9ae2af",
            "dced6f505bf44c90be4b60c35ed9c0d2",
            "770751fcca504a0c9affe9a73cbcab14",
            "a50a745d1dd143a4b0ed2d4e4f094edd",
            "c078ce5ec4094356b48a88f197fbf341",
            "00873c1da11849e88bd5979e79467bd2",
            "fc08983a943f462dab9a3c69f4559c0a",
            "a3ba763ea08345bc91f11df2e5b1d650",
            "b85fb6b6697c4c26a0b9dcfe9a0cc66e",
            "01581ac673144927be13cb5aa9f44202"
          ]
        }
      },
      "id": "R_RxqMA2bC6o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "########## TESTING WITH pipeline(return_full_text=False) ##########\n",
            "--- Initializing LLM for Test (return_full_text=False) ---\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-2b-it...\n",
            "BNB config created for google/gemma-2b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-2b-it (Quantization: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a21fba4cc298413e8dfddeb442bd1808"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-2b-it initialized successfully.\n",
            "\n",
            "--- Running Test: Test 1 (Simple Q&A) (Pipeline return_full_text=False) ---\n",
            "Formatted Prompt (sent to pipeline):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "What is 2+2?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            ">>>>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM call took 1.47 seconds.\n",
            "Raw 'outputs' from hf_pipeline:\n",
            "<<<<<\n",
            "[{'generated_text': 'The answer is 4. 2+2 is a simple addition problem that can be solved by'}]\n",
            ">>>>>\n",
            "'generated_text' from LLM (len 71):\n",
            "<<<<<\n",
            "The answer is 4. 2+2 is a simple addition problem that can be solved by\n",
            ">>>>>\n",
            "Assistant Response (since is_pipeline_returning_full_text=False):\n",
            "<<<<<\n",
            "The answer is 4. 2+2 is a simple addition problem that can be solved by\n",
            ">>>>>\n",
            "\n",
            "--- Running Test: Test 2 (Simple Instruction) (Pipeline return_full_text=False) ---\n",
            "Formatted Prompt (sent to pipeline):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "List three primary colors.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            ">>>>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM call took 1.32 seconds.\n",
            "Raw 'outputs' from hf_pipeline:\n",
            "<<<<<\n",
            "[{'generated_text': 'Sure, here are the three primary colors:\\n\\n1. Red\\n2. Yellow\\n3. Blue'}]\n",
            ">>>>>\n",
            "'generated_text' from LLM (len 66):\n",
            "<<<<<\n",
            "Sure, here are the three primary colors:\n",
            "\n",
            "1. Red\n",
            "2. Yellow\n",
            "3. Blue\n",
            ">>>>>\n",
            "Assistant Response (since is_pipeline_returning_full_text=False):\n",
            "<<<<<\n",
            "Sure, here are the three primary colors:\n",
            "\n",
            "1. Red\n",
            "2. Yellow\n",
            "3. Blue\n",
            ">>>>>\n",
            "\n",
            "--- Running Test: Test 3 (Simple JSON, no context) (Pipeline return_full_text=False) ---\n",
            "Formatted Prompt (sent to pipeline):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "Provide a JSON object with a \"fruit\" key and a \"color\" key.\n",
            "        Example: {\"fruit\": \"apple\", \"color\": \"red\"}\n",
            "        Your JSON:<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            ">>>>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM call took 1.86 seconds.\n",
            "Raw 'outputs' from hf_pipeline:\n",
            "<<<<<\n",
            "[{'generated_text': '```json\\n{\\n  \"fruit\": \"banana\",\\n  \"color\": \"yellow\"\\n}\\n```'}]\n",
            ">>>>>\n",
            "'generated_text' from LLM (len 56):\n",
            "<<<<<\n",
            "```json\n",
            "{\n",
            "  \"fruit\": \"banana\",\n",
            "  \"color\": \"yellow\"\n",
            "}\n",
            "```\n",
            ">>>>>\n",
            "Assistant Response (since is_pipeline_returning_full_text=False):\n",
            "<<<<<\n",
            "```json\n",
            "{\n",
            "  \"fruit\": \"banana\",\n",
            "  \"color\": \"yellow\"\n",
            "}\n",
            "```\n",
            ">>>>>\n",
            "\n",
            "--- Running Test: Test 4 (Motif Prompt, Short Context) (Pipeline return_full_text=False) ---\n",
            "Formatted Prompt (sent to pipeline):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "Your task is to identify up to 2 key recurring themes.\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 1-2 short phrases that often appear in the text (surface forms)\n",
            "\n",
            "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
            "Example of one object in the list:\n",
            "{\n",
            "  \"label\": \"[EXAMPLE_LABEL]\",\n",
            "  \"description\": \"A concise description of the example theme.\",\n",
            "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
            "}\n",
            "If no clear motifs are found, output an empty JSON list: `[]`.\n",
            "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
            "\n",
            "Set of comments to analyze:\n",
            "\"\"\"\n",
            "The user expressed concerns about data privacy. Another user mentioned data security. Access control was also discussed as important.\n",
            "\"\"\"\n",
            "\n",
            "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            ">>>>>\n",
            "LLM call took 2.78 seconds.\n",
            "Raw 'outputs' from hf_pipeline:\n",
            "<<<<<\n",
            "[{'generated_text': '{\\n  \"label\": \"[DATA_PRIVACY]\",\\n  \"description\": \"A concern about data privacy.\",\\n  \"surface_forms\": [\"concerns about data privacy\"]\\n}\\n```'}]\n",
            ">>>>>\n",
            "'generated_text' from LLM (len 137):\n",
            "<<<<<\n",
            "{\n",
            "  \"label\": \"[DATA_PRIVACY]\",\n",
            "  \"description\": \"A concern about data privacy.\",\n",
            "  \"surface_forms\": [\"concerns about data privacy\"]\n",
            "}\n",
            "```\n",
            ">>>>>\n",
            "Assistant Response (since is_pipeline_returning_full_text=False):\n",
            "<<<<<\n",
            "{\n",
            "  \"label\": \"[DATA_PRIVACY]\",\n",
            "  \"description\": \"A concern about data privacy.\",\n",
            "  \"surface_forms\": [\"concerns about data privacy\"]\n",
            "}\n",
            "```\n",
            ">>>>>\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "########## TESTING WITH pipeline(return_full_text=True) ##########\n",
            "--- Initializing LLM for Test (return_full_text=True) ---\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-2b-it...\n",
            "BNB config created for google/gemma-2b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-2b-it (Quantization: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9342a1062ca94a2ba4448081e0c76413"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-2b-it initialized successfully.\n",
            "\n",
            "--- Running Test: Test 5 (Simple Q&A, RFF=True) (Pipeline return_full_text=True) ---\n",
            "Formatted Prompt (sent to pipeline):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "What is 2+2?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            ">>>>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM call took 1.73 seconds.\n",
            "Raw 'outputs' from hf_pipeline:\n",
            "<<<<<\n",
            "[{'generated_text': '<bos><start_of_turn>user\\nWhat is 2+2?<end_of_turn>\\n<start_of_turn>model\\nThe answer is 4. 2+2 is a simple addition problem that can be solved by adding the two numbers together.'}]\n",
            ">>>>>\n",
            "'generated_text' from LLM (len 176):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "What is 2+2?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The answer is 4. 2+2 is a simple addition problem that can be solved by adding the two numbers together.\n",
            ">>>>>\n",
            "Attempting to strip prompt (since is_pipeline_returning_full_text=True)...\n",
            "Isolated Assistant Response (exact prompt strip):\n",
            "<<<<<\n",
            "The answer is 4. 2+2 is a simple addition problem that can be solved by adding the two numbers together.\n",
            ">>>>>\n",
            "\n",
            "--- Running Test: Test 6 (Motif Prompt, Short Context, RFF=True) (Pipeline return_full_text=True) ---\n",
            "Formatted Prompt (sent to pipeline):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "Your task is to identify up to 2 key recurring themes.\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 1-2 short phrases that often appear in the text (surface forms)\n",
            "\n",
            "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
            "Example of one object in the list:\n",
            "{\n",
            "  \"label\": \"[EXAMPLE_LABEL]\",\n",
            "  \"description\": \"A concise description of the example theme.\",\n",
            "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
            "}\n",
            "If no clear motifs are found, output an empty JSON list: `[]`.\n",
            "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
            "\n",
            "Set of comments to analyze:\n",
            "\"\"\"\n",
            "The user expressed concerns about data privacy. Another user mentioned data security. Access control was also discussed as important.\n",
            "\"\"\"\n",
            "\n",
            "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            ">>>>>\n",
            "LLM call took 2.61 seconds.\n",
            "Raw 'outputs' from hf_pipeline:\n",
            "<<<<<\n",
            "[{'generated_text': '<bos><start_of_turn>user\\nYou will receive a set of comments from different people answering the same question.\\nYour task is to identify up to 2 key recurring themes.\\nFor each theme, provide:\\n- A short label like [DATA_PRIVACY]\\n- A 1-sentence definition\\n- 1-2 short phrases that often appear in the text (surface forms)\\n\\nOutput MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\\nExample of one object in the list:\\n{\\n  \"label\": \"[EXAMPLE_LABEL]\",\\n  \"description\": \"A concise description of the example theme.\",\\n  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\\n}\\nIf no clear motifs are found, output an empty JSON list: `[]`.\\nDo not include any other text, explanations, or markdown code fences around the JSON.\\n\\nSet of comments to analyze:\\n\"\"\"\\nThe user expressed concerns about data privacy. Another user mentioned data security. Access control was also discussed as important.\\n\"\"\"\\n\\nValid JSON Output (ensure it\\'s a list of objects, or an empty list [] if no themes):<end_of_turn>\\n<start_of_turn>model\\n{\\n  \"label\": \"[DATA_PRIVACY]\",\\n  \"description\": \"A concern about data privacy.\",\\n  \"surface_forms\": [\"concerns about data privacy\"]\\n}\\n```'}]\n",
            ">>>>>\n",
            "'generated_text' from LLM (len 1227):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "Your task is to identify up to 2 key recurring themes.\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 1-2 short phrases that often appear in the text (surface forms)\n",
            "\n",
            "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
            "Example of one object in the list:\n",
            "{\n",
            "  \"label\": \"[EXAMPLE_LABEL]\",\n",
            "  \"description\": \"A concise description of the example theme.\",\n",
            "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
            "}\n",
            "If no clear motifs are found, output an empty JSON list: `[]`.\n",
            "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
            "\n",
            "Set of comments to analyze:\n",
            "\"\"\"\n",
            "The user expressed concerns about data privacy. Another user mentioned data security. Access control was also discussed as important.\n",
            "\"\"\"\n",
            "\n",
            "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):<end_of_turn>\n",
            "<start_of_turn>model\n",
            "{\n",
            "  \"label\": \"[DATA_PRIVACY]\",\n",
            "  \"description\": \"A concern about data privacy.\",\n",
            "  \"surface_forms\": [\"concerns about data privacy\"]\n",
            "}\n",
            "```\n",
            ">>>>>\n",
            "Attempting to strip prompt (since is_pipeline_returning_full_text=True)...\n",
            "Isolated Assistant Response (exact prompt strip):\n",
            "<<<<<\n",
            "{\n",
            "  \"label\": \"[DATA_PRIVACY]\",\n",
            "  \"description\": \"A concern about data privacy.\",\n",
            "  \"surface_forms\": [\"concerns about data privacy\"]\n",
            "}\n",
            "```\n",
            ">>>>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simple llm prompting test\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import time\n",
        "\n",
        "# --- Configuration (Adjust as needed) ---\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it' # Or your 'gemma-3n-e4b-it' if that's the one you are targeting\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "MAX_NEW_TOKENS_TEST = 150 # Start with a reasonable number for simple tests\n",
        "\n",
        "# --- LLM Initialization (from your main script) ---\n",
        "def initialize_llm_pipeline_for_test(return_full_text_setting):\n",
        "    print(f\"--- Initializing LLM for Test (return_full_text={return_full_text_setting}) ---\")\n",
        "    local_llm_pipeline_instance = None\n",
        "    local_llm_tokenizer_instance = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {LOCAL_LLM_MODEL_ID}...\")\n",
        "        local_llm_tokenizer_instance = AutoTokenizer.from_pretrained(LOCAL_LLM_MODEL_ID)\n",
        "\n",
        "        # Ensure pad_token is set if tokenizer doesn't have one (Gemma usually doesn't)\n",
        "        if local_llm_tokenizer_instance.pad_token is None:\n",
        "            print(\"Tokenizer does not have a pad_token, setting it to eos_token.\")\n",
        "            local_llm_tokenizer_instance.pad_token = local_llm_tokenizer_instance.eos_token\n",
        "\n",
        "        bnb_config = None\n",
        "        quant_active = False\n",
        "        if USE_QUANTIZATION_FOR_LOCAL_LLM and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n",
        "                quant_active = True\n",
        "                print(f\"BNB config created for {LOCAL_LLM_MODEL_ID}, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb:\n",
        "                print(f\"WARN: Failed to create BitsAndBytesConfig: {e_bnb}. Quantization disabled.\")\n",
        "\n",
        "        print(f\"Loading local model {LOCAL_LLM_MODEL_ID} (Quantization: {quant_active})...\")\n",
        "        model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "        if quant_active: model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        else:\n",
        "            if device.type == 'cuda': model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "        local_llm_model_instance = AutoModelForCausalLM.from_pretrained(LOCAL_LLM_MODEL_ID, **model_kwargs)\n",
        "\n",
        "        local_llm_pipeline_instance = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=local_llm_model_instance,\n",
        "            tokenizer=local_llm_tokenizer_instance,\n",
        "            return_full_text=return_full_text_setting # Configurable\n",
        "        )\n",
        "        print(f\"Local LLM pipeline for {LOCAL_LLM_MODEL_ID} initialized successfully.\")\n",
        "        return local_llm_pipeline_instance, local_llm_tokenizer_instance\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to initialize local LLM pipeline for test: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# --- Test Function ---\n",
        "def run_llm_test(test_name, user_prompt_content, hf_pipeline, hf_tokenizer, max_new_tok=MAX_NEW_TOKENS_TEST):\n",
        "    print(f\"\\n--- Running Test: {test_name} ---\")\n",
        "    if not hf_pipeline or not hf_tokenizer:\n",
        "        print(\"LLM Pipeline not initialized. Skipping test.\")\n",
        "        return\n",
        "\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": user_prompt_content.strip()}]\n",
        "\n",
        "    # Important: For Gemma, apply_chat_template often expects a list of dicts with 'role' and 'content'\n",
        "    # and add_generation_prompt=True is crucial to add the '<start_of_turn>model\\n' token.\n",
        "    try:\n",
        "        prompt_formatted_for_llm = hf_tokenizer.apply_chat_template(\n",
        "            messages_for_template,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True # This adds the turn for the model to start generating\n",
        "        )\n",
        "    except Exception as e_template:\n",
        "        print(f\"ERROR applying chat template: {e_template}\")\n",
        "        print(\"Make sure your tokenizer has a chat template configured (e.g., tokenizer.chat_template).\")\n",
        "        return\n",
        "\n",
        "    print(f\"Formatted Prompt (sent to pipeline):\\n<<<<<\\n{prompt_formatted_for_llm}\\n>>>>>\")\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": max_new_tok,\n",
        "        \"do_sample\": False, # Keep False for predictable output during debugging\n",
        "        \"pad_token_id\": hf_tokenizer.eos_token_id\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        outputs = hf_pipeline(prompt_formatted_for_llm, **generation_args)\n",
        "        end_time = time.time()\n",
        "        print(f\"LLM call took {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "        print(f\"Raw 'outputs' from hf_pipeline:\\n<<<<<\\n{outputs}\\n>>>>>\")\n",
        "\n",
        "        if outputs and isinstance(outputs, list) and outputs[0] and isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
        "            generated_text_full = outputs[0]['generated_text']\n",
        "            print(f\"'generated_text' from LLM (len {len(generated_text_full)}):\\n<<<<<\\n{generated_text_full}\\n>>>>>\")\n",
        "\n",
        "            # Attempt to isolate assistant's response if prompt was echoed (when return_full_text=True)\n",
        "            if hf_pipeline.return_full_text: # Check the setting of the pipeline\n",
        "                if generated_text_full.startswith(prompt_formatted_for_llm):\n",
        "                    assistant_response = generated_text_full[len(prompt_formatted_for_llm):].strip()\n",
        "                    print(f\"Isolated Assistant Response (if prompt echoed):\\n<<<<<\\n{assistant_response}\\n>>>>>\")\n",
        "                else:\n",
        "                    # Fallback for Gemma template if prompt not exactly at start\n",
        "                    model_turn_start_token = \"<start_of_turn>model\"\n",
        "                    if model_turn_start_token in generated_text_full:\n",
        "                        last_occurrence_index = generated_text_full.rfind(model_turn_start_token)\n",
        "                        assistant_response = generated_text_full[last_occurrence_index + len(model_turn_start_token):].strip()\n",
        "                        if assistant_response.startswith(\"\\n\"): assistant_response = assistant_response[1:].strip()\n",
        "                        print(f\"Isolated Assistant Response (Gemma template heuristic):\\n<<<<<\\n{assistant_response}\\n>>>>>\")\n",
        "                    else:\n",
        "                        print(\"Could not reliably strip prompt from 'generated_text'. The full text is shown above.\")\n",
        "            else: # if return_full_text=False, generated_text is already just the new tokens\n",
        "                print(f\"Assistant Response (since return_full_text=False):\\n<<<<<\\n{generated_text_full}\\n>>>>>\")\n",
        "\n",
        "        else:\n",
        "            print(\"LLM output structure was not as expected (e.g., no 'generated_text').\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during LLM test call or processing: {e}\")\n",
        "\n",
        "# --- Main Test Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Test with return_full_text=False first ---\n",
        "    test_pipeline_rff_false, test_tokenizer_rff_false = initialize_llm_pipeline_for_test(return_full_text_setting=False)\n",
        "\n",
        "    if test_pipeline_rff_false:\n",
        "        # Test 1: Simplest possible prompt\n",
        "        prompt1 = \"What is 2+2?\"\n",
        "        run_llm_test(\"Test 1 (Simple Q&A, return_full_text=False)\", prompt1, test_pipeline_rff_false, test_tokenizer_rff_false, max_new_tok=20)\n",
        "\n",
        "        # Test 2: Simple instruction following\n",
        "        prompt2 = \"List three colors.\"\n",
        "        run_llm_test(\"Test 2 (Simple Instruction, return_full_text=False)\", prompt2, test_pipeline_rff_false, test_tokenizer_rff_false, max_new_tok=30)\n",
        "\n",
        "        # Test 3: Instruction to produce structured output (no context text yet)\n",
        "        prompt3 = \"\"\"\n",
        "        Provide a JSON object with a \"fruit\" key and a \"color\" key.\n",
        "        Example: {\"fruit\": \"apple\", \"color\": \"red\"}\n",
        "        Your JSON:\n",
        "        \"\"\"\n",
        "        run_llm_test(\"Test 3 (Simple JSON, no context, return_full_text=False)\", prompt3, test_pipeline_rff_false, test_tokenizer_rff_false, max_new_tok=50)\n",
        "\n",
        "        # Test 4: Your motif prompt with VERY SHORT placeholder context\n",
        "        short_context = \"Privacy is important. Data security is a concern. We need good access control.\"\n",
        "        motif_prompt_template = f\"\"\"You will receive a set of comments from different people answering the same question.\n",
        "\n",
        "Your task is to identify up to 2 key recurring themes.\n",
        "\n",
        "For each theme, provide:\n",
        "- A short label like [DATA_PRIVACY]\n",
        "- A 1-sentence definition\n",
        "- 1-2 short phrases that often appear in the text (surface forms)\n",
        "\n",
        "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
        "Example of one object in the list:\n",
        "{{\n",
        "  \"label\": \"[EXAMPLE_LABEL]\",\n",
        "  \"description\": \"A concise description of the example theme.\",\n",
        "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
        "}}\n",
        "If no clear motifs are found, output an empty JSON list: `[]`.\n",
        "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
        "\n",
        "Set of comments to analyze:\n",
        "\\\"\\\"\\\"\n",
        "{short_context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):\n",
        "\"\"\"\n",
        "        run_llm_test(\"Test 4 (Motif Prompt, Short Context, return_full_text=False)\", motif_prompt_template, test_pipeline_rff_false, test_tokenizer_rff_false, max_new_tok=300) # Increased max_new_tokens for JSON\n",
        "    else:\n",
        "        print(\"Skipping tests for return_full_text=False due to pipeline init failure.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # --- Test with return_full_text=True ---\n",
        "    # test_pipeline_rff_true, test_tokenizer_rff_true = initialize_llm_pipeline_for_test(return_full_text_setting=True)\n",
        "    # if test_pipeline_rff_true:\n",
        "    #     run_llm_test(\"Test 5 (Simple Q&A, return_full_text=True)\", prompt1, test_pipeline_rff_true, test_tokenizer_rff_true, max_new_tok=20)\n",
        "    #     run_llm_test(\"Test 6 (Motif Prompt, Short Context, return_full_text=True)\", motif_prompt_template, test_pipeline_rff_true, test_tokenizer_rff_true, max_new_tok=300)\n",
        "    # else:\n",
        "    #     print(\"Skipping tests for return_full_text=True due to pipeline init failure.\")\n",
        "\n",
        "    # Clean up (optional, if GPU memory is an issue between runs)\n",
        "    # del test_pipeline_rff_false, test_tokenizer_rff_false, test_pipeline_rff_true, test_tokenizer_rff_true\n",
        "    # torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8qiDsH4jYNrd",
        "outputId": "92392728-0fe4-4a17-f70c-a8669acab39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d224745da5b243898d51995805e993fd",
            "f34a78926243449088936a63e3391b83",
            "2ba36adf986545138b5b77974274a38e",
            "7c0edf9d536c40db96dcafaea2b9bad7",
            "6fbe1ad790d1466686e940746412f2ee",
            "24659fba736e4d45abc34779e731b6e2",
            "3d5cfecdb6fe487bb98ac0afe7c6997b",
            "c8bb546e611f428c917a74e92d44777e",
            "7639ab401c3a48c9a05b4bb2a11b8baa",
            "5d82ca5e0b3547dca20f4ff0b548684c",
            "2a7b7d9129184c73bd6c9bd621d404ee",
            "75fca020536047a2bdb4f2aec0deda87",
            "46775fc9f5ae4098aa0eeacc58df0a07",
            "9accd69da0494da781611cd44df2d13a",
            "47beeb3fdb564bc39f126f0d36836d30",
            "5dd88221987a4cd49ab4932aadffa3be",
            "b810986cbc104b899eb84e1e71cc04db",
            "035304554df740d1b8b527b6bc59f00a",
            "53fff4ebf541469bb3c7fecf5c2448fc",
            "a8bbf11fb7b8465889ad69d4f1e9b455",
            "cd5ff10ffc36471ea22e36c891542dd5",
            "18a7d258945544f8ad17de47ad0f3f39",
            "f4949b8049254640aafb0774728d3a49",
            "be98287f1cb1474eb41c9672b74714cf",
            "81a6266760b84e1d87e70fb025b4bee8",
            "2ab077a6a55448a58ca7f37e183b8163",
            "6b7875bd803c4e418e8561e622a63153",
            "738aae361b68489081b882339cc81482",
            "300ba463ee794a53acb6490bfd1b299b",
            "35bafdbc19be4d62a7a7bf249f6c56ec",
            "15be8bea5c254c6f85e60fb33da56586",
            "5331cd7f6ca74492bd887cb56f45571f",
            "cefec5b93d3b4540ad1bb937921b17b4",
            "080a713c867445fe8a2777f0792772f0",
            "1049fa02cd0648ada93ee10c9624669d",
            "d0c54f402d0d42f0b8d1927279110d48",
            "1dcfd5dfca9a4017ac3d6748414a6d32",
            "418f07476d0f4f4ea94caa7b34b7c83f",
            "32e986998a3f4cb4bf54872ebc9c2c91",
            "0292e030616c46b98cb37bd61d1a92d8",
            "edba6b259340447e951d7cb34a46dbba",
            "9241af92762a4e6d83a0075fa12e7349",
            "3b69c81812644cbf81d5ad0b6a5049c2",
            "2150c1812bc04898bb3b103ea2a78137",
            "46c17c4ced914b1d8f3a7c02cc9bc47a",
            "3206e6a816474f2a8ca56bc5d1f2695d",
            "e7f07947823249c9be9698a57ba1071d",
            "f8df8b5275fc4929ad8f854838325627",
            "0053e39f66484a8fa8c9c21de927b9ea",
            "acf10cfda9384aadad17f5a5f0663b7b",
            "4dd9673ab305465cbd18e65b72d52d4e",
            "9d2bbc2a497f4d14be4a6b74bf240019",
            "51d617334acf491893c385d98ca77cd7",
            "3042d9c6f746453aa1927cd49825dcd0",
            "9dda1a00a4a54e09874ab864fada8536",
            "61eddf1106d7440cbda812954e413edb",
            "b96debc777f14cd7b040ccd7f3d446c8",
            "4b8d6723fba6490fb4c6e52f625b19bd",
            "b2b83bb0d65d401090ce0dadf5df9e95",
            "0d77c3636eba48deba98b43b488df771",
            "995074e734bf4b22b032f14b2d6efa91",
            "2f5ca7c6a3d84fc0b59b4bbeed320913",
            "63ca04b3ee414c2989ca96716812788b",
            "d8de3310eccc4f248f5c5ad817bae65c",
            "d314ef166f1b40899065c3dd6a15d10b",
            "a2b56e52995c4a58ad34ee47b9194dd7",
            "aa8cd143666c435586306e025b171263",
            "444c9abb956046c7b31acf9e6c85bd77",
            "b0eaaab19576444c81d20e8ac172d1e4",
            "5b120b328d4c45ebbc7f5ba041647bdc",
            "84844dcfca08432594ebce64022c556a",
            "dd3f60df45cc41518fe1631fba735023",
            "3a96966df2034ea9962104e430c9cb27",
            "535d4696a3c44e4eb30f692d3e5d4ab0",
            "a197c176f240416f8ec9f8d111f655f1",
            "631681e611ce46f79f03a61b773b944d",
            "7320464df0d14a5f8f6e4ca037af97e1",
            "da218f6cd702439f8b0d2807fc5f5ff1",
            "e2fc40dad4724f5eb8f640798af57a45",
            "fe35820c2d8c4c20a01c82bb5f810a25",
            "629042d965e14a1680b2d917317642cf",
            "1080bea490684700a3c227bad451c11d",
            "62ee46f2eeef48e995ea8932c422dd92",
            "2a250e38cc244fbf8655409260d231a2",
            "8240628da6d741d3958937f403c864f5",
            "6390cddb130f4b65ad3024ebd5e7c758",
            "31d8a2e536284082b18c972dc80ff4d0",
            "7f505a52be90451e989d1fdc9c1a7fe3",
            "9802031d26cf414abe6e5d15cc23370d",
            "02af6cf0b16b4ed1aebffa113e565c5f",
            "e05e800bd96f4125a09aa31c7939c8fa",
            "cbc16800773440df810fefa6e74d90c3",
            "90cb4280dc9f4c268d4ff42f4f404d74",
            "7d413be70e9043b8814ce9e851bd1483",
            "58b50a4e6b8c4ab9b6d330d4cd14e6e7",
            "71d28e971ddf42ebb4306a7e40b0420a",
            "c1e348b5fcca4ac985cad4b59aabcd42",
            "45ca9a8aea234f0ab0d6e9b881f43b5f",
            "9b331fd8db0c43559be5a6e7516f1255",
            "308ef4a3942b4cc484671f9556b7f386",
            "af9118f38d6940f68edcf459b31ccec6",
            "c530bfa2c60241caaac991c17e0004f1",
            "69d57e45f99d4ea6ab9050c5c47f3d9f",
            "90a947613c6142d2b4250dbdfe57f688",
            "b82efef224464bf4aebc288e22e8747a",
            "4f09e950217c4105ab117d0121eab161",
            "8aca71d5d1b44472b9b216e17195e85a",
            "c4f7097f38474555bdfb74ae284cad0d",
            "3716ddce58c54737aec67e66e7a39a89",
            "8fb5998f0cbb4e998e8c4fab32334b08",
            "e94cb294862940c1b667df1a9eff29e6",
            "34cf4b7f66be4024bf3b709f903241b4",
            "71a9f304c38e4c908c9cde4d0d8c9548",
            "3109d1d07e834bedad2bc7967f3614a7",
            "59e3fdd6993348afa10c22ee9202b385",
            "53745263c2de4cb4ad69f51c624fb4df",
            "d3ab3acee02f422ca9c4c05c19d6d8cc",
            "a51dda8cdc284018aa92c68c1b328cc3",
            "fabe63dd0d314a93a614ffdc7713c5e0",
            "5350b263bb384f68a17b93527f13d559",
            "0090e89f5db046b7842dacf2a22f51ff"
          ]
        }
      },
      "id": "8qiDsH4jYNrd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing LLM for Test (return_full_text=False) ---\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-2b-it...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d224745da5b243898d51995805e993fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75fca020536047a2bdb4f2aec0deda87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4949b8049254640aafb0774728d3a49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "080a713c867445fe8a2777f0792772f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BNB config created for google/gemma-2b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-2b-it (Quantization: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46c17c4ced914b1d8f3a7c02cc9bc47a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61eddf1106d7440cbda812954e413edb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa8cd143666c435586306e025b171263"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da218f6cd702439f8b0d2807fc5f5ff1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9802031d26cf414abe6e5d15cc23370d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "308ef4a3942b4cc484671f9556b7f386"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e94cb294862940c1b667df1a9eff29e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-2b-it initialized successfully.\n",
            "\n",
            "--- Running Test: Test 1 (Simple Q&A, return_full_text=False) ---\n",
            "Formatted Prompt (sent to pipeline):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "What is 2+2?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            ">>>>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM call took 1.80 seconds.\n",
            "Raw 'outputs' from hf_pipeline:\n",
            "<<<<<\n",
            "[{'generated_text': 'The answer is 4. 2+2 is a simple addition problem that can be solved by'}]\n",
            ">>>>>\n",
            "'generated_text' from LLM (len 71):\n",
            "<<<<<\n",
            "The answer is 4. 2+2 is a simple addition problem that can be solved by\n",
            ">>>>>\n",
            "ERROR during LLM test call or processing: 'TextGenerationPipeline' object has no attribute 'return_full_text'\n",
            "\n",
            "--- Running Test: Test 2 (Simple Instruction, return_full_text=False) ---\n",
            "Formatted Prompt (sent to pipeline):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "List three colors.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            ">>>>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM call took 1.19 seconds.\n",
            "Raw 'outputs' from hf_pipeline:\n",
            "<<<<<\n",
            "[{'generated_text': 'Sure, here are three colors:\\n\\n1. Red\\n2. Yellow\\n3. Blue'}]\n",
            ">>>>>\n",
            "'generated_text' from LLM (len 54):\n",
            "<<<<<\n",
            "Sure, here are three colors:\n",
            "\n",
            "1. Red\n",
            "2. Yellow\n",
            "3. Blue\n",
            ">>>>>\n",
            "ERROR during LLM test call or processing: 'TextGenerationPipeline' object has no attribute 'return_full_text'\n",
            "\n",
            "--- Running Test: Test 3 (Simple JSON, no context, return_full_text=False) ---\n",
            "Formatted Prompt (sent to pipeline):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "Provide a JSON object with a \"fruit\" key and a \"color\" key.\n",
            "        Example: {\"fruit\": \"apple\", \"color\": \"red\"}\n",
            "        Your JSON:<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            ">>>>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM call took 1.44 seconds.\n",
            "Raw 'outputs' from hf_pipeline:\n",
            "<<<<<\n",
            "[{'generated_text': '```json\\n{\\n  \"fruit\": \"banana\",\\n  \"color\": \"yellow\"\\n}\\n```'}]\n",
            ">>>>>\n",
            "'generated_text' from LLM (len 56):\n",
            "<<<<<\n",
            "```json\n",
            "{\n",
            "  \"fruit\": \"banana\",\n",
            "  \"color\": \"yellow\"\n",
            "}\n",
            "```\n",
            ">>>>>\n",
            "ERROR during LLM test call or processing: 'TextGenerationPipeline' object has no attribute 'return_full_text'\n",
            "\n",
            "--- Running Test: Test 4 (Motif Prompt, Short Context, return_full_text=False) ---\n",
            "Formatted Prompt (sent to pipeline):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 2 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 1-2 short phrases that often appear in the text (surface forms)\n",
            "\n",
            "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
            "Example of one object in the list:\n",
            "{\n",
            "  \"label\": \"[EXAMPLE_LABEL]\",\n",
            "  \"description\": \"A concise description of the example theme.\",\n",
            "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
            "}\n",
            "If no clear motifs are found, output an empty JSON list: `[]`.\n",
            "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
            "\n",
            "Set of comments to analyze:\n",
            "\"\"\"\n",
            "Privacy is important. Data security is a concern. We need good access control.\n",
            "\"\"\"\n",
            "\n",
            "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            ">>>>>\n",
            "LLM call took 3.18 seconds.\n",
            "Raw 'outputs' from hf_pipeline:\n",
            "<<<<<\n",
            "[{'generated_text': '```json\\n[\\n  {\\n    \"label\": \"[DATA_PRIVACY]\",\\n    \"description\": \"A concern related to the protection of personal data.\",\\n    \"surface_forms\": [\"Privacy is important.\"]\\n  }\\n]\\n```'}]\n",
            ">>>>>\n",
            "'generated_text' from LLM (len 177):\n",
            "<<<<<\n",
            "```json\n",
            "[\n",
            "  {\n",
            "    \"label\": \"[DATA_PRIVACY]\",\n",
            "    \"description\": \"A concern related to the protection of personal data.\",\n",
            "    \"surface_forms\": [\"Privacy is important.\"]\n",
            "  }\n",
            "]\n",
            "```\n",
            ">>>>>\n",
            "ERROR during LLM test call or processing: 'TextGenerationPipeline' object has no attribute 'return_full_text'\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1st June\n",
        "\n",
        "Debugging empty motifs returned"
      ],
      "metadata": {
        "id": "j3QgjmpQLLo4"
      },
      "id": "j3QgjmpQLLo4"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title revised call_local_llm_for_motifs()\n",
        "def call_local_llm_for_motifs(prompt_str: str, hf_pipeline, hf_tokenizer, qid_for_log: str, chunk_idx_for_log: int) -> str: # Added qid and chunk_idx for logging\n",
        "    \"\"\"Makes the actual call to the local LLM pipeline and returns raw text.\"\"\"\n",
        "    messages_for_template = [{\"role\": \"user\", \"content\": prompt_str}]\n",
        "    prompt_formatted_for_llm = hf_tokenizer.apply_chat_template(\n",
        "        messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": 700,\n",
        "        \"do_sample\": False,    # Keep this False for deterministic output for now\n",
        "        \"pad_token_id\": hf_tokenizer.eos_token_id,\n",
        "        # The warning \"generation flags are not valid\" for temp, top_p, top_k is fine when do_sample=False\n",
        "    }\n",
        "\n",
        "    print(f\"    DEBUG (call_local_llm): Sending prompt to LLM for QID {qid_for_log}, Chunk {chunk_idx_for_log} (Prompt length: {len(prompt_formatted_for_llm)} chars). First 300 chars of formatted prompt:\\n<<<<<\\n{prompt_formatted_for_llm[:300]}...\\n>>>>>\") # Log part of the prompt\n",
        "\n",
        "    outputs = hf_pipeline(prompt_formatted_for_llm, **generation_args)\n",
        "\n",
        "    print(f\"    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{outputs}\\n>>>>>\") # VERY IMPORTANT DEBUG LINE\n",
        "\n",
        "    if not outputs or not isinstance(outputs, list) or not outputs[0] or not isinstance(outputs[0], dict) or 'generated_text' not in outputs[0] or not outputs[0]['generated_text']:\n",
        "        print(f\"    WARN (call_local_llm): LLM pipeline returned unexpected or empty structure for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return \"\" # Return empty string on pipeline failure\n",
        "\n",
        "    generated_text_full = outputs[0]['generated_text']\n",
        "    print(f\"    DEBUG (call_local_llm): 'generated_text_full' for QID {qid_for_log}, Chunk {chunk_idx_for_log} (len {len(generated_text_full)}):\\n<<<<<\\n{generated_text_full[:1000]}...\\n>>>>>\") # Log first 1000 chars\n",
        "\n",
        "    # Extract only the assistant's response part\n",
        "    assistant_response_text = \"\"\n",
        "    # Logic for stripping prompt if return_full_text=True (which is typical for pipeline)\n",
        "    if generated_text_full.startswith(prompt_formatted_for_llm):\n",
        "        assistant_response_text = generated_text_full[len(prompt_formatted_for_llm):].strip()\n",
        "    else:\n",
        "        # Fallback for models/templates that might structure output differently\n",
        "        # (e.g., with specific role tokens like Gemma's <start_of_turn>model)\n",
        "        model_turn_start_token = \"<start_of_turn>model\"\n",
        "        if model_turn_start_token in generated_text_full:\n",
        "            # Find the *last* occurrence of the model turn start, in case of complex multi-turn internal thoughts\n",
        "            last_occurrence_index = generated_text_full.rfind(model_turn_start_token)\n",
        "            potential_response = generated_text_full[last_occurrence_index + len(model_turn_start_token):].strip()\n",
        "            if potential_response.startswith(\"\\n\"):\n",
        "                potential_response = potential_response[1:].strip()\n",
        "\n",
        "            # Heuristic: If this potential response is much shorter than the full text, it's likely correct\n",
        "            # Also check if it doesn't look like the prompt itself\n",
        "            if len(potential_response) < len(generated_text_full) * 0.8 and \"TEXT TO ANALYZE:\" not in potential_response:\n",
        "                 assistant_response_text = potential_response\n",
        "            else: # If heuristic fails, maybe the prompt wasn't echoed as expected\n",
        "                 assistant_response_text = generated_text_full.strip()\n",
        "                 print(f\"    WARN (call_local_llm): Could not reliably strip prompt/template. Using full output as response for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        else:\n",
        "            assistant_response_text = generated_text_full.strip()\n",
        "            print(f\"    WARN (call_local_llm): Prompt/template start not found. Using full output as response for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "\n",
        "    print(f\"    DEBUG (call_local_llm): 'assistant_response_text' (after stripping) for QID {qid_for_log}, Chunk {chunk_idx_for_log} (len {len(assistant_response_text)}):\\n<<<<<\\n{assistant_response_text[:1000]}...\\n>>>>>\")\n",
        "    return assistant_response_text"
      ],
      "metadata": {
        "id": "Pwb7cLqlLeL-"
      },
      "id": "Pwb7cLqlLeL-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 31th May"
      ],
      "metadata": {
        "id": "WwjN3dyXr66v"
      },
      "id": "WwjN3dyXr66v"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Batching and Revised Prompting\n",
        "# --- Imports ---\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict # For type hinting\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # !!! EXAMPLE - UPDATE THIS PATH !!!\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"] # Process only Q4 for this example\n",
        "\n",
        "# --- BDM and LLM Model Configuration ---\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-3-4b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "LLM_BATCH_SIZE = 5 # Number of responses per chunk for LLM processing\n",
        "LLM_RETRY_ATTEMPTS = 2 # Number of retries for LLM calls\n",
        "MAX_TEXT_PER_LLM_PROMPT_CHUNK = 7500 # Max characters for the text_block in a single LLM prompt\n",
        "\n",
        "# --- Token-Based L(H) Configuration ---\n",
        "MOTIF_SYMBOLIC_LABEL_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TEXT_BASE_COST = 0.5\n",
        "MOTIF_DESCRIPTION_TOKEN_COST = 0.1\n",
        "MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.25\n",
        "MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.1\n",
        "\n",
        "# --- Logging File ---\n",
        "LLM_DEBUG_LOG_FILE = \"llm_motif_debug_log.txt\"\n",
        "\n",
        "# --- Helper Function Definitions (Tokenization, L(H), BDM, Compression) ---\n",
        "\n",
        "def tokenize_phrase(phrase_text): # Renamed for clarity\n",
        "    \"\"\"Simple tokenizer for phrases/definitions.\"\"\"\n",
        "    if not isinstance(phrase_text, str): return []\n",
        "    phrase_text = phrase_text.lower()\n",
        "    # Basic cleaning, can be expanded\n",
        "    tokens = phrase_text.split()\n",
        "    return [t for t in tokens if t]\n",
        "\n",
        "def calculate_L_H_token_based_structured(structured_motifs_list: List[Dict]) -> float:\n",
        "    \"\"\"Calculates L(H) using token-based costs from structured motif objects.\"\"\"\n",
        "    if not structured_motifs_list: return 0.0\n",
        "    total_lh_cost = 0.0\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        current_motif_lh = 0\n",
        "        label_str = motif_obj.get('label', \"\")\n",
        "        if label_str and isinstance(label_str, str) and label_str.strip():\n",
        "            current_motif_lh += MOTIF_SYMBOLIC_LABEL_COST\n",
        "\n",
        "        description_str = motif_obj.get('description', \"\") # Using \"description\" key\n",
        "        if description_str and isinstance(description_str, str) and description_str.strip():\n",
        "            current_motif_lh += MOTIF_DESCRIPTION_TEXT_BASE_COST\n",
        "            current_motif_lh += len(tokenize_phrase(description_str)) * MOTIF_DESCRIPTION_TOKEN_COST\n",
        "\n",
        "        surface_forms_list = motif_obj.get('surface_forms', [])\n",
        "        if surface_forms_list and isinstance(surface_forms_list, list):\n",
        "            valid_sfs_for_lh = [sf for sf in surface_forms_list if isinstance(sf, str) and sf.strip()]\n",
        "            if valid_sfs_for_lh:\n",
        "                current_motif_lh += MOTIF_SURFACE_FORMS_LIST_BASE_COST\n",
        "                for sf_str in valid_sfs_for_lh:\n",
        "                    current_motif_lh += len(tokenize_phrase(sf_str)) * MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH\n",
        "        total_lh_cost += current_motif_lh\n",
        "    return total_lh_cost\n",
        "\n",
        "def llm_compress_text_structured(text_to_compress: str, structured_motifs_list: List[Dict]) -> str:\n",
        "    \"\"\"Compresses text by replacing surface forms with their motif labels.\"\"\"\n",
        "    if not isinstance(text_to_compress, str): return \"\"\n",
        "    if not structured_motifs_list: return text_to_compress.lower()\n",
        "\n",
        "    compressed_text = text_to_compress.lower()\n",
        "\n",
        "    # For this version, iterate through motifs as provided.\n",
        "    # More advanced: sort motifs by some priority (e.g. length of longest SF)\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "\n",
        "        label = motif_obj.get('label', None)\n",
        "        surface_forms = motif_obj.get('surface_forms', [])\n",
        "\n",
        "        if not label or not surface_forms or not isinstance(surface_forms, list):\n",
        "            continue\n",
        "\n",
        "        placeholder = label # Use the actual symbolic label\n",
        "\n",
        "        # Sort this motif's own surface forms by length (descending) for greedy matching\n",
        "        sorted_sfs_for_this_motif = sorted(\n",
        "            [sf for sf in surface_forms if isinstance(sf, str) and sf.strip()],\n",
        "            key=len,\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        for sf_str in sorted_sfs_for_this_motif:\n",
        "            try:\n",
        "                # Important: Ensure surface forms are also lowercased for matching\n",
        "                compressed_text = re.sub(re.escape(sf_str.lower()), placeholder, compressed_text, flags=re.IGNORECASE)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for SF '{sf_str}' of motif '{label}': {re_e}. Skipping.\")\n",
        "                continue\n",
        "    return compressed_text\n",
        "\n",
        "def text_to_binary_matrix(text_input: str, size=MATRIX_SIZE_GLOBAL) -> np.ndarray:\n",
        "    \"\"\"Converts text to a binary matrix using SHA256 hash.\"\"\"\n",
        "    if not text_input or not isinstance(text_input, str) or not text_input.strip():\n",
        "        return np.zeros(size, dtype=int)\n",
        "    hash_obj = hashlib.sha256(text_input.encode('utf-8', 'ignore'))\n",
        "    hash_digest = hash_obj.hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string = bin(int(hash_digest, 16))[2:].zfill(256) # SHA256 produces 256 bits\n",
        "    binary_string_padded = binary_string.ljust(required_bits, '0') # Pad if required_bits > 256\n",
        "    bits = [int(b) for b in binary_string_padded[:required_bits]]\n",
        "    return np.array(bits).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input: str, bdm_instance: BDM, matrix_s=MATRIX_SIZE_GLOBAL) -> float:\n",
        "    \"\"\"Computes BDM for a given text string.\"\"\"\n",
        "    if not text_input or not isinstance(text_input, str) or not text_input.strip() : return 0.0\n",
        "\n",
        "    MAX_TEXT_FOR_BDM_HASH = 2000 # Consider if this truncation is still desired\n",
        "    text_for_hash = text_input if len(text_input) <= MAX_TEXT_FOR_BDM_HASH else text_input[:MAX_TEXT_FOR_BDM_HASH]\n",
        "\n",
        "    matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        bdm_value = bdm_instance.bdm(matrix)\n",
        "        return bdm_value\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0 # Indicate error\n",
        "\n",
        "def compute_mdl_cost_for_text_block(full_qid_corpus_str: str,\n",
        "                                    final_consolidated_motifs: List[Dict],\n",
        "                                    bdm_instance: BDM,\n",
        "                                    matrix_s=MATRIX_SIZE_GLOBAL) -> tuple[float, float, float]:\n",
        "    \"\"\"Computes L(H), L(D|H), and Total MDL for a text block given consolidated motifs.\"\"\"\n",
        "    if not isinstance(full_qid_corpus_str, str) : full_qid_corpus_str = \"\"\n",
        "\n",
        "    l_h = calculate_L_H_token_based_structured(final_consolidated_motifs)\n",
        "\n",
        "    compressed_text_block = llm_compress_text_structured(full_qid_corpus_str, final_consolidated_motifs)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "\n",
        "    if l_d_h < 0: # BDM error\n",
        "        return l_h, -1.0, -1.0\n",
        "\n",
        "    return l_h, l_d_h, l_h + l_d_h\n",
        "\n",
        "# --- LLM Motif Extraction (Batched, with your prompt) ---\n",
        "\n",
        "def build_llm_prompt_for_motifs(text_block_for_prompt: str) -> str:\n",
        "    \"\"\"Builds the prompt for the LLM to extract motifs.\"\"\"\n",
        "    # Ensure text_block_for_prompt is not excessively long for the prompt itself\n",
        "    if len(text_block_for_prompt) > MAX_TEXT_PER_LLM_PROMPT_CHUNK:\n",
        "        text_block_for_prompt = text_block_for_prompt[:MAX_TEXT_PER_LLM_PROMPT_CHUNK]\n",
        "        # print(f\"    Note: Text block for LLM prompt truncated to {MAX_TEXT_PER_LLM_PROMPT_CHUNK} chars.\")\n",
        "\n",
        "    prompt = f\"\"\"You will receive a set of comments from different people answering the same question.\n",
        "\n",
        "Your task is to identify up to 5 key recurring themes.\n",
        "\n",
        "For each theme, provide:\n",
        "- A short label like [DATA_PRIVACY]\n",
        "- A 1-sentence definition\n",
        "- 2–3 short phrases that often appear in the text (surface forms)\n",
        "\n",
        "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
        "Example of one object in the list:\n",
        "{{\n",
        "  \"label\": \"[EXAMPLE_LABEL]\",\n",
        "  \"description\": \"A concise description of the example theme.\",\n",
        "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
        "}}\n",
        "If no clear motifs are found, output an empty JSON list: `[]`.\n",
        "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
        "\n",
        "Set of comments to analyze:\n",
        "'''\n",
        "{text_block_for_prompt}\n",
        "'''\n",
        "\n",
        "Valid JSON Output (ensure it's a list of objects, or an empty list [] if no themes):\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "\n",
        "# def call_local_llm_for_motifs(prompt_str: str, hf_pipeline, hf_tokenizer) -> str:\n",
        "#     \"\"\"Makes the actual call to the local LLM pipeline and returns raw text.\"\"\"\n",
        "#     messages_for_template = [{\"role\": \"user\", \"content\": prompt_str}]\n",
        "#     prompt_formatted_for_llm = hf_tokenizer.apply_chat_template(\n",
        "#         messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "#     )\n",
        "\n",
        "#     generation_args = {\n",
        "#         \"max_new_tokens\": 700, # Allow space for JSON with multiple motifs\n",
        "#         \"do_sample\": False,\n",
        "#         \"pad_token_id\": hf_tokenizer.eos_token_id,\n",
        "#     }\n",
        "#     outputs = hf_pipeline(prompt_formatted_for_llm, **generation_args)\n",
        "\n",
        "#     if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n",
        "#         # print(f\"    LLM pipeline returned unexpected/empty output.\") # Logged by caller\n",
        "#         return \"\" # Return empty string on pipeline failure\n",
        "\n",
        "#     generated_text_full = outputs[0]['generated_text']\n",
        "\n",
        "#     # Extract only the assistant's response part\n",
        "#     assistant_response_text = \"\"\n",
        "#     if generated_text_full.startswith(prompt_formatted_for_llm): # Check if prompt is echoed\n",
        "#         assistant_response_text = generated_text_full[len(prompt_formatted_for_llm):].strip()\n",
        "#     else:\n",
        "#         model_turn_start_token = \"<start_of_turn>model\" # Common for Gemma instruct\n",
        "#         if model_turn_start_token in generated_text_full:\n",
        "#             last_occurrence_index = generated_text_full.rfind(model_turn_start_token)\n",
        "#             assistant_response_text = generated_text_full[last_occurrence_index + len(model_turn_start_token):].strip()\n",
        "#             if assistant_response_text.startswith(\"\\n\"): # Remove leading newline if present\n",
        "#                 assistant_response_text = assistant_response_text[1:].strip()\n",
        "#         else:\n",
        "#             assistant_response_text = generated_text_full.strip() # Fallback\n",
        "#     return assistant_response_text\n",
        "\n",
        "\n",
        "def extract_motifs_from_llm_response(llm_response_str: str, qid_for_log:str, chunk_idx_for_log:int, prompt_sent:str) -> List[Dict]:\n",
        "    \"\"\"Parses the LLM's raw text response to extract structured motifs.\"\"\"\n",
        "    # print(f\"    DEBUG (extract_motifs): Raw LLM response for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{llm_response_str}\\n>>>>>\")\n",
        "\n",
        "    json_str_candidate = llm_response_str # Start with the full response\n",
        "\n",
        "    # Heuristic to find JSON block if LLM wraps it (though prompt asks not to)\n",
        "    json_block_match_markdown = re.search(r\"```json\\s*([\\s\\S]*?)\\s*```\", llm_response_str)\n",
        "    if json_block_match_markdown:\n",
        "        json_str_candidate = json_block_match_markdown.group(1).strip()\n",
        "    else:\n",
        "        # If no markdown, try to find first '[' and last ']' as a simpler heuristic\n",
        "        first_bracket = llm_response_str.find('[')\n",
        "        last_bracket = llm_response_str.rfind(']')\n",
        "        if first_bracket != -1 and last_bracket != -1 and last_bracket > first_bracket:\n",
        "            json_str_candidate = llm_response_str[first_bracket : last_bracket+1].strip()\n",
        "        # Else, assume the whole response is the JSON string (might fail if it's not)\n",
        "\n",
        "    # print(f\"    DEBUG (extract_motifs): Final json_str candidate for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{json_str_candidate}\\n>>>>>\")\n",
        "\n",
        "    if not json_str_candidate or json_str_candidate.lower() == \"[]\":\n",
        "        # print(f\"    LLM indicated no themes or JSON was empty for QID {qid_for_log}, Chunk {chunk_idx_for_log}.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        parsed_json = json.loads(json_str_candidate)\n",
        "        if not isinstance(parsed_json, list):\n",
        "            # print(f\"    WARN (extract_motifs): LLM output was not a JSON list for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {parsed_json}\")\n",
        "            raise ValueError(\"Response was not a list\")\n",
        "\n",
        "        valid_motifs_from_json = []\n",
        "        for item in parsed_json:\n",
        "            if isinstance(item, dict) and \\\n",
        "               'label' in item and isinstance(item['label'], str) and \\\n",
        "               'description' in item and isinstance(item['description'], str) and \\\n",
        "               item['label'].startswith('[') and item['label'].endswith(']'):\n",
        "\n",
        "                sf = item.get('surface_forms', [])\n",
        "                if not isinstance(sf, list) or not all(isinstance(s, str) for s in sf):\n",
        "                    sf = []\n",
        "\n",
        "                valid_motifs_from_json.append({\n",
        "                    \"label\": item['label'].strip(),\n",
        "                    \"description\": item['description'].strip(),\n",
        "                    \"surface_forms\": [s.strip() for s in sf if s.strip()]\n",
        "                })\n",
        "            # else:\n",
        "                # print(f\"    WARN (extract_motifs): Invalid item structure in LLM JSON for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {item}\")\n",
        "        return valid_motifs_from_json\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"    [WARN] Motif JSON parsing failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {e}\")\n",
        "        # Log the problematic parts for debugging\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} ---\\n\")\n",
        "            f.write(f\"PROMPT SENT (first 500 chars):\\n{prompt_sent[:500]}...\\n\")\n",
        "            f.write(f\"RAW LLM RESPONSE (导致 JSONDecodeError):\\n{llm_response_str}\\n\")\n",
        "            f.write(f\"EXTRACTED JSON STRING CANDIDATE (导致 JSONDecodeError):\\n{json_str_candidate}\\n\")\n",
        "        return []\n",
        "    except ValueError as ve: # Handles \"Response was not a list\"\n",
        "        print(f\"    [WARN] Motif structure validation failed for QID {qid_for_log}, Chunk {chunk_idx_for_log}: {ve}\")\n",
        "        with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx_for_log} ---\\n\")\n",
        "            f.write(f\"PROMPT SENT (first 500 chars):\\n{prompt_sent[:500]}...\\n\")\n",
        "            f.write(f\"RAW LLM RESPONSE (导致 ValueError):\\n{llm_response_str}\\n\")\n",
        "            f.write(f\"EXTRACTED JSON STRING CANDIDATE (导致 ValueError):\\n{json_str_candidate}\\n\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def get_motifs_for_text_chunks(\n",
        "    list_of_response_strings: List[str],\n",
        "    batch_size: int,\n",
        "    hf_pipeline,\n",
        "    hf_tokenizer,\n",
        "    qid_for_log: str # For logging context\n",
        "    ) -> List[Dict]:\n",
        "    \"\"\"Processes responses in batches, calls LLM for each, aggregates motifs.\"\"\"\n",
        "\n",
        "    all_extracted_motifs_from_all_chunks = []\n",
        "\n",
        "    # Create chunks of text, where each chunk is a join of 'batch_size' responses\n",
        "    batched_text_blocks = []\n",
        "    for i in range(0, len(list_of_response_strings), batch_size):\n",
        "        current_batch_responses = list_of_response_strings[i:i + batch_size]\n",
        "        # Join individual responses within a batch with a clear separator\n",
        "        # This text_block is what goes into the LLM prompt\n",
        "        text_block_for_chunk = \"\\n\\n<RSP_SEP>\\n\\n\".join(current_batch_responses)\n",
        "        batched_text_blocks.append(text_block_for_chunk)\n",
        "\n",
        "    print(f\"  QID {qid_for_log}: Processing {len(list_of_response_strings)} responses in {len(batched_text_blocks)} chunks (batch size: {batch_size} responses).\")\n",
        "\n",
        "    for chunk_idx, text_chunk_to_analyze in enumerate(batched_text_blocks):\n",
        "        print(f\"    Analyzing chunk {chunk_idx + 1}/{len(batched_text_blocks)} for QID {qid_for_log} (len: {len(text_chunk_to_analyze)} chars)...\")\n",
        "\n",
        "        if len(text_chunk_to_analyze.strip()) < 50: # Skip very small chunks\n",
        "            print(f\"      Chunk {chunk_idx+1} for QID {qid_for_log} too short, skipping.\")\n",
        "            continue\n",
        "\n",
        "        prompt_for_llm = build_llm_prompt_for_motifs(text_chunk_to_analyze)\n",
        "        motifs_from_this_chunk = []\n",
        "\n",
        "        for attempt in range(LLM_RETRY_ATTEMPTS):\n",
        "            try:\n",
        "                # raw_llm_response_text = call_local_llm_for_motifs(prompt_for_llm, hf_pipeline, hf_tokenizer)\n",
        "                # VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\n",
        "                # FIX IS HERE: Pass qid_for_log and chunk_idx + 1\n",
        "                # VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\n",
        "                raw_llm_response_text = call_local_llm_for_motifs(\n",
        "                    prompt_for_llm,\n",
        "                    hf_pipeline,\n",
        "                    hf_tokenizer,\n",
        "                    qid_for_log,      # Pass the QID for this whole batch operation\n",
        "                    chunk_idx + 1     # Pass the current chunk number (1-based for logging)\n",
        "                )\n",
        "                # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "                # END OF FIX\n",
        "                # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "                if not raw_llm_response_text:\n",
        "                    print(f\"      LLM call attempt {attempt + 1} for chunk {chunk_idx+1} (QID {qid_for_log}) returned empty. Retrying if possible...\")\n",
        "                    if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "                    continue # Go to next attempt or fail\n",
        "\n",
        "                motifs_attempt_from_chunk = extract_motifs_from_llm_response(raw_llm_response_text, qid_for_log, chunk_idx+1, prompt_for_llm)\n",
        "\n",
        "                if motifs_attempt_from_chunk: # If non-empty list (successful parse and valid structure)\n",
        "                    motifs_from_this_chunk = motifs_attempt_from_chunk\n",
        "                    break # Success, exit retry loop\n",
        "                else: # Parsing failed or LLM returned empty valid JSON like []\n",
        "                    print(f\"      Motif extraction/parsing attempt {attempt + 1} yielded no valid motifs for chunk {chunk_idx+1} (QID {qid_for_log}). Retrying if possible...\")\n",
        "                    if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "\n",
        "            except Exception as e_call: # Catch errors from call_local_llm_for_motifs itself\n",
        "                 print(f\"      Critical error during LLM call attempt {attempt + 1} for chunk {chunk_idx+1} (QID {qid_for_log}): {e_call}\")\n",
        "                 if attempt < LLM_RETRY_ATTEMPTS - 1: time.sleep(1)\n",
        "                 # Log error to debug file\n",
        "                 with open(LLM_DEBUG_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(f\"\\n--- QID: {qid_for_log} --- CHUNK: {chunk_idx+1} --- ATTEMPT: {attempt+1} ---\\n\")\n",
        "                    f.write(f\"PROMPT SENT (first 500 chars):\\n{prompt_for_llm[:500]}...\\n\")\n",
        "                    f.write(f\"CRITICAL LLM CALL ERROR:\\n{e_call}\\n\")\n",
        "\n",
        "\n",
        "        if motifs_from_this_chunk:\n",
        "            print(f\"      Extracted {len(motifs_from_this_chunk)} motif objects from chunk {chunk_idx+1} (QID {qid_for_log}).\")\n",
        "            all_extracted_motifs_from_all_chunks.extend(motifs_from_this_chunk)\n",
        "        else:\n",
        "            print(f\"      No valid motifs extracted from chunk {chunk_idx+1} (QID {qid_for_log}) after {LLM_RETRY_ATTEMPTS} attempts.\")\n",
        "\n",
        "    return all_extracted_motifs_from_all_chunks\n",
        "\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "def main():\n",
        "    print(\"--- MDL Prototype: Batched LLM Motif Extraction, Structured Motifs, Token-L(H) ---\")\n",
        "    print(f\"Using L(H) Cost Params: Label={MOTIF_SYMBOLIC_LABEL_COST}, DescBase={MOTIF_DESCRIPTION_TEXT_BASE_COST}, DescToken={MOTIF_DESCRIPTION_TOKEN_COST}, SFListBase={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, SFTokenInLH={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\")\n",
        "    print(f\"LLM Batch Size: {LLM_BATCH_SIZE} responses. LLM Retries: {LLM_RETRY_ATTEMPTS}. Max Text per Prompt Chunk: {MAX_TEXT_PER_LLM_PROMPT_CHUNK}\")\n",
        "\n",
        "    # Clear or create debug log file\n",
        "    with open(LLM_DEBUG_LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"LLM Motif Debug Log - {time.asctime()}\\n\")\n",
        "        f.write(f\"Model: {LOCAL_LLM_MODEL_ID}\\n\")\n",
        "\n",
        "    local_llm_pipeline_instance = None\n",
        "    local_llm_tokenizer_instance = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {LOCAL_LLM_MODEL_ID}...\")\n",
        "        local_llm_tokenizer_instance = AutoTokenizer.from_pretrained(LOCAL_LLM_MODEL_ID)\n",
        "        bnb_config = None\n",
        "        quant_active = False\n",
        "        if USE_QUANTIZATION_FOR_LOCAL_LLM and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n",
        "                quant_active = True\n",
        "                print(f\"BNB config created for {LOCAL_LLM_MODEL_ID}, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb:\n",
        "                print(f\"WARN: Failed to create BitsAndBytesConfig: {e_bnb}. Quantization disabled.\")\n",
        "        print(f\"Loading local model {LOCAL_LLM_MODEL_ID} (Quantization: {quant_active})...\")\n",
        "        model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "        if quant_active: model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        else:\n",
        "            if device.type == 'cuda': model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "        local_llm_model_instance = AutoModelForCausalLM.from_pretrained(LOCAL_LLM_MODEL_ID, **model_kwargs)\n",
        "        local_llm_pipeline_instance = pipeline(\"text-generation\", model=local_llm_model_instance, tokenizer=local_llm_tokenizer_instance, return_full_text=True) # return_full_text=True for easier splitting\n",
        "        print(f\"Local LLM pipeline for {LOCAL_LLM_MODEL_ID} initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to initialize local LLM pipeline: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        bdm_instance_main = BDM(ndim=2) # CTM often slower, NKS default is fine\n",
        "        print(\"BDM instance initialized successfully.\")\n",
        "    except Exception as e_bdm_init:\n",
        "        print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(P2_COLLATED_FILE):\n",
        "        print(f\"ERROR: Phase 2 output file not found: {P2_COLLATED_FILE}\")\n",
        "        return\n",
        "    print(f\"Loading Phase 2 output from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f:\n",
        "            phase2_data_content = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or parsing {P2_COLLATED_FILE}: {e}\")\n",
        "        return\n",
        "\n",
        "    all_qid_mdl_results = []\n",
        "    qids_to_process_final = []\n",
        "    aggregated_content_by_qid_map = {}\n",
        "\n",
        "    if phase2_data_content:\n",
        "        aggregated_content_by_qid_map = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "\n",
        "    if not aggregated_content_by_qid_map:\n",
        "        print(f\"No 'aggregated_pdf_content_by_qid' key found or data is empty in {P2_COLLATED_FILE}.\")\n",
        "        return\n",
        "\n",
        "    if P3_QIDS_TO_PROCESS_THEMATICALLY and isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and P3_QIDS_TO_PROCESS_THEMATICALLY:\n",
        "        qids_to_process_final = [qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY if qid in aggregated_content_by_qid_map]\n",
        "        print(f\"Targeting specific QIDs from P3_QIDS_TO_PROCESS_THEMATICALLY: {qids_to_process_final}\")\n",
        "        if not qids_to_process_final:\n",
        "            print(f\"Warning: None of QIDs {P3_QIDS_TO_PROCESS_THEMATICALLY} found in loaded data.\")\n",
        "            return\n",
        "    else:\n",
        "        # Fallback to process a limited number of QIDs if P3_QIDS_TO_PROCESS_THEMATICALLY is not set\n",
        "        qids_to_process_limit_fallback = 1\n",
        "        print(f\"P3_QIDS_TO_PROCESS_THEMATICALLY not set or empty. Processing up to {qids_to_process_limit_fallback} QID(s) as a fallback.\")\n",
        "        qids_to_process_final = list(aggregated_content_by_qid_map.keys())[:qids_to_process_limit_fallback]\n",
        "        if not qids_to_process_final:\n",
        "            print(\"No QIDs available to process based on fallback.\")\n",
        "            return\n",
        "    print(f\"\\nMDL analysis will run for these QIDs: {qids_to_process_final}\\n\")\n",
        "\n",
        "    for qid_str in qids_to_process_final:\n",
        "        list_of_individual_responses = aggregated_content_by_qid_map.get(qid_str) # This should be a list of text dicts\n",
        "\n",
        "        if not list_of_individual_responses or not isinstance(list_of_individual_responses, list):\n",
        "            print(f\"No valid responses found for QID {qid_str}, or format is incorrect. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Extract just the text from the response dicts\n",
        "        actual_response_text_strings = [\n",
        "            item.get(\"text\", \"\") for item in list_of_individual_responses\n",
        "            if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()\n",
        "        ]\n",
        "\n",
        "        if not actual_response_text_strings:\n",
        "            print(f\"No valid text strings extracted from responses for QID {qid_str}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"--- Analyzing Aggregated Text for QID: {qid_str} ---\")\n",
        "        # The full corpus for this QID is needed for baseline and final L(D|H)\n",
        "        full_corpus_for_qid_str = \"\\n\\n<RSP_SEP>\\n\\n\".join(actual_response_text_strings)\n",
        "\n",
        "        if len(full_corpus_for_qid_str.strip()) < 100: # Arbitrary short corpus threshold\n",
        "            print(f\"  Skipping QID {qid_str}: combined text too short ({len(full_corpus_for_qid_str)} chars).\")\n",
        "            continue\n",
        "\n",
        "        num_responses_for_qid = len(actual_response_text_strings)\n",
        "        print(f\"  Combined corpus for QID {qid_str} has {len(full_corpus_for_qid_str)} chars from {num_responses_for_qid} individual responses.\")\n",
        "\n",
        "        # Calculate baseline L(D) for the entire QID's text\n",
        "        baseline_l_d_original = compute_bdm_for_text(full_corpus_for_qid_str, bdm_instance_main, MATRIX_SIZE_GLOBAL)\n",
        "        if baseline_l_d_original < 0:\n",
        "            print(f\"  Error computing baseline BDM for QID {qid_str}. Skipping this QID.\")\n",
        "            continue\n",
        "        baseline_total_mdl_cost = baseline_l_d_original # L(H) is 0 for baseline\n",
        "        print(f\"  Baseline MDL for QID {qid_str} (L(D_orig)): {baseline_total_mdl_cost:.4f}\")\n",
        "\n",
        "        # --- New Batched Motif Extraction ---\n",
        "        raw_motifs_from_all_chunks = get_motifs_for_text_chunks(\n",
        "            actual_response_text_strings, # Pass list of individual response strings\n",
        "            LLM_BATCH_SIZE,\n",
        "            local_llm_pipeline_instance,\n",
        "            local_llm_tokenizer_instance,\n",
        "            qid_str # For logging\n",
        "        )\n",
        "\n",
        "        if not raw_motifs_from_all_chunks:\n",
        "            print(f\"  No raw motifs extracted by LLM for QID {qid_str} from any chunk.\")\n",
        "            # Log baseline result\n",
        "            all_qid_mdl_results.append({\n",
        "                \"qid\": qid_str, \"corpus_len_for_qid\": len(full_corpus_for_qid_str), \"num_responses\": num_responses_for_qid,\n",
        "                \"baseline_mdl\": baseline_total_mdl_cost, \"final_motifs\": [],\n",
        "                \"l_h_motifs\": 0.0, \"l_d_h_motifs\": baseline_total_mdl_cost, \"total_mdl_motifs\": baseline_total_mdl_cost,\n",
        "                \"compression_achieved\": 0.0, \"num_raw_motifs_extracted\": 0, \"num_consolidated_motifs\": 0\n",
        "            })\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        print(f\"  Extracted {len(raw_motifs_from_all_chunks)} raw motif objects from LLM for QID {qid_str} (across all chunks).\")\n",
        "\n",
        "        # --- Motif Consolidation Step ---\n",
        "        consolidated_motifs_temp_dict = {}\n",
        "        for motif_obj_raw in raw_motifs_from_all_chunks:\n",
        "            label = motif_obj_raw.get(\"label\")\n",
        "            description = motif_obj_raw.get(\"description\")\n",
        "            surface_forms = motif_obj_raw.get(\"surface_forms\")\n",
        "\n",
        "            if label and description and surface_forms is not None: # Basic validation\n",
        "                if label not in consolidated_motifs_temp_dict:\n",
        "                    consolidated_motifs_temp_dict[label] = motif_obj_raw\n",
        "                else: # Label exists, merge surface forms (simple union)\n",
        "                    existing_sfs_set = set(consolidated_motifs_temp_dict[label].get(\"surface_forms\", []))\n",
        "                    new_sfs_set = set(surface_forms)\n",
        "                    # Keep original description from first encounter of label\n",
        "                    consolidated_motifs_temp_dict[label][\"surface_forms\"] = sorted(list(existing_sfs_set.union(new_sfs_set)))\n",
        "            # else:\n",
        "                # print(f\"    [WARN] Invalid raw motif object structure skipped during QID {qid_str} consolidation: {motif_obj_raw}\")\n",
        "\n",
        "        final_motifs_for_qid = list(consolidated_motifs_temp_dict.values())\n",
        "        print(f\"  Consolidated into {len(final_motifs_for_qid)} unique motifs (by label) for QID {qid_str}.\")\n",
        "\n",
        "        if not final_motifs_for_qid:\n",
        "            print(f\"  No unique motifs left after consolidation for QID {qid_str}.\")\n",
        "            # Log baseline result\n",
        "            all_qid_mdl_results.append({\n",
        "                \"qid\": qid_str, \"corpus_len_for_qid\": len(full_corpus_for_qid_str), \"num_responses\": num_responses_for_qid,\n",
        "                \"baseline_mdl\": baseline_total_mdl_cost, \"final_motifs\": [],\n",
        "                \"l_h_motifs\": 0.0, \"l_d_h_motifs\": baseline_total_mdl_cost, \"total_mdl_motifs\": baseline_total_mdl_cost,\n",
        "                \"compression_achieved\": 0.0, \"num_raw_motifs_extracted\": len(raw_motifs_from_all_chunks), \"num_consolidated_motifs\": 0\n",
        "            })\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        print(f\"  Final Consolidated Motifs for QID {qid_str} (for MDL eval):\")\n",
        "        for mo_final in final_motifs_for_qid:\n",
        "            print(f\"    Label: {mo_final.get('label')}, Desc: {mo_final.get('description','N/A')[:60]}..., SFs: {mo_final.get('surface_forms',[])}\")\n",
        "\n",
        "\n",
        "        # Calculate MDL cost using the full original corpus for this QID and the consolidated motifs\n",
        "        l_h_final, l_d_h_final, total_mdl_with_final_motifs = compute_mdl_cost_for_text_block(\n",
        "            full_corpus_for_qid_str,\n",
        "            final_motifs_for_qid,\n",
        "            bdm_instance_main,\n",
        "            MATRIX_SIZE_GLOBAL\n",
        "        )\n",
        "\n",
        "        if l_d_h_final < 0: # Check if BDM error occurred for L(D|H)\n",
        "            print(f\"  Error computing MDL cost with final motifs for QID {qid_str} (BDM error in L(D|H)). Skipping result.\")\n",
        "            # Log error result\n",
        "            all_qid_mdl_results.append({\n",
        "                \"qid\": qid_str, \"corpus_len_for_qid\": len(full_corpus_for_qid_str), \"num_responses\": num_responses_for_qid,\n",
        "                \"baseline_mdl\": baseline_total_mdl_cost, \"final_motifs\": final_motifs_for_qid, # Store attempted motifs\n",
        "                \"l_h_motifs\": l_h_final if l_h_final >=0 else \"L(H)_OK_BDM_ERR_IN_L(D|H)\",\n",
        "                \"l_d_h_motifs\": -1.0, \"total_mdl_motifs\": -1.0,\n",
        "                \"compression_achieved\": \"BDM_ERROR_IN_L(D|H)\",\n",
        "                \"num_raw_motifs_extracted\": len(raw_motifs_from_all_chunks),\n",
        "                \"num_consolidated_motifs\": len(final_motifs_for_qid)\n",
        "            })\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        print(f\"  L(H) (Token-based Structured) for final motifs of QID {qid_str}: {l_h_final:.4f}\")\n",
        "        print(f\"  L(D|H) (BDM-based) compressed full corpus complexity for QID {qid_str}: {l_d_h_final:.4f}\")\n",
        "        print(f\"  Total MDL cost with final motifs for QID {qid_str}: {total_mdl_with_final_motifs:.4f}\")\n",
        "\n",
        "        compression_final = baseline_total_mdl_cost - total_mdl_with_final_motifs\n",
        "        result_status_str = \"\"\n",
        "        if compression_final > 0.0001: # Using a small threshold for \"significant\"\n",
        "            result_status_str = f\"SUCCESS: Compression achieved: {compression_final:.4f}\"\n",
        "        else:\n",
        "            result_status_str = f\"NOTE: No significant compression (or cost increased). Diff: {compression_final:.4f}\"\n",
        "        print(f\"  {result_status_str}\")\n",
        "\n",
        "        all_qid_mdl_results.append({\n",
        "            \"qid\": qid_str,\n",
        "            \"corpus_len_for_qid\": len(full_corpus_for_qid_str),\n",
        "            \"num_responses\": num_responses_for_qid,\n",
        "            \"baseline_mdl\": baseline_total_mdl_cost,\n",
        "            \"final_motifs\": final_motifs_for_qid,\n",
        "            \"l_h_motifs\": l_h_final,\n",
        "            \"l_d_h_motifs\": l_d_h_final,\n",
        "            \"total_mdl_motifs\": total_mdl_with_final_motifs,\n",
        "            \"compression_achieved\": compression_final,\n",
        "            \"num_raw_motifs_extracted\": len(raw_motifs_from_all_chunks),\n",
        "            \"num_consolidated_motifs\": len(final_motifs_for_qid)\n",
        "        })\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # --- Summary Printing and Saving Results ---\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary (Batched Local LLM, Structured Motifs, Token-L(H)) ---\")\n",
        "    if not all_qid_mdl_results:\n",
        "        print(\"No QIDs were processed or no valid results generated.\")\n",
        "    else:\n",
        "        valid_results_for_stats = [r for r in all_qid_mdl_results if isinstance(r.get('compression_achieved'), float) and r.get('l_h_motifs', -1.0) >= 0]\n",
        "        num_compressed_qids = sum(1 for r in valid_results_for_stats if r['compression_achieved'] > 0.0001)\n",
        "        successful_compressions_values = [r['compression_achieved'] for r in valid_results_for_stats if r['compression_achieved'] > 0.0001]\n",
        "\n",
        "        avg_compression_val = np.mean(successful_compressions_values) if successful_compressions_values else 0\n",
        "        max_compression_val = np.max(successful_compressions_values) if successful_compressions_values else 0\n",
        "\n",
        "        print(f\"Total QIDs targeted for analysis: {len(qids_to_process_final)}\")\n",
        "        print(f\"Total QID results logged: {len(all_qid_mdl_results)}\")\n",
        "        print(f\"Number of QIDs where compression was achieved (from valid results): {num_compressed_qids}\")\n",
        "        if num_compressed_qids > 0:\n",
        "            print(f\"  Average compression (for successful cases): {avg_compression_val:.4f}\")\n",
        "            print(f\"  Maximum compression achieved across QIDs: {max_compression_val:.4f}\")\n",
        "\n",
        "        output_filename_qids_final = \"mdl_analysis_per_qid_batched_llm_v1.json\"\n",
        "        try:\n",
        "            with open(output_filename_qids_final, \"w\", encoding=\"utf-8\") as f_out:\n",
        "                json.dump(all_qid_mdl_results, f_out, indent=2)\n",
        "            print(f\"Detailed QID-based results saved to {output_filename_qids_final}\")\n",
        "        except Exception as e_save:\n",
        "            print(f\"Error saving QID-based results to {output_filename_qids_final}: {e_save}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "15e04d3b2e13454e80b5d7a93d216a1e",
            "6f47b73152c5440cb61c982ebab9899c",
            "f737dbe0c95d432d9a386e9b64a9de0c",
            "33af13d07b1245a89a09eedae4d6c5aa",
            "dc8e4a3a333e400e93390acd535f9993",
            "0ede814e3fe74153a7c236cec38d92c4",
            "5c9f7e411021498695ddfe14d2619096",
            "72bcc4f24bce49e3b29f394249a8590c",
            "7707003214fb429fa7215a893d9ae008",
            "2f5f00d4412b4e25916ea134d991f225",
            "c9137173812b42d4a5f31974d989759f"
          ]
        },
        "id": "Kl-NSIL29UP6",
        "outputId": "fb896a1e-ffd7-418d-fb33-a4f08543db89"
      },
      "id": "Kl-NSIL29UP6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- MDL Prototype: Batched LLM Motif Extraction, Structured Motifs, Token-L(H) ---\n",
            "Using L(H) Cost Params: Label=0.5, DescBase=0.5, DescToken=0.1, SFListBase=0.25, SFTokenInLH=0.1\n",
            "LLM Batch Size: 5 responses. LLM Retries: 2. Max Text per Prompt Chunk: 7500\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-3-4b-it...\n",
            "BNB config created for google/gemma-3-4b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-3-4b-it (Quantization: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15e04d3b2e13454e80b5d7a93d216a1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-3-4b-it initialized successfully.\n",
            "BDM instance initialized successfully.\n",
            "Loading Phase 2 output from: /content/drive/MyDrive/Colab Notebooks/Legal/Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json...\n",
            "Targeting specific QIDs from P3_QIDS_TO_PROCESS_THEMATICALLY: ['Q4']\n",
            "\n",
            "MDL analysis will run for these QIDs: ['Q4']\n",
            "\n",
            "--- Analyzing Aggregated Text for QID: Q4 ---\n",
            "  Combined corpus for QID Q4 has 30351 chars from 26 individual responses.\n",
            "  Baseline MDL for QID Q4 (L(D_orig)): 122.0250\n",
            "  QID Q4: Processing 26 responses in 6 chunks (batch size: 5 responses).\n",
            "    Analyzing chunk 1/6 for QID Q4 (len: 4702 chars)...\n",
            "    DEBUG (call_local_llm): Sending prompt to LLM for QID Q4, Chunk 1 (Prompt length: 5661 chars). First 300 chars of formatted prompt:\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the ...\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID Q4, Chunk 1:\n",
            "<<<<<\n",
            "[{'generated_text': '<bos><start_of_turn>user\\nYou will receive a set of comments from different people answering the same question.\\n\\nYour task is to identify up to 5 key recurring themes.\\n\\nFor each theme, provide:\\n- A short label like [DATA_PRIVACY]\\n- A 1-sentence definition\\n- 2–3 short phrases that often appear in the text (surface forms)\\n\\nOutput MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\\nExample of one object in the list:\\n{\\n  \"label\": \"[EXAMPLE_LABEL]\",\\n  \"description\": \"A concise description of the example theme.\",\\n  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\\n}\\nIf no clear motifs are found, output an empty JSON list: `[]`.\\nDo not include any other text, explanations, or markdown code fences around the JSON.\\n\\nSet of comments to analyze:\\n\\'\\'\\'\\nThis excerpt highlights a significant concern regarding privacy rights in Australia, specifically concerning the availability of property data. The Attorney-Generals Department points to the Privacy Act 2022, referencing principles 12 and 13, and proposes new rights for individuals in proposals 18.1, 18.2, and 18.3. The core issue is that real estate websites like Realestate.com.au are collecting and displaying property images, potentially violating individuals’ right to control their personal information and privacy.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, capturing the key points in approximately 3 sentences:\\nThe Australian Privacy Act currently restricts personal information processing based on the GDPR’s stricter requirements for data protection, leading to a significant divergence between the two frameworks. Proposals 18.1 and 18.2 aim to introduce new rights allowing individuals to access and understand their data held by organizations, mirroring GDPR Article 15. However, these proposals face challenges, particularly Proposal 18.2, which shifts the focus from a straightforward right to challenge data processing to a more complex process requiring demonstrating a legitimate reason for the entity’s actions, potentially creating a burden on individuals.\\n\\n<RSP_SEP>\\n\\nThis excerpt highlights a critical challenge regarding the application of privacy principles to employment in Australia. It emphasizes that fees cannot be included in technology, and specific exceptions are needed to address situations where compliance with requests could conflict with public interests, particularly law enforcement. The proposals require careful consideration of jurisdictional limitations, with a focus on Australia-specific applications. The excerpt suggests a need for significant revisions to Australian privacy laws to demonstrate credibility and address existing excuses regarding privacy breaches.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, capturing the key points as requested:\\n**Summary:**\\nThis document highlights the growing need for reform in Australian privacy law and human rights law due to the increasing use of data and algorithmic management in the workplace. Australia currently lacks a federal human rights framework and statutory privacy laws, but the Victorian Charter of Human Rights and Responsibilities Act 2006 offers a potential legal basis for interpreting privacy rights. While the GDPR provides some valuable guidance and offers a benchmark for global data protection, its effectiveness in addressing employee rights remains debated, particularly considering the absence of a common law right to privacy in Australia. Furthermore, the document emphasizes the need for broader reforms across discrimination law and human rights law to better respond to the challenges posed by algorithmic tools and potential discrimination.\\n---\\n**Key Takeaways from the Excerpt:**\\n* **Lack of Federal Framework:** Australia lacks a federal human rights law specifically addressing privacy.\\n* **GDPR Influence:** The GDPR offers some benefits, though scholars question its effectiveness in protecting employee rights.\\n* **Alternative Legal Basis:** The Victorian Charter provides a potential legal foundation for privacy rights.\\n* **Need for Reform:** Significant reforms\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, capturing the essence in three sentences:\\nThe Australian Privacy Legislation Amendment Act has increased penalties for privacy breaches, highlighting the need for continued reform to ensure adequate protection for Australians in the digital age. The legislation mandates that individuals have control over their personal information and that changes to data collection practices by platforms must be approved by the Privacy Act. Furthermore, Proposal 25.5 suggests a direct right of action to seek redress for breaches impacting individuals, with the potential for court-ordered remedies. Finally, Proposal 26.1 proposes a mechanism for courts to issue orders to address privacy violations.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThe recent legislation strengthening privacy protections in Australia has introduced penalties for breaches and mandates that individuals have agency over their data. Crucially, the updated Privacy Act now prohibits unilateral changes to data collection practices by platforms, requiring approval from the legislation. Adding to this, Proposal 25.5 proposes a direct right to sue for breaches impacting individuals, allowing courts to issue orders to address the issue. Finally, Proposal 26.1 suggests a mechanism for courts to intervene and impose remedies when privacy violations occur.\\n\\'\\'\\'\\n\\nValid JSON Output (ensure it\\'s a list of objects, or an empty list [] if no themes):<end_of_turn>\\n<start_of_turn>model\\n'}]\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'generated_text_full' for QID Q4, Chunk 1 (len 5661):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the text (surface forms)\n",
            "\n",
            "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
            "Example of one object in the list:\n",
            "{\n",
            "  \"label\": \"[EXAMPLE_LABEL]\",\n",
            "  \"description\": \"A concise description of the example theme.\",\n",
            "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
            "}\n",
            "If no clear motifs are found, output an empty JSON list: `[]`.\n",
            "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
            "\n",
            "Set of comments to analyze:\n",
            "'''\n",
            "This excerpt highlights a significant concern regarding privacy rights in Australia, specifically concerning the availability of property data. The Attorney-Generals ...\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'assistant_response_text' (after stripping) for QID Q4, Chunk 1 (len 0):\n",
            "<<<<<\n",
            "...\n",
            ">>>>>\n",
            "      LLM call attempt 1 for chunk 1 (QID Q4) returned empty. Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DEBUG (call_local_llm): Sending prompt to LLM for QID Q4, Chunk 1 (Prompt length: 5661 chars). First 300 chars of formatted prompt:\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the ...\n",
            ">>>>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID Q4, Chunk 1:\n",
            "<<<<<\n",
            "[{'generated_text': '<bos><start_of_turn>user\\nYou will receive a set of comments from different people answering the same question.\\n\\nYour task is to identify up to 5 key recurring themes.\\n\\nFor each theme, provide:\\n- A short label like [DATA_PRIVACY]\\n- A 1-sentence definition\\n- 2–3 short phrases that often appear in the text (surface forms)\\n\\nOutput MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\\nExample of one object in the list:\\n{\\n  \"label\": \"[EXAMPLE_LABEL]\",\\n  \"description\": \"A concise description of the example theme.\",\\n  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\\n}\\nIf no clear motifs are found, output an empty JSON list: `[]`.\\nDo not include any other text, explanations, or markdown code fences around the JSON.\\n\\nSet of comments to analyze:\\n\\'\\'\\'\\nThis excerpt highlights a significant concern regarding privacy rights in Australia, specifically concerning the availability of property data. The Attorney-Generals Department points to the Privacy Act 2022, referencing principles 12 and 13, and proposes new rights for individuals in proposals 18.1, 18.2, and 18.3. The core issue is that real estate websites like Realestate.com.au are collecting and displaying property images, potentially violating individuals’ right to control their personal information and privacy.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, capturing the key points in approximately 3 sentences:\\nThe Australian Privacy Act currently restricts personal information processing based on the GDPR’s stricter requirements for data protection, leading to a significant divergence between the two frameworks. Proposals 18.1 and 18.2 aim to introduce new rights allowing individuals to access and understand their data held by organizations, mirroring GDPR Article 15. However, these proposals face challenges, particularly Proposal 18.2, which shifts the focus from a straightforward right to challenge data processing to a more complex process requiring demonstrating a legitimate reason for the entity’s actions, potentially creating a burden on individuals.\\n\\n<RSP_SEP>\\n\\nThis excerpt highlights a critical challenge regarding the application of privacy principles to employment in Australia. It emphasizes that fees cannot be included in technology, and specific exceptions are needed to address situations where compliance with requests could conflict with public interests, particularly law enforcement. The proposals require careful consideration of jurisdictional limitations, with a focus on Australia-specific applications. The excerpt suggests a need for significant revisions to Australian privacy laws to demonstrate credibility and address existing excuses regarding privacy breaches.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, capturing the key points as requested:\\n**Summary:**\\nThis document highlights the growing need for reform in Australian privacy law and human rights law due to the increasing use of data and algorithmic management in the workplace. Australia currently lacks a federal human rights framework and statutory privacy laws, but the Victorian Charter of Human Rights and Responsibilities Act 2006 offers a potential legal basis for interpreting privacy rights. While the GDPR provides some valuable guidance and offers a benchmark for global data protection, its effectiveness in addressing employee rights remains debated, particularly considering the absence of a common law right to privacy in Australia. Furthermore, the document emphasizes the need for broader reforms across discrimination law and human rights law to better respond to the challenges posed by algorithmic tools and potential discrimination.\\n---\\n**Key Takeaways from the Excerpt:**\\n* **Lack of Federal Framework:** Australia lacks a federal human rights law specifically addressing privacy.\\n* **GDPR Influence:** The GDPR offers some benefits, though scholars question its effectiveness in protecting employee rights.\\n* **Alternative Legal Basis:** The Victorian Charter provides a potential legal foundation for privacy rights.\\n* **Need for Reform:** Significant reforms\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, capturing the essence in three sentences:\\nThe Australian Privacy Legislation Amendment Act has increased penalties for privacy breaches, highlighting the need for continued reform to ensure adequate protection for Australians in the digital age. The legislation mandates that individuals have control over their personal information and that changes to data collection practices by platforms must be approved by the Privacy Act. Furthermore, Proposal 25.5 suggests a direct right of action to seek redress for breaches impacting individuals, with the potential for court-ordered remedies. Finally, Proposal 26.1 proposes a mechanism for courts to issue orders to address privacy violations.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThe recent legislation strengthening privacy protections in Australia has introduced penalties for breaches and mandates that individuals have agency over their data. Crucially, the updated Privacy Act now prohibits unilateral changes to data collection practices by platforms, requiring approval from the legislation. Adding to this, Proposal 25.5 proposes a direct right to sue for breaches impacting individuals, allowing courts to issue orders to address the issue. Finally, Proposal 26.1 suggests a mechanism for courts to intervene and impose remedies when privacy violations occur.\\n\\'\\'\\'\\n\\nValid JSON Output (ensure it\\'s a list of objects, or an empty list [] if no themes):<end_of_turn>\\n<start_of_turn>model\\n'}]\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'generated_text_full' for QID Q4, Chunk 1 (len 5661):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the text (surface forms)\n",
            "\n",
            "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
            "Example of one object in the list:\n",
            "{\n",
            "  \"label\": \"[EXAMPLE_LABEL]\",\n",
            "  \"description\": \"A concise description of the example theme.\",\n",
            "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
            "}\n",
            "If no clear motifs are found, output an empty JSON list: `[]`.\n",
            "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
            "\n",
            "Set of comments to analyze:\n",
            "'''\n",
            "This excerpt highlights a significant concern regarding privacy rights in Australia, specifically concerning the availability of property data. The Attorney-Generals ...\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'assistant_response_text' (after stripping) for QID Q4, Chunk 1 (len 0):\n",
            "<<<<<\n",
            "...\n",
            ">>>>>\n",
            "      LLM call attempt 2 for chunk 1 (QID Q4) returned empty. Retrying if possible...\n",
            "      No valid motifs extracted from chunk 1 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 2/6 for QID Q4 (len: 5979 chars)...\n",
            "    DEBUG (call_local_llm): Sending prompt to LLM for QID Q4, Chunk 2 (Prompt length: 6938 chars). First 300 chars of formatted prompt:\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the ...\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID Q4, Chunk 2:\n",
            "<<<<<\n",
            "[{'generated_text': '<bos><start_of_turn>user\\nYou will receive a set of comments from different people answering the same question.\\n\\nYour task is to identify up to 5 key recurring themes.\\n\\nFor each theme, provide:\\n- A short label like [DATA_PRIVACY]\\n- A 1-sentence definition\\n- 2–3 short phrases that often appear in the text (surface forms)\\n\\nOutput MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\\nExample of one object in the list:\\n{\\n  \"label\": \"[EXAMPLE_LABEL]\",\\n  \"description\": \"A concise description of the example theme.\",\\n  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\\n}\\nIf no clear motifs are found, output an empty JSON list: `[]`.\\nDo not include any other text, explanations, or markdown code fences around the JSON.\\n\\nSet of comments to analyze:\\n\\'\\'\\'\\nHere’s a concise summary of the excerpt, focusing on the core points and relating them to the question:\\nThe excerpt highlights the current limitations of Australian privacy laws regarding data rights, particularly concerning large companies facing significant financial burdens. It emphasizes the need for broader rights – including access, knowledge, and control – over data, suggesting a desire for more comprehensive protections. Currently, Australia lacks robust data rights, especially for political organizations and small businesses, and requires more flexible rules to address this issue.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThe excerpt argues that Australian privacy laws currently offer inadequate data protection, especially for large organizations facing financial difficulties. It advocates for broader rights – including access, knowledge, and control – over data, suggesting a need for more flexible rules to ensure individuals have greater agency over their personal information. The current framework is particularly weak when applied to political parties, charities, and small businesses due to existing exemptions.\\n\\n<RSP_SEP>\\n\\nHere’s a summary of the excerpt in approximately three sentences, capturing the essence of the relevant points:\\nThe author supports the Australian government’s efforts to protect children and young people online by advocating for increased privacy protections and addressing potential risks of harm. Recognizing that adolescents between 13 and 15 are equally vulnerable to exploitation, the author supports proposals that reduce risk and offer redress for harm. Furthermore, AHISA recommends incorporating children’s privacy as a guiding principle within the Privacy Act, aligning with international standards like the UN Convention on the Rights of the Child, and supports the right to erasure for children, potentially requiring the involvement of legal guardians or trustees.\\n---\\n**Alternative Summary (Slightly More Concise):**\\nThe excerpt highlights the Australian government’s commitment to protecting children online, supporting proposals to reduce harm and offering redress. It emphasizes that children aged 13-15 are equally vulnerable and advocates for incorporating their privacy into the Privacy Act, suggesting the right to erasure and the involvement of legal representatives. Finally, the author supports the UN Convention and suggests utilizing the Privacy Act to accommodate requests for information access, correction, or erasure.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, focusing on the core points and relating them to the question:\\n**Summary:**\\nThe Australian Privacy Act 1988 provides strong protections for individuals’ privacy, and the current review offers enhancements to this framework. However, existing protections may not be sufficient given employers’ data collection practices, particularly concerning the sheer volume of personal information they collect from employees. The proposed revisions, particularly regarding the Employee Records exemption, aim to align the Act with public expectations and bolster trust in employer data handling while acknowledging the potential risks posed by AI-driven decision-making. Finally, the proposed right to request information about automated decisions is a positive step towards ensuring fairness and transparency.\\n---\\nWould you like me to elaborate on any specific aspect of this summary, or perhaps explore potential exceptions or further considerations?\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, capturing the key points in approximately three sentences:\\nThe excerpt highlights the need for robust privacy protections in Australia’s Privacy Principles, particularly regarding secure data handling and pseudonymization/anonymization. Specifically, WESNET advocates for entities to proactively secure their data and protect it from identification through data analysis – like tracking keyboard usage – which could lead to targeted targeting. Furthermore, the document emphasizes the importance of entities taking responsibility for fair and reasonable data handling practices when required by the Privacy Principles.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThe excerpt emphasizes the requirement for entities to protect personal information through security measures like masking and pseudonymization, as proposed by WESNET. It stresses that data collection should be carefully monitored and that entities are responsible for handling the information fairly and responsibly, aligning with the Privacy Principles. Finally, the excerpt underscores the potential for data analysis to identify individuals through tracking methods, highlighting the need for clear guidance on responsible data handling.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, focusing on the key points relevant to your question:\\n**Summary:**\\nForHumanity, a company focused on AI safety and government support, is responding to the Australian Privacy Act Review by offering assistance to agencies managing AI risks. They are encouraged by the draft proposals for the Act, specifically the proposed rights outlined in proposals 18.1, 18.2, and 18.3. Currently, the company is hesitant to provide rights related to these proposals, but acknowledges the need to consider how these rights might be interpreted within the context of their work.\\n**Further Breakdown of the Core Points:**\\n* **Focus on AI Safety & Government Support:** ForHumanity is actively involved in developing and providing solutions for AI safety, particularly for government agencies.\\n* **Responding to Legislation:** They are actively engaging with the Australian Privacy Act Review and offering assistance.\\n* **Hesitant Approach to Rights:** The company is cautious about offering specific rights related to the proposed individual rights, indicating a need for careful consideration.\\nWould you like me to elaborate on any particular aspect of this excerpt?\\n\\'\\'\\'\\n\\nValid JSON Output (ensure it\\'s a list of objects, or an empty list [] if no themes):<end_of_turn>\\n<start_of_turn>model\\n'}]\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'generated_text_full' for QID Q4, Chunk 2 (len 6938):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the text (surface forms)\n",
            "\n",
            "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
            "Example of one object in the list:\n",
            "{\n",
            "  \"label\": \"[EXAMPLE_LABEL]\",\n",
            "  \"description\": \"A concise description of the example theme.\",\n",
            "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
            "}\n",
            "If no clear motifs are found, output an empty JSON list: `[]`.\n",
            "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
            "\n",
            "Set of comments to analyze:\n",
            "'''\n",
            "Here’s a concise summary of the excerpt, focusing on the core points and relating them to the question:\n",
            "The excerpt highlights the current limitations of Australian p...\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'assistant_response_text' (after stripping) for QID Q4, Chunk 2 (len 0):\n",
            "<<<<<\n",
            "...\n",
            ">>>>>\n",
            "      LLM call attempt 1 for chunk 2 (QID Q4) returned empty. Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DEBUG (call_local_llm): Sending prompt to LLM for QID Q4, Chunk 2 (Prompt length: 6938 chars). First 300 chars of formatted prompt:\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the ...\n",
            ">>>>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID Q4, Chunk 2:\n",
            "<<<<<\n",
            "[{'generated_text': '<bos><start_of_turn>user\\nYou will receive a set of comments from different people answering the same question.\\n\\nYour task is to identify up to 5 key recurring themes.\\n\\nFor each theme, provide:\\n- A short label like [DATA_PRIVACY]\\n- A 1-sentence definition\\n- 2–3 short phrases that often appear in the text (surface forms)\\n\\nOutput MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\\nExample of one object in the list:\\n{\\n  \"label\": \"[EXAMPLE_LABEL]\",\\n  \"description\": \"A concise description of the example theme.\",\\n  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\\n}\\nIf no clear motifs are found, output an empty JSON list: `[]`.\\nDo not include any other text, explanations, or markdown code fences around the JSON.\\n\\nSet of comments to analyze:\\n\\'\\'\\'\\nHere’s a concise summary of the excerpt, focusing on the core points and relating them to the question:\\nThe excerpt highlights the current limitations of Australian privacy laws regarding data rights, particularly concerning large companies facing significant financial burdens. It emphasizes the need for broader rights – including access, knowledge, and control – over data, suggesting a desire for more comprehensive protections. Currently, Australia lacks robust data rights, especially for political organizations and small businesses, and requires more flexible rules to address this issue.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThe excerpt argues that Australian privacy laws currently offer inadequate data protection, especially for large organizations facing financial difficulties. It advocates for broader rights – including access, knowledge, and control – over data, suggesting a need for more flexible rules to ensure individuals have greater agency over their personal information. The current framework is particularly weak when applied to political parties, charities, and small businesses due to existing exemptions.\\n\\n<RSP_SEP>\\n\\nHere’s a summary of the excerpt in approximately three sentences, capturing the essence of the relevant points:\\nThe author supports the Australian government’s efforts to protect children and young people online by advocating for increased privacy protections and addressing potential risks of harm. Recognizing that adolescents between 13 and 15 are equally vulnerable to exploitation, the author supports proposals that reduce risk and offer redress for harm. Furthermore, AHISA recommends incorporating children’s privacy as a guiding principle within the Privacy Act, aligning with international standards like the UN Convention on the Rights of the Child, and supports the right to erasure for children, potentially requiring the involvement of legal guardians or trustees.\\n---\\n**Alternative Summary (Slightly More Concise):**\\nThe excerpt highlights the Australian government’s commitment to protecting children online, supporting proposals to reduce harm and offering redress. It emphasizes that children aged 13-15 are equally vulnerable and advocates for incorporating their privacy into the Privacy Act, suggesting the right to erasure and the involvement of legal representatives. Finally, the author supports the UN Convention and suggests utilizing the Privacy Act to accommodate requests for information access, correction, or erasure.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, focusing on the core points and relating them to the question:\\n**Summary:**\\nThe Australian Privacy Act 1988 provides strong protections for individuals’ privacy, and the current review offers enhancements to this framework. However, existing protections may not be sufficient given employers’ data collection practices, particularly concerning the sheer volume of personal information they collect from employees. The proposed revisions, particularly regarding the Employee Records exemption, aim to align the Act with public expectations and bolster trust in employer data handling while acknowledging the potential risks posed by AI-driven decision-making. Finally, the proposed right to request information about automated decisions is a positive step towards ensuring fairness and transparency.\\n---\\nWould you like me to elaborate on any specific aspect of this summary, or perhaps explore potential exceptions or further considerations?\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, capturing the key points in approximately three sentences:\\nThe excerpt highlights the need for robust privacy protections in Australia’s Privacy Principles, particularly regarding secure data handling and pseudonymization/anonymization. Specifically, WESNET advocates for entities to proactively secure their data and protect it from identification through data analysis – like tracking keyboard usage – which could lead to targeted targeting. Furthermore, the document emphasizes the importance of entities taking responsibility for fair and reasonable data handling practices when required by the Privacy Principles.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThe excerpt emphasizes the requirement for entities to protect personal information through security measures like masking and pseudonymization, as proposed by WESNET. It stresses that data collection should be carefully monitored and that entities are responsible for handling the information fairly and responsibly, aligning with the Privacy Principles. Finally, the excerpt underscores the potential for data analysis to identify individuals through tracking methods, highlighting the need for clear guidance on responsible data handling.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, focusing on the key points relevant to your question:\\n**Summary:**\\nForHumanity, a company focused on AI safety and government support, is responding to the Australian Privacy Act Review by offering assistance to agencies managing AI risks. They are encouraged by the draft proposals for the Act, specifically the proposed rights outlined in proposals 18.1, 18.2, and 18.3. Currently, the company is hesitant to provide rights related to these proposals, but acknowledges the need to consider how these rights might be interpreted within the context of their work.\\n**Further Breakdown of the Core Points:**\\n* **Focus on AI Safety & Government Support:** ForHumanity is actively involved in developing and providing solutions for AI safety, particularly for government agencies.\\n* **Responding to Legislation:** They are actively engaging with the Australian Privacy Act Review and offering assistance.\\n* **Hesitant Approach to Rights:** The company is cautious about offering specific rights related to the proposed individual rights, indicating a need for careful consideration.\\nWould you like me to elaborate on any particular aspect of this excerpt?\\n\\'\\'\\'\\n\\nValid JSON Output (ensure it\\'s a list of objects, or an empty list [] if no themes):<end_of_turn>\\n<start_of_turn>model\\n'}]\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'generated_text_full' for QID Q4, Chunk 2 (len 6938):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the text (surface forms)\n",
            "\n",
            "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
            "Example of one object in the list:\n",
            "{\n",
            "  \"label\": \"[EXAMPLE_LABEL]\",\n",
            "  \"description\": \"A concise description of the example theme.\",\n",
            "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
            "}\n",
            "If no clear motifs are found, output an empty JSON list: `[]`.\n",
            "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
            "\n",
            "Set of comments to analyze:\n",
            "'''\n",
            "Here’s a concise summary of the excerpt, focusing on the core points and relating them to the question:\n",
            "The excerpt highlights the current limitations of Australian p...\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'assistant_response_text' (after stripping) for QID Q4, Chunk 2 (len 0):\n",
            "<<<<<\n",
            "...\n",
            ">>>>>\n",
            "      LLM call attempt 2 for chunk 2 (QID Q4) returned empty. Retrying if possible...\n",
            "      No valid motifs extracted from chunk 2 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 3/6 for QID Q4 (len: 6403 chars)...\n",
            "    DEBUG (call_local_llm): Sending prompt to LLM for QID Q4, Chunk 3 (Prompt length: 7362 chars). First 300 chars of formatted prompt:\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the ...\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID Q4, Chunk 3:\n",
            "<<<<<\n",
            "[{'generated_text': '<bos><start_of_turn>user\\nYou will receive a set of comments from different people answering the same question.\\n\\nYour task is to identify up to 5 key recurring themes.\\n\\nFor each theme, provide:\\n- A short label like [DATA_PRIVACY]\\n- A 1-sentence definition\\n- 2–3 short phrases that often appear in the text (surface forms)\\n\\nOutput MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\\nExample of one object in the list:\\n{\\n  \"label\": \"[EXAMPLE_LABEL]\",\\n  \"description\": \"A concise description of the example theme.\",\\n  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\\n}\\nIf no clear motifs are found, output an empty JSON list: `[]`.\\nDo not include any other text, explanations, or markdown code fences around the JSON.\\n\\nSet of comments to analyze:\\n\\'\\'\\'\\nHere’s a concise summary of the excerpt, capturing the key points in approximately three sentences:\\nTransurban acknowledges the existing privacy rights for employees under Australian Privacy Principles 12 and 13, but believes these are insufficient for employment contexts. They support formalizing additional protections for employees’ personal information and sensitive data, but acknowledge the complexity of the issue within the employment context. Furthermore, Transurban suggests further consultation and potential exceptions are needed for specific rights like access, particularly in response to frivolous requests.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nTransurban emphasizes a commitment to employee privacy, noting existing rights and proposing additional protections for employees’ personal information within the employment context. However, they argue that the current Australian privacy laws aren’t fully suitable for this scenario, highlighting the complexities of the situation as echoed in existing reports. They also support the proposed exceptions, particularly regarding access requests, and suggest further discussion and potential new rules if necessary.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, focusing on the core points and relating them to the question:\\nThe excerpt highlights concerns about the use of predictive analytics tools in Victoria and NSW, which are being used to target vulnerable communities and potentially victimize individuals. It emphasizes the importance of privacy as a fundamental human right, arguing that its absence can severely restrict freedom of speech and expression and limit accountability. Furthermore, the excerpt suggests that the right to privacy is crucial for protecting individual integrity and enabling resistance against oppressive systems.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThe excerpt raises concerns about the potential misuse of predictive analytics tools in policing, which could lead to targeted harm and increased surveillance of vulnerable communities. It underscores the fundamental importance of privacy rights, arguing that their absence would hinder freedom of speech and expression and limit the ability to hold authorities accountable. Finally, the excerpt suggests that protecting privacy safeguards individual integrity and empowers resistance against unjust systems.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, capturing the essence in approximately three sentences:\\nThe excerpt highlights a critical need for stronger legislation in Australia regarding privacy rights, particularly regarding data handling practices. Recent reforms require organizations to be covered by the Privacy Act, and robust enforcement powers are crucial to address the growing issue of large-scale data collection and the potential for data breaches. Small businesses, often facing security risks and lacking adequate safeguards, are particularly vulnerable to exploitation by criminals due to their sensitive data collection and lack of protective measures. Therefore, the current exemption system for small businesses is inadequate and creates a dangerous situation where data breaches can have severe consequences.\\n---\\n**Alternative Summary (Slightly More Focused):**\\nThe excerpt emphasizes the urgent need for updated privacy legislation in Australia to protect individuals’ rights in the face of increasing data collection. Existing exemptions for small businesses are insufficient due to their vulnerability to data breaches and the lack of robust security measures. This situation creates a high-risk environment where sensitive data is collected and misused, potentially leading to significant legal and personal harm.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, focusing on the core points and relating to the question:\\nThe excerpt highlights concerns about the Australian government’s privacy reforms, particularly regarding the limitations of current privacy principles and the potential for misuse of private information. The report identifies a failure to adequately address threats to privacy arising from corporate influence and the failure to identify a significant current risk. Furthermore, it suggests that the government’s survey-driven approach has inadvertently exacerbated these risks, leading to exploitation and a lack of proactive protection for individuals. The core issue is that the current legal framework isn’t sufficient to prevent the abuse of private information, particularly as corporations are actively exploiting this vulnerability.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThe excerpt criticizes the Australian government’s privacy reforms, arguing that the current laws are inadequate in protecting individuals from potential abuse of private information. The report notes that the Privacy Act Review identifies a significant threat – the potential for corporations to exploit private data – and suggests the government’s survey-based approach has inadvertently fueled this problem. The document argues that the government has failed to proactively identify and mitigate the risks of private information abuse, leading to exploitation and\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, focusing on the core points and relating to the question:\\nThe excerpt highlights that Australian Privacy Principles 12 and 13 offer protections for individuals, allowing for safe communication with adults and referral to safeguarding bodies for investigation. However, restrictive provisions within the Privacy Act hinder banks and other financial institutions from acting on information for financial abuse reporting, limiting their ability to assist vulnerable adults. Furthermore, the ADC suggests that current banking practices often lack the necessary consent for such actions.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThis excerpt addresses the challenge of safeguarding vulnerable adults in the Australian context, referencing the Privacy Principles and proposed safeguards. It points out that while protections exist for communication and referral, restrictions within the Privacy Act impede banks’ ability to actively investigate financial abuse, potentially jeopardizing the well-being of individuals. The ADC’s observation underscores the need for revised banking practices to better support these situations.\\n\\'\\'\\'\\n\\nValid JSON Output (ensure it\\'s a list of objects, or an empty list [] if no themes):<end_of_turn>\\n<start_of_turn>model\\n'}]\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'generated_text_full' for QID Q4, Chunk 3 (len 7362):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the text (surface forms)\n",
            "\n",
            "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
            "Example of one object in the list:\n",
            "{\n",
            "  \"label\": \"[EXAMPLE_LABEL]\",\n",
            "  \"description\": \"A concise description of the example theme.\",\n",
            "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
            "}\n",
            "If no clear motifs are found, output an empty JSON list: `[]`.\n",
            "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
            "\n",
            "Set of comments to analyze:\n",
            "'''\n",
            "Here’s a concise summary of the excerpt, capturing the key points in approximately three sentences:\n",
            "Transurban acknowledges the existing privacy rights for employees ...\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'assistant_response_text' (after stripping) for QID Q4, Chunk 3 (len 0):\n",
            "<<<<<\n",
            "...\n",
            ">>>>>\n",
            "      LLM call attempt 1 for chunk 3 (QID Q4) returned empty. Retrying if possible...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DEBUG (call_local_llm): Sending prompt to LLM for QID Q4, Chunk 3 (Prompt length: 7362 chars). First 300 chars of formatted prompt:\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the ...\n",
            ">>>>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID Q4, Chunk 3:\n",
            "<<<<<\n",
            "[{'generated_text': '<bos><start_of_turn>user\\nYou will receive a set of comments from different people answering the same question.\\n\\nYour task is to identify up to 5 key recurring themes.\\n\\nFor each theme, provide:\\n- A short label like [DATA_PRIVACY]\\n- A 1-sentence definition\\n- 2–3 short phrases that often appear in the text (surface forms)\\n\\nOutput MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\\nExample of one object in the list:\\n{\\n  \"label\": \"[EXAMPLE_LABEL]\",\\n  \"description\": \"A concise description of the example theme.\",\\n  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\\n}\\nIf no clear motifs are found, output an empty JSON list: `[]`.\\nDo not include any other text, explanations, or markdown code fences around the JSON.\\n\\nSet of comments to analyze:\\n\\'\\'\\'\\nHere’s a concise summary of the excerpt, capturing the key points in approximately three sentences:\\nTransurban acknowledges the existing privacy rights for employees under Australian Privacy Principles 12 and 13, but believes these are insufficient for employment contexts. They support formalizing additional protections for employees’ personal information and sensitive data, but acknowledge the complexity of the issue within the employment context. Furthermore, Transurban suggests further consultation and potential exceptions are needed for specific rights like access, particularly in response to frivolous requests.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nTransurban emphasizes a commitment to employee privacy, noting existing rights and proposing additional protections for employees’ personal information within the employment context. However, they argue that the current Australian privacy laws aren’t fully suitable for this scenario, highlighting the complexities of the situation as echoed in existing reports. They also support the proposed exceptions, particularly regarding access requests, and suggest further discussion and potential new rules if necessary.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, focusing on the core points and relating them to the question:\\nThe excerpt highlights concerns about the use of predictive analytics tools in Victoria and NSW, which are being used to target vulnerable communities and potentially victimize individuals. It emphasizes the importance of privacy as a fundamental human right, arguing that its absence can severely restrict freedom of speech and expression and limit accountability. Furthermore, the excerpt suggests that the right to privacy is crucial for protecting individual integrity and enabling resistance against oppressive systems.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThe excerpt raises concerns about the potential misuse of predictive analytics tools in policing, which could lead to targeted harm and increased surveillance of vulnerable communities. It underscores the fundamental importance of privacy rights, arguing that their absence would hinder freedom of speech and expression and limit the ability to hold authorities accountable. Finally, the excerpt suggests that protecting privacy safeguards individual integrity and empowers resistance against unjust systems.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, capturing the essence in approximately three sentences:\\nThe excerpt highlights a critical need for stronger legislation in Australia regarding privacy rights, particularly regarding data handling practices. Recent reforms require organizations to be covered by the Privacy Act, and robust enforcement powers are crucial to address the growing issue of large-scale data collection and the potential for data breaches. Small businesses, often facing security risks and lacking adequate safeguards, are particularly vulnerable to exploitation by criminals due to their sensitive data collection and lack of protective measures. Therefore, the current exemption system for small businesses is inadequate and creates a dangerous situation where data breaches can have severe consequences.\\n---\\n**Alternative Summary (Slightly More Focused):**\\nThe excerpt emphasizes the urgent need for updated privacy legislation in Australia to protect individuals’ rights in the face of increasing data collection. Existing exemptions for small businesses are insufficient due to their vulnerability to data breaches and the lack of robust security measures. This situation creates a high-risk environment where sensitive data is collected and misused, potentially leading to significant legal and personal harm.\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, focusing on the core points and relating to the question:\\nThe excerpt highlights concerns about the Australian government’s privacy reforms, particularly regarding the limitations of current privacy principles and the potential for misuse of private information. The report identifies a failure to adequately address threats to privacy arising from corporate influence and the failure to identify a significant current risk. Furthermore, it suggests that the government’s survey-driven approach has inadvertently exacerbated these risks, leading to exploitation and a lack of proactive protection for individuals. The core issue is that the current legal framework isn’t sufficient to prevent the abuse of private information, particularly as corporations are actively exploiting this vulnerability.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThe excerpt criticizes the Australian government’s privacy reforms, arguing that the current laws are inadequate in protecting individuals from potential abuse of private information. The report notes that the Privacy Act Review identifies a significant threat – the potential for corporations to exploit private data – and suggests the government’s survey-based approach has inadvertently fueled this problem. The document argues that the government has failed to proactively identify and mitigate the risks of private information abuse, leading to exploitation and\\n\\n<RSP_SEP>\\n\\nHere’s a concise summary of the excerpt, focusing on the core points and relating to the question:\\nThe excerpt highlights that Australian Privacy Principles 12 and 13 offer protections for individuals, allowing for safe communication with adults and referral to safeguarding bodies for investigation. However, restrictive provisions within the Privacy Act hinder banks and other financial institutions from acting on information for financial abuse reporting, limiting their ability to assist vulnerable adults. Furthermore, the ADC suggests that current banking practices often lack the necessary consent for such actions.\\n---\\n**Alternative Summary (Slightly More Detailed):**\\nThis excerpt addresses the challenge of safeguarding vulnerable adults in the Australian context, referencing the Privacy Principles and proposed safeguards. It points out that while protections exist for communication and referral, restrictions within the Privacy Act impede banks’ ability to actively investigate financial abuse, potentially jeopardizing the well-being of individuals. The ADC’s observation underscores the need for revised banking practices to better support these situations.\\n\\'\\'\\'\\n\\nValid JSON Output (ensure it\\'s a list of objects, or an empty list [] if no themes):<end_of_turn>\\n<start_of_turn>model\\n'}]\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'generated_text_full' for QID Q4, Chunk 3 (len 7362):\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the text (surface forms)\n",
            "\n",
            "Output MUST be a valid JSON list of objects, where each object has \"label\", \"description\", and \"surface_forms\" keys.\n",
            "Example of one object in the list:\n",
            "{\n",
            "  \"label\": \"[EXAMPLE_LABEL]\",\n",
            "  \"description\": \"A concise description of the example theme.\",\n",
            "  \"surface_forms\": [\"short repeated phrase 1\", \"another short repeated phrase\"]\n",
            "}\n",
            "If no clear motifs are found, output an empty JSON list: `[]`.\n",
            "Do not include any other text, explanations, or markdown code fences around the JSON.\n",
            "\n",
            "Set of comments to analyze:\n",
            "'''\n",
            "Here’s a concise summary of the excerpt, capturing the key points in approximately three sentences:\n",
            "Transurban acknowledges the existing privacy rights for employees ...\n",
            ">>>>>\n",
            "    DEBUG (call_local_llm): 'assistant_response_text' (after stripping) for QID Q4, Chunk 3 (len 0):\n",
            "<<<<<\n",
            "...\n",
            ">>>>>\n",
            "      LLM call attempt 2 for chunk 3 (QID Q4) returned empty. Retrying if possible...\n",
            "      No valid motifs extracted from chunk 3 (QID Q4) after 2 attempts.\n",
            "    Analyzing chunk 4/6 for QID Q4 (len: 6400 chars)...\n",
            "    DEBUG (call_local_llm): Sending prompt to LLM for QID Q4, Chunk 4 (Prompt length: 7359 chars). First 300 chars of formatted prompt:\n",
            "<<<<<\n",
            "<bos><start_of_turn>user\n",
            "You will receive a set of comments from different people answering the same question.\n",
            "\n",
            "Your task is to identify up to 5 key recurring themes.\n",
            "\n",
            "For each theme, provide:\n",
            "- A short label like [DATA_PRIVACY]\n",
            "- A 1-sentence definition\n",
            "- 2–3 short phrases that often appear in the ...\n",
            ">>>>>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-23261fd1200b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-23261fd1200b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# --- New Batched Motif Extraction ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         raw_motifs_from_all_chunks = get_motifs_for_text_chunks(\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mactual_response_text_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Pass list of individual response strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mLLM_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-23261fd1200b>\u001b[0m in \u001b[0;36mget_motifs_for_text_chunks\u001b[0;34m(list_of_response_strings, batch_size, hf_pipeline, hf_tokenizer, qid_for_log)\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# FIX IS HERE: Pass qid_for_log and chunk_idx + 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0;31m# VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                 raw_llm_response_text = call_local_llm_for_motifs(\n\u001b[0m\u001b[1;32m    338\u001b[0m                     \u001b[0mprompt_for_llm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                     \u001b[0mhf_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-7eacac9f1b3b>\u001b[0m in \u001b[0;36mcall_local_llm_for_motifs\u001b[0;34m(prompt_str, hf_pipeline, hf_tokenizer, qid_for_log, chunk_idx_for_log)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"    DEBUG (call_local_llm): Sending prompt to LLM for QID {qid_for_log}, Chunk {chunk_idx_for_log} (Prompt length: {len(prompt_formatted_for_llm)} chars). First 300 chars of formatted prompt:\\n<<<<<\\n{prompt_formatted_for_llm[:300]}...\\n>>>>>\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Log part of the prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_formatted_for_llm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgeneration_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"    DEBUG (call_local_llm): Raw 'outputs' from hf_pipeline for QID {qid_for_log}, Chunk {chunk_idx_for_log}:\\n<<<<<\\n{outputs}\\n>>>>>\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# VERY IMPORTANT DEBUG LINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m             )\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1336\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2598\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3558\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3560\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3562\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1346\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **lm_kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         )\n\u001b[0;32m-> 1208\u001b[0;31m         outputs = self.language_model(\n\u001b[0m\u001b[1;32m   1209\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m                 )\n\u001b[1;32m    653\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    655\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0mposition_embeddings_global\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_embeddings_global\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    464\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;31m# backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m ) -> Tuple[torch.Tensor, None]:\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_key_value_groups\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeat_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeat_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_rep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_key_value_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_key_value_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Token-based L(H)\n",
        "# --- Imports ---\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # !!! EXAMPLE - UPDATE THIS PATH !!!\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"] # EXAMPLE: Process only Q4\n",
        "\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "\n",
        "# --- Token-Based L(H) Costs for Structured Motifs ---\n",
        "MOTIF_SYMBOLIC_LABEL_COST = 0.5\n",
        "MOTIF_DEFINITION_TEXT_BASE_COST = 0.25\n",
        "MOTIF_DEFINITION_TOKEN_COST = 0.05\n",
        "MOTIF_SURFACE_FORMS_LIST_BASE_COST = 0.1\n",
        "MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH = 0.05\n",
        "\n",
        "# --- Helper Function Definitions ---\n",
        "\n",
        "def llm_extract_motifs_local_hf(text_to_analyze, hf_pipeline, hf_tokenizer):\n",
        "    MAX_TEXT_FOR_LLM = 7000 # Or your chosen limit\n",
        "    text_for_llm_step1 = text_to_analyze\n",
        "    if len(text_to_analyze) > MAX_TEXT_FOR_LLM:\n",
        "        text_for_llm_step1 = text_to_analyze[:MAX_TEXT_FOR_LLM]\n",
        "\n",
        "    # --- Step 1: Extract Candidate Phrases ---\n",
        "    print(\"    Executing LLM Step 1: Extracting candidate phrases...\")\n",
        "    prompt_step1_content = f\"\"\"\n",
        "    From the text below, extract up to 7 distinct, short, recurring keyword phrases or noun phrases (typically 2 to 5 words each) that seem to represent key themes or concepts.\n",
        "    Focus on phrases that you believe are used multiple times or are central to the discussion.\n",
        "    List each distinct phrase on a new line, with no other formatting or explanations.\n",
        "    If no such phrases are found, output the exact text: NO_PHRASES_FOUND\n",
        "\n",
        "    TEXT TO ANALYZE:\n",
        "    '''\n",
        "    {text_for_llm_step1}\n",
        "    '''\n",
        "\n",
        "    KEY PHRASES (each on a new line):\n",
        "    \"\"\"\n",
        "    messages_step1 = [{\"role\": \"user\", \"content\": prompt_step1_content.strip()}]\n",
        "    prompt_formatted_step1 = hf_tokenizer.apply_chat_template(messages_step1, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    generation_args_step1 = {\"max_new_tokens\": 150, \"do_sample\": False, \"pad_token_id\": hf_tokenizer.eos_token_id}\n",
        "\n",
        "    candidate_phrases = []\n",
        "    try:\n",
        "        outputs_step1 = hf_pipeline(prompt_formatted_step1, **generation_args_step1)\n",
        "        if outputs_step1 and outputs_step1[0].get('generated_text'):\n",
        "            full_response_step1 = outputs_step1[0]['generated_text']\n",
        "            # Extract only the assistant's response part\n",
        "            assistant_response_step1 = \"\"\n",
        "            if full_response_step1.startswith(prompt_formatted_step1):\n",
        "                assistant_response_step1 = full_response_step1[len(prompt_formatted_step1):].strip()\n",
        "            else: # Fallback for models that might not echo prompt fully\n",
        "                model_turn_start_token = \"<start_of_turn>model\"\n",
        "                if model_turn_start_token in full_response_step1:\n",
        "                    last_occurrence_index = full_response_step1.rfind(model_turn_start_token)\n",
        "                    assistant_response_step1 = full_response_step1[last_occurrence_index + len(model_turn_start_token):].strip()\n",
        "                    if assistant_response_step1.startswith(\"\\n\"):\n",
        "                        assistant_response_step1 = assistant_response_step1[1:].strip()\n",
        "                else:\n",
        "                     assistant_response_step1 = full_response_step1.strip()\n",
        "\n",
        "            print(f\"    DEBUG Step 1 LLM Raw Output: '{assistant_response_step1}'\")\n",
        "            if \"NO_PHRASES_FOUND\" not in assistant_response_step1.upper():\n",
        "                candidate_phrases = [p.strip() for p in assistant_response_step1.split('\\n') if p.strip() and len(p.strip().split()) >= 2 and len(p.strip().split()) <= 5] # Ensure phrases meet length criteria\n",
        "                candidate_phrases = list(dict.fromkeys(candidate_phrases)) # Deduplicate\n",
        "                print(f\"    LLM Step 1 Extracted Candidate Phrases: {candidate_phrases}\")\n",
        "            else:\n",
        "                print(\"    LLM Step 1 indicated NO_PHRASES_FOUND.\")\n",
        "        else:\n",
        "            print(\"    Error: LLM Step 1 returned empty/unexpected output.\")\n",
        "    except Exception as e_step1:\n",
        "        print(f\"    Error in LLM Step 1 (Extracting Phrases): {e_step1}\")\n",
        "        return [] # Return empty if step 1 fails\n",
        "\n",
        "    if not candidate_phrases:\n",
        "        return []\n",
        "\n",
        "    # --- Step 2: Structure Each Phrase into a Motif Object ---\n",
        "    structured_motifs_list = []\n",
        "    # Provide a smaller context snippet for step 2 to avoid overwhelming the LLM again with long text\n",
        "    context_snippet_for_step2 = text_for_llm_step1[:1000] # e.g., first 1000 chars of the (potentially truncated) text\n",
        "\n",
        "    for i, phrase in enumerate(candidate_phrases[:5]): # Process up to 5 phrases from step 1\n",
        "        print(f\"    Executing LLM Step 2: Structuring phrase '{phrase}'...\")\n",
        "        prompt_step2_content = f\"\"\"\n",
        "        You are an expert in thematic analysis.\n",
        "        The following KEY PHRASE was identified as potentially important from a larger document: \"{phrase}\"\n",
        "\n",
        "        For context, here is a brief snippet from the beginning of the larger document:\n",
        "        '''\n",
        "        {context_snippet_for_step2}\n",
        "        '''\n",
        "\n",
        "        Your task is to structure this KEY PHRASE into a thematic motif. Provide:\n",
        "        1. A short, symbolic label in [UPPER_SNAKE_CASE_WITH_BRACKETS] that captures the essence of the KEY PHRASE.\n",
        "        2. A concise, 1-sentence human-readable description of the theme represented by this KEY PHRASE.\n",
        "        3. A list called \"surface_forms\". This list MUST include the original KEY PHRASE: \"{phrase}\". You can optionally add 1 or 2 other very close variations or synonyms if they strongly represent the same immediate concept. Prefer short, common variations.\n",
        "\n",
        "        Output MUST be a single valid JSON object with \"label\", \"description\", and \"surface_forms\" keys.\n",
        "        Do not include any other text, explanations, or markdown code fences around the JSON.\n",
        "\n",
        "        Example of ONE JSON object:\n",
        "        {{\n",
        "          \"label\": \"[EXAMPLE_LABEL]\",\n",
        "          \"description\": \"A concise description of the theme related to the input phrase.\",\n",
        "          \"surface_forms\": [\"original input phrase\", \"very close variant 1\"]\n",
        "        }}\n",
        "\n",
        "        JSON Output for KEY PHRASE \"{phrase}\":\n",
        "        \"\"\"\n",
        "        messages_step2 = [{\"role\": \"user\", \"content\": prompt_step2_content.strip()}]\n",
        "        prompt_formatted_step2 = hf_tokenizer.apply_chat_template(messages_step2, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        generation_args_step2 = {\"max_new_tokens\": 250, \"do_sample\": False, \"pad_token_id\": hf_tokenizer.eos_token_id} # Max tokens for a single JSON object\n",
        "\n",
        "        try:\n",
        "            outputs_step2 = hf_pipeline(prompt_formatted_step2, **generation_args_step2)\n",
        "            if outputs_step2 and outputs_step2[0].get('generated_text'):\n",
        "                full_response_step2 = outputs_step2[0]['generated_text']\n",
        "                assistant_response_step2 = \"\"\n",
        "                if full_response_step2.startswith(prompt_formatted_step2):\n",
        "                     assistant_response_step2 = full_response_step2[len(prompt_formatted_step2):].strip()\n",
        "                else: # Fallback for models that might not echo prompt fully\n",
        "                    model_turn_start_token = \"<start_of_turn>model\"\n",
        "                    if model_turn_start_token in full_response_step2:\n",
        "                        last_occurrence_index = full_response_step2.rfind(model_turn_start_token)\n",
        "                        assistant_response_step2 = full_response_step2[last_occurrence_index + len(model_turn_start_token):].strip()\n",
        "                        if assistant_response_step2.startswith(\"\\n\"):\n",
        "                             assistant_response_step2 = assistant_response_step2[1:].strip()\n",
        "                    else:\n",
        "                        assistant_response_step2 = full_response_step2.strip()\n",
        "\n",
        "                print(f\"    DEBUG Step 2 LLM Raw Output for '{phrase}': '{assistant_response_step2}'\")\n",
        "\n",
        "                json_str_step2 = assistant_response_step2\n",
        "                # Minimal JSON extraction (assuming LLM is mostly compliant)\n",
        "                first_bracket_s2 = json_str_step2.find('{')\n",
        "                last_bracket_s2 = json_str_step2.rfind('}')\n",
        "                if first_bracket_s2 != -1 and last_bracket_s2 != -1 and last_bracket_s2 > first_bracket_s2:\n",
        "                    json_str_step2 = json_str_step2[first_bracket_s2 : last_bracket_s2+1].strip()\n",
        "\n",
        "                # print(f\"    DEBUG Step 2 Extracted JSON string for '{phrase}': '{json_str_step2}'\")\n",
        "\n",
        "                try:\n",
        "                    parsed_json_obj = json.loads(json_str_step2)\n",
        "                    if isinstance(parsed_json_obj, dict) and \\\n",
        "                       'label' in parsed_json_obj and isinstance(parsed_json_obj['label'], str) and \\\n",
        "                       'description' in parsed_json_obj and isinstance(parsed_json_obj['description'], str) and \\\n",
        "                       parsed_json_obj['label'].startswith('[') and parsed_json_obj['label'].endswith(']'):\n",
        "\n",
        "                        sf = parsed_json_obj.get('surface_forms', [])\n",
        "                        if not isinstance(sf, list) or not all(isinstance(s, str) for s in sf):\n",
        "                            sf = [phrase] # Default to the input phrase if SFs are bad\n",
        "                        elif phrase.lower() not in [s.lower() for s in sf]: # Ensure original phrase is in SFs\n",
        "                            sf.insert(0, phrase)\n",
        "\n",
        "                        structured_motifs_list.append({\n",
        "                            \"label\": parsed_json_obj['label'].strip(),\n",
        "                            \"description\": parsed_json_obj['description'].strip(),\n",
        "                            \"surface_forms\": list(dict.fromkeys([s.strip() for s in sf if s.strip()])) # Deduplicate SFs\n",
        "                        })\n",
        "                        print(f\"    LLM Step 2 Structured Motif: {structured_motifs_list[-1]}\")\n",
        "                    # else:\n",
        "                        # print(f\"    WARN: Invalid JSON object structure from LLM Step 2 for '{phrase}': {parsed_json_obj}\")\n",
        "                except json.JSONDecodeError as e_s2:\n",
        "                    print(f\"    Error decoding JSON from LLM Step 2 for '{phrase}': {e_s2}\")\n",
        "                    print(f\"    Problematic JSON string was: '{json_str_step2}'\")\n",
        "            else:\n",
        "                print(f\"    Error: LLM Step 2 returned empty/unexpected output for phrase '{phrase}'.\")\n",
        "        except Exception as e_step2:\n",
        "            print(f\"    Error in LLM Step 2 (Structuring Phrase '{phrase}'): {e_step2}\")\n",
        "            # Continue to next phrase even if one fails\n",
        "\n",
        "    return structured_motifs_list\n",
        "\n",
        "def calculate_L_H_token_based_structured(structured_motifs_list):\n",
        "    if not structured_motifs_list: return 0.0\n",
        "    total_lh_cost = 0.0\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        current_motif_lh = 0\n",
        "        label_str = motif_obj.get('label', \"\")\n",
        "        if label_str and isinstance(label_str, str) and label_str.strip():\n",
        "            current_motif_lh += MOTIF_SYMBOLIC_LABEL_COST\n",
        "        definition_str = motif_obj.get('definition', \"\")\n",
        "        if definition_str and isinstance(definition_str, str) and definition_str.strip():\n",
        "            current_motif_lh += MOTIF_DEFINITION_TEXT_BASE_COST\n",
        "            current_motif_lh += len(definition_str.split()) * MOTIF_DEFINITION_TOKEN_COST\n",
        "        surface_forms_list = motif_obj.get('surface_forms', [])\n",
        "        if surface_forms_list and isinstance(surface_forms_list, list):\n",
        "            valid_sfs_for_lh = [sf for sf in surface_forms_list if isinstance(sf, str) and sf.strip()]\n",
        "            if valid_sfs_for_lh:\n",
        "                current_motif_lh += MOTIF_SURFACE_FORMS_LIST_BASE_COST\n",
        "                for sf_str in valid_sfs_for_lh:\n",
        "                    current_motif_lh += len(sf_str.split()) * MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH\n",
        "        total_lh_cost += current_motif_lh\n",
        "    return total_lh_cost\n",
        "\n",
        "def llm_compress_text_structured(text_to_compress, structured_motifs_list):\n",
        "    if not isinstance(text_to_compress, str): return \"\"\n",
        "    compressed = text_to_compress.lower()\n",
        "    if not structured_motifs_list: return compressed\n",
        "    for motif_obj in structured_motifs_list:\n",
        "        if not isinstance(motif_obj, dict): continue\n",
        "        label = motif_obj.get('label', \"[UNKNOWN_LABEL]\")\n",
        "        surface_forms = motif_obj.get('surface_forms', [])\n",
        "        if not label or not surface_forms or not isinstance(surface_forms, list): continue\n",
        "        sorted_sfs_for_this_motif = sorted(\n",
        "            [sf for sf in surface_forms if isinstance(sf, str) and sf.strip()],\n",
        "            key=len, reverse=True\n",
        "        )\n",
        "        placeholder = label\n",
        "        for sf in sorted_sfs_for_this_motif:\n",
        "            try:\n",
        "                compressed = re.sub(re.escape(sf.lower()), placeholder, compressed, flags=re.IGNORECASE)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error for SF '{sf}' of motif '{label}': {re_e}. Skipping.\")\n",
        "                continue\n",
        "    return compressed\n",
        "\n",
        "def text_to_binary_matrix(text_input, size=MATRIX_SIZE_GLOBAL):\n",
        "    if not text_input or not isinstance(text_input, str): return np.zeros(size, dtype=int)\n",
        "    hash_digest = hashlib.sha256(text_input.encode('utf-8', 'ignore')).hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string = bin(int(hash_digest, 16))[2:].zfill(required_bits)\n",
        "    binary_string_padded = binary_string.ljust(required_bits, '0')\n",
        "    bits = [int(b) for b in binary_string_padded[:required_bits]]\n",
        "    return np.array(bits).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input, bdm_instance, matrix_s=MATRIX_SIZE_GLOBAL):\n",
        "    if not text_input or not isinstance(text_input, str) : return 0.0\n",
        "    if not text_input.strip(): return 0.0\n",
        "    MAX_TEXT_FOR_BDM_HASH = 2000\n",
        "    text_for_hash = text_input if len(text_input) <= MAX_TEXT_FOR_BDM_HASH else text_input[:MAX_TEXT_FOR_BDM_HASH]\n",
        "    matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        return bdm_instance.bdm(matrix)\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0\n",
        "\n",
        "def compute_mdl_cost_for_text_block(text_block_str, structured_motifs_list, bdm_instance, matrix_s=MATRIX_SIZE_GLOBAL):\n",
        "    if not isinstance(text_block_str, str) : text_block_str = \"\"\n",
        "    l_h = calculate_L_H_token_based_structured(structured_motifs_list)\n",
        "    compressed_text_block = llm_compress_text_structured(text_block_str, structured_motifs_list)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "    if l_d_h < 0: return l_h, -1.0, -1.0\n",
        "    return l_h, l_d_h, l_h + l_d_h\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "def main():\n",
        "    print(\"--- MDL Prototype: Analyzing Per-QID Aggregated Text (Local LLM, Structured Motifs, Token-L(H)) ---\")\n",
        "    print(f\"Using L(H) Cost Params: Label={MOTIF_SYMBOLIC_LABEL_COST}, DefBase={MOTIF_DEFINITION_TEXT_BASE_COST}, DefToken={MOTIF_DEFINITION_TOKEN_COST}, SFListBase={MOTIF_SURFACE_FORMS_LIST_BASE_COST}, SFTokenInLH={MOTIF_SURFACE_FORM_TOKEN_COST_IN_LH}\\n\")\n",
        "\n",
        "    local_llm_pipeline = None\n",
        "    local_llm_tokenizer = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {LOCAL_LLM_MODEL_ID}...\")\n",
        "        local_llm_tokenizer = AutoTokenizer.from_pretrained(LOCAL_LLM_MODEL_ID)\n",
        "        bnb_config = None; quant_active = False\n",
        "        if USE_QUANTIZATION_FOR_LOCAL_LLM and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n",
        "                quant_active = True; print(f\"BNB config created for {LOCAL_LLM_MODEL_ID}, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb: print(f\"WARN: Failed to create BitsAndBytesConfig: {e_bnb}. Quantization disabled.\")\n",
        "\n",
        "        print(f\"Loading local model {LOCAL_LLM_MODEL_ID} (Quantization: {quant_active})...\")\n",
        "        model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "        if quant_active: model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        else:\n",
        "            if device.type == 'cuda': model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "        local_llm_model = AutoModelForCausalLM.from_pretrained(LOCAL_LLM_MODEL_ID, **model_kwargs)\n",
        "        local_llm_pipeline = pipeline(\"text-generation\", model=local_llm_model, tokenizer=local_llm_tokenizer)\n",
        "        print(f\"Local LLM pipeline for {LOCAL_LLM_MODEL_ID} initialized successfully.\")\n",
        "    except Exception as e: print(f\"CRITICAL: Failed to initialize local LLM pipeline: {e}\"); return\n",
        "\n",
        "    try:\n",
        "        bdm_instance = BDM(ndim=2); print(\"BDM instance initialized successfully.\")\n",
        "    except Exception as e_bdm_init: print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\"); return\n",
        "\n",
        "    if not os.path.exists(P2_COLLATED_FILE): print(f\"ERROR: Phase 2 output file not found: {P2_COLLATED_FILE}\"); return\n",
        "    print(f\"Loading Phase 2 output from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f: phase2_data_content = json.load(f)\n",
        "    except Exception as e: print(f\"Error loading or parsing {P2_COLLATED_FILE}: {e}\"); return\n",
        "\n",
        "    all_qid_results = []; qids_to_target = []; aggregated_content_by_qid = {}\n",
        "    if phase2_data_content: aggregated_content_by_qid = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "    if not aggregated_content_by_qid: print(f\"No 'aggregated_pdf_content_by_qid' key found or data loaded from {P2_COLLATED_FILE}.\"); return\n",
        "\n",
        "    if P3_QIDS_TO_PROCESS_THEMATICALLY and isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and len(P3_QIDS_TO_PROCESS_THEMATICALLY) > 0:\n",
        "        qids_to_target = [qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY if qid in aggregated_content_by_qid]\n",
        "        print(f\"Targeting specific QIDs: {qids_to_target}\")\n",
        "        if not qids_to_target: print(f\"WARN: Specified QIDs not found.\"); return\n",
        "    else:\n",
        "        qids_to_process_limit_fallback = 1; print(f\"Processing up to {qids_to_process_limit_fallback} QID(s).\")\n",
        "        temp_qids_to_target = [qid_key for q_idx, qid_key in enumerate(aggregated_content_by_qid.keys()) if q_idx < qids_to_process_limit_fallback]\n",
        "        qids_to_target = temp_qids_to_target\n",
        "        if not qids_to_target: print(\"No QIDs to process.\"); return\n",
        "    print(f\"\\nMDL analysis will run for QIDs: {qids_to_target}\\n\")\n",
        "\n",
        "    for qid in qids_to_target:\n",
        "        text_items_list = aggregated_content_by_qid.get(qid)\n",
        "        if not text_items_list or not isinstance(text_items_list, list): continue\n",
        "        print(f\"--- Analyzing Aggregated Text for QID: {qid} ---\")\n",
        "        all_texts_for_qid = [item.get(\"text\", \"\") for item in text_items_list if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()]\n",
        "        corpus_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(all_texts_for_qid)\n",
        "        if len(corpus_for_qid.strip()) < 100: print(f\"  Skipping QID {qid}: combined text too short.\"); continue\n",
        "\n",
        "        num_responses_for_qid = len(all_texts_for_qid)\n",
        "        print(f\"  Combined corpus for QID {qid} has {len(corpus_for_qid)} chars from {num_responses_for_qid} responses.\")\n",
        "        baseline_l_d_h = compute_bdm_for_text(corpus_for_qid, bdm_instance, MATRIX_SIZE_GLOBAL)\n",
        "        if baseline_l_d_h < 0: print(f\"  Error computing baseline BDM for QID {qid}. Skipping.\"); continue\n",
        "        baseline_total_mdl = baseline_l_d_h\n",
        "        print(f\"  Baseline MDL (L(D_orig)): {baseline_total_mdl:.4f}\")\n",
        "\n",
        "        structured_motifs = llm_extract_motifs_local_hf_json(corpus_for_qid, local_llm_pipeline, local_llm_tokenizer)\n",
        "        print(f\"  Local LLM Extracted Structured Motifs for QID {qid}:\")\n",
        "        if structured_motifs:\n",
        "            for mo in structured_motifs: print(f\"    Label: {mo.get('label')}, Def: {mo.get('definition', '')[:60]}..., SFs: {mo.get('surface_forms')}\")\n",
        "        else: print(\"    None extracted.\")\n",
        "\n",
        "        if not structured_motifs:\n",
        "            print(\"  No valid structured motifs extracted by Local LLM.\")\n",
        "            all_qid_results.append({\"qid\": qid, \"baseline_mdl\": baseline_total_mdl, \"motifs\": [], \"l_h_motifs\": 0.0, \"l_d_h_motifs\": baseline_total_mdl, \"total_mdl_motifs\": baseline_total_mdl, \"compression_achieved\": 0.0})\n",
        "            print(\"-\" * 40); continue\n",
        "\n",
        "        l_h, l_d_h, total_mdl_with_motifs = compute_mdl_cost_for_text_block(corpus_for_qid, structured_motifs, bdm_instance, MATRIX_SIZE_GLOBAL)\n",
        "        if l_d_h < 0:\n",
        "            print(f\"  Error computing MDL L(D|H) with motifs. Skipping.\"); all_qid_results.append({\"qid\": qid, \"baseline_mdl\": baseline_total_mdl, \"motifs\": structured_motifs, \"l_h_motifs\": l_h, \"l_d_h_motifs\": -1.0, \"total_mdl_motifs\": -1.0, \"compression_achieved\": \"BDM_ERROR_LDH\"})\n",
        "            print(\"-\" * 40); continue\n",
        "\n",
        "        print(f\"  L(H) (Token-based Structured) motif complexity for QID {qid}: {l_h:.4f}\")\n",
        "        print(f\"  L(D|H) (BDM-based) compressed corpus complexity for QID {qid}: {l_d_h:.4f}\")\n",
        "        print(f\"  Total MDL cost with motifs for QID {qid}: {total_mdl_with_motifs:.4f}\")\n",
        "        compression = baseline_total_mdl - total_mdl_with_motifs\n",
        "        result_status = f\"NOTE: No significant compression. Diff: {compression:.4f}\"\n",
        "        # Fixed typo in variable name for SUCCESS case:\n",
        "        if compression > 0.0001: result_status = f\"SUCCESS: Compression achieved: {compression:.4f}\"\n",
        "        print(f\"  {result_status}\")\n",
        "        all_qid_results.append({\"qid\": qid, \"baseline_mdl\": baseline_total_mdl, \"motifs\": structured_motifs, \"l_h_motifs\": l_h, \"l_d_h_motifs\": l_d_h, \"total_mdl_motifs\": total_mdl_with_motifs, \"compression_achieved\": compression})\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary (Local LLM, Structured Motifs, Token-L(H)) ---\")\n",
        "    if not all_qid_results: print(\"No QIDs processed/results generated.\")\n",
        "    else:\n",
        "        valid_results = [r for r in all_qid_results if isinstance(r.get('compression_achieved', 'error'), float)]\n",
        "        num_compressed = sum(1 for r in valid_results if r['compression_achieved'] > 0.0001)\n",
        "        success_comps = [r['compression_achieved'] for r in valid_results if r['compression_achieved'] > 0.0001]\n",
        "        avg_comp = np.mean(success_comps) if success_comps else 0\n",
        "        max_comp = np.max(success_comps) if success_comps else 0\n",
        "        print(f\"Total QIDs targeted: {len(qids_to_target)}, Results logged: {len(all_qid_results)}\")\n",
        "        print(f\"QIDs with compression: {num_compressed}\")\n",
        "        if num_compressed > 0: print(f\"  Avg compression: {avg_comp:.4f}, Max compression: {max_comp:.4f}\")\n",
        "        output_filename = \"mdl_analysis_per_qid_local_llm_structured_placeholders_v1.json\" # New filename\n",
        "        try:\n",
        "            with open(output_filename, \"w\", encoding=\"utf-8\") as f_out: json.dump(all_qid_results, f_out, indent=2)\n",
        "            print(f\"Detailed results saved to {output_filename}\")\n",
        "        except Exception as e_save: print(f\"Error saving results: {e_save}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571,
          "referenced_widgets": [
            "d5230b4536b048c9bee0a10faff8bfbd",
            "6fcda94bd47c42a7989947f2c44678db",
            "0c0c9c82d9e04a7798c39871b99839af",
            "4fe2b3fb2033466c939a5e8088dfb6ef",
            "b670f488182340c297bc8bee844153ef",
            "86c6e3f7bd514914a9989c4e49838ebb",
            "14cd06caf864448983c3c4141489adfb",
            "9d3f814138504a39a0a0afbd5214f3cc",
            "cd4ca90eba674edcb7ccfc77c6b8424c",
            "3059ddac1d404a73b20fbe488919c5ed",
            "7addaa8aa2c04c1896bce670258f15a0"
          ]
        },
        "id": "iGHtc_n-3H_2",
        "outputId": "34c0a37d-9b4a-44a4-ece2-f73aefd4e3b7"
      },
      "id": "iGHtc_n-3H_2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- MDL Prototype: Analyzing Per-QID Aggregated Text (Local LLM, Structured Motifs, Token-L(H)) ---\n",
            "Using L(H) Cost Params: Label=0.5, DefBase=0.25, DefToken=0.05, SFListBase=0.1, SFTokenInLH=0.05\n",
            "\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-2b-it...\n",
            "BNB config created for google/gemma-2b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-2b-it (Quantization: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5230b4536b048c9bee0a10faff8bfbd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-2b-it initialized successfully.\n",
            "BDM instance initialized successfully.\n",
            "Loading Phase 2 output from: /content/drive/MyDrive/Colab Notebooks/Legal/Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json...\n",
            "Targeting specific QIDs: ['Q4']\n",
            "\n",
            "MDL analysis will run for QIDs: ['Q4']\n",
            "\n",
            "--- Analyzing Aggregated Text for QID: Q4 ---\n",
            "  Combined corpus for QID Q4 has 129501 chars from 209 responses.\n",
            "  Baseline MDL (L(D_orig)): 121.3693\n",
            "  Local LLM Extracted Structured Motifs for QID Q4:\n",
            "    None extracted.\n",
            "  No valid structured motifs extracted by Local LLM.\n",
            "----------------------------------------\n",
            "\n",
            "--- Overall QID-based MDL Analysis Summary (Local LLM, Structured Motifs, Token-L(H)) ---\n",
            "Total QIDs targeted: 1, Results logged: 1\n",
            "QIDs with compression: 0\n",
            "Detailed results saved to mdl_analysis_per_qid_local_llm_structured_placeholders_v1.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict\n",
        "\n",
        "def build_prompt(response_texts: List[str]) -> str:\n",
        "    text_block = \"\\n\\n\".join(response_texts)\n",
        "    prompt = f\"\"\"You will receive a set of responses to the same question.\n",
        "\n",
        "Your task is to identify up to 5 key recurring themes.\n",
        "\n",
        "For each theme, provide:\n",
        "- A short label like [DATA_PRIVACY]\n",
        "- A 1-sentence definition\n",
        "- 2–3 short phrases that often appear in the text (surface forms)\n",
        "\n",
        "Respond in JSON format like this:\n",
        "[\n",
        "  {{\n",
        "    \"label\": \"[LABEL]\",\n",
        "    \"description\": \"Short definition...\",\n",
        "    \"surface_forms\": [\"form1\", \"form2\"]\n",
        "  }}\n",
        "]\n",
        "\n",
        "Responses:\n",
        "{text_block}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def extract_structured_motifs_from_responses(all_responses: List[str], batch_size: int = 10) -> List[Dict]:\n",
        "    all_motifs = []\n",
        "\n",
        "    for i in range(0, len(all_responses), batch_size):\n",
        "        batch = all_responses[i:i + batch_size]\n",
        "        prompt = build_prompt(batch)\n",
        "\n",
        "        try:\n",
        "            # Replace this with your actual LLM call (local model or remote)\n",
        "            llm_response = local_llm.generate(prompt)\n",
        "            motifs = json.loads(llm_response)\n",
        "\n",
        "            if isinstance(motifs, list):\n",
        "                all_motifs.extend(motifs)\n",
        "            else:\n",
        "                raise ValueError(\"Response was not a list\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed to extract motifs for batch {i//batch_size + 1}: {e}\")\n",
        "            with open(\"llm_motif_debug.txt\", \"a\") as f:\n",
        "                f.write(f\"\\n--- Batch {i//batch_size + 1} Prompt ---\\n{prompt}\")\n",
        "                f.write(f\"\\n--- LLM Response ---\\n{llm_response}\\n\")\n",
        "\n",
        "    return all_motifs\n"
      ],
      "metadata": {
        "id": "zUjnPtkUYuxv"
      },
      "id": "zUjnPtkUYuxv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_motifs = extract_structured_motifs_from_responses(qid_responses[\"Q4\"], batch_size=10)\n",
        "\n",
        "# Proceed to MDL cost estimation\n",
        "l_h = calculate_L_H_token_based(structured_motifs)\n",
        "l_d_given_h = estimate_bdm_cost_with_placeholders(corpus, structured_motifs)\n"
      ],
      "metadata": {
        "id": "femb58otYsuB"
      },
      "id": "femb58otYsuB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 30th May"
      ],
      "metadata": {
        "id": "FawZ4x3vliNJ"
      },
      "id": "FawZ4x3vliNJ"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pybdm nltk tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cXf6ZNslnbf",
        "outputId": "0158ad3a-6220-498c-93ce-cef46c3b331c"
      },
      "id": "4cXf6ZNslnbf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybdm\n",
            "  Downloading pybdm-0.1.0-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.11/dist-packages (from pybdm) (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Downloading pybdm-0.1.0-py2.py3-none-any.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybdm\n",
            "Successfully installed pybdm-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import hashlib\n",
        "\n",
        "# Initialize the BDM estimator for 2D matrices\n",
        "bdm = BDM(ndim=2)\n",
        "\n",
        "def text_to_binary_matrix(text, size=(8, 8)):\n",
        "    \"\"\"\n",
        "    Converts a text string into a binary matrix via SHA-256 hashing.\n",
        "    This ensures fixed-length binary output suitable for BDM input.\n",
        "    \"\"\"\n",
        "    # Create SHA-256 hash and convert to binary string\n",
        "    hash_digest = hashlib.sha256(text.encode()).hexdigest()\n",
        "    binary_string = bin(int(hash_digest, 16))[2:].zfill(256)  # Ensure 256 bits\n",
        "\n",
        "    # Take only enough bits to fill the matrix\n",
        "    binary_list = [int(bit) for bit in binary_string[:size[0] * size[1]]]\n",
        "\n",
        "    return np.array(binary_list).reshape(size)\n",
        "\n",
        "# Example usage\n",
        "motif = \"privacy rights\"\n",
        "matrix = text_to_binary_matrix(motif)\n",
        "complexity = bdm.bdm(matrix)\n",
        "\n",
        "print(\"Motif:\", motif)\n",
        "print(\"Matrix:\\n\", matrix)\n",
        "print(\"BDM Complexity:\", complexity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68gL0B6hmvjH",
        "outputId": "f4170474-eccd-4c57-da54-fd62346cc7af"
      },
      "id": "68gL0B6hmvjH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Motif: privacy rights\n",
            "Matrix:\n",
            " [[0 0 1 0 1 0 0 1]\n",
            " [0 1 1 0 1 0 1 1]\n",
            " [1 0 0 1 1 1 0 1]\n",
            " [0 0 0 1 1 0 0 1]\n",
            " [1 1 1 1 0 1 0 1]\n",
            " [0 0 1 0 1 0 1 1]\n",
            " [1 0 1 0 0 0 1 1]\n",
            " [0 0 1 0 1 0 0 1]]\n",
            "BDM Complexity: 129.57258675661535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from pybdm import BDM\n",
        "import hashlib\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Initialize BDM (2D)\n",
        "bdm = BDM(ndim=2)\n",
        "MATRIX_SIZE = (8, 8)\n",
        "MIN_NGRAM = 2\n",
        "MAX_NGRAM = 4\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "def discover_motifs(tokens, min_freq=2, n_range=(2,4)):\n",
        "    counts = Counter()\n",
        "    for n in range(n_range[0], n_range[1]+1):\n",
        "        counts.update(ngrams(tokens, n))\n",
        "    motifs = { ' '.join(k): v for k, v in counts.items() if v >= min_freq }\n",
        "    return motifs\n",
        "\n",
        "def text_to_binary_matrix(text, size=MATRIX_SIZE):\n",
        "    hash_digest = hashlib.sha256(text.encode()).hexdigest()\n",
        "    binary_string = bin(int(hash_digest, 16))[2:].zfill(size[0]*size[1])\n",
        "    bits = [int(b) for b in binary_string[:size[0]*size[1]]]\n",
        "    return np.array(bits).reshape(size)\n",
        "\n",
        "def encode_text_as_motifs(tokens, motifs):\n",
        "    encoded = []\n",
        "    motif_keys = sorted(motifs.keys(), key=lambda x: -len(x.split()))\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        matched = False\n",
        "        for motif in motif_keys:\n",
        "            motif_tokens = motif.split()\n",
        "            if tokens[i:i+len(motif_tokens)] == motif_tokens:\n",
        "                encoded.append(f\"<{motif}>\")\n",
        "                i += len(motif_tokens)\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched:\n",
        "            encoded.append(tokens[i])\n",
        "            i += 1\n",
        "    return encoded\n",
        "\n",
        "def chunk_sequence_to_matrices(sequence, chunk_len=64):\n",
        "    # Convert sequence (list of strings) into chunks, then each chunk to binary matrices\n",
        "    matrices = []\n",
        "    # Join sequence into a string with space separators for hashing\n",
        "    seq_str = ' '.join(sequence)\n",
        "    for i in range(0, len(seq_str), chunk_len):\n",
        "        chunk = seq_str[i:i+chunk_len]\n",
        "        matrices.append(text_to_binary_matrix(chunk))\n",
        "    return matrices\n",
        "\n",
        "def mdl_cost(motifs, encoded_seq):\n",
        "    # Calculate L(H)\n",
        "    l_h = sum(bdm.bdm(text_to_binary_matrix(m)) for m in motifs.keys())\n",
        "\n",
        "    # Calculate L(D|H)\n",
        "    matrices = chunk_sequence_to_matrices(encoded_seq)\n",
        "    l_d_h = sum(bdm.bdm(m) for m in matrices)\n",
        "\n",
        "    return l_h, l_d_h, l_h + l_d_h\n",
        "\n",
        "# Sample JSONL input for Q4\n",
        "sample_jsonl_line = '''\n",
        "{\n",
        "  \"Q4\": {\n",
        "    \"question_text\": \"Noting the current individual rights contained in Australian Privacy Principles 12 and 13, and the proposed individual rights in proposals 18.1, 18.2 and 18.3, what specific exceptions (if any) should apply to these rights in the employment context?\",\n",
        "    \"status\": \"success_summarized\",\n",
        "    \"extracted_passages\": [\n",
        "      \"13 March 2023 Attorney-Generals Department Email: To whom it may concern Thank you for the opportunity to provide feedback on the Privacy Act Review Report. I would like to raise an issue that undermines the privacy rights of Australians. Property data is available for everyone and websites such as Realestate.com.au display past and current pictures of your home, even though the property is off market.\"\n",
        "    ],\n",
        "    \"top_passage_score\": 0.6251732707023621,\n",
        "    \"summary\": \"This excerpt highlights a significant concern regarding privacy rights in Australia, specifically concerning the availability of property data. The Attorney-Generals Department points to the Privacy Act 2022, referencing principles 12 and 13, and proposes new rights for individuals in proposals 18.1, 18.2, and 18.3. The core issue is that real estate websites like Realestate.com.au are collecting and displaying property images, potentially violating individuals’ right to control their personal information and privacy.\"\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "def run_example(jsonl_line):\n",
        "    data = json.loads(jsonl_line)\n",
        "    qid = next(iter(data))\n",
        "    passage = data[qid]['extracted_passages'][0]\n",
        "    tokens = normalize_text(passage)\n",
        "    motifs = discover_motifs(tokens)\n",
        "    encoded_seq = encode_text_as_motifs(tokens, motifs)\n",
        "    l_h, l_d_h, total = mdl_cost(motifs, encoded_seq)\n",
        "\n",
        "    print(f\"QID: {qid}\")\n",
        "    print(f\"Motifs (L(H)):\\n{motifs}\")\n",
        "    print(f\"Encoded sequence:\\n{encoded_seq}\")\n",
        "    print(f\"L(H) (motifs complexity): {l_h}\")\n",
        "    print(f\"L(D|H) (encoded text complexity): {l_d_h}\")\n",
        "    print(f\"Total MDL cost: {total}\")\n",
        "\n",
        "run_example(sample_jsonl_line)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9qBENuroQsT",
        "outputId": "6305b034-ccb2-4e2d-964c-50cadeafbc31"
      },
      "id": "Q9qBENuroQsT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QID: Q4\n",
            "Motifs (L(H)):\n",
            "{'the privacy': 2}\n",
            "Encoded sequence:\n",
            "['13', 'march', '2023', 'attorneygenerals', 'department', 'email', 'to', 'whom', 'it', 'may', 'concern', 'thank', 'you', 'for', 'the', 'opportunity', 'to', 'provide', 'feedback', 'on', '<the privacy>', 'act', 'review', 'report', 'i', 'would', 'like', 'to', 'raise', 'an', 'issue', 'that', 'undermines', '<the privacy>', 'rights', 'of', 'australians', 'property', 'data', 'is', 'available', 'for', 'everyone', 'and', 'websites', 'such', 'as', 'realestatecomau', 'display', 'past', 'and', 'current', 'pictures', 'of', 'your', 'home', 'even', 'though', 'the', 'property', 'is', 'off', 'market']\n",
            "L(H) (motifs complexity): 125.47929777772686\n",
            "L(D|H) (encoded text complexity): 868.968392491515\n",
            "Total MDL cost: 994.4476902692419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MWP using Q4"
      ],
      "metadata": {
        "id": "x2kJYKbyp1Dn"
      },
      "id": "x2kJYKbyp1Dn"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLicpX0rp-j_",
        "outputId": "70e69180-2eb7-4d0e-ce09-d79e3673754f"
      },
      "id": "VLicpX0rp-j_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.16.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.4.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lgp5oS5yqKzP"
      },
      "id": "Lgp5oS5yqKzP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "y0w-hNjbOaxY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "y0w-hNjbOaxY"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from pybdm import BDM\n",
        "\n",
        "bdm = BDM(ndim=2)\n",
        "MATRIX_SIZE = (8, 8)\n",
        "\n",
        "# Initialize Gemini client once (make sure GEMINI_API_KEY is set in your env)\n",
        "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "model = \"gemma-3n-e4b-it\"\n",
        "\n",
        "def llm_extract_motifs(text):\n",
        "    prompt = f\"\"\"\n",
        "    Extract a concise list of 5 key themes or motifs from the following text. List them separated by commas:\n",
        "\n",
        "    Text:\n",
        "    \\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "    \"\"\"\n",
        "\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[types.Part.from_text(text=prompt)]\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    config = types.GenerateContentConfig(response_mime_type=\"text/plain\")\n",
        "\n",
        "    response = client.models.generate_content(model=model, contents=contents, config=config)\n",
        "    motifs_text = response.text.strip()\n",
        "\n",
        "    motifs = [m.strip() for m in motifs_text.split(\",\") if m.strip()]\n",
        "    return motifs\n",
        "\n",
        "def llm_compress_text(text, motifs):\n",
        "    compressed = text.lower()\n",
        "    for motif in motifs:\n",
        "        compressed = compressed.replace(motif, f\"<{motif}>\")\n",
        "    return compressed\n",
        "\n",
        "def text_to_binary_matrix(text, size=MATRIX_SIZE):\n",
        "    hash_digest = hashlib.sha256(text.encode()).hexdigest()\n",
        "    binary_string = bin(int(hash_digest, 16))[2:].zfill(size[0]*size[1])\n",
        "    bits = [int(b) for b in binary_string[:size[0]*size[1]]]\n",
        "    return np.array(bits).reshape(size)\n",
        "\n",
        "def compute_mdl_cost(text, motifs):\n",
        "    l_h = sum(bdm.bdm(text_to_binary_matrix(m)) for m in motifs)\n",
        "    compressed_text = llm_compress_text(text, motifs)\n",
        "    l_d_h = bdm.bdm(text_to_binary_matrix(compressed_text))\n",
        "    return l_h, l_d_h, l_h + l_d_h\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sample_text = (\n",
        "        \"13 March 2023 Attorney-Generals Department Email: To whom it may concern \"\n",
        "        \"Thank you for the opportunity to provide feedback on the Privacy Act Review Report. \"\n",
        "        \"I would like to raise an issue that undermines the privacy rights of Australians. \"\n",
        "        \"Property data is available for everyone and websites such as Realestate.com.au display \"\n",
        "        \"past and current pictures of your home, even though the property is off market.\"\n",
        "    )\n",
        "\n",
        "    motifs = llm_extract_motifs(sample_text)\n",
        "    print(\"Extracted motifs:\", motifs)\n",
        "\n",
        "    l_h, l_d_h, total = compute_mdl_cost(sample_text, motifs)\n",
        "    print(f\"L(H) motif complexity: {l_h:.4f}\")\n",
        "    print(f\"L(D|H) compressed text complexity: {l_d_h:.4f}\")\n",
        "    print(f\"Total MDL cost: {total:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alHOjPZEp31P",
        "outputId": "63cf60d7-e0f7-426a-821c-486c7ffcc7b4"
      },
      "id": "alHOjPZEp31P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted motifs: ['Privacy rights', 'Data accessibility', 'Property data exposure', 'Online information sharing', 'Off-market property information.']\n",
            "L(H) motif complexity: 603.6927\n",
            "L(D|H) compressed text complexity: 128.5227\n",
            "Total MDL cost: 732.2154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input text extracted using existing Thematic Analysis Pipeline"
      ],
      "metadata": {
        "id": "Ij_ypWQlvD6v"
      },
      "id": "Ij_ypWQlvD6v"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KgoDOXWvhr-"
      },
      "id": "2KgoDOXWvhr-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Previous BDM MDL Prototype Code (Imports, BDM init, text_to_binary_matrix, etc.) ---\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# --- Configuration from your larger codebase (Cell 2) ---\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # EXAMPLE - UPDATE\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json') # Corrected variable name\n",
        "\n",
        "# --- Your existing BDM MDL functions (ensure they are defined above) ---\n",
        "# (BDM, MATRIX_SIZE, client, model from your BDM prototype)\n",
        "# Function definitions (llm_extract_motifs, llm_compress_text, text_to_binary_matrix,\n",
        "# compute_bdm_for_text, compute_mdl_cost_for_text_block)\n",
        "\n",
        "def llm_extract_motifs(text_to_analyze, genai_client, llm_model_name):\n",
        "    # Your existing llm_extract_motifs function\n",
        "    prompt = f\"\"\"\n",
        "    From the following text, which comprises several responses to a single question,\n",
        "    extract a concise list of up to 5 key recurring themes or motifs.\n",
        "    List them separated by commas, with no introductory text.\n",
        "    If no clear themes, output \"NO_THEMES_FOUND\".\n",
        "\n",
        "    Text:\n",
        "    \\\"\\\"\\\"{text_to_analyze}\\\"\\\"\\\"\n",
        "\n",
        "    Recurring Themes/Motifs:\n",
        "    \"\"\"\n",
        "    # Limit text_to_analyze to avoid exceeding LLM context window for this prototype\n",
        "    # A more robust solution would involve chunking the text if it's too long.\n",
        "    MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION = 15000 # Approx characters, adjust based on model\n",
        "    if len(text_to_analyze) > MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION:\n",
        "        print(f\"    Warning: Text for motif extraction is long ({len(text_to_analyze)} chars), truncating to {MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION} for LLM.\")\n",
        "        text_to_analyze = text_to_analyze[:MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION]\n",
        "\n",
        "\n",
        "    contents = [types.Content(role=\"user\", parts=[types.Part.from_text(text=prompt)])]\n",
        "    config = types.GenerateContentConfig(response_mime_type=\"text/plain\")\n",
        "\n",
        "    try:\n",
        "        response = genai_client.models.generate_content(model=llm_model_name, contents=contents, config=config)\n",
        "        motifs_text = response.text.strip()\n",
        "        if \"NO_THEMES_FOUND\" in motifs_text or not motifs_text:\n",
        "            return []\n",
        "        motifs = [m.strip() for m in motifs_text.split(\",\") if m.strip() and len(m.strip()) > 3] # Slightly longer min motif\n",
        "        return motifs[:5] # Max 5 motifs\n",
        "    except Exception as e:\n",
        "        print(f\"Error in llm_extract_motifs for text starting with '{text_to_analyze[:50]}...': {e}\")\n",
        "        return []\n",
        "\n",
        "def llm_compress_text(text_to_compress, motifs_list):\n",
        "    # Your existing llm_compress_text function\n",
        "    compressed = text_to_compress.lower() # Consistent case for replacement\n",
        "    for motif in motifs_list:\n",
        "        if motif and isinstance(motif, str): # Ensure motif is a non-empty string\n",
        "            placeholder = f\"<MOTIF_{motif.replace(' ', '_').upper().replace('[','').replace(']','').replace(':','_')[:20]}>\" # Sanitize and shorten placeholder\n",
        "            try:\n",
        "                # Use re.escape to handle special characters in motifs\n",
        "                compressed = re.sub(re.escape(motif.lower()), placeholder, compressed, flags=re.IGNORECASE)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for motif '{motif}': {re_e}. Skipping this motif for compression.\")\n",
        "                continue # Skip problematic motif\n",
        "    return compressed\n",
        "\n",
        "def text_to_binary_matrix(text_input, size=(8,8)):\n",
        "    # Your existing text_to_binary_matrix function\n",
        "    if not text_input or not isinstance(text_input, str):\n",
        "        return np.zeros(size, dtype=int)\n",
        "    hash_digest = hashlib.sha256(text_input.encode('utf-8', 'ignore')).hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string = bin(int(hash_digest, 16))[2:].zfill(required_bits)\n",
        "    binary_string_padded = binary_string.ljust(required_bits, '0')\n",
        "    bits = [int(b) for b in binary_string_padded[:required_bits]]\n",
        "    return np.array(bits).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input, bdm_instance, matrix_s=(8,8)):\n",
        "    if not text_input: return 0.0\n",
        "    # BDM can be slow for very long strings if we try to make one giant matrix.\n",
        "    # For this prototype, we'll still use one matrix per text block (motif or compressed text).\n",
        "    # A more advanced version might chunk long texts for BDM.\n",
        "    MAX_BDM_TEXT_LEN = 2000 # Heuristic limit for direct BDM to avoid excessive slowness\n",
        "    if len(text_input) > MAX_BDM_TEXT_LEN:\n",
        "        # Simple strategy: take BDM of a representative sample (e.g. hash of truncated)\n",
        "        # This is a simplification for the prototype.\n",
        "        # print(f\"      BDM input text long ({len(text_input)}), using BDM of truncated hash for speed.\")\n",
        "        text_input_for_bdm = text_input[:MAX_BDM_TEXT_LEN]\n",
        "    else:\n",
        "        text_input_for_bdm = text_input\n",
        "\n",
        "    matrix = text_to_binary_matrix(text_input_for_bdm, size=matrix_s)\n",
        "    return bdm_instance.bdm(matrix)\n",
        "\n",
        "def compute_mdl_cost_for_text_block(text_block_str, motifs_list, bdm_instance, matrix_s=(8,8)):\n",
        "    if not motifs_list:\n",
        "        l_h = 0.0\n",
        "    else:\n",
        "        valid_motifs = [m for m in motifs_list if isinstance(m, str) and m.strip()]\n",
        "        if not valid_motifs:\n",
        "            l_h = 0.0\n",
        "        else:\n",
        "            l_h = sum(compute_bdm_for_text(m, bdm_instance, matrix_s) for m in valid_motifs)\n",
        "\n",
        "    compressed_text_block = llm_compress_text(text_block_str, motifs_list)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "\n",
        "    return l_h, l_d_h, l_h + l_d_h\n",
        "\n",
        "# --- Main Processing Logic ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Initialize BDM and LLM Client ---\n",
        "    if 'GEMINI_API_KEY' not in os.environ:\n",
        "        print(\"CRITICAL: GEMINI_API_KEY environment variable not set.\")\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI_API_KEY')\n",
        "            if 'GEMINI_API_KEY' not in os.environ or not os.environ['GEMINI_API_KEY']:\n",
        "                 print(\"CRITICAL: GEMINI_API_KEY still not found after Colab check.\")\n",
        "                 exit()\n",
        "            print(\"Loaded GEMINI_API_KEY from Colab secrets.\")\n",
        "        except ImportError:\n",
        "            print(\"CRITICAL: Not in Colab and GEMINI_API_KEY not set directly.\")\n",
        "            exit()\n",
        "        except Exception as e_key:\n",
        "            print(f\"CRITICAL: Error loading GEMINI_API_KEY from Colab secrets: {e_key}\")\n",
        "            exit()\n",
        "\n",
        "    try:\n",
        "        g_client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "        g_model_name = \"gemma-3n-e4b-it\"\n",
        "        MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "        print(f\"Successfully initialized GenAI client for model {g_model_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to initialize GenAI client: {e}\")\n",
        "        exit()\n",
        "\n",
        "    bdm_instance = BDM(ndim=2)\n",
        "\n",
        "    # --- Load and Process Data from Phase 2 Output ---\n",
        "    if not os.path.exists(P2_COLLATED_FILE):\n",
        "        print(f\"ERROR: Phase 2 output file not found: {P2_COLLATED_FILE}\")\n",
        "        exit()\n",
        "\n",
        "    print(f\"Loading Phase 2 output from: {P2_COLLATED_FILE}...\")\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f:\n",
        "            phase2_data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or parsing {P2_COLLATED_FILE}: {e}\")\n",
        "        exit()\n",
        "\n",
        "    all_qid_results = []\n",
        "    # Limit QIDs to process for this prototype run\n",
        "    qids_to_process_limit = 2 # Process first X QIDs found in the file\n",
        "\n",
        "    print(f\"\\nProcessing up to {qids_to_process_limit} QIDs for MDL analysis...\\n\")\n",
        "\n",
        "    qids_processed_count = 0\n",
        "    aggregated_content_by_qid = phase2_data.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "    if not aggregated_content_by_qid:\n",
        "        print(f\"No 'aggregated_pdf_content_by_qid' key found in {P2_COLLATED_FILE}. Check file structure.\")\n",
        "        exit()\n",
        "\n",
        "    for qid, text_items_list in aggregated_content_by_qid.items():\n",
        "        if qids_processed_count >= qids_to_process_limit:\n",
        "            break\n",
        "\n",
        "        if not text_items_list or not isinstance(text_items_list, list):\n",
        "            # print(f\"Skipping QID {qid}: no text items or not a list.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"--- Analyzing Aggregated Text for QID: {qid} ---\")\n",
        "\n",
        "        # 1. Concatenate all text for this QID\n",
        "        #    Prioritize 'pdf_passages' if available, else 'pdf_summary' if that's what Phase 2 collated\n",
        "        #    Your Phase 2 logic seems to choose one or the other already.\n",
        "        all_texts_for_qid = [item.get(\"text\", \"\") for item in text_items_list if isinstance(item, dict) and item.get(\"text\")]\n",
        "\n",
        "        # Join with a clear separator that's unlikely to be part of normal text\n",
        "        # This separator itself adds to the \"complexity\" of the original text block from BDM's perspective.\n",
        "        corpus_for_qid = \"\\n\\n<RESPONSE_SEPARATOR>\\n\\n\".join(filter(None, all_texts_for_qid))\n",
        "\n",
        "        if len(corpus_for_qid.strip()) < 100: # Skip if the combined corpus for this QID is too short\n",
        "            print(f\"  Skipping QID {qid}: combined text too short ({len(corpus_for_qid)} chars).\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  Combined corpus for QID {qid} has {len(corpus_for_qid)} characters from {len(all_texts_for_qid)} responses.\")\n",
        "        # print(f\"  Corpus Snippet (first 200 chars): {corpus_for_qid[:200].replace(chr(10), ' ')}...\")\n",
        "\n",
        "\n",
        "        # 2. Baseline MDL cost (original aggregated text for this QID)\n",
        "        baseline_l_d_h = compute_bdm_for_text(corpus_for_qid, bdm_instance, MATRIX_SIZE_GLOBAL)\n",
        "        baseline_total_mdl = baseline_l_d_h # L(H) is 0 for baseline\n",
        "        print(f\"  Baseline MDL for QID {qid} (L(D_orig_corpus_for_qid)): {baseline_total_mdl:.4f}\")\n",
        "\n",
        "        # 3. Extract motifs using LLM from this aggregated QID corpus\n",
        "        #    The prompt for llm_extract_motifs is already geared towards \"recurring themes\"\n",
        "        extracted_motifs = llm_extract_motifs(corpus_for_qid, g_client, g_model_name)\n",
        "        print(f\"  LLM Extracted Motifs for QID {qid}: {extracted_motifs}\")\n",
        "\n",
        "        if not extracted_motifs:\n",
        "            print(\"  No motifs extracted by LLM for this QID, skipping MDL cost calculation with motifs.\")\n",
        "            all_qid_results.append({\n",
        "                \"qid\": qid, \"corpus_len_for_qid\": len(corpus_for_qid),\n",
        "                \"baseline_mdl\": baseline_total_mdl, \"motifs\": [],\n",
        "                \"l_h_motifs\": 0, \"l_d_h_motifs\": baseline_total_mdl, \"total_mdl_motifs\": baseline_total_mdl,\n",
        "                \"compression_achieved\": 0\n",
        "            })\n",
        "            qids_processed_count += 1\n",
        "            continue\n",
        "\n",
        "        # 4. Compute MDL cost with LLM-extracted motifs for this QID's corpus\n",
        "        l_h, l_d_h, total_mdl_with_motifs = compute_mdl_cost_for_text_block(\n",
        "            corpus_for_qid, extracted_motifs, bdm_instance, MATRIX_SIZE_GLOBAL\n",
        "        )\n",
        "\n",
        "        print(f\"  L(H) motif complexity for QID {qid}: {l_h:.4f}\")\n",
        "        print(f\"  L(D|H) compressed corpus complexity for QID {qid}: {l_d_h:.4f}\")\n",
        "        print(f\"  Total MDL cost with motifs for QID {qid}: {total_mdl_with_motifs:.4f}\")\n",
        "\n",
        "        compression = baseline_total_mdl - total_mdl_with_motifs\n",
        "        if compression > 0.0001: # Use a small epsilon\n",
        "            print(f\"  SUCCESS for QID {qid}: Compression achieved: {compression:.4f}\")\n",
        "        else:\n",
        "            print(f\"  NOTE for QID {qid}: No significant compression achieved (or cost increased). Diff: {compression:.4f}\")\n",
        "\n",
        "        all_qid_results.append({\n",
        "            \"qid\": qid, \"corpus_len_for_qid\": len(corpus_for_qid),\n",
        "            \"baseline_mdl\": baseline_total_mdl, \"motifs\": extracted_motifs,\n",
        "            \"l_h_motifs\": l_h, \"l_d_h_motifs\": l_d_h, \"total_mdl_motifs\": total_mdl_with_motifs,\n",
        "            \"compression_achieved\": compression\n",
        "        })\n",
        "        qids_processed_count += 1\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary ---\")\n",
        "    if not all_qid_results:\n",
        "        print(\"No QIDs were processed or no valid results generated.\")\n",
        "    else:\n",
        "        num_compressed_qids = sum(1 for r in all_qid_results if r['compression_achieved'] > 0.0001)\n",
        "        successful_compressions = [r['compression_achieved'] for r in all_qid_results if r['compression_achieved'] > 0.0001]\n",
        "\n",
        "        avg_compression = np.mean(successful_compressions) if successful_compressions else 0\n",
        "        max_compression = np.max(successful_compressions) if successful_compressions else 0\n",
        "\n",
        "        print(f\"Total QIDs analyzed: {len(all_qid_results)}\")\n",
        "        print(f\"Number of QIDs where compression was achieved: {num_compressed_qids}\")\n",
        "        if num_compressed_qids > 0:\n",
        "            print(f\"  Average compression (for successful QIDs): {avg_compression:.4f}\")\n",
        "            print(f\"  Maximum compression achieved across QIDs: {max_compression:.4f}\")\n",
        "\n",
        "        output_filename_qids = \"mdl_analysis_per_qid.json\"\n",
        "        try:\n",
        "            with open(output_filename_qids, \"w\", encoding=\"utf-8\") as f_out:\n",
        "                json.dump(all_qid_results, f_out, indent=2)\n",
        "            print(f\"Detailed QID-based results saved to {output_filename_qids}\")\n",
        "        except Exception as e_save:\n",
        "            print(f\"Error saving QID-based results to {output_filename_qids}: {e_save}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "dvnwKICyvTu_",
        "outputId": "fec85c9e-f578-49a9-a228-4f199c9e9a1c"
      },
      "id": "dvnwKICyvTu_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRITICAL: GEMINI_API_KEY environment variable not set.\n",
            "Loaded GEMINI_API_KEY from Colab secrets.\n",
            "Successfully initialized GenAI client for model gemma-3n-e4b-it.\n",
            "ERROR: Phase 2 output file not found: /content/drive/MyDrive/Colab Notebooks/Legal/Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json\n",
            "Loading Phase 2 output from: /content/drive/MyDrive/Colab Notebooks/Legal/Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json...\n",
            "Error loading or parsing /content/drive/MyDrive/Colab Notebooks/Legal/Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Legal/Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json'\n",
            "\n",
            "Processing up to 2 QIDs for MDL analysis...\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'phase2_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e7fe9acf172e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mqids_processed_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0maggregated_content_by_qid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphase2_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"aggregated_pdf_content_by_qid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0maggregated_content_by_qid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No 'aggregated_pdf_content_by_qid' key found in {P2_COLLATED_FILE}. Check file structure.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'phase2_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports (ensure all necessary imports are at the top) ---\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "from google import genai # Assuming these are needed if not already imported from other cells\n",
        "from google.genai import types # Assuming these are needed\n",
        "\n",
        "# --- Configuration (Define paths and constants) ---\n",
        "# These would typically be in your \"Cell 2: Global Project Configuration\"\n",
        "# For this standalone cell, ensure they are defined or adjust paths as needed.\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # !!! EXAMPLE - UPDATE THIS PATH !!!\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "# This is the output file from your \"Cell 4: Phase 2 - Collation\" script\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "# --- BDM and LLM Model Configuration ---\n",
        "MATRIX_SIZE_GLOBAL = (8, 8) # For BDM text_to_binary_matrix\n",
        "\n",
        "# --- Helper Function Definitions ---\n",
        "\n",
        "def llm_extract_motifs(text_to_analyze, genai_client, llm_model_name):\n",
        "    \"\"\"Extracts motifs from text using the specified LLM.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    From the following text, which comprises several responses to a single question,\n",
        "    extract a concise list of up to 5 key recurring themes or motifs.\n",
        "    List them separated by commas, with no introductory text.\n",
        "    If no clear themes or the text is too generic, output \"NO_THEMES_FOUND\".\n",
        "\n",
        "    Text (analyse for recurring themes):\n",
        "    \\\"\\\"\\\"{text_to_analyze}\\\"\\\"\\\"\n",
        "\n",
        "    Recurring Themes/Motifs (comma-separated, max 5):\n",
        "    \"\"\"\n",
        "    MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION = 15000 # Approx characters\n",
        "    original_len = len(text_to_analyze)\n",
        "    if original_len > MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION:\n",
        "        # print(f\"    Warning: Text for motif extraction is long ({original_len} chars), truncating to {MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION} for LLM.\")\n",
        "        text_to_analyze = text_to_analyze[:MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION]\n",
        "\n",
        "    contents = [types.Content(role=\"user\", parts=[types.Part.from_text(text=prompt)])]\n",
        "    config = types.GenerateContentConfig(response_mime_type=\"text/plain\")\n",
        "\n",
        "    try:\n",
        "        response = genai_client.models.generate_content(model=llm_model_name, contents=contents, config=config)\n",
        "        motifs_text = response.text.strip()\n",
        "        if \"NO_THEMES_FOUND\" in motifs_text.upper() or not motifs_text:\n",
        "            return []\n",
        "        motifs = [m.strip() for m in motifs_text.split(\",\") if m.strip() and len(m.strip()) > 3]\n",
        "        return motifs[:5] # Max 5 motifs\n",
        "    except Exception as e:\n",
        "        print(f\"    Error in llm_extract_motifs (text len {original_len}, truncated to {len(text_to_analyze)}): {e}\")\n",
        "        return []\n",
        "\n",
        "def llm_compress_text(text_to_compress, motifs_list):\n",
        "    \"\"\"Compresses text by replacing motifs with placeholders.\"\"\"\n",
        "    if not isinstance(text_to_compress, str): return \"\" # Handle non-string input\n",
        "    compressed = text_to_compress.lower()\n",
        "    if not motifs_list: return compressed\n",
        "\n",
        "    for motif in motifs_list:\n",
        "        if motif and isinstance(motif, str) and motif.strip():\n",
        "            # Sanitize motif for placeholder: replace non-alphanum with underscore, uppercase, limit length\n",
        "            safe_placeholder_name = re.sub(r'\\W+', '_', motif).upper()[:20]\n",
        "            placeholder = f\"<MOTIF_{safe_placeholder_name}>\"\n",
        "            try:\n",
        "                compressed = re.sub(re.escape(motif.lower()), placeholder, compressed, flags=re.IGNORECASE)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for motif '{motif}': {re_e}. Skipping.\")\n",
        "                continue\n",
        "    return compressed\n",
        "\n",
        "def text_to_binary_matrix(text_input, size=MATRIX_SIZE_GLOBAL):\n",
        "    \"\"\"Converts text to a binary matrix via hashing for BDM.\"\"\"\n",
        "    if not text_input or not isinstance(text_input, str):\n",
        "        return np.zeros(size, dtype=int)\n",
        "    hash_digest = hashlib.sha256(text_input.encode('utf-8', 'ignore')).hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string = bin(int(hash_digest, 16))[2:].zfill(required_bits)\n",
        "    binary_string_padded = binary_string.ljust(required_bits, '0')\n",
        "    bits = [int(b) for b in binary_string_padded[:required_bits]]\n",
        "    return np.array(bits).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input, bdm_instance, matrix_s=MATRIX_SIZE_GLOBAL):\n",
        "    \"\"\"Computes BDM for a given text string.\"\"\"\n",
        "    if not text_input or not isinstance(text_input, str) : return 0.0 # Treat BDM of empty/non-string as 0\n",
        "\n",
        "    # BDM complexity can be sensitive to very small variations if matrix is small\n",
        "    # For consistency, ensure text_input is not just whitespace\n",
        "    if not text_input.strip(): return 0.0\n",
        "\n",
        "    # Simplification: BDM applied to a hash of the text (truncated if very long)\n",
        "    # This makes BDM less sensitive to actual text length for L(D|H) but consistent.\n",
        "    MAX_TEXT_FOR_BDM_HASH = 2000 # Apply hash to a potentially truncated version for BDM consistency\n",
        "    text_for_hash = text_input if len(text_input) <= MAX_TEXT_FOR_BDM_HASH else text_input[:MAX_TEXT_FOR_BDM_HASH]\n",
        "\n",
        "    matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        return bdm_instance.bdm(matrix)\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0 # Indicate BDM error\n",
        "\n",
        "def compute_mdl_cost_for_text_block(text_block_str, motifs_list, bdm_instance, matrix_s=MATRIX_SIZE_GLOBAL):\n",
        "    \"\"\"Computes L(H), L(D|H), and total MDL cost for a text block and its motifs.\"\"\"\n",
        "    if not isinstance(text_block_str, str) : text_block_str = \"\" # Ensure it's a string\n",
        "\n",
        "    l_h = 0.0\n",
        "    valid_motifs_for_lh = []\n",
        "    if motifs_list:\n",
        "        valid_motifs_for_lh = [m for m in motifs_list if isinstance(m, str) and m.strip()]\n",
        "        if valid_motifs_for_lh:\n",
        "            l_h = sum(compute_bdm_for_text(m, bdm_instance, matrix_s) for m in valid_motifs_for_lh)\n",
        "\n",
        "    # L(D|H): Cost of the data (text_block) compressed with these motifs\n",
        "    # Use all original motifs_list for compression, even if some were empty/invalid for L(H)\n",
        "    compressed_text_block = llm_compress_text(text_block_str, motifs_list)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "\n",
        "    # Handle BDM error indication\n",
        "    if l_h < 0 or l_d_h < 0:\n",
        "        return -1.0, -1.0, -1.0 # Indicate error in MDL cost calculation\n",
        "\n",
        "    return l_h, l_d_h, l_h + l_d_h\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "def main():\n",
        "    print(\"--- MDL Prototype: Analyzing Per-QID Aggregated Text from Phase 2 ---\")\n",
        "    # --- Initialize BDM and LLM Client ---\n",
        "    global g_client, g_model_name # Make them accessible if defined outside main\n",
        "\n",
        "    if 'GEMINI_API_KEY' not in os.environ:\n",
        "        print(\"CRITICAL: GEMINI_API_KEY environment variable not set.\")\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            gemini_api_key_val = userdata.get('GEMINI_API_KEY')\n",
        "            if not gemini_api_key_val:\n",
        "                print(\"CRITICAL: GEMINI_API_KEY not found in Colab secrets.\")\n",
        "                return # Use return instead of exit() if in a function\n",
        "            os.environ['GEMINI_API_KEY'] = gemini_api_key_val\n",
        "            print(\"Loaded GEMINI_API_KEY from Colab secrets.\")\n",
        "        except ImportError:\n",
        "            print(\"CRITICAL: Not in Colab and GEMINI_API_KEY not set directly.\")\n",
        "            return\n",
        "        except Exception as e_key:\n",
        "            print(f\"CRITICAL: Error loading GEMINI_API_KEY from Colab secrets: {e_key}\")\n",
        "            return\n",
        "\n",
        "    try:\n",
        "        g_client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "        g_model_name = \"gemma-3n-e4b-it\" # The model that worked for you\n",
        "        print(f\"Successfully initialized GenAI client for model {g_model_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to initialize GenAI client: {e}\")\n",
        "        return\n",
        "\n",
        "    # bdm_instance = BDM(ndim=2, block_size=None, use_ctm=False) # Example BDM config, adjust if needed\n",
        "    bdm_instance = BDM(ndim=2) # Example BDM config, adjust if needed\n",
        "\n",
        "    # --- Load and Process Data from Phase 2 Output ---\n",
        "    if not os.path.exists(P2_COLLATED_FILE):\n",
        "        print(f\"ERROR: Phase 2 output file not found: {P2_COLLATED_FILE}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loading Phase 2 output from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f:\n",
        "            phase2_data_content = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or parsing {P2_COLLATED_FILE}: {e}\")\n",
        "        return\n",
        "\n",
        "    all_qid_results = []\n",
        "    qids_to_process_limit = 3 # Process first X QIDs found in the file for speed\n",
        "\n",
        "    print(f\"\\nProcessing up to {qids_to_process_limit} QIDs for MDL analysis...\\n\")\n",
        "\n",
        "    qids_processed_count = 0\n",
        "\n",
        "    if phase2_data_content is None:\n",
        "        print(f\"CRITICAL: Failed to load data from {P2_COLLATED_FILE}. Cannot proceed.\")\n",
        "        aggregated_content_by_qid = {}\n",
        "    else:\n",
        "        aggregated_content_by_qid = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "\n",
        "    if not aggregated_content_by_qid:\n",
        "        print(f\"No 'aggregated_pdf_content_by_qid' key found or data loaded from {P2_COLLATED_FILE}.\")\n",
        "        return\n",
        "\n",
        "    for qid, text_items_list in aggregated_content_by_qid.items():\n",
        "        if qids_processed_count >= qids_to_process_limit:\n",
        "            break\n",
        "\n",
        "        if not text_items_list or not isinstance(text_items_list, list):\n",
        "            continue\n",
        "\n",
        "        print(f\"--- Analyzing Aggregated Text for QID: {qid} ---\")\n",
        "\n",
        "        all_texts_for_qid = [\n",
        "            item.get(\"text\", \"\") for item in text_items_list\n",
        "            if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()\n",
        "        ]\n",
        "\n",
        "        corpus_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(all_texts_for_qid) # Shorter separator\n",
        "\n",
        "        if len(corpus_for_qid.strip()) < 100:\n",
        "            print(f\"  Skipping QID {qid}: combined text too short ({len(corpus_for_qid)} chars).\")\n",
        "            continue\n",
        "\n",
        "        num_responses_for_qid = len(all_texts_for_qid)\n",
        "        print(f\"  Combined corpus for QID {qid} has {len(corpus_for_qid)} chars from {num_responses_for_qid} responses.\")\n",
        "\n",
        "        baseline_l_d_h = compute_bdm_for_text(corpus_for_qid, bdm_instance, MATRIX_SIZE_GLOBAL)\n",
        "        if baseline_l_d_h < 0: # BDM Error\n",
        "            print(f\"  Error computing baseline BDM for QID {qid}. Skipping.\")\n",
        "            continue\n",
        "        baseline_total_mdl = baseline_l_d_h\n",
        "        print(f\"  Baseline MDL for QID {qid} (L(D_orig_corpus_for_qid)): {baseline_total_mdl:.4f}\")\n",
        "\n",
        "        extracted_motifs = llm_extract_motifs(corpus_for_qid, g_client, g_model_name)\n",
        "        print(f\"  LLM Extracted Motifs for QID {qid}: {extracted_motifs}\")\n",
        "\n",
        "        if not extracted_motifs:\n",
        "            print(\"  No valid motifs extracted by LLM for this QID.\")\n",
        "            # Store result even if no motifs, for completeness\n",
        "            all_qid_results.append({\n",
        "                \"qid\": qid, \"corpus_len_for_qid\": len(corpus_for_qid), \"num_responses\": num_responses_for_qid,\n",
        "                \"baseline_mdl\": baseline_total_mdl, \"motifs\": [],\n",
        "                \"l_h_motifs\": 0, \"l_d_h_motifs\": baseline_total_mdl, \"total_mdl_motifs\": baseline_total_mdl,\n",
        "                \"compression_achieved\": 0.0\n",
        "            })\n",
        "            qids_processed_count += 1\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        l_h, l_d_h, total_mdl_with_motifs = compute_mdl_cost_for_text_block(\n",
        "            corpus_for_qid, extracted_motifs, bdm_instance, MATRIX_SIZE_GLOBAL\n",
        "        )\n",
        "\n",
        "        if total_mdl_with_motifs < 0: # BDM Error during motif MDL calculation\n",
        "            print(f\"  Error computing MDL cost with motifs for QID {qid}. Skipping this result.\")\n",
        "            # Still append a result indicating error if needed, or just skip\n",
        "            all_qid_results.append({\n",
        "                \"qid\": qid, \"corpus_len_for_qid\": len(corpus_for_qid), \"num_responses\": num_responses_for_qid,\n",
        "                \"baseline_mdl\": baseline_total_mdl, \"motifs\": extracted_motifs,\n",
        "                \"l_h_motifs\": -1.0, \"l_d_h_motifs\": -1.0, \"total_mdl_motifs\": -1.0,\n",
        "                \"compression_achieved\": \"BDM_ERROR\"\n",
        "            })\n",
        "            qids_processed_count += 1\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        print(f\"  L(H) motif complexity for QID {qid}: {l_h:.4f}\")\n",
        "        print(f\"  L(D|H) compressed corpus complexity for QID {qid}: {l_d_h:.4f}\")\n",
        "        print(f\"  Total MDL cost with motifs for QID {qid}: {total_mdl_with_motifs:.4f}\")\n",
        "\n",
        "        compression = baseline_total_mdl - total_mdl_with_motifs\n",
        "        result_status = \"\"\n",
        "        if compression > 0.0001:\n",
        "            result_status = f\"SUCCESS: Compression achieved: {compression:.4f}\"\n",
        "        else:\n",
        "            result_status = f\"NOTE: No significant compression (or cost increased). Diff: {compression:.4f}\"\n",
        "        print(f\"  {result_status}\")\n",
        "\n",
        "        all_qid_results.append({\n",
        "            \"qid\": qid, \"corpus_len_for_qid\": len(corpus_for_qid), \"num_responses\": num_responses_for_qid,\n",
        "            \"baseline_mdl\": baseline_total_mdl, \"motifs\": extracted_motifs,\n",
        "            \"l_h_motifs\": l_h, \"l_d_h_motifs\": l_d_h, \"total_mdl_motifs\": total_mdl_with_motifs,\n",
        "            \"compression_achieved\": compression\n",
        "        })\n",
        "        qids_processed_count += 1\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary ---\")\n",
        "    if not all_qid_results:\n",
        "        print(\"No QIDs were processed or no valid results generated.\")\n",
        "    else:\n",
        "        # Filter out error results before calculating stats\n",
        "        valid_results_for_stats = [r for r in all_qid_results if not isinstance(r['compression_achieved'], str) and r['l_h_motifs'] >= 0]\n",
        "\n",
        "        num_compressed_qids = sum(1 for r in valid_results_for_stats if r['compression_achieved'] > 0.0001)\n",
        "        successful_compressions = [r['compression_achieved'] for r in valid_results_for_stats if r['compression_achieved'] > 0.0001]\n",
        "\n",
        "        avg_compression = np.mean(successful_compressions) if successful_compressions else 0\n",
        "        max_compression = np.max(successful_compressions) if successful_compressions else 0\n",
        "\n",
        "        print(f\"Total QIDs attempted: {qids_processed_count} (out of {len(aggregated_content_by_qid)} with content)\")\n",
        "        print(f\"Total QID results logged: {len(all_qid_results)}\")\n",
        "        print(f\"Number of QIDs where compression was achieved (from valid results): {num_compressed_qids}\")\n",
        "        if num_compressed_qids > 0:\n",
        "            print(f\"  Average compression (for successful cases): {avg_compression:.4f}\")\n",
        "            print(f\"  Maximum compression achieved across QIDs: {max_compression:.4f}\")\n",
        "\n",
        "        output_filename_qids = \"mdl_analysis_per_qid_v2.json\" # Changed filename slightly\n",
        "        try:\n",
        "            with open(output_filename_qids, \"w\", encoding=\"utf-8\") as f_out:\n",
        "                json.dump(all_qid_results, f_out, indent=2)\n",
        "            print(f\"Detailed QID-based results saved to {output_filename_qids}\")\n",
        "        except Exception as e_save:\n",
        "            print(f\"Error saving QID-based results to {output_filename_qids}: {e_save}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This structure allows defining g_client and g_model_name globally if needed by helper functions\n",
        "    # without explicitly passing them, though passing is cleaner.\n",
        "    # For this script, helper functions now accept client and model_name.\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuklprZ2w-fq",
        "outputId": "22fec667-8d8a-43bb-9b33-49a2ad52040c"
      },
      "id": "XuklprZ2w-fq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- MDL Prototype: Analyzing Per-QID Aggregated Text from Phase 2 ---\n",
            "Successfully initialized GenAI client for model gemma-3n-e4b-it.\n",
            "Loading Phase 2 output from: /content/drive/MyDrive/Colab Notebooks/Legal/Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json...\n",
            "\n",
            "Processing up to 3 QIDs for MDL analysis...\n",
            "\n",
            "--- Analyzing Aggregated Text for QID: Q4 ---\n",
            "  Combined corpus for QID Q4 has 129501 chars from 209 responses.\n",
            "  Baseline MDL for QID Q4 (L(D_orig_corpus_for_qid)): 121.3693\n",
            "    Error in llm_extract_motifs (text len 129501, truncated to 15000): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemma-3-4b'}, 'quotaValue': '15000'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '50s'}]}}\n",
            "  LLM Extracted Motifs for QID Q4: []\n",
            "  No valid motifs extracted by LLM for this QID.\n",
            "----------------------------------------\n",
            "--- Analyzing Aggregated Text for QID: Q5 ---\n",
            "  Combined corpus for QID Q5 has 134321 chars from 213 responses.\n",
            "  Baseline MDL for QID Q5 (L(D_orig_corpus_for_qid)): 122.0170\n",
            "    Error in llm_extract_motifs (text len 134321, truncated to 15000): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemma-3-4b'}, 'quotaValue': '15000'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '50s'}]}}\n",
            "  LLM Extracted Motifs for QID Q5: []\n",
            "  No valid motifs extracted by LLM for this QID.\n",
            "----------------------------------------\n",
            "--- Analyzing Aggregated Text for QID: Q6 ---\n",
            "  Combined corpus for QID Q6 has 115506 chars from 195 responses.\n",
            "  Baseline MDL for QID Q6 (L(D_orig_corpus_for_qid)): 123.0722\n",
            "    Error in llm_extract_motifs (text len 115506, truncated to 15000): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemma-3-4b'}, 'quotaValue': '15000'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '50s'}]}}\n",
            "  LLM Extracted Motifs for QID Q6: []\n",
            "  No valid motifs extracted by LLM for this QID.\n",
            "----------------------------------------\n",
            "\n",
            "--- Overall QID-based MDL Analysis Summary ---\n",
            "Total QIDs attempted: 3 (out of 36 with content)\n",
            "Total QID results logged: 3\n",
            "Number of QIDs where compression was achieved (from valid results): 0\n",
            "Detailed QID-based results saved to mdl_analysis_per_qid_v2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Gemma 3 locally"
      ],
      "metadata": {
        "id": "A4pyJUx2z9B_"
      },
      "id": "A4pyJUx2z9B_"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Add/Ensure these imports are at the top of your cell ---\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "import time # For potential sleeps if needed, though less critical with local model\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "# from google import genai # No longer needed for LLM calls if using local\n",
        "# from google.genai import types # No longer needed for LLM calls\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # !!! EXAMPLE - UPDATE THIS PATH !!!\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "\n",
        "# LLM Model Configuration for Local Hugging Face Model\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True # Recommended for 2b model on Colab\n",
        "\n",
        "# --- Helper Function Definitions (tokenize_sentence, llm_compress_text, text_to_binary_matrix, etc. remain the same) ---\n",
        "# ... (Keep your existing helper functions for BDM, text processing) ...\n",
        "\n",
        "def llm_extract_motifs_local_hf(text_to_analyze, hf_pipeline, hf_tokenizer):\n",
        "    \"\"\"Extracts motifs from text using a local Hugging Face pipeline.\"\"\"\n",
        "    prompt_template = [ # Gemma instruction format\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"\n",
        "From the following text, which comprises several responses to a single question,\n",
        "extract a concise list of up to 5 key recurring themes or motifs.\n",
        "List them separated by commas, with no introductory text.\n",
        "If no clear themes or the text is too generic, output \"NO_THEMES_FOUND\".\n",
        "\n",
        "Text (analyse for recurring themes):\n",
        "\\\"\\\"\\\"{text_to_analyze}\\\"\\\"\\\"\n",
        "\n",
        "Recurring Themes/Motifs (comma-separated, max 5):\n",
        "\"\"\"}\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    prompt_formatted = hf_tokenizer.apply_chat_template(prompt_template, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION = 7000 # Gemma 2b context is ~8k tokens, prompt takes some.\n",
        "                                           # Adjust based on typical token/char ratio of your text.\n",
        "                                           # For characters, this might be around 20k-25k chars.\n",
        "                                           # Let's use characters for direct comparison with previous truncation.\n",
        "    original_len = len(text_to_analyze)\n",
        "\n",
        "    # Truncate the *content* part of the prompt if necessary\n",
        "    # This is a bit more complex as the full prompt_formatted includes instructions\n",
        "    # A simpler way for now is to truncate text_to_analyze *before* formatting the prompt.\n",
        "    if len(text_to_analyze) > MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION:\n",
        "        # print(f\"    Warning: Text for motif extraction is long ({len(text_to_analyze)} chars), truncating to {MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION} for local LLM.\")\n",
        "        text_to_analyze_for_prompt = text_to_analyze[:MAX_TEXT_FOR_LLM_MOTIF_EXTRACTION]\n",
        "\n",
        "        # Re-create prompt with truncated text\n",
        "        prompt_template_truncated = [\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"\n",
        "From the following text, which comprises several responses to a single question,\n",
        "extract a concise list of up to 5 key recurring themes or motifs.\n",
        "List them separated by commas, with no introductory text.\n",
        "If no clear themes or the text is too generic, output \"NO_THEMES_FOUND\".\n",
        "\n",
        "Text (analyse for recurring themes):\n",
        "\\\"\\\"\\\"{text_to_analyze_for_prompt}\\\"\\\"\\\"\n",
        "\n",
        "Recurring Themes/Motifs (comma-separated, max 5):\n",
        "\"\"\"}\n",
        "        ]\n",
        "        prompt_formatted = hf_tokenizer.apply_chat_template(prompt_template_truncated, tokenize=False, add_generation_prompt=True)\n",
        "    else:\n",
        "        text_to_analyze_for_prompt = text_to_analyze\n",
        "\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": 100, # Motifs should be short\n",
        "        \"do_sample\": False,    # For deterministic output\n",
        "        \"pad_token_id\": hf_tokenizer.eos_token_id # Gemma uses eos_token for padding\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        outputs = hf_pipeline(prompt_formatted, **generation_args)\n",
        "        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n",
        "            print(f\"    Error: Local LLM pipeline returned unexpected/empty output for text (len {original_len}).\")\n",
        "            return []\n",
        "\n",
        "        generated_text_full = outputs[0]['generated_text']\n",
        "\n",
        "        # Strip the prompt part from the generated text\n",
        "        # This is crucial for local models as they often include the prompt in the output.\n",
        "        if generated_text_full.startswith(prompt_formatted):\n",
        "            motifs_text = generated_text_full[len(prompt_formatted):].strip()\n",
        "        else:\n",
        "            # A common alternative is that the model just appends to the last turn.\n",
        "            # For Gemma, it often just gives the assistant's response.\n",
        "            # We might need to find the start of the assistant's actual response if the template is complex.\n",
        "            # For this specific template, it's usually clean.\n",
        "            motifs_text = generated_text_full # Assume for now it's just the response\n",
        "            # More robust stripping for chat templates:\n",
        "            # Find the last assistant marker if one was added by add_generation_prompt=True\n",
        "            # or if the model generates it. For Gemma, it might be simple.\n",
        "            # If using apply_chat_template with add_generation_prompt=False, then you'd add the marker manually.\n",
        "\n",
        "            # A simple check: if \"Recurring Themes/Motifs\" is in the prompt and output, strip from there.\n",
        "            last_prompt_line = \"Recurring Themes/Motifs (comma-separated, max 5):\"\n",
        "            if last_prompt_line in motifs_text: # If prompt is included\n",
        "                 motifs_text = motifs_text.split(last_prompt_line, 1)[-1].strip()\n",
        "\n",
        "\n",
        "        if \"NO_THEMES_FOUND\" in motifs_text.upper() or not motifs_text:\n",
        "            return []\n",
        "        motifs = [m.strip() for m in motifs_text.split(\",\") if m.strip() and len(m.strip()) > 3]\n",
        "        return motifs[:5]\n",
        "    except Exception as e:\n",
        "        print(f\"    Error in llm_extract_motifs_local_hf (text len {original_len}, truncated to {len(text_to_analyze_for_prompt)}): {e}\")\n",
        "        # print(traceback.format_exc()) # For detailed debugging\n",
        "        return []\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "def main():\n",
        "    print(\"--- MDL Prototype: Analyzing Per-QID Aggregated Text (Local LLM) ---\")\n",
        "\n",
        "    # --- Initialize Local Hugging Face LLM ---\n",
        "    local_llm_pipeline = None\n",
        "    local_llm_tokenizer = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {LOCAL_LLM_MODEL_ID}...\")\n",
        "        local_llm_tokenizer = AutoTokenizer.from_pretrained(LOCAL_LLM_MODEL_ID)\n",
        "\n",
        "        bnb_config = None\n",
        "        quant_active = False\n",
        "        if USE_QUANTIZATION_FOR_LOCAL_LLM and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=compute_dtype,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                )\n",
        "                quant_active = True\n",
        "                print(f\"BNB config created for {LOCAL_LLM_MODEL_ID}, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb:\n",
        "                print(f\"WARN: Failed to create BitsAndBytesConfig (is bitsandbytes installed?): {e_bnb}. Quantization disabled.\")\n",
        "                quant_active = False\n",
        "\n",
        "        print(f\"Loading local model {LOCAL_LLM_MODEL_ID} (Quantization: {quant_active})...\")\n",
        "        model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True} # Gemma needs trust_remote_code\n",
        "        if quant_active:\n",
        "            model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        else: # If not quantizing, explicitly set dtype for GPU or let CPU use default\n",
        "            if device.type == 'cuda':\n",
        "                 model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "\n",
        "        local_llm_model = AutoModelForCausalLM.from_pretrained(LOCAL_LLM_MODEL_ID, **model_kwargs)\n",
        "\n",
        "        local_llm_pipeline = pipeline(\n",
        "            \"text-generation\", # For Gemma chat/instruct, \"text-generation\" is appropriate\n",
        "            model=local_llm_model,\n",
        "            tokenizer=local_llm_tokenizer,\n",
        "            # device=0 if device.type == 'cuda' else -1 # pipeline device argument\n",
        "        )\n",
        "        print(f\"Local LLM pipeline for {LOCAL_LLM_MODEL_ID} initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to initialize local LLM pipeline: {e}\")\n",
        "        # print(traceback.format_exc())\n",
        "        return\n",
        "\n",
        "    # --- Initialize BDM ---\n",
        "    try:\n",
        "        bdm_instance = BDM(ndim=2)\n",
        "        print(\"BDM instance initialized successfully.\")\n",
        "    except Exception as e_bdm_init:\n",
        "        print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\")\n",
        "        return\n",
        "\n",
        "    # --- Load and Process Data from Phase 2 Output ---\n",
        "    # (This part of main remains largely the same, just calls the new LLM function)\n",
        "    if not os.path.exists(P2_COLLATED_FILE):\n",
        "        print(f\"ERROR: Phase 2 output file not found: {P2_COLLATED_FILE}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loading Phase 2 output from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f:\n",
        "            phase2_data_content = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or parsing {P2_COLLATED_FILE}: {e}\")\n",
        "        return\n",
        "\n",
        "    all_qid_results = []\n",
        "    qids_to_process_limit = 3\n",
        "\n",
        "    print(f\"\\nProcessing up to {qids_to_process_limit} QIDs for MDL analysis using LOCAL LLM...\\n\")\n",
        "    qids_processed_count = 0\n",
        "\n",
        "    if phase2_data_content is None:\n",
        "        aggregated_content_by_qid = {}\n",
        "    else:\n",
        "        aggregated_content_by_qid = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "\n",
        "    if not aggregated_content_by_qid:\n",
        "        print(f\"No 'aggregated_pdf_content_by_qid' key found or data loaded from {P2_COLLATED_FILE}.\")\n",
        "        return\n",
        "\n",
        "    for qid, text_items_list in aggregated_content_by_qid.items():\n",
        "        if qids_processed_count >= qids_to_process_limit: break\n",
        "        if not text_items_list or not isinstance(text_items_list, list): continue\n",
        "\n",
        "        print(f\"--- Analyzing Aggregated Text for QID: {qid} ---\")\n",
        "        all_texts_for_qid = [\n",
        "            item.get(\"text\", \"\") for item in text_items_list\n",
        "            if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()\n",
        "        ]\n",
        "        corpus_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(all_texts_for_qid)\n",
        "\n",
        "        if len(corpus_for_qid.strip()) < 100:\n",
        "            print(f\"  Skipping QID {qid}: combined text too short ({len(corpus_for_qid)} chars).\")\n",
        "            continue\n",
        "\n",
        "        num_responses_for_qid = len(all_texts_for_qid)\n",
        "        print(f\"  Combined corpus for QID {qid} has {len(corpus_for_qid)} chars from {num_responses_for_qid} responses.\")\n",
        "\n",
        "        baseline_l_d_h = compute_bdm_for_text(corpus_for_qid, bdm_instance, MATRIX_SIZE_GLOBAL)\n",
        "        if baseline_l_d_h < 0:\n",
        "            print(f\"  Error computing baseline BDM for QID {qid}. Skipping.\")\n",
        "            continue\n",
        "        baseline_total_mdl = baseline_l_d_h\n",
        "        print(f\"  Baseline MDL for QID {qid}: {baseline_total_mdl:.4f}\")\n",
        "\n",
        "        # Call the new local LLM function\n",
        "        extracted_motifs = llm_extract_motifs_local_hf(corpus_for_qid, local_llm_pipeline, local_llm_tokenizer)\n",
        "        print(f\"  Local LLM Extracted Motifs for QID {qid}: {extracted_motifs}\")\n",
        "\n",
        "        if not extracted_motifs:\n",
        "            print(\"  No valid motifs extracted by Local LLM for this QID.\")\n",
        "            # Store result even if no motifs\n",
        "            all_qid_results.append({\n",
        "                \"qid\": qid, \"corpus_len_for_qid\": len(corpus_for_qid), \"num_responses\": num_responses_for_qid,\n",
        "                \"baseline_mdl\": baseline_total_mdl, \"motifs\": [],\n",
        "                \"l_h_motifs\": 0, \"l_d_h_motifs\": baseline_total_mdl, \"total_mdl_motifs\": baseline_total_mdl,\n",
        "                \"compression_achieved\": 0.0\n",
        "            })\n",
        "            qids_processed_count += 1\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        l_h, l_d_h, total_mdl_with_motifs = compute_mdl_cost_for_text_block(\n",
        "            corpus_for_qid, extracted_motifs, bdm_instance, MATRIX_SIZE_GLOBAL\n",
        "        )\n",
        "\n",
        "        if total_mdl_with_motifs < 0:\n",
        "            print(f\"  Error computing MDL cost with motifs for QID {qid}. Skipping.\")\n",
        "            all_qid_results.append({\n",
        "                \"qid\": qid, \"corpus_len_for_qid\": len(corpus_for_qid), \"num_responses\": num_responses_for_qid,\n",
        "                \"baseline_mdl\": baseline_total_mdl, \"motifs\": extracted_motifs,\n",
        "                \"l_h_motifs\": -1.0, \"l_d_h_motifs\": -1.0, \"total_mdl_motifs\": -1.0,\n",
        "                \"compression_achieved\": \"BDM_ERROR\"\n",
        "            })\n",
        "            qids_processed_count += 1\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        print(f\"  L(H) motif complexity for QID {qid}: {l_h:.4f}\")\n",
        "        print(f\"  L(D|H) compressed corpus complexity for QID {qid}: {l_d_h:.4f}\")\n",
        "        print(f\"  Total MDL cost with motifs for QID {qid}: {total_mdl_with_motifs:.4f}\")\n",
        "\n",
        "        compression = baseline_total_mdl - total_mdl_with_motifs\n",
        "        result_status = \"\"\n",
        "        if compression > 0.0001:\n",
        "            result_status = f\"SUCCESS: Compression achieved: {compression:.4f}\"\n",
        "        else:\n",
        "            result_status = f\"NOTE: No significant compression (or cost increased). Diff: {compression:.4f}\"\n",
        "        print(f\"  {result_status}\")\n",
        "\n",
        "        all_qid_results.append({\n",
        "            \"qid\": qid, \"corpus_len_for_qid\": len(corpus_for_qid), \"num_responses\": num_responses_for_qid,\n",
        "            \"baseline_mdl\": baseline_total_mdl, \"motifs\": extracted_motifs,\n",
        "            \"l_h_motifs\": l_h, \"l_d_h_motifs\": l_d_h, \"total_mdl_motifs\": total_mdl_with_motifs,\n",
        "            \"compression_achieved\": compression\n",
        "        })\n",
        "        qids_processed_count += 1\n",
        "        print(\"-\" * 40)\n",
        "        # Optional: Add a small delay if GPU is very busy, though less of an API rate limit issue\n",
        "        # time.sleep(1)\n",
        "\n",
        "\n",
        "    # (Summary printing and saving results - same as before)\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary (Local LLM) ---\")\n",
        "    if not all_qid_results:\n",
        "        print(\"No QIDs were processed or no valid results generated.\")\n",
        "    else:\n",
        "        valid_results_for_stats = [r for r in all_qid_results if not isinstance(r['compression_achieved'], str) and r['l_h_motifs'] >= 0]\n",
        "        num_compressed_qids = sum(1 for r in valid_results_for_stats if r['compression_achieved'] > 0.0001)\n",
        "        successful_compressions = [r['compression_achieved'] for r in valid_results_for_stats if r['compression_achieved'] > 0.0001]\n",
        "        avg_compression = np.mean(successful_compressions) if successful_compressions else 0\n",
        "        max_compression = np.max(successful_compressions) if successful_compressions else 0\n",
        "        print(f\"Total QIDs attempted: {qids_processed_count} (out of {len(aggregated_content_by_qid)} with content)\")\n",
        "        print(f\"Total QID results logged: {len(all_qid_results)}\")\n",
        "        print(f\"Number of QIDs where compression was achieved (from valid results): {num_compressed_qids}\")\n",
        "        if num_compressed_qids > 0:\n",
        "            print(f\"  Average compression (for successful cases): {avg_compression:.4f}\")\n",
        "            print(f\"  Maximum compression achieved across QIDs: {max_compression:.4f}\")\n",
        "        output_filename_qids = \"mdl_analysis_per_qid_local_llm.json\"\n",
        "        try:\n",
        "            with open(output_filename_qids, \"w\", encoding=\"utf-8\") as f_out:\n",
        "                json.dump(all_qid_results, f_out, indent=2)\n",
        "            print(f\"Detailed QID-based results saved to {output_filename_qids}\")\n",
        "        except Exception as e_save:\n",
        "            print(f\"Error saving QID-based results to {output_filename_qids}: {e_save}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532,
          "referenced_widgets": [
            "8a64773925744baea29eea08a545553a",
            "0df849aab8d940fc9d77e2f59c234eb8",
            "7f8a30976cd34f2aafa500726823dabf",
            "97299b0d95c04763bb841cf8b6e97e3a",
            "e8ac434ab6a34412837fad8064bb014e",
            "a57a12b412f84bc7b42c2fdc5dbcb44e",
            "62e99469688045e8909e9a613baf5fd8",
            "9c96cb39cdb143afadbed3cdfee0e99d",
            "7d08e648164c4022bf4ea71787abe737",
            "dcdda177c4d44b37a507033d455d5406",
            "ba814d8b3dc742f3ba28309d94d51480",
            "1b362802b0ac422fad1e85f8e49baced",
            "1a1154118fbf49fab9b62c8dc63101ac",
            "57e7c031c4f2450eb3e4ab0b1f924d1a",
            "aabdfed0252849ef8dbaa5fb933fbda1",
            "aba11a26abe4416aa242cc386017495b",
            "ed6dc9f7c39747b29c3591410d899e20",
            "9f95347e08fc432f91a12e7a4d1a14e4",
            "73b2b6d7c673462b9b5e347c2c7be66a",
            "3263c5eb70164bd8b9b93b2e6e1c122b",
            "529f156b48da492fbc42194052174d0e",
            "45d7a57c207a4b0ab2ed5469545becc2",
            "ea2554699c314899927546a5f81b1765",
            "a2f98fa8bd9a4734bcb0036ab3294a7c",
            "4ae4477d4ae24ae0afea5e6a8da99716",
            "286d1f5e53fe4fd5ac8bf2a43d94948e",
            "b0e555643bc9431d9ca508f52e42dcb4",
            "1b78b9de59d643d29d8cf7afe7269187",
            "77a33da84d844784b6b0f3b96d896678",
            "e0330a06f3f6428faa01bdc1d34c3467",
            "cf7cf3d2859b4482a297570d277326c3",
            "a68d2cd4d99247499cd7b632d71e0f1b",
            "4740d2159c45443f912f82871d7ff232",
            "7c0f050fa3b64bcbbf86daa03ee7a70e",
            "b97af678587a4f31a49c92506fbd066a",
            "3e74e36d19914759a639279a0dc4de9b",
            "2661d0f3b81a403cbc9b48b276c025cd",
            "4dfc9632d4e341afaaf98d9011d8d05c",
            "fa87e009cee84839b87a48820e076b6e",
            "1a8e5ba53a7f4e4eab79bc47fa509f13",
            "2e6bbbcadf294dfa81250d6dc6fb69c8",
            "72b454d2b5204d4ab1caf1c453e7aa14",
            "ddac919f5b6047cda17259f30aa2f5bf",
            "0b5173e1f85b4516aab2c84d7052f6ac",
            "1dbcf09f3f97496894851b4e4ce034bd",
            "b1cfb632222e43f8b9eb616143daaab0",
            "93e6fb28c09c45a7b0beaf976e16bc91",
            "36baa56e1c444af4ab70ab8065ad00da",
            "d62c9483f02f42d2822426db33c43694",
            "bb8b864025354ff7b59b2787cadf75c0",
            "e0494862437f40078501dbebba0b70ed",
            "c43bdc9efb504b129df5f4312d79acf9",
            "dcc649facd1742d29458b5cf1d3528cd",
            "74b5865376d142f2979aaeeec02eb83c",
            "ce334170aa8b4317b74efdc9f4ef1c78",
            "849749e91cc84fc2889f92db97784a82",
            "dc0430db45df4d26baf98a1e4c933d46",
            "4d73a5a35b4f4f5e87b396ff866038ac",
            "ab4f71e9df6841a8a408c05a8534ef15",
            "eef99c7acdb64985bbc606523494b1da",
            "9dab3ca19e6b4e8789621e47fd5bfc83",
            "0d39a60d68214b11ad241abd655262d5",
            "07eacbf79eb94431b8b28eb4159f5a0f",
            "0e4c6e81f5224d79b48f5afd3e48c8c0",
            "abe08f958e3b45748923aed4292c4a07",
            "4b77549e761446e789ed02746d5e57d7",
            "a0ec648c027c4f519514bc0643caa33f",
            "3a844260d126437b9f9a5aeac387650a",
            "09d21b8e23504017854e79fa4677d41d",
            "91bf0cf56c5744feb508e670677ff67b",
            "a378a7d3247c44fba6120720f21d185f",
            "4f82c66e947640008d95c5a8386322ce",
            "22a4f7157a254ccf876a3fc7aa98b9d7",
            "f2ae5b69cef64cf5bfdbf7a1a02bcbba",
            "665130a4de9043cfa16fc5c7b9c3f1f5",
            "6b84493da552405b9af086c57634f0bd",
            "a947f668b07e4591b6cc45e81c1d0922",
            "28648067915945da8687b691d6a902a1",
            "c157add43598447aaf5bb493ea2e66aa",
            "8530ddd8cb924ad180ada0a1cd0ba3c4",
            "f8f19ca1c54c40afb40fee47731cd848",
            "5107e8c660eb438e9f4c7d074b09868c",
            "8eb1d11644cd4598a84e134511999375",
            "1872ee846ea441ea80c638e7d88cecfe",
            "49c62f684baa4dbeae5381a6ce23eb03",
            "99add5ea5ced471f8461cb42bb651158",
            "1b0129787ddf46899e5b4c1d9ea82ca0",
            "b5f8406419884add905b864558538091",
            "8dfadfdb88e54bc3b4ab7e6ebebdae4a",
            "cdaf8a63740e405c96684195cf8de1e4",
            "3671b9f22cda468dbc92b1c5322365f1",
            "023c85da967341e8abdb871c52cc543b",
            "f3be97b863604b5cbfc391e27f3f3aae",
            "93649707ebb0410ea51462903f8cafb8",
            "4e55717324084a8cbb574fb1309b4458",
            "4ba0ba4fa2914701a5f707a4691efae9",
            "92292e5a7d884474b0ec559764106e96",
            "9bef668f3a244dcfbf66bd34fbfa476d",
            "8813bcc6ec274192a013d0a330fc4be9",
            "23bad0724f494b6d9bd8fa51e2af497d",
            "f60fb5773b6745fb9eab3af67ab58cff",
            "d0e32ff1ef394b8bac73bb6513b1fd4f",
            "09bc4e3534cc458799ebf828974fc95a",
            "ac735f0111144b8baa9c0630b2ce88aa",
            "2e8ca3d5a3864e098b3543d552ed0053",
            "3522032fa59c4cf8a01983376719e5c7",
            "5de21c45629b413a888c3d76f55f4219",
            "6e8862b500f940fb98ea14f4bea0566b",
            "e3a1bffff40f4fdc89ca6c1c6501c413",
            "e7cd7c511942446a96950ca08cb3087b",
            "a2c3549678e64b2abc18b30c1cc86ce4",
            "1054a496e9e84bb287c363c052ca56f4",
            "0d93b1b80e444ea4b5b95dc8a9ea4840",
            "3572193e8fcd4acf9d08549fe86c7c67",
            "4e64f390f49b4bcc9a186a904746c768",
            "b7b5e1b1d8004a8cbec48c1f0e612bdf",
            "f83b3fdc72f8465aaeb4fbb881750806",
            "7e92b74b62e1405483100eb5a561591d",
            "411242327ada41538187b0bdd7472cc9",
            "33100a1dce7045afbebbd851dde6b181",
            "251c5ffae56a4028bb21fb9ebe847065"
          ]
        },
        "id": "68x19Uzg0Aop",
        "outputId": "e17373ef-f391-4d12-8aaf-ed4c57e8d6dc"
      },
      "id": "68x19Uzg0Aop",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- MDL Prototype: Analyzing Per-QID Aggregated Text (Local LLM) ---\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-2b-it...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a64773925744baea29eea08a545553a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b362802b0ac422fad1e85f8e49baced"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea2554699c314899927546a5f81b1765"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c0f050fa3b64bcbbf86daa03ee7a70e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BNB config created for google/gemma-2b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-2b-it (Quantization: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dbcf09f3f97496894851b4e4ce034bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "849749e91cc84fc2889f92db97784a82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0ec648c027c4f519514bc0643caa33f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28648067915945da8687b691d6a902a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dfadfdb88e54bc3b4ab7e6ebebdae4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23bad0724f494b6d9bd8fa51e2af497d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2c3549678e64b2abc18b30c1cc86ce4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-2b-it initialized successfully.\n",
            "CRITICAL: Failed to initialize BDM instance: _Partition.__init__() got an unexpected keyword argument 'use_ctm'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Per QID processing\n",
        "# --- Imports (ensure all necessary imports are at the top) ---\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from pybdm import BDM\n",
        "import re\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "# from google import genai # No longer needed for LLM calls if using local\n",
        "# from google.genai import types # No longer needed for LLM calls\n",
        "\n",
        "# --- Configuration (Define paths and constants) ---\n",
        "# These would typically be in your \"Cell 2: Global Project Configuration\"\n",
        "# For this standalone cell, ensure they are defined or adjust paths as needed.\n",
        "BASE_PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Legal/' # !!! EXAMPLE - UPDATE THIS PATH !!!\n",
        "PHASE2_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'Phase2_PDF_Collated_Texts/')\n",
        "P2_COLLATED_FILE = os.path.join(PHASE2_OUTPUT_DIR, 'phase2_collated_pdf_texts.json')\n",
        "\n",
        "# Parameter from your Cell 2, to control which QIDs are processed\n",
        "# Set to None or empty list to process all (up to qids_to_process_limit)\n",
        "# Set to e.g., [\"Q4\"] to process only Q4\n",
        "# P3_QIDS_TO_PROCESS_THEMATICALLY = None\n",
        "P3_QIDS_TO_PROCESS_THEMATICALLY = [\"Q4\"] # EXAMPLE: Process only Q4\n",
        "\n",
        "# --- BDM and LLM Model Configuration ---\n",
        "MATRIX_SIZE_GLOBAL = (8, 8)\n",
        "LOCAL_LLM_MODEL_ID = 'google/gemma-2b-it'\n",
        "USE_QUANTIZATION_FOR_LOCAL_LLM = True\n",
        "\n",
        "# --- Helper Function Definitions (llm_extract_motifs_local_hf, llm_compress_text, etc. - keep as before) ---\n",
        "def llm_extract_motifs_local_hf(text_to_analyze, hf_pipeline, hf_tokenizer):\n",
        "    \"\"\"Extracts motifs from text using a local Hugging Face pipeline.\"\"\"\n",
        "    # Using a simplified prompt structure that Gemma instruct models usually handle well\n",
        "    # by just appending their response.\n",
        "    prompt_content = f\"\"\"\n",
        "From the following text, which comprises several responses to a single question,\n",
        "extract a concise list of up to 5 key recurring themes or motifs.\n",
        "List them separated by commas, with no introductory text.\n",
        "If no clear themes or the text is too generic, output \"NO_THEMES_FOUND\".\n",
        "\n",
        "Text (analyse for recurring themes):\n",
        "\\\"\\\"\\\"{text_to_analyze}\\\"\\\"\\\"\n",
        "\n",
        "Recurring Themes/Motifs (comma-separated, max 5):\n",
        "\"\"\"\n",
        "    # For Gemma instruct models, often just the user prompt is enough,\n",
        "    # and the model appends its response.\n",
        "    # The apply_chat_template is good, but we need to be careful about stripping.\n",
        "    # Let's try a simpler prompt construction if direct text-generation is used.\n",
        "\n",
        "    # Constructing the prompt for Gemma instruct/chat models\n",
        "    # The key is to get only the assistant's part of the response.\n",
        "    messages_for_template = [\n",
        "        {\"role\": \"user\", \"content\": prompt_content}\n",
        "    ]\n",
        "    # This adds the necessary tokens for the model to know it should generate a response.\n",
        "    prompt_formatted_for_llm = hf_tokenizer.apply_chat_template(\n",
        "        messages_for_template,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True # Crucial for instruct models\n",
        "    )\n",
        "\n",
        "    MAX_TEXT_IN_PROMPT_FOR_LLM = 7000 # Max characters of *user content* (text_to_analyze)\n",
        "    original_len = len(text_to_analyze)\n",
        "\n",
        "    # Truncate text_to_analyze *before* putting it into the prompt_content\n",
        "    if original_len > MAX_TEXT_IN_PROMPT_FOR_LLM:\n",
        "        text_to_analyze_for_prompt = text_to_analyze[:MAX_TEXT_IN_PROMPT_FOR_LLM]\n",
        "        # Re-create prompt_content and prompt_formatted_for_llm with truncated text\n",
        "        prompt_content = f\"\"\"\n",
        "From the following text, which comprises several responses to a single question,\n",
        "extract a concise list of up to 5 key recurring themes or motifs.\n",
        "List them separated by commas, with no introductory text.\n",
        "If no clear themes or the text is too generic, output \"NO_THEMES_FOUND\".\n",
        "\n",
        "Text (analyse for recurring themes):\n",
        "\\\"\\\"\\\"{text_to_analyze_for_prompt}\\\"\\\"\\\"\n",
        "\n",
        "Recurring Themes/Motifs (comma-separated, max 5):\n",
        "\"\"\"\n",
        "        messages_for_template = [{\"role\": \"user\", \"content\": prompt_content}]\n",
        "        prompt_formatted_for_llm = hf_tokenizer.apply_chat_template(\n",
        "            messages_for_template, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    else:\n",
        "        text_to_analyze_for_prompt = text_to_analyze # Used for logging length\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": 150, # Increased slightly to allow for a preamble if LLM insists\n",
        "        \"do_sample\": False,\n",
        "        \"pad_token_id\": hf_tokenizer.eos_token_id\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        outputs = hf_pipeline(prompt_formatted_for_llm, **generation_args)\n",
        "        if not outputs or not isinstance(outputs, list) or not outputs[0].get('generated_text'):\n",
        "            print(f\"    Error: Local LLM pipeline returned unexpected/empty output for text (len {original_len}).\")\n",
        "            return []\n",
        "\n",
        "        generated_text_full = outputs[0]['generated_text']\n",
        "\n",
        "        # --- More Robust Stripping of Prompt and Model Preamble ---\n",
        "        # 1. Strip the input prompt from the beginning of the output\n",
        "        if generated_text_full.startswith(prompt_formatted_for_llm):\n",
        "            actual_response_text = generated_text_full[len(prompt_formatted_for_llm):].strip()\n",
        "        else:\n",
        "            # If prompt not found at start, it might be that the pipeline only returned the new tokens.\n",
        "            # Or, the model added its own chat turn markers.\n",
        "            # For Gemma with add_generation_prompt=True, the output usually starts right after the prompt.\n",
        "            # Let's try to find common start-of-response markers if the above fails.\n",
        "            # A simpler approach if the LLM is good: it just appends its answer.\n",
        "            # The `apply_chat_template` with `add_generation_prompt=True` should mean the `generated_text_full`\n",
        "            # contains the prompt AND the model's response.\n",
        "            # The challenge is that `prompt_formatted_for_llm` itself might end with a model turn token.\n",
        "            # A more reliable way can be to look for text *after* the last user message in the prompt.\n",
        "\n",
        "            # Try splitting based on a known part of the prompt that precedes the answer:\n",
        "            key_phrase_before_answer = \"Recurring Themes/Motifs (comma-separated, max 5):\"\n",
        "            if key_phrase_before_answer in generated_text_full:\n",
        "                # Take text after the last occurrence of this key phrase\n",
        "                actual_response_text = generated_text_full.split(key_phrase_before_answer)[-1].strip()\n",
        "            else:\n",
        "                # Fallback: assume the pipeline might have stripped the prompt for us,\n",
        "                # or the model just gave the answer without special markers.\n",
        "                # This part is tricky and model-dependent.\n",
        "                actual_response_text = generated_text_full # Could be risky, might need more cleanup\n",
        "                # print(f\"    WARN: Could not reliably strip prompt. Full output: {actual_response_text[:200]}\")\n",
        "\n",
        "\n",
        "        # 2. Clean up common LLM preambles from the *actual_response_text*\n",
        "        preambles_to_remove = [\n",
        "            \"Sure, here's a concise summary of the recurring themes and motifs in the text:\",\n",
        "            \"Sure, here is a concise list of the recurring themes and motifs in the text:\",\n",
        "            \"Here's a concise summary of the recurring themes and motifs in the text:\",\n",
        "            \"Here are the recurring themes and motifs from the text:\",\n",
        "            \"Okay, here are some key themes:\",\n",
        "            \"Sure, here are 5 key themes or motifs from the text:\",\n",
        "            \"Here are 5 key themes or motifs from the text:\",\n",
        "            \"- \" # If it starts listing with hyphens immediately\n",
        "        ]\n",
        "        cleaned_response_text = actual_response_text\n",
        "        for preamble in preambles_to_remove:\n",
        "            if cleaned_response_text.lower().startswith(preamble.lower()):\n",
        "                cleaned_response_text = cleaned_response_text[len(preamble):].strip()\n",
        "                break # Remove first matching preamble\n",
        "\n",
        "        # Also remove potential bullet points or leading hyphens from each line if motifs are listed that way\n",
        "        motifs_lines = cleaned_response_text.split('\\n')\n",
        "        processed_motifs_text = []\n",
        "        for line in motifs_lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\"- \"):\n",
        "                line = line[2:].strip()\n",
        "            elif line.startswith(\"* \"):\n",
        "                line = line[2:].strip()\n",
        "            if line: # Add line if it's not empty after stripping\n",
        "                processed_motifs_text.append(line)\n",
        "\n",
        "        # Join back if motifs were on separate lines, then split by comma\n",
        "        # Or, if each line is a motif, just use them.\n",
        "        # The prompt asks for comma-separated.\n",
        "        final_motifs_string = \", \".join(processed_motifs_text) # If motifs were on new lines, this joins them with comma.\n",
        "                                                              # If they were already comma sep on one line, it might add extra commas.\n",
        "                                                              # Better to assume LLM tries for comma separation.\n",
        "\n",
        "        if not final_motifs_string.strip() or \"NO_THEMES_FOUND\" in final_motifs_string.upper():\n",
        "            # print(f\"    LLM indicated no themes or response was empty after cleaning. Cleaned: '{final_motifs_string}'\")\n",
        "            return []\n",
        "\n",
        "        # Now split the cleaned string by comma\n",
        "        motifs = [m.strip() for m in final_motifs_string.split(',') if m.strip() and len(m.strip()) > 3]\n",
        "\n",
        "        # print(f\"    DEBUG: Raw LLM output after prompt strip: '{actual_response_text[:200]}'\")\n",
        "        # print(f\"    DEBUG: Cleaned response text for motif splitting: '{final_motifs_string}'\")\n",
        "        # print(f\"    DEBUG: Final parsed motifs: {motifs[:5]}\")\n",
        "\n",
        "        return motifs[:5]\n",
        "    except Exception as e:\n",
        "        print(f\"    Error in llm_extract_motifs_local_hf (text len {original_len}, truncated to {len(text_to_analyze_for_prompt)}): {e}\")\n",
        "        # import traceback\n",
        "        # print(traceback.format_exc()) # For detailed debugging\n",
        "        return []\n",
        "\n",
        "def llm_compress_text(text_to_compress, motifs_list):\n",
        "    if not isinstance(text_to_compress, str): return \"\"\n",
        "    compressed = text_to_compress.lower()\n",
        "    if not motifs_list: return compressed\n",
        "    for motif in motifs_list:\n",
        "        if motif and isinstance(motif, str) and motif.strip():\n",
        "            safe_placeholder_name = re.sub(r'\\W+', '_', motif).upper()[:20]\n",
        "            placeholder = f\"<MOTIF_{safe_placeholder_name}>\"\n",
        "            try:\n",
        "                compressed = re.sub(re.escape(motif.lower()), placeholder, compressed, flags=re.IGNORECASE)\n",
        "            except re.error as re_e:\n",
        "                print(f\"    Regex error during compression for motif '{motif}': {re_e}. Skipping.\")\n",
        "                continue\n",
        "    return compressed\n",
        "\n",
        "def text_to_binary_matrix(text_input, size=MATRIX_SIZE_GLOBAL):\n",
        "    if not text_input or not isinstance(text_input, str):\n",
        "        return np.zeros(size, dtype=int)\n",
        "    hash_digest = hashlib.sha256(text_input.encode('utf-8', 'ignore')).hexdigest()\n",
        "    required_bits = size[0] * size[1]\n",
        "    binary_string = bin(int(hash_digest, 16))[2:].zfill(required_bits)\n",
        "    binary_string_padded = binary_string.ljust(required_bits, '0')\n",
        "    bits = [int(b) for b in binary_string_padded[:required_bits]]\n",
        "    return np.array(bits).reshape(size)\n",
        "\n",
        "def compute_bdm_for_text(text_input, bdm_instance, matrix_s=MATRIX_SIZE_GLOBAL):\n",
        "    if not text_input or not isinstance(text_input, str) : return 0.0\n",
        "    if not text_input.strip(): return 0.0\n",
        "    MAX_TEXT_FOR_BDM_HASH = 2000\n",
        "    text_for_hash = text_input if len(text_input) <= MAX_TEXT_FOR_BDM_HASH else text_input[:MAX_TEXT_FOR_BDM_HASH]\n",
        "    matrix = text_to_binary_matrix(text_for_hash, size=matrix_s)\n",
        "    try:\n",
        "        return bdm_instance.bdm(matrix)\n",
        "    except Exception as e_bdm:\n",
        "        print(f\"      Error during BDM calculation for text (len {len(text_input)}, hashed part len {len(text_for_hash)}): {e_bdm}\")\n",
        "        return -1.0\n",
        "\n",
        "def compute_mdl_cost_for_text_block(text_block_str, motifs_list, bdm_instance, matrix_s=MATRIX_SIZE_GLOBAL):\n",
        "    if not isinstance(text_block_str, str) : text_block_str = \"\"\n",
        "    l_h = 0.0\n",
        "    valid_motifs_for_lh = []\n",
        "    if motifs_list:\n",
        "        valid_motifs_for_lh = [m for m in motifs_list if isinstance(m, str) and m.strip()]\n",
        "        if valid_motifs_for_lh:\n",
        "            l_h = sum(compute_bdm_for_text(m, bdm_instance, matrix_s) for m in valid_motifs_for_lh)\n",
        "    compressed_text_block = llm_compress_text(text_block_str, motifs_list)\n",
        "    l_d_h = compute_bdm_for_text(compressed_text_block, bdm_instance, matrix_s)\n",
        "    if l_h < 0 or l_d_h < 0: return -1.0, -1.0, -1.0\n",
        "    return l_h, l_d_h, l_h + l_d_h\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "def main():\n",
        "    print(\"--- MDL Prototype: Analyzing Per-QID Aggregated Text (Local LLM) ---\")\n",
        "\n",
        "    local_llm_pipeline = None\n",
        "    local_llm_tokenizer = None\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading tokenizer for {LOCAL_LLM_MODEL_ID}...\")\n",
        "        local_llm_tokenizer = AutoTokenizer.from_pretrained(LOCAL_LLM_MODEL_ID)\n",
        "        bnb_config = None\n",
        "        quant_active = False\n",
        "        if USE_QUANTIZATION_FOR_LOCAL_LLM and torch.cuda.is_available():\n",
        "            try:\n",
        "                compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=True)\n",
        "                quant_active = True\n",
        "                print(f\"BNB config created for {LOCAL_LLM_MODEL_ID}, compute_dtype: {compute_dtype}.\")\n",
        "            except Exception as e_bnb:\n",
        "                print(f\"WARN: Failed to create BitsAndBytesConfig: {e_bnb}. Quantization disabled.\")\n",
        "        print(f\"Loading local model {LOCAL_LLM_MODEL_ID} (Quantization: {quant_active})...\")\n",
        "        model_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
        "        if quant_active: model_kwargs[\"quantization_config\"] = bnb_config\n",
        "        else:\n",
        "            if device.type == 'cuda': model_kwargs[\"torch_dtype\"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "        local_llm_model = AutoModelForCausalLM.from_pretrained(LOCAL_LLM_MODEL_ID, **model_kwargs)\n",
        "        local_llm_pipeline = pipeline(\"text-generation\", model=local_llm_model, tokenizer=local_llm_tokenizer)\n",
        "        print(f\"Local LLM pipeline for {LOCAL_LLM_MODEL_ID} initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL: Failed to initialize local LLM pipeline: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        bdm_instance = BDM(ndim=2)\n",
        "        print(\"BDM instance initialized successfully.\")\n",
        "    except Exception as e_bdm_init:\n",
        "        print(f\"CRITICAL: Failed to initialize BDM instance: {e_bdm_init}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(P2_COLLATED_FILE):\n",
        "        print(f\"ERROR: Phase 2 output file not found: {P2_COLLATED_FILE}\")\n",
        "        return\n",
        "    print(f\"Loading Phase 2 output from: {P2_COLLATED_FILE}...\")\n",
        "    phase2_data_content = None\n",
        "    try:\n",
        "        with open(P2_COLLATED_FILE, 'r', encoding='utf-8') as f:\n",
        "            phase2_data_content = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or parsing {P2_COLLATED_FILE}: {e}\")\n",
        "        return\n",
        "\n",
        "    all_qid_results = []\n",
        "\n",
        "    # Use P3_QIDS_TO_PROCESS_THEMATICALLY if defined and not empty,\n",
        "    # otherwise, qids_to_process_limit applies to all QIDs.\n",
        "    qids_to_target = []\n",
        "    aggregated_content_by_qid = {}\n",
        "\n",
        "    if phase2_data_content:\n",
        "        aggregated_content_by_qid = phase2_data_content.get(\"aggregated_pdf_content_by_qid\", {})\n",
        "\n",
        "    if not aggregated_content_by_qid:\n",
        "        print(f\"No 'aggregated_pdf_content_by_qid' key found or data loaded from {P2_COLLATED_FILE}.\")\n",
        "        return\n",
        "\n",
        "    if P3_QIDS_TO_PROCESS_THEMATICALLY and isinstance(P3_QIDS_TO_PROCESS_THEMATICALLY, list) and len(P3_QIDS_TO_PROCESS_THEMATICALLY) > 0:\n",
        "        qids_to_target = [qid for qid in P3_QIDS_TO_PROCESS_THEMATICALLY if qid in aggregated_content_by_qid]\n",
        "        print(f\"Targeting specific QIDs based on P3_QIDS_TO_PROCESS_THEMATICALLY: {qids_to_target}\")\n",
        "        if not qids_to_target:\n",
        "            print(f\"Warning: None of the QIDs specified in P3_QIDS_TO_PROCESS_THEMATICALLY ({P3_QIDS_TO_PROCESS_THEMATICALLY}) were found in the loaded data.\")\n",
        "            return # Or process all if preferred fallback\n",
        "    else:\n",
        "        qids_to_process_limit = 1 # Example: Process only the first QID if no specific list\n",
        "        print(f\"P3_QIDS_TO_PROCESS_THEMATICALLY not set or empty. Processing up to {qids_to_process_limit} QID(s) from the data.\")\n",
        "        # Iterate through available QIDs up to the limit\n",
        "        temp_qids_to_target = []\n",
        "        for q_idx, qid_key in enumerate(aggregated_content_by_qid.keys()):\n",
        "            if q_idx < qids_to_process_limit:\n",
        "                temp_qids_to_target.append(qid_key)\n",
        "            else:\n",
        "                break\n",
        "        qids_to_target = temp_qids_to_target\n",
        "        if not qids_to_target:\n",
        "            print(\"No QIDs found to process based on the limit.\")\n",
        "            return\n",
        "\n",
        "    print(f\"\\nMDL analysis will run for these QIDs: {qids_to_target}\\n\")\n",
        "\n",
        "    for qid in qids_to_target:\n",
        "        text_items_list = aggregated_content_by_qid.get(qid) # Get items for the target QID\n",
        "\n",
        "        if not text_items_list or not isinstance(text_items_list, list):\n",
        "            print(f\"Skipping QID {qid}: no text items or not a list.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"--- Analyzing Aggregated Text for QID: {qid} ---\")\n",
        "        all_texts_for_qid = [\n",
        "            item.get(\"text\", \"\") for item in text_items_list\n",
        "            if isinstance(item, dict) and isinstance(item.get(\"text\"), str) and item.get(\"text\",\"\").strip()\n",
        "        ]\n",
        "        corpus_for_qid = \"\\n\\n<RSP_SEP>\\n\\n\".join(all_texts_for_qid)\n",
        "\n",
        "        if len(corpus_for_qid.strip()) < 100:\n",
        "            print(f\"  Skipping QID {qid}: combined text too short ({len(corpus_for_qid)} chars).\")\n",
        "            continue\n",
        "\n",
        "        num_responses_for_qid = len(all_texts_for_qid)\n",
        "        print(f\"  Combined corpus for QID {qid} has {len(corpus_for_qid)} chars from {num_responses_for_qid} responses.\")\n",
        "\n",
        "        baseline_l_d_h = compute_bdm_for_text(corpus_for_qid, bdm_instance, MATRIX_SIZE_GLOBAL)\n",
        "        if baseline_l_d_h < 0:\n",
        "            print(f\"  Error computing baseline BDM for QID {qid}. Skipping.\")\n",
        "            continue\n",
        "        baseline_total_mdl = baseline_l_d_h\n",
        "        print(f\"  Baseline MDL for QID {qid} (L(D_orig_corpus_for_qid)): {baseline_total_mdl:.4f}\")\n",
        "\n",
        "        extracted_motifs = llm_extract_motifs_local_hf(corpus_for_qid, local_llm_pipeline, local_llm_tokenizer)\n",
        "        print(f\"  Local LLM Extracted Motifs for QID {qid}: {extracted_motifs}\")\n",
        "\n",
        "        if not extracted_motifs:\n",
        "            print(\"  No valid motifs extracted by Local LLM for this QID.\")\n",
        "            all_qid_results.append({\n",
        "                \"qid\": qid, \"corpus_len_for_qid\": len(corpus_for_qid), \"num_responses\": num_responses_for_qid,\n",
        "                \"baseline_mdl\": baseline_total_mdl, \"motifs\": [],\n",
        "                \"l_h_motifs\": 0, \"l_d_h_motifs\": baseline_total_mdl, \"total_mdl_motifs\": baseline_total_mdl,\n",
        "                \"compression_achieved\": 0.0\n",
        "            })\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        l_h, l_d_h, total_mdl_with_motifs = compute_mdl_cost_for_text_block(\n",
        "            corpus_for_qid, extracted_motifs, bdm_instance, MATRIX_SIZE_GLOBAL\n",
        "        )\n",
        "\n",
        "        if total_mdl_with_motifs < 0:\n",
        "            print(f\"  Error computing MDL cost with motifs for QID {qid}. Skipping this result.\")\n",
        "            all_qid_results.append({\n",
        "                \"qid\": qid, \"corpus_len_for_qid\": len(corpus_for_qid), \"num_responses\": num_responses_for_qid,\n",
        "                \"baseline_mdl\": baseline_total_mdl, \"motifs\": extracted_motifs,\n",
        "                \"l_h_motifs\": -1.0, \"l_d_h_motifs\": -1.0, \"total_mdl_motifs\": -1.0,\n",
        "                \"compression_achieved\": \"BDM_ERROR\"\n",
        "            })\n",
        "            print(\"-\" * 40)\n",
        "            continue\n",
        "\n",
        "        print(f\"  L(H) motif complexity for QID {qid}: {l_h:.4f}\")\n",
        "        print(f\"  L(D|H) compressed corpus complexity for QID {qid}: {l_d_h:.4f}\")\n",
        "        print(f\"  Total MDL cost with motifs for QID {qid}: {total_mdl_with_motifs:.4f}\")\n",
        "\n",
        "        compression = baseline_total_mdl - total_mdl_with_motifs\n",
        "        result_status = \"\"\n",
        "        if compression > 0.0001: result_status = f\"SUCCESS: Compression achieved: {compression:.4f}\"\n",
        "        else: result_status = f\"NOTE: No significant compression (or cost increased). Diff: {compression:.4f}\"\n",
        "        print(f\"  {result_status}\")\n",
        "\n",
        "        all_qid_results.append({\n",
        "            \"qid\": qid, \"corpus_len_for_qid\": len(corpus_for_qid), \"num_responses\": num_responses_for_qid,\n",
        "            \"baseline_mdl\": baseline_total_mdl, \"motifs\": extracted_motifs,\n",
        "            \"l_h_motifs\": l_h, \"l_d_h_motifs\": l_d_h, \"total_mdl_motifs\": total_mdl_with_motifs,\n",
        "            \"compression_achieved\": compression\n",
        "        })\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # (Summary printing and saving results - same as before)\n",
        "    print(\"\\n--- Overall QID-based MDL Analysis Summary (Local LLM) ---\")\n",
        "    if not all_qid_results:\n",
        "        print(\"No QIDs were processed or no valid results generated.\")\n",
        "    else:\n",
        "        valid_results_for_stats = [r for r in all_qid_results if not isinstance(r['compression_achieved'], str) and r['l_h_motifs'] >= 0]\n",
        "        num_compressed_qids = sum(1 for r in valid_results_for_stats if r['compression_achieved'] > 0.0001)\n",
        "        successful_compressions = [r['compression_achieved'] for r in valid_results_for_stats if r['compression_achieved'] > 0.0001]\n",
        "        avg_compression = np.mean(successful_compressions) if successful_compressions else 0\n",
        "        max_compression = np.max(successful_compressions) if successful_compressions else 0\n",
        "        # Note: qids_processed_count was removed as qids_to_target now defines how many are attempted.\n",
        "        print(f\"Total QIDs targeted for analysis: {len(qids_to_target)}\")\n",
        "        print(f\"Total QID results logged: {len(all_qid_results)}\")\n",
        "        print(f\"Number of QIDs where compression was achieved (from valid results): {num_compressed_qids}\")\n",
        "        if num_compressed_qids > 0:\n",
        "            print(f\"  Average compression (for successful cases): {avg_compression:.4f}\")\n",
        "            print(f\"  Maximum compression achieved across QIDs: {max_compression:.4f}\")\n",
        "        output_filename_qids = \"mdl_analysis_per_qid_local_llm_v2.json\" # Changed filename slightly\n",
        "        try:\n",
        "            with open(output_filename_qids, \"w\", encoding=\"utf-8\") as f_out:\n",
        "                json.dump(all_qid_results, f_out, indent=2)\n",
        "            print(f\"Detailed QID-based results saved to {output_filename_qids}\")\n",
        "        except Exception as e_save:\n",
        "            print(f\"Error saving QID-based results to {output_filename_qids}: {e_save}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589,
          "referenced_widgets": [
            "402cf05514504aa49906e88f9d09ffb3",
            "a54e97ec4bc346188387c304d9d7b0a8",
            "da4a9204cafe49f48ae6d686d9c07c41",
            "a04c05645e264ab18ed1e5660e2dd8a7",
            "b43aee5b8c5e4ebd922c65c583f65d7e",
            "b37d1f17f00145b68fef00c1f02d61ad",
            "c5487c1f6e544ab4b2338fb860eacb05",
            "baaedaa7654f436f8d4faf261424e34e",
            "49af9b578b4b4d148ba9e7517c5bb06d",
            "e789af69fab34de8a5bf9ba34723b4b2",
            "c66857da5b7547c8992a920cce5c8174"
          ]
        },
        "id": "luu06TqJ2e2P",
        "outputId": "e135ef08-05e1-48f8-c94f-5d5d72f90999"
      },
      "id": "luu06TqJ2e2P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- MDL Prototype: Analyzing Per-QID Aggregated Text (Local LLM) ---\n",
            "Using device: cuda\n",
            "Loading tokenizer for google/gemma-2b-it...\n",
            "BNB config created for google/gemma-2b-it, compute_dtype: torch.bfloat16.\n",
            "Loading local model google/gemma-2b-it (Quantization: True)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "402cf05514504aa49906e88f9d09ffb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM pipeline for google/gemma-2b-it initialized successfully.\n",
            "BDM instance initialized successfully.\n",
            "Loading Phase 2 output from: /content/drive/MyDrive/Colab Notebooks/Legal/Phase2_PDF_Collated_Texts/phase2_collated_pdf_texts.json...\n",
            "Targeting specific QIDs based on P3_QIDS_TO_PROCESS_THEMATICALLY: ['Q4']\n",
            "\n",
            "MDL analysis will run for these QIDs: ['Q4']\n",
            "\n",
            "--- Analyzing Aggregated Text for QID: Q4 ---\n",
            "  Combined corpus for QID Q4 has 129501 chars from 209 responses.\n",
            "  Baseline MDL for QID Q4 (L(D_orig_corpus_for_qid)): 121.3693\n",
            "  Local LLM Extracted Motifs for QID Q4: ['Exceptions to individual rights', 'Balancing individual rights and competing public interests', 'Robust privacy protections in the digital economy', 'Stricter data rights for employees', 'Balancing employee rights with the need for data security']\n",
            "  L(H) motif complexity for QID Q4: 609.4512\n",
            "  L(D|H) compressed corpus complexity for QID Q4: 118.2032\n",
            "  Total MDL cost with motifs for QID Q4: 727.6544\n",
            "  NOTE: No significant compression (or cost increased). Diff: -606.2851\n",
            "----------------------------------------\n",
            "\n",
            "--- Overall QID-based MDL Analysis Summary (Local LLM) ---\n",
            "Total QIDs targeted for analysis: 1\n",
            "Total QID results logged: 1\n",
            "Number of QIDs where compression was achieved (from valid results): 0\n",
            "Detailed QID-based results saved to mdl_analysis_per_qid_local_llm_v2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM7T2MRLledX"
      },
      "source": [
        "# 🧪 Minimal Working Prototype: Synthetic Data via Embedding Translation\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "- Embed a few real (or mock) samples\n",
        "- Learn a transformation into a new space (simulating vec2vec)\n",
        "- Generate synthetic text from translated embeddings\n",
        "- Evaluate semantic similarity and privacy risk"
      ],
      "id": "mM7T2MRLledX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0APC-bnledg"
      },
      "source": [
        "# Step 1: Install dependencies\n",
        "!pip install -q sentence-transformers scikit-learn numpy transformers"
      ],
      "id": "M0APC-bnledg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG9Gz2FOledi"
      },
      "source": [
        "# Step 2: Embed original and simulated target samples\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Simulated 'real' text (de-identified)\n",
        "real_texts = [\n",
        "    'I am concerned about data sharing with third parties.',\n",
        "    'The reform must ensure algorithmic transparency.',\n",
        "    'Consent should be more meaningful and not just a checkbox.'\n",
        "]\n",
        "\n",
        "# Simulated 'synthetic corpus' as new target space\n",
        "synthetic_texts = [\n",
        "    'Ensure user control over personal data.',\n",
        "    'Explain how machine learning models make decisions.',\n",
        "    'Design opt-in mechanisms that reflect informed choice.'\n",
        "]\n",
        "\n",
        "real_embeddings = normalize(model.encode(real_texts))\n",
        "target_embeddings = normalize(model.encode(synthetic_texts))"
      ],
      "id": "iG9Gz2FOledi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "144RVk5Kledk"
      },
      "source": [
        "# Step 3: Learn and apply translation (Procrustes)\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "\n",
        "W, _ = orthogonal_procrustes(real_embeddings, target_embeddings)\n",
        "translated_embeddings = real_embeddings @ W"
      ],
      "id": "144RVk5Kledk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ-BCy_Qledk"
      },
      "source": [
        "# Step 4: Use nearest neighbors to recover semantically-aligned synthetic prompts\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sim_matrix = cosine_similarity(translated_embeddings, target_embeddings)\n",
        "for i, row in enumerate(sim_matrix):\n",
        "    match_idx = row.argmax()\n",
        "    print(f\"Original: {real_texts[i]}\\n → Synthetic: {synthetic_texts[match_idx]}\\n\")"
      ],
      "id": "fZ-BCy_Qledk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7stmD0lledl"
      },
      "source": [
        "## ✅ Next Steps\n",
        "- Use translated embeddings to prompt an LLM for full text generation\n",
        "- Use this embedding alignment method to create synthetic corpus in bulk\n",
        "- Validate outputs via cosine similarity + human review\n",
        "\n",
        "To integrate with `synthetic-data-kit` or `unsloth`, use these outputs to generate QA pairs or summaries."
      ],
      "id": "v7stmD0lledl"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d5230b4536b048c9bee0a10faff8bfbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fcda94bd47c42a7989947f2c44678db",
              "IPY_MODEL_0c0c9c82d9e04a7798c39871b99839af",
              "IPY_MODEL_4fe2b3fb2033466c939a5e8088dfb6ef"
            ],
            "layout": "IPY_MODEL_b670f488182340c297bc8bee844153ef"
          }
        },
        "6fcda94bd47c42a7989947f2c44678db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86c6e3f7bd514914a9989c4e49838ebb",
            "placeholder": "​",
            "style": "IPY_MODEL_14cd06caf864448983c3c4141489adfb",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0c0c9c82d9e04a7798c39871b99839af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d3f814138504a39a0a0afbd5214f3cc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd4ca90eba674edcb7ccfc77c6b8424c",
            "value": 2
          }
        },
        "4fe2b3fb2033466c939a5e8088dfb6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3059ddac1d404a73b20fbe488919c5ed",
            "placeholder": "​",
            "style": "IPY_MODEL_7addaa8aa2c04c1896bce670258f15a0",
            "value": " 2/2 [00:21&lt;00:00,  8.76s/it]"
          }
        },
        "b670f488182340c297bc8bee844153ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86c6e3f7bd514914a9989c4e49838ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14cd06caf864448983c3c4141489adfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d3f814138504a39a0a0afbd5214f3cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd4ca90eba674edcb7ccfc77c6b8424c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3059ddac1d404a73b20fbe488919c5ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7addaa8aa2c04c1896bce670258f15a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a64773925744baea29eea08a545553a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0df849aab8d940fc9d77e2f59c234eb8",
              "IPY_MODEL_7f8a30976cd34f2aafa500726823dabf",
              "IPY_MODEL_97299b0d95c04763bb841cf8b6e97e3a"
            ],
            "layout": "IPY_MODEL_e8ac434ab6a34412837fad8064bb014e"
          }
        },
        "0df849aab8d940fc9d77e2f59c234eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a57a12b412f84bc7b42c2fdc5dbcb44e",
            "placeholder": "​",
            "style": "IPY_MODEL_62e99469688045e8909e9a613baf5fd8",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7f8a30976cd34f2aafa500726823dabf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c96cb39cdb143afadbed3cdfee0e99d",
            "max": 34173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d08e648164c4022bf4ea71787abe737",
            "value": 34173
          }
        },
        "97299b0d95c04763bb841cf8b6e97e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcdda177c4d44b37a507033d455d5406",
            "placeholder": "​",
            "style": "IPY_MODEL_ba814d8b3dc742f3ba28309d94d51480",
            "value": " 34.2k/34.2k [00:00&lt;00:00, 1.99MB/s]"
          }
        },
        "e8ac434ab6a34412837fad8064bb014e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a57a12b412f84bc7b42c2fdc5dbcb44e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62e99469688045e8909e9a613baf5fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c96cb39cdb143afadbed3cdfee0e99d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d08e648164c4022bf4ea71787abe737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcdda177c4d44b37a507033d455d5406": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba814d8b3dc742f3ba28309d94d51480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b362802b0ac422fad1e85f8e49baced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a1154118fbf49fab9b62c8dc63101ac",
              "IPY_MODEL_57e7c031c4f2450eb3e4ab0b1f924d1a",
              "IPY_MODEL_aabdfed0252849ef8dbaa5fb933fbda1"
            ],
            "layout": "IPY_MODEL_aba11a26abe4416aa242cc386017495b"
          }
        },
        "1a1154118fbf49fab9b62c8dc63101ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed6dc9f7c39747b29c3591410d899e20",
            "placeholder": "​",
            "style": "IPY_MODEL_9f95347e08fc432f91a12e7a4d1a14e4",
            "value": "tokenizer.model: 100%"
          }
        },
        "57e7c031c4f2450eb3e4ab0b1f924d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73b2b6d7c673462b9b5e347c2c7be66a",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3263c5eb70164bd8b9b93b2e6e1c122b",
            "value": 4241003
          }
        },
        "aabdfed0252849ef8dbaa5fb933fbda1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_529f156b48da492fbc42194052174d0e",
            "placeholder": "​",
            "style": "IPY_MODEL_45d7a57c207a4b0ab2ed5469545becc2",
            "value": " 4.24M/4.24M [00:00&lt;00:00, 53.9MB/s]"
          }
        },
        "aba11a26abe4416aa242cc386017495b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed6dc9f7c39747b29c3591410d899e20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f95347e08fc432f91a12e7a4d1a14e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73b2b6d7c673462b9b5e347c2c7be66a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3263c5eb70164bd8b9b93b2e6e1c122b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "529f156b48da492fbc42194052174d0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45d7a57c207a4b0ab2ed5469545becc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea2554699c314899927546a5f81b1765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2f98fa8bd9a4734bcb0036ab3294a7c",
              "IPY_MODEL_4ae4477d4ae24ae0afea5e6a8da99716",
              "IPY_MODEL_286d1f5e53fe4fd5ac8bf2a43d94948e"
            ],
            "layout": "IPY_MODEL_b0e555643bc9431d9ca508f52e42dcb4"
          }
        },
        "a2f98fa8bd9a4734bcb0036ab3294a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b78b9de59d643d29d8cf7afe7269187",
            "placeholder": "​",
            "style": "IPY_MODEL_77a33da84d844784b6b0f3b96d896678",
            "value": "tokenizer.json: 100%"
          }
        },
        "4ae4477d4ae24ae0afea5e6a8da99716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0330a06f3f6428faa01bdc1d34c3467",
            "max": 17518497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf7cf3d2859b4482a297570d277326c3",
            "value": 17518497
          }
        },
        "286d1f5e53fe4fd5ac8bf2a43d94948e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a68d2cd4d99247499cd7b632d71e0f1b",
            "placeholder": "​",
            "style": "IPY_MODEL_4740d2159c45443f912f82871d7ff232",
            "value": " 17.5M/17.5M [00:00&lt;00:00, 213MB/s]"
          }
        },
        "b0e555643bc9431d9ca508f52e42dcb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b78b9de59d643d29d8cf7afe7269187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77a33da84d844784b6b0f3b96d896678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0330a06f3f6428faa01bdc1d34c3467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf7cf3d2859b4482a297570d277326c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a68d2cd4d99247499cd7b632d71e0f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4740d2159c45443f912f82871d7ff232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c0f050fa3b64bcbbf86daa03ee7a70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b97af678587a4f31a49c92506fbd066a",
              "IPY_MODEL_3e74e36d19914759a639279a0dc4de9b",
              "IPY_MODEL_2661d0f3b81a403cbc9b48b276c025cd"
            ],
            "layout": "IPY_MODEL_4dfc9632d4e341afaaf98d9011d8d05c"
          }
        },
        "b97af678587a4f31a49c92506fbd066a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa87e009cee84839b87a48820e076b6e",
            "placeholder": "​",
            "style": "IPY_MODEL_1a8e5ba53a7f4e4eab79bc47fa509f13",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "3e74e36d19914759a639279a0dc4de9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e6bbbcadf294dfa81250d6dc6fb69c8",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72b454d2b5204d4ab1caf1c453e7aa14",
            "value": 636
          }
        },
        "2661d0f3b81a403cbc9b48b276c025cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddac919f5b6047cda17259f30aa2f5bf",
            "placeholder": "​",
            "style": "IPY_MODEL_0b5173e1f85b4516aab2c84d7052f6ac",
            "value": " 636/636 [00:00&lt;00:00, 75.6kB/s]"
          }
        },
        "4dfc9632d4e341afaaf98d9011d8d05c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa87e009cee84839b87a48820e076b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a8e5ba53a7f4e4eab79bc47fa509f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e6bbbcadf294dfa81250d6dc6fb69c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72b454d2b5204d4ab1caf1c453e7aa14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ddac919f5b6047cda17259f30aa2f5bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b5173e1f85b4516aab2c84d7052f6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dbcf09f3f97496894851b4e4ce034bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1cfb632222e43f8b9eb616143daaab0",
              "IPY_MODEL_93e6fb28c09c45a7b0beaf976e16bc91",
              "IPY_MODEL_36baa56e1c444af4ab70ab8065ad00da"
            ],
            "layout": "IPY_MODEL_d62c9483f02f42d2822426db33c43694"
          }
        },
        "b1cfb632222e43f8b9eb616143daaab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb8b864025354ff7b59b2787cadf75c0",
            "placeholder": "​",
            "style": "IPY_MODEL_e0494862437f40078501dbebba0b70ed",
            "value": "config.json: 100%"
          }
        },
        "93e6fb28c09c45a7b0beaf976e16bc91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c43bdc9efb504b129df5f4312d79acf9",
            "max": 627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcc649facd1742d29458b5cf1d3528cd",
            "value": 627
          }
        },
        "36baa56e1c444af4ab70ab8065ad00da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74b5865376d142f2979aaeeec02eb83c",
            "placeholder": "​",
            "style": "IPY_MODEL_ce334170aa8b4317b74efdc9f4ef1c78",
            "value": " 627/627 [00:00&lt;00:00, 75.3kB/s]"
          }
        },
        "d62c9483f02f42d2822426db33c43694": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb8b864025354ff7b59b2787cadf75c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0494862437f40078501dbebba0b70ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c43bdc9efb504b129df5f4312d79acf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcc649facd1742d29458b5cf1d3528cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74b5865376d142f2979aaeeec02eb83c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce334170aa8b4317b74efdc9f4ef1c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "849749e91cc84fc2889f92db97784a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc0430db45df4d26baf98a1e4c933d46",
              "IPY_MODEL_4d73a5a35b4f4f5e87b396ff866038ac",
              "IPY_MODEL_ab4f71e9df6841a8a408c05a8534ef15"
            ],
            "layout": "IPY_MODEL_eef99c7acdb64985bbc606523494b1da"
          }
        },
        "dc0430db45df4d26baf98a1e4c933d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dab3ca19e6b4e8789621e47fd5bfc83",
            "placeholder": "​",
            "style": "IPY_MODEL_0d39a60d68214b11ad241abd655262d5",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "4d73a5a35b4f4f5e87b396ff866038ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07eacbf79eb94431b8b28eb4159f5a0f",
            "max": 13489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e4c6e81f5224d79b48f5afd3e48c8c0",
            "value": 13489
          }
        },
        "ab4f71e9df6841a8a408c05a8534ef15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abe08f958e3b45748923aed4292c4a07",
            "placeholder": "​",
            "style": "IPY_MODEL_4b77549e761446e789ed02746d5e57d7",
            "value": " 13.5k/13.5k [00:00&lt;00:00, 1.06MB/s]"
          }
        },
        "eef99c7acdb64985bbc606523494b1da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dab3ca19e6b4e8789621e47fd5bfc83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d39a60d68214b11ad241abd655262d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07eacbf79eb94431b8b28eb4159f5a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e4c6e81f5224d79b48f5afd3e48c8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "abe08f958e3b45748923aed4292c4a07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b77549e761446e789ed02746d5e57d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0ec648c027c4f519514bc0643caa33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a844260d126437b9f9a5aeac387650a",
              "IPY_MODEL_09d21b8e23504017854e79fa4677d41d",
              "IPY_MODEL_91bf0cf56c5744feb508e670677ff67b"
            ],
            "layout": "IPY_MODEL_a378a7d3247c44fba6120720f21d185f"
          }
        },
        "3a844260d126437b9f9a5aeac387650a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f82c66e947640008d95c5a8386322ce",
            "placeholder": "​",
            "style": "IPY_MODEL_22a4f7157a254ccf876a3fc7aa98b9d7",
            "value": "Fetching 2 files: 100%"
          }
        },
        "09d21b8e23504017854e79fa4677d41d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2ae5b69cef64cf5bfdbf7a1a02bcbba",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_665130a4de9043cfa16fc5c7b9c3f1f5",
            "value": 2
          }
        },
        "91bf0cf56c5744feb508e670677ff67b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b84493da552405b9af086c57634f0bd",
            "placeholder": "​",
            "style": "IPY_MODEL_a947f668b07e4591b6cc45e81c1d0922",
            "value": " 2/2 [00:25&lt;00:00, 25.35s/it]"
          }
        },
        "a378a7d3247c44fba6120720f21d185f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f82c66e947640008d95c5a8386322ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22a4f7157a254ccf876a3fc7aa98b9d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2ae5b69cef64cf5bfdbf7a1a02bcbba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "665130a4de9043cfa16fc5c7b9c3f1f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b84493da552405b9af086c57634f0bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a947f668b07e4591b6cc45e81c1d0922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28648067915945da8687b691d6a902a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c157add43598447aaf5bb493ea2e66aa",
              "IPY_MODEL_8530ddd8cb924ad180ada0a1cd0ba3c4",
              "IPY_MODEL_f8f19ca1c54c40afb40fee47731cd848"
            ],
            "layout": "IPY_MODEL_5107e8c660eb438e9f4c7d074b09868c"
          }
        },
        "c157add43598447aaf5bb493ea2e66aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eb1d11644cd4598a84e134511999375",
            "placeholder": "​",
            "style": "IPY_MODEL_1872ee846ea441ea80c638e7d88cecfe",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "8530ddd8cb924ad180ada0a1cd0ba3c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49c62f684baa4dbeae5381a6ce23eb03",
            "max": 67121608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99add5ea5ced471f8461cb42bb651158",
            "value": 67121608
          }
        },
        "f8f19ca1c54c40afb40fee47731cd848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b0129787ddf46899e5b4c1d9ea82ca0",
            "placeholder": "​",
            "style": "IPY_MODEL_b5f8406419884add905b864558538091",
            "value": " 67.1M/67.1M [00:00&lt;00:00, 179MB/s]"
          }
        },
        "5107e8c660eb438e9f4c7d074b09868c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eb1d11644cd4598a84e134511999375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1872ee846ea441ea80c638e7d88cecfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49c62f684baa4dbeae5381a6ce23eb03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99add5ea5ced471f8461cb42bb651158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b0129787ddf46899e5b4c1d9ea82ca0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f8406419884add905b864558538091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dfadfdb88e54bc3b4ab7e6ebebdae4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdaf8a63740e405c96684195cf8de1e4",
              "IPY_MODEL_3671b9f22cda468dbc92b1c5322365f1",
              "IPY_MODEL_023c85da967341e8abdb871c52cc543b"
            ],
            "layout": "IPY_MODEL_f3be97b863604b5cbfc391e27f3f3aae"
          }
        },
        "cdaf8a63740e405c96684195cf8de1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93649707ebb0410ea51462903f8cafb8",
            "placeholder": "​",
            "style": "IPY_MODEL_4e55717324084a8cbb574fb1309b4458",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "3671b9f22cda468dbc92b1c5322365f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ba0ba4fa2914701a5f707a4691efae9",
            "max": 4945242264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92292e5a7d884474b0ec559764106e96",
            "value": 4945242264
          }
        },
        "023c85da967341e8abdb871c52cc543b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bef668f3a244dcfbf66bd34fbfa476d",
            "placeholder": "​",
            "style": "IPY_MODEL_8813bcc6ec274192a013d0a330fc4be9",
            "value": " 4.95G/4.95G [00:25&lt;00:00, 259MB/s]"
          }
        },
        "f3be97b863604b5cbfc391e27f3f3aae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93649707ebb0410ea51462903f8cafb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e55717324084a8cbb574fb1309b4458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ba0ba4fa2914701a5f707a4691efae9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92292e5a7d884474b0ec559764106e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bef668f3a244dcfbf66bd34fbfa476d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8813bcc6ec274192a013d0a330fc4be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23bad0724f494b6d9bd8fa51e2af497d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f60fb5773b6745fb9eab3af67ab58cff",
              "IPY_MODEL_d0e32ff1ef394b8bac73bb6513b1fd4f",
              "IPY_MODEL_09bc4e3534cc458799ebf828974fc95a"
            ],
            "layout": "IPY_MODEL_ac735f0111144b8baa9c0630b2ce88aa"
          }
        },
        "f60fb5773b6745fb9eab3af67ab58cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e8ca3d5a3864e098b3543d552ed0053",
            "placeholder": "​",
            "style": "IPY_MODEL_3522032fa59c4cf8a01983376719e5c7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d0e32ff1ef394b8bac73bb6513b1fd4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5de21c45629b413a888c3d76f55f4219",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e8862b500f940fb98ea14f4bea0566b",
            "value": 2
          }
        },
        "09bc4e3534cc458799ebf828974fc95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3a1bffff40f4fdc89ca6c1c6501c413",
            "placeholder": "​",
            "style": "IPY_MODEL_e7cd7c511942446a96950ca08cb3087b",
            "value": " 2/2 [00:28&lt;00:00, 11.64s/it]"
          }
        },
        "ac735f0111144b8baa9c0630b2ce88aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e8ca3d5a3864e098b3543d552ed0053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3522032fa59c4cf8a01983376719e5c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5de21c45629b413a888c3d76f55f4219": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e8862b500f940fb98ea14f4bea0566b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3a1bffff40f4fdc89ca6c1c6501c413": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7cd7c511942446a96950ca08cb3087b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2c3549678e64b2abc18b30c1cc86ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1054a496e9e84bb287c363c052ca56f4",
              "IPY_MODEL_0d93b1b80e444ea4b5b95dc8a9ea4840",
              "IPY_MODEL_3572193e8fcd4acf9d08549fe86c7c67"
            ],
            "layout": "IPY_MODEL_4e64f390f49b4bcc9a186a904746c768"
          }
        },
        "1054a496e9e84bb287c363c052ca56f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7b5e1b1d8004a8cbec48c1f0e612bdf",
            "placeholder": "​",
            "style": "IPY_MODEL_f83b3fdc72f8465aaeb4fbb881750806",
            "value": "generation_config.json: 100%"
          }
        },
        "0d93b1b80e444ea4b5b95dc8a9ea4840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e92b74b62e1405483100eb5a561591d",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_411242327ada41538187b0bdd7472cc9",
            "value": 137
          }
        },
        "3572193e8fcd4acf9d08549fe86c7c67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33100a1dce7045afbebbd851dde6b181",
            "placeholder": "​",
            "style": "IPY_MODEL_251c5ffae56a4028bb21fb9ebe847065",
            "value": " 137/137 [00:00&lt;00:00, 9.59kB/s]"
          }
        },
        "4e64f390f49b4bcc9a186a904746c768": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7b5e1b1d8004a8cbec48c1f0e612bdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83b3fdc72f8465aaeb4fbb881750806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e92b74b62e1405483100eb5a561591d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "411242327ada41538187b0bdd7472cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33100a1dce7045afbebbd851dde6b181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "251c5ffae56a4028bb21fb9ebe847065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "402cf05514504aa49906e88f9d09ffb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a54e97ec4bc346188387c304d9d7b0a8",
              "IPY_MODEL_da4a9204cafe49f48ae6d686d9c07c41",
              "IPY_MODEL_a04c05645e264ab18ed1e5660e2dd8a7"
            ],
            "layout": "IPY_MODEL_b43aee5b8c5e4ebd922c65c583f65d7e"
          }
        },
        "a54e97ec4bc346188387c304d9d7b0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b37d1f17f00145b68fef00c1f02d61ad",
            "placeholder": "​",
            "style": "IPY_MODEL_c5487c1f6e544ab4b2338fb860eacb05",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "da4a9204cafe49f48ae6d686d9c07c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baaedaa7654f436f8d4faf261424e34e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49af9b578b4b4d148ba9e7517c5bb06d",
            "value": 2
          }
        },
        "a04c05645e264ab18ed1e5660e2dd8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e789af69fab34de8a5bf9ba34723b4b2",
            "placeholder": "​",
            "style": "IPY_MODEL_c66857da5b7547c8992a920cce5c8174",
            "value": " 2/2 [00:18&lt;00:00,  7.79s/it]"
          }
        },
        "b43aee5b8c5e4ebd922c65c583f65d7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b37d1f17f00145b68fef00c1f02d61ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5487c1f6e544ab4b2338fb860eacb05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "baaedaa7654f436f8d4faf261424e34e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49af9b578b4b4d148ba9e7517c5bb06d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e789af69fab34de8a5bf9ba34723b4b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c66857da5b7547c8992a920cce5c8174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a21fba4cc298413e8dfddeb442bd1808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d6cf6cb97524e939ec83feac69a494a",
              "IPY_MODEL_81201a97fa71413ba7ee0743b391368f",
              "IPY_MODEL_4ea4e6357aba4a5f92e192aa8beb9ce0"
            ],
            "layout": "IPY_MODEL_b555f7f3c57148c0afed1d3a30548488"
          }
        },
        "7d6cf6cb97524e939ec83feac69a494a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a38b994f5ca4a13a4b484012c06b793",
            "placeholder": "​",
            "style": "IPY_MODEL_1475e693f7f14cafa806a84e7d76c899",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "81201a97fa71413ba7ee0743b391368f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20b847c1cbe84e0881237c16f9f81817",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_557ef3ddcb61472fad04269774283785",
            "value": 2
          }
        },
        "4ea4e6357aba4a5f92e192aa8beb9ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c33753344ad245618155cb809574fa9d",
            "placeholder": "​",
            "style": "IPY_MODEL_a7e7478becde448daf516f44105aad61",
            "value": " 2/2 [00:24&lt;00:00, 10.07s/it]"
          }
        },
        "b555f7f3c57148c0afed1d3a30548488": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a38b994f5ca4a13a4b484012c06b793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1475e693f7f14cafa806a84e7d76c899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20b847c1cbe84e0881237c16f9f81817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "557ef3ddcb61472fad04269774283785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c33753344ad245618155cb809574fa9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7e7478becde448daf516f44105aad61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9342a1062ca94a2ba4448081e0c76413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77011edf3a09404ea3dc93d68b9ae2af",
              "IPY_MODEL_dced6f505bf44c90be4b60c35ed9c0d2",
              "IPY_MODEL_770751fcca504a0c9affe9a73cbcab14"
            ],
            "layout": "IPY_MODEL_a50a745d1dd143a4b0ed2d4e4f094edd"
          }
        },
        "77011edf3a09404ea3dc93d68b9ae2af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c078ce5ec4094356b48a88f197fbf341",
            "placeholder": "​",
            "style": "IPY_MODEL_00873c1da11849e88bd5979e79467bd2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "dced6f505bf44c90be4b60c35ed9c0d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc08983a943f462dab9a3c69f4559c0a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3ba763ea08345bc91f11df2e5b1d650",
            "value": 2
          }
        },
        "770751fcca504a0c9affe9a73cbcab14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b85fb6b6697c4c26a0b9dcfe9a0cc66e",
            "placeholder": "​",
            "style": "IPY_MODEL_01581ac673144927be13cb5aa9f44202",
            "value": " 2/2 [00:23&lt;00:00,  9.91s/it]"
          }
        },
        "a50a745d1dd143a4b0ed2d4e4f094edd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c078ce5ec4094356b48a88f197fbf341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00873c1da11849e88bd5979e79467bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc08983a943f462dab9a3c69f4559c0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ba763ea08345bc91f11df2e5b1d650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b85fb6b6697c4c26a0b9dcfe9a0cc66e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01581ac673144927be13cb5aa9f44202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d224745da5b243898d51995805e993fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f34a78926243449088936a63e3391b83",
              "IPY_MODEL_2ba36adf986545138b5b77974274a38e",
              "IPY_MODEL_7c0edf9d536c40db96dcafaea2b9bad7"
            ],
            "layout": "IPY_MODEL_6fbe1ad790d1466686e940746412f2ee"
          }
        },
        "f34a78926243449088936a63e3391b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24659fba736e4d45abc34779e731b6e2",
            "placeholder": "​",
            "style": "IPY_MODEL_3d5cfecdb6fe487bb98ac0afe7c6997b",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "2ba36adf986545138b5b77974274a38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8bb546e611f428c917a74e92d44777e",
            "max": 34173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7639ab401c3a48c9a05b4bb2a11b8baa",
            "value": 34173
          }
        },
        "7c0edf9d536c40db96dcafaea2b9bad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d82ca5e0b3547dca20f4ff0b548684c",
            "placeholder": "​",
            "style": "IPY_MODEL_2a7b7d9129184c73bd6c9bd621d404ee",
            "value": " 34.2k/34.2k [00:00&lt;00:00, 539kB/s]"
          }
        },
        "6fbe1ad790d1466686e940746412f2ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24659fba736e4d45abc34779e731b6e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d5cfecdb6fe487bb98ac0afe7c6997b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8bb546e611f428c917a74e92d44777e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7639ab401c3a48c9a05b4bb2a11b8baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d82ca5e0b3547dca20f4ff0b548684c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a7b7d9129184c73bd6c9bd621d404ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75fca020536047a2bdb4f2aec0deda87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46775fc9f5ae4098aa0eeacc58df0a07",
              "IPY_MODEL_9accd69da0494da781611cd44df2d13a",
              "IPY_MODEL_47beeb3fdb564bc39f126f0d36836d30"
            ],
            "layout": "IPY_MODEL_5dd88221987a4cd49ab4932aadffa3be"
          }
        },
        "46775fc9f5ae4098aa0eeacc58df0a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b810986cbc104b899eb84e1e71cc04db",
            "placeholder": "​",
            "style": "IPY_MODEL_035304554df740d1b8b527b6bc59f00a",
            "value": "tokenizer.model: 100%"
          }
        },
        "9accd69da0494da781611cd44df2d13a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53fff4ebf541469bb3c7fecf5c2448fc",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8bbf11fb7b8465889ad69d4f1e9b455",
            "value": 4241003
          }
        },
        "47beeb3fdb564bc39f126f0d36836d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd5ff10ffc36471ea22e36c891542dd5",
            "placeholder": "​",
            "style": "IPY_MODEL_18a7d258945544f8ad17de47ad0f3f39",
            "value": " 4.24M/4.24M [00:00&lt;00:00, 39.1MB/s]"
          }
        },
        "5dd88221987a4cd49ab4932aadffa3be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b810986cbc104b899eb84e1e71cc04db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "035304554df740d1b8b527b6bc59f00a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53fff4ebf541469bb3c7fecf5c2448fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8bbf11fb7b8465889ad69d4f1e9b455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd5ff10ffc36471ea22e36c891542dd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a7d258945544f8ad17de47ad0f3f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4949b8049254640aafb0774728d3a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be98287f1cb1474eb41c9672b74714cf",
              "IPY_MODEL_81a6266760b84e1d87e70fb025b4bee8",
              "IPY_MODEL_2ab077a6a55448a58ca7f37e183b8163"
            ],
            "layout": "IPY_MODEL_6b7875bd803c4e418e8561e622a63153"
          }
        },
        "be98287f1cb1474eb41c9672b74714cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_738aae361b68489081b882339cc81482",
            "placeholder": "​",
            "style": "IPY_MODEL_300ba463ee794a53acb6490bfd1b299b",
            "value": "tokenizer.json: 100%"
          }
        },
        "81a6266760b84e1d87e70fb025b4bee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35bafdbc19be4d62a7a7bf249f6c56ec",
            "max": 17518497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15be8bea5c254c6f85e60fb33da56586",
            "value": 17518497
          }
        },
        "2ab077a6a55448a58ca7f37e183b8163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5331cd7f6ca74492bd887cb56f45571f",
            "placeholder": "​",
            "style": "IPY_MODEL_cefec5b93d3b4540ad1bb937921b17b4",
            "value": " 17.5M/17.5M [00:00&lt;00:00, 73.3MB/s]"
          }
        },
        "6b7875bd803c4e418e8561e622a63153": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738aae361b68489081b882339cc81482": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "300ba463ee794a53acb6490bfd1b299b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35bafdbc19be4d62a7a7bf249f6c56ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15be8bea5c254c6f85e60fb33da56586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5331cd7f6ca74492bd887cb56f45571f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cefec5b93d3b4540ad1bb937921b17b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "080a713c867445fe8a2777f0792772f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1049fa02cd0648ada93ee10c9624669d",
              "IPY_MODEL_d0c54f402d0d42f0b8d1927279110d48",
              "IPY_MODEL_1dcfd5dfca9a4017ac3d6748414a6d32"
            ],
            "layout": "IPY_MODEL_418f07476d0f4f4ea94caa7b34b7c83f"
          }
        },
        "1049fa02cd0648ada93ee10c9624669d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32e986998a3f4cb4bf54872ebc9c2c91",
            "placeholder": "​",
            "style": "IPY_MODEL_0292e030616c46b98cb37bd61d1a92d8",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "d0c54f402d0d42f0b8d1927279110d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edba6b259340447e951d7cb34a46dbba",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9241af92762a4e6d83a0075fa12e7349",
            "value": 636
          }
        },
        "1dcfd5dfca9a4017ac3d6748414a6d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b69c81812644cbf81d5ad0b6a5049c2",
            "placeholder": "​",
            "style": "IPY_MODEL_2150c1812bc04898bb3b103ea2a78137",
            "value": " 636/636 [00:00&lt;00:00, 16.8kB/s]"
          }
        },
        "418f07476d0f4f4ea94caa7b34b7c83f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32e986998a3f4cb4bf54872ebc9c2c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0292e030616c46b98cb37bd61d1a92d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edba6b259340447e951d7cb34a46dbba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9241af92762a4e6d83a0075fa12e7349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b69c81812644cbf81d5ad0b6a5049c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2150c1812bc04898bb3b103ea2a78137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46c17c4ced914b1d8f3a7c02cc9bc47a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3206e6a816474f2a8ca56bc5d1f2695d",
              "IPY_MODEL_e7f07947823249c9be9698a57ba1071d",
              "IPY_MODEL_f8df8b5275fc4929ad8f854838325627"
            ],
            "layout": "IPY_MODEL_0053e39f66484a8fa8c9c21de927b9ea"
          }
        },
        "3206e6a816474f2a8ca56bc5d1f2695d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acf10cfda9384aadad17f5a5f0663b7b",
            "placeholder": "​",
            "style": "IPY_MODEL_4dd9673ab305465cbd18e65b72d52d4e",
            "value": "config.json: 100%"
          }
        },
        "e7f07947823249c9be9698a57ba1071d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d2bbc2a497f4d14be4a6b74bf240019",
            "max": 627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51d617334acf491893c385d98ca77cd7",
            "value": 627
          }
        },
        "f8df8b5275fc4929ad8f854838325627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3042d9c6f746453aa1927cd49825dcd0",
            "placeholder": "​",
            "style": "IPY_MODEL_9dda1a00a4a54e09874ab864fada8536",
            "value": " 627/627 [00:00&lt;00:00, 19.3kB/s]"
          }
        },
        "0053e39f66484a8fa8c9c21de927b9ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acf10cfda9384aadad17f5a5f0663b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dd9673ab305465cbd18e65b72d52d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d2bbc2a497f4d14be4a6b74bf240019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51d617334acf491893c385d98ca77cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3042d9c6f746453aa1927cd49825dcd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dda1a00a4a54e09874ab864fada8536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61eddf1106d7440cbda812954e413edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b96debc777f14cd7b040ccd7f3d446c8",
              "IPY_MODEL_4b8d6723fba6490fb4c6e52f625b19bd",
              "IPY_MODEL_b2b83bb0d65d401090ce0dadf5df9e95"
            ],
            "layout": "IPY_MODEL_0d77c3636eba48deba98b43b488df771"
          }
        },
        "b96debc777f14cd7b040ccd7f3d446c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_995074e734bf4b22b032f14b2d6efa91",
            "placeholder": "​",
            "style": "IPY_MODEL_2f5ca7c6a3d84fc0b59b4bbeed320913",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "4b8d6723fba6490fb4c6e52f625b19bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63ca04b3ee414c2989ca96716812788b",
            "max": 13489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8de3310eccc4f248f5c5ad817bae65c",
            "value": 13489
          }
        },
        "b2b83bb0d65d401090ce0dadf5df9e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d314ef166f1b40899065c3dd6a15d10b",
            "placeholder": "​",
            "style": "IPY_MODEL_a2b56e52995c4a58ad34ee47b9194dd7",
            "value": " 13.5k/13.5k [00:00&lt;00:00, 665kB/s]"
          }
        },
        "0d77c3636eba48deba98b43b488df771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "995074e734bf4b22b032f14b2d6efa91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f5ca7c6a3d84fc0b59b4bbeed320913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63ca04b3ee414c2989ca96716812788b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8de3310eccc4f248f5c5ad817bae65c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d314ef166f1b40899065c3dd6a15d10b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2b56e52995c4a58ad34ee47b9194dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa8cd143666c435586306e025b171263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_444c9abb956046c7b31acf9e6c85bd77",
              "IPY_MODEL_b0eaaab19576444c81d20e8ac172d1e4",
              "IPY_MODEL_5b120b328d4c45ebbc7f5ba041647bdc"
            ],
            "layout": "IPY_MODEL_84844dcfca08432594ebce64022c556a"
          }
        },
        "444c9abb956046c7b31acf9e6c85bd77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd3f60df45cc41518fe1631fba735023",
            "placeholder": "​",
            "style": "IPY_MODEL_3a96966df2034ea9962104e430c9cb27",
            "value": "Fetching 2 files: 100%"
          }
        },
        "b0eaaab19576444c81d20e8ac172d1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_535d4696a3c44e4eb30f692d3e5d4ab0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a197c176f240416f8ec9f8d111f655f1",
            "value": 2
          }
        },
        "5b120b328d4c45ebbc7f5ba041647bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_631681e611ce46f79f03a61b773b944d",
            "placeholder": "​",
            "style": "IPY_MODEL_7320464df0d14a5f8f6e4ca037af97e1",
            "value": " 2/2 [01:34&lt;00:00, 94.02s/it]"
          }
        },
        "84844dcfca08432594ebce64022c556a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd3f60df45cc41518fe1631fba735023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a96966df2034ea9962104e430c9cb27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "535d4696a3c44e4eb30f692d3e5d4ab0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a197c176f240416f8ec9f8d111f655f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "631681e611ce46f79f03a61b773b944d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7320464df0d14a5f8f6e4ca037af97e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da218f6cd702439f8b0d2807fc5f5ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2fc40dad4724f5eb8f640798af57a45",
              "IPY_MODEL_fe35820c2d8c4c20a01c82bb5f810a25",
              "IPY_MODEL_629042d965e14a1680b2d917317642cf"
            ],
            "layout": "IPY_MODEL_1080bea490684700a3c227bad451c11d"
          }
        },
        "e2fc40dad4724f5eb8f640798af57a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62ee46f2eeef48e995ea8932c422dd92",
            "placeholder": "​",
            "style": "IPY_MODEL_2a250e38cc244fbf8655409260d231a2",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "fe35820c2d8c4c20a01c82bb5f810a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8240628da6d741d3958937f403c864f5",
            "max": 4945242264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6390cddb130f4b65ad3024ebd5e7c758",
            "value": 4945242264
          }
        },
        "629042d965e14a1680b2d917317642cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31d8a2e536284082b18c972dc80ff4d0",
            "placeholder": "​",
            "style": "IPY_MODEL_7f505a52be90451e989d1fdc9c1a7fe3",
            "value": " 4.95G/4.95G [01:33&lt;00:00, 21.1MB/s]"
          }
        },
        "1080bea490684700a3c227bad451c11d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62ee46f2eeef48e995ea8932c422dd92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a250e38cc244fbf8655409260d231a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8240628da6d741d3958937f403c864f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6390cddb130f4b65ad3024ebd5e7c758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31d8a2e536284082b18c972dc80ff4d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f505a52be90451e989d1fdc9c1a7fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9802031d26cf414abe6e5d15cc23370d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02af6cf0b16b4ed1aebffa113e565c5f",
              "IPY_MODEL_e05e800bd96f4125a09aa31c7939c8fa",
              "IPY_MODEL_cbc16800773440df810fefa6e74d90c3"
            ],
            "layout": "IPY_MODEL_90cb4280dc9f4c268d4ff42f4f404d74"
          }
        },
        "02af6cf0b16b4ed1aebffa113e565c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d413be70e9043b8814ce9e851bd1483",
            "placeholder": "​",
            "style": "IPY_MODEL_58b50a4e6b8c4ab9b6d330d4cd14e6e7",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "e05e800bd96f4125a09aa31c7939c8fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71d28e971ddf42ebb4306a7e40b0420a",
            "max": 67121608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1e348b5fcca4ac985cad4b59aabcd42",
            "value": 67121608
          }
        },
        "cbc16800773440df810fefa6e74d90c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45ca9a8aea234f0ab0d6e9b881f43b5f",
            "placeholder": "​",
            "style": "IPY_MODEL_9b331fd8db0c43559be5a6e7516f1255",
            "value": " 67.1M/67.1M [00:00&lt;00:00, 118MB/s]"
          }
        },
        "90cb4280dc9f4c268d4ff42f4f404d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d413be70e9043b8814ce9e851bd1483": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58b50a4e6b8c4ab9b6d330d4cd14e6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71d28e971ddf42ebb4306a7e40b0420a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1e348b5fcca4ac985cad4b59aabcd42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45ca9a8aea234f0ab0d6e9b881f43b5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b331fd8db0c43559be5a6e7516f1255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "308ef4a3942b4cc484671f9556b7f386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af9118f38d6940f68edcf459b31ccec6",
              "IPY_MODEL_c530bfa2c60241caaac991c17e0004f1",
              "IPY_MODEL_69d57e45f99d4ea6ab9050c5c47f3d9f"
            ],
            "layout": "IPY_MODEL_90a947613c6142d2b4250dbdfe57f688"
          }
        },
        "af9118f38d6940f68edcf459b31ccec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b82efef224464bf4aebc288e22e8747a",
            "placeholder": "​",
            "style": "IPY_MODEL_4f09e950217c4105ab117d0121eab161",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c530bfa2c60241caaac991c17e0004f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aca71d5d1b44472b9b216e17195e85a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4f7097f38474555bdfb74ae284cad0d",
            "value": 2
          }
        },
        "69d57e45f99d4ea6ab9050c5c47f3d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3716ddce58c54737aec67e66e7a39a89",
            "placeholder": "​",
            "style": "IPY_MODEL_8fb5998f0cbb4e998e8c4fab32334b08",
            "value": " 2/2 [00:24&lt;00:00, 10.25s/it]"
          }
        },
        "90a947613c6142d2b4250dbdfe57f688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b82efef224464bf4aebc288e22e8747a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f09e950217c4105ab117d0121eab161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8aca71d5d1b44472b9b216e17195e85a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4f7097f38474555bdfb74ae284cad0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3716ddce58c54737aec67e66e7a39a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fb5998f0cbb4e998e8c4fab32334b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e94cb294862940c1b667df1a9eff29e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34cf4b7f66be4024bf3b709f903241b4",
              "IPY_MODEL_71a9f304c38e4c908c9cde4d0d8c9548",
              "IPY_MODEL_3109d1d07e834bedad2bc7967f3614a7"
            ],
            "layout": "IPY_MODEL_59e3fdd6993348afa10c22ee9202b385"
          }
        },
        "34cf4b7f66be4024bf3b709f903241b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53745263c2de4cb4ad69f51c624fb4df",
            "placeholder": "​",
            "style": "IPY_MODEL_d3ab3acee02f422ca9c4c05c19d6d8cc",
            "value": "generation_config.json: 100%"
          }
        },
        "71a9f304c38e4c908c9cde4d0d8c9548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a51dda8cdc284018aa92c68c1b328cc3",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fabe63dd0d314a93a614ffdc7713c5e0",
            "value": 137
          }
        },
        "3109d1d07e834bedad2bc7967f3614a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5350b263bb384f68a17b93527f13d559",
            "placeholder": "​",
            "style": "IPY_MODEL_0090e89f5db046b7842dacf2a22f51ff",
            "value": " 137/137 [00:00&lt;00:00, 8.88kB/s]"
          }
        },
        "59e3fdd6993348afa10c22ee9202b385": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53745263c2de4cb4ad69f51c624fb4df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3ab3acee02f422ca9c4c05c19d6d8cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a51dda8cdc284018aa92c68c1b328cc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fabe63dd0d314a93a614ffdc7713c5e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5350b263bb384f68a17b93527f13d559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0090e89f5db046b7842dacf2a22f51ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15e04d3b2e13454e80b5d7a93d216a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f47b73152c5440cb61c982ebab9899c",
              "IPY_MODEL_f737dbe0c95d432d9a386e9b64a9de0c",
              "IPY_MODEL_33af13d07b1245a89a09eedae4d6c5aa"
            ],
            "layout": "IPY_MODEL_dc8e4a3a333e400e93390acd535f9993"
          }
        },
        "6f47b73152c5440cb61c982ebab9899c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ede814e3fe74153a7c236cec38d92c4",
            "placeholder": "​",
            "style": "IPY_MODEL_5c9f7e411021498695ddfe14d2619096",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f737dbe0c95d432d9a386e9b64a9de0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72bcc4f24bce49e3b29f394249a8590c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7707003214fb429fa7215a893d9ae008",
            "value": 2
          }
        },
        "33af13d07b1245a89a09eedae4d6c5aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f5f00d4412b4e25916ea134d991f225",
            "placeholder": "​",
            "style": "IPY_MODEL_c9137173812b42d4a5f31974d989759f",
            "value": " 2/2 [00:41&lt;00:00, 20.21s/it]"
          }
        },
        "dc8e4a3a333e400e93390acd535f9993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ede814e3fe74153a7c236cec38d92c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9f7e411021498695ddfe14d2619096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72bcc4f24bce49e3b29f394249a8590c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7707003214fb429fa7215a893d9ae008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f5f00d4412b4e25916ea134d991f225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9137173812b42d4a5f31974d989759f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "544b149ee9b84729a55d6a455bac9c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1e8a0ec78214c5b99e7eb0f813c0906",
              "IPY_MODEL_838b815131e44d2dbbdba57ecb103208",
              "IPY_MODEL_f6b3cf07405549bbabe7f906bed98a65"
            ],
            "layout": "IPY_MODEL_ab8c4bd7599f4ed5a1f5204905cca6e9"
          }
        },
        "a1e8a0ec78214c5b99e7eb0f813c0906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d431f912399f4ce8a47c27ab803350f0",
            "placeholder": "​",
            "style": "IPY_MODEL_29032a12246d4526a764e77a8207b11a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "838b815131e44d2dbbdba57ecb103208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df418e4f62b94f18a290f5043fd87ac4",
            "max": 34173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60aa7572f23a451ba601c7336dcf997e",
            "value": 34173
          }
        },
        "f6b3cf07405549bbabe7f906bed98a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83fe78a19cee47a5b2538f0eec11c150",
            "placeholder": "​",
            "style": "IPY_MODEL_c0c036733d58472f98f18e9444e052b5",
            "value": " 34.2k/34.2k [00:00&lt;00:00, 1.18MB/s]"
          }
        },
        "ab8c4bd7599f4ed5a1f5204905cca6e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d431f912399f4ce8a47c27ab803350f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29032a12246d4526a764e77a8207b11a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df418e4f62b94f18a290f5043fd87ac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60aa7572f23a451ba601c7336dcf997e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83fe78a19cee47a5b2538f0eec11c150": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c036733d58472f98f18e9444e052b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e027f0310bde4079aedf69540abbf080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4a69a214ac14f63a66704063a06bdb0",
              "IPY_MODEL_6f9f41fee36e465b87e78564c3bfc87d",
              "IPY_MODEL_811c614ae93e4c798705941379b27581"
            ],
            "layout": "IPY_MODEL_c4921b98f5c5435dad071cb78c7b6264"
          }
        },
        "f4a69a214ac14f63a66704063a06bdb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faec9076954b4065b325e10da878d9e3",
            "placeholder": "​",
            "style": "IPY_MODEL_8001c3d1c3eb4052ac41cf515948d44d",
            "value": "tokenizer.model: 100%"
          }
        },
        "6f9f41fee36e465b87e78564c3bfc87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9207e813ed24c73918fdaceb3e560cb",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e09f19832ef4940bc0a27d35cb19d4f",
            "value": 4241003
          }
        },
        "811c614ae93e4c798705941379b27581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bc7c8bf36a5409e8d9c7dc4254e03bf",
            "placeholder": "​",
            "style": "IPY_MODEL_2100ab3c1d2e4983a19720058f2cd4e6",
            "value": " 4.24M/4.24M [00:00&lt;00:00, 15.5MB/s]"
          }
        },
        "c4921b98f5c5435dad071cb78c7b6264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faec9076954b4065b325e10da878d9e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8001c3d1c3eb4052ac41cf515948d44d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9207e813ed24c73918fdaceb3e560cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e09f19832ef4940bc0a27d35cb19d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8bc7c8bf36a5409e8d9c7dc4254e03bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2100ab3c1d2e4983a19720058f2cd4e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1ecc31a0a3944b0b8c538fae95477b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03550f00a9dd4b30993d30e7aa284085",
              "IPY_MODEL_cbbc7d0133da4cfd96fc714177923ade",
              "IPY_MODEL_40ea27218c4d420fb396dfddc2a93e95"
            ],
            "layout": "IPY_MODEL_c1601eaf9d8c469cbaf3dfd029848add"
          }
        },
        "03550f00a9dd4b30993d30e7aa284085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe83604d163d494dbd9fd63a56855c9b",
            "placeholder": "​",
            "style": "IPY_MODEL_b3dda36901cb41c7bc17afc78255c20a",
            "value": "tokenizer.json: 100%"
          }
        },
        "cbbc7d0133da4cfd96fc714177923ade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af4a463d5c67432998ce063b4a911861",
            "max": 17518497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6817babbc89e44a19ddbcba27d1a1e8c",
            "value": 17518497
          }
        },
        "40ea27218c4d420fb396dfddc2a93e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a91bb0bdab1e497b9961149a14b5fa43",
            "placeholder": "​",
            "style": "IPY_MODEL_deffbfbcebf54d25945e364b67ca51d8",
            "value": " 17.5M/17.5M [00:00&lt;00:00, 59.1MB/s]"
          }
        },
        "c1601eaf9d8c469cbaf3dfd029848add": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe83604d163d494dbd9fd63a56855c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3dda36901cb41c7bc17afc78255c20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af4a463d5c67432998ce063b4a911861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6817babbc89e44a19ddbcba27d1a1e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a91bb0bdab1e497b9961149a14b5fa43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deffbfbcebf54d25945e364b67ca51d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c5c170836e54bc9996cbd66c7b885ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d611e8a6ba74429196a534dd4bbc4174",
              "IPY_MODEL_8e2b0a3c9f31434f9f9909a1b60c0ec6",
              "IPY_MODEL_3761b8f77c9d4f2aa27187f1b3971775"
            ],
            "layout": "IPY_MODEL_b5e5badcbc6f4e5eb27965f1c7c3adbe"
          }
        },
        "d611e8a6ba74429196a534dd4bbc4174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8e4da327c9945c9ace894be067615b1",
            "placeholder": "​",
            "style": "IPY_MODEL_b00f58216a5e4c1ea5deb180e948d705",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "8e2b0a3c9f31434f9f9909a1b60c0ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f29ab6b845f648ea99765a6a5f3ff9f8",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2ca733d2047402aa7d240966e9e1e1f",
            "value": 636
          }
        },
        "3761b8f77c9d4f2aa27187f1b3971775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a5be45728414aa0b2c0a2da72fde0ad",
            "placeholder": "​",
            "style": "IPY_MODEL_45cbcd4a1a364eb6836a171f0fdd8a73",
            "value": " 636/636 [00:00&lt;00:00, 19.3kB/s]"
          }
        },
        "b5e5badcbc6f4e5eb27965f1c7c3adbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8e4da327c9945c9ace894be067615b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b00f58216a5e4c1ea5deb180e948d705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f29ab6b845f648ea99765a6a5f3ff9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ca733d2047402aa7d240966e9e1e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a5be45728414aa0b2c0a2da72fde0ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45cbcd4a1a364eb6836a171f0fdd8a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cb96c1713f04077a66e80efc3bcd0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_674d9d9dc3c84b2985516202ac73b193",
              "IPY_MODEL_4647bf04a5734b4b8a03d831e27d71ea",
              "IPY_MODEL_73f43d4a00cf4225b1f0695480de9121"
            ],
            "layout": "IPY_MODEL_4566e5044db245e3a859ace1f92a37b7"
          }
        },
        "674d9d9dc3c84b2985516202ac73b193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3edfcaf8a6c4ea78cb8bbded7ada100",
            "placeholder": "​",
            "style": "IPY_MODEL_0dc2469a30cd49cc89ff95b43465cce4",
            "value": "config.json: 100%"
          }
        },
        "4647bf04a5734b4b8a03d831e27d71ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e964980f8ab477f939cfe88163ab13f",
            "max": 627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1af4509d8b934606a8e849cb3acf46bc",
            "value": 627
          }
        },
        "73f43d4a00cf4225b1f0695480de9121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dea710f63bca4e2380716a814414e6b9",
            "placeholder": "​",
            "style": "IPY_MODEL_86de0c21e1d44771a008deb1f9dfbab7",
            "value": " 627/627 [00:00&lt;00:00, 26.1kB/s]"
          }
        },
        "4566e5044db245e3a859ace1f92a37b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3edfcaf8a6c4ea78cb8bbded7ada100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dc2469a30cd49cc89ff95b43465cce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e964980f8ab477f939cfe88163ab13f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1af4509d8b934606a8e849cb3acf46bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dea710f63bca4e2380716a814414e6b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86de0c21e1d44771a008deb1f9dfbab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58852eab7f804c1f960a9dd2f8af254c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff36e206409e461db9a9d48e843db499",
              "IPY_MODEL_801f5fb175a6475aad9c48c2a797587a",
              "IPY_MODEL_1f96711539df4f048a7f5ae06af77f41"
            ],
            "layout": "IPY_MODEL_05daee421ab0412c965f9e87d6edcf49"
          }
        },
        "ff36e206409e461db9a9d48e843db499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d5296ae7bf84f1a91a0c798422436a5",
            "placeholder": "​",
            "style": "IPY_MODEL_ed9067e2a2f2434482f7ca34dd6006a3",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "801f5fb175a6475aad9c48c2a797587a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dff8e3bcddbb4fb19d9291b7e99b1bc5",
            "max": 13489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5b2b276571648ceab5c3827422b2ab5",
            "value": 13489
          }
        },
        "1f96711539df4f048a7f5ae06af77f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4e148c54f104204af69b21ebfb583e4",
            "placeholder": "​",
            "style": "IPY_MODEL_97a086416fa14f56ad74c7a118e66888",
            "value": " 13.5k/13.5k [00:00&lt;00:00, 718kB/s]"
          }
        },
        "05daee421ab0412c965f9e87d6edcf49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d5296ae7bf84f1a91a0c798422436a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed9067e2a2f2434482f7ca34dd6006a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dff8e3bcddbb4fb19d9291b7e99b1bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5b2b276571648ceab5c3827422b2ab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4e148c54f104204af69b21ebfb583e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97a086416fa14f56ad74c7a118e66888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a8f0ed61ef34a80a03771fc3ecbae99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26332e5d98594c75ad7e9dedb5f7977d",
              "IPY_MODEL_beeb087ff3a54fc7ad3e6e4299ec0406",
              "IPY_MODEL_1a2beae78a2c492bb62f6afb1ebe4264"
            ],
            "layout": "IPY_MODEL_06c990e1edb94353888eb005b4a82288"
          }
        },
        "26332e5d98594c75ad7e9dedb5f7977d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58a69f8304064ce588b20163ce7906b5",
            "placeholder": "​",
            "style": "IPY_MODEL_72b88ab3cd5e4380998bff7874f89155",
            "value": "Fetching 2 files: 100%"
          }
        },
        "beeb087ff3a54fc7ad3e6e4299ec0406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1361d9b262a14baeaf9f9429ba72b73c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0575b7e6f6744c1aeef931c7130861e",
            "value": 2
          }
        },
        "1a2beae78a2c492bb62f6afb1ebe4264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6163977591b4165a715c4f30dce2a2e",
            "placeholder": "​",
            "style": "IPY_MODEL_1272689efb714b04b244891fb4618d6f",
            "value": " 2/2 [00:50&lt;00:00, 50.14s/it]"
          }
        },
        "06c990e1edb94353888eb005b4a82288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58a69f8304064ce588b20163ce7906b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72b88ab3cd5e4380998bff7874f89155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1361d9b262a14baeaf9f9429ba72b73c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0575b7e6f6744c1aeef931c7130861e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6163977591b4165a715c4f30dce2a2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1272689efb714b04b244891fb4618d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c717b35804754afca3e04a55b5d63a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c918bb59e3794beca4809a56a3214c63",
              "IPY_MODEL_7d0cb3fe712341058189a55645d45c02",
              "IPY_MODEL_95ddbae01df2472183e966de0a8d34ee"
            ],
            "layout": "IPY_MODEL_9c34ae0ff78447eb965021109ce276a9"
          }
        },
        "c918bb59e3794beca4809a56a3214c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9649b9d582d451c9079e7358e854cbf",
            "placeholder": "​",
            "style": "IPY_MODEL_7dc46cb490d3453e857755d0bb6c814d",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "7d0cb3fe712341058189a55645d45c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_954617cf030c4982afafdcc463c97ca2",
            "max": 67121608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f2cdf0e06324b30ae1b90fd48d64c3a",
            "value": 67121608
          }
        },
        "95ddbae01df2472183e966de0a8d34ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6715ac484f764fe58eed1e21827f7c80",
            "placeholder": "​",
            "style": "IPY_MODEL_5a7f12aa5873413dbd0116f21c1da02b",
            "value": " 67.1M/67.1M [00:00&lt;00:00, 91.2MB/s]"
          }
        },
        "9c34ae0ff78447eb965021109ce276a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9649b9d582d451c9079e7358e854cbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc46cb490d3453e857755d0bb6c814d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "954617cf030c4982afafdcc463c97ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f2cdf0e06324b30ae1b90fd48d64c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6715ac484f764fe58eed1e21827f7c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a7f12aa5873413dbd0116f21c1da02b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e69dfcb15a8a40b79ba6c0206728b7fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b1fb2a9280b4a98852b65302129f74d",
              "IPY_MODEL_2a7f91f7003b49d289d1ce1863083b3c",
              "IPY_MODEL_0cc168a176b446f5a4aceca19573b9ba"
            ],
            "layout": "IPY_MODEL_330652077f3445f8965a5ac39919ce27"
          }
        },
        "1b1fb2a9280b4a98852b65302129f74d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9dc7edb1ef242c58fce49ce6eb575a7",
            "placeholder": "​",
            "style": "IPY_MODEL_f56bace57e88409f866e1c3c258a4600",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "2a7f91f7003b49d289d1ce1863083b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e62144c209654e1eb5b57a7da630c9b1",
            "max": 4945242264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94fa7f0ca1a9457bbc6bfbe49bf813b7",
            "value": 4945242264
          }
        },
        "0cc168a176b446f5a4aceca19573b9ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f9f59f27ec94374acaf7660be552681",
            "placeholder": "​",
            "style": "IPY_MODEL_38c785f41a7b478e862cdeb07c1fcc99",
            "value": " 4.95G/4.95G [00:49&lt;00:00, 87.5MB/s]"
          }
        },
        "330652077f3445f8965a5ac39919ce27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9dc7edb1ef242c58fce49ce6eb575a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f56bace57e88409f866e1c3c258a4600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e62144c209654e1eb5b57a7da630c9b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94fa7f0ca1a9457bbc6bfbe49bf813b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f9f59f27ec94374acaf7660be552681": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38c785f41a7b478e862cdeb07c1fcc99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b9d0eb98a904bb6b05c8155b726c417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_837c5ac1a2e5467cacd148c840a87bc2",
              "IPY_MODEL_f4a8cceeb58b4bb4ac314d7edfaa2754",
              "IPY_MODEL_6f2c14a138664eebb9dc75c2849a4396"
            ],
            "layout": "IPY_MODEL_f4bcdf8ed29c4dfab23ddd02dfc31bfa"
          }
        },
        "837c5ac1a2e5467cacd148c840a87bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64da5c42a33f4c90bf4b70abc8393742",
            "placeholder": "​",
            "style": "IPY_MODEL_610fdabe80704e95bea9628075c9cfc2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f4a8cceeb58b4bb4ac314d7edfaa2754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed596c3733804b96bbb3df6b6bf2fea5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cd33871c73a4d108cd6fc1ad0a45fba",
            "value": 2
          }
        },
        "6f2c14a138664eebb9dc75c2849a4396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37ee374d4b35449588155870a424a0af",
            "placeholder": "​",
            "style": "IPY_MODEL_3fc0668356914f748d611fb1bb95522a",
            "value": " 2/2 [00:23&lt;00:00,  9.89s/it]"
          }
        },
        "f4bcdf8ed29c4dfab23ddd02dfc31bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64da5c42a33f4c90bf4b70abc8393742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "610fdabe80704e95bea9628075c9cfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed596c3733804b96bbb3df6b6bf2fea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd33871c73a4d108cd6fc1ad0a45fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37ee374d4b35449588155870a424a0af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fc0668356914f748d611fb1bb95522a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b19d8269d4e14c74a921ecb216a0dfe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58f7a3b11d7a499482f8145879782a20",
              "IPY_MODEL_49c797ab9e1c480f9a9d7682db941aaf",
              "IPY_MODEL_809819349c364d91bef83386a5a3dcaa"
            ],
            "layout": "IPY_MODEL_5fdfc2d3558e46b4834a1419bc9cd2a2"
          }
        },
        "58f7a3b11d7a499482f8145879782a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8cff635f52643b0a02f0307b9b2c3e6",
            "placeholder": "​",
            "style": "IPY_MODEL_e089aebba59b455a8299149b80af992f",
            "value": "generation_config.json: 100%"
          }
        },
        "49c797ab9e1c480f9a9d7682db941aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecda6762438f49f5995bb3d61851d62c",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f56bd992f94425eb9c5acdb6c7d4fed",
            "value": 137
          }
        },
        "809819349c364d91bef83386a5a3dcaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3238d3d52e534d488b129a544ced2b60",
            "placeholder": "​",
            "style": "IPY_MODEL_bf0737078bb44b51b837a49f524dd5a3",
            "value": " 137/137 [00:00&lt;00:00, 8.74kB/s]"
          }
        },
        "5fdfc2d3558e46b4834a1419bc9cd2a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8cff635f52643b0a02f0307b9b2c3e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e089aebba59b455a8299149b80af992f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecda6762438f49f5995bb3d61851d62c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f56bd992f94425eb9c5acdb6c7d4fed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3238d3d52e534d488b129a544ced2b60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf0737078bb44b51b837a49f524dd5a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbae32a5ed2942649c8b2b2dd81c9598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9dc17b55cf6a4e8d9d316ddd5759ad39",
              "IPY_MODEL_1a56b46a59d7447cab5b50b84f8164ab",
              "IPY_MODEL_2584ea08f90c4a6a90dde60576f3ceb1"
            ],
            "layout": "IPY_MODEL_4d5c08d8bef24b358d77b094bacdb460"
          }
        },
        "9dc17b55cf6a4e8d9d316ddd5759ad39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b297ed3417bc4a79891b13355e900ab5",
            "placeholder": "​",
            "style": "IPY_MODEL_f1dc021730bb402a97ad30eb9c8340b8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1a56b46a59d7447cab5b50b84f8164ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8371dab2ab641e7adf5e444fa4c6910",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d99f7b4468324a898287b3c4f6ae3554",
            "value": 2
          }
        },
        "2584ea08f90c4a6a90dde60576f3ceb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_414d7e0acf72421aa61e4e5302c90962",
            "placeholder": "​",
            "style": "IPY_MODEL_a2acc6f54c204c10beaeed31995454bb",
            "value": " 2/2 [00:13&lt;00:00,  5.45s/it]"
          }
        },
        "4d5c08d8bef24b358d77b094bacdb460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b297ed3417bc4a79891b13355e900ab5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1dc021730bb402a97ad30eb9c8340b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8371dab2ab641e7adf5e444fa4c6910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d99f7b4468324a898287b3c4f6ae3554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "414d7e0acf72421aa61e4e5302c90962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2acc6f54c204c10beaeed31995454bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e30a632dfc043b487fe015d2f4425a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11d336596bf74eeb81a5853d30f0688a",
              "IPY_MODEL_eb867fd7ce9145918709fb4485b94afa",
              "IPY_MODEL_150690c97f654b0595c19cf33ea07a69"
            ],
            "layout": "IPY_MODEL_98dbe79659444665b0f36907e0d90ae0"
          }
        },
        "11d336596bf74eeb81a5853d30f0688a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_429b10f41894411ba6e442a42d64b3f7",
            "placeholder": "​",
            "style": "IPY_MODEL_c400181857da49fcb75675be19a98769",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "eb867fd7ce9145918709fb4485b94afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6164bab89c1746d2a6e4fdd4e8663d89",
            "max": 34173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_802d1a122825460f8664263b7ba78f3a",
            "value": 34173
          }
        },
        "150690c97f654b0595c19cf33ea07a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad0595bc26584579b6913ab42bea72d3",
            "placeholder": "​",
            "style": "IPY_MODEL_d72b5964eea7460fb1e5a1365da7050b",
            "value": " 34.2k/34.2k [00:00&lt;00:00, 3.35MB/s]"
          }
        },
        "98dbe79659444665b0f36907e0d90ae0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "429b10f41894411ba6e442a42d64b3f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c400181857da49fcb75675be19a98769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6164bab89c1746d2a6e4fdd4e8663d89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802d1a122825460f8664263b7ba78f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad0595bc26584579b6913ab42bea72d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d72b5964eea7460fb1e5a1365da7050b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23633088a11e4f7d8e06f0b424a55b5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51ab088fea0046fab6f7bfc9606d895f",
              "IPY_MODEL_bd2446f7dae14fbb82c6b820c943281b",
              "IPY_MODEL_fa1f3158a230476485f7fae7642b3d17"
            ],
            "layout": "IPY_MODEL_9dd679e6ca1e4130827ecd25cb72515f"
          }
        },
        "51ab088fea0046fab6f7bfc9606d895f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ccd59cf05a42f0912b20289bc507d0",
            "placeholder": "​",
            "style": "IPY_MODEL_46381cfbfea341a3bf19d6766a0cea1e",
            "value": "tokenizer.model: 100%"
          }
        },
        "bd2446f7dae14fbb82c6b820c943281b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1424b6efe4f4bfe9fa2502297985935",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b800f4c979e4d0fa3fb26b10ef2d247",
            "value": 4241003
          }
        },
        "fa1f3158a230476485f7fae7642b3d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b300830b69d4d40b661e4e2d2d482c5",
            "placeholder": "​",
            "style": "IPY_MODEL_dd73de3dbe38471085314549e4967679",
            "value": " 4.24M/4.24M [00:00&lt;00:00, 63.6MB/s]"
          }
        },
        "9dd679e6ca1e4130827ecd25cb72515f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ccd59cf05a42f0912b20289bc507d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46381cfbfea341a3bf19d6766a0cea1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1424b6efe4f4bfe9fa2502297985935": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b800f4c979e4d0fa3fb26b10ef2d247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b300830b69d4d40b661e4e2d2d482c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd73de3dbe38471085314549e4967679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58391969e96542f79f28ced1640ecd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c2c145685ab47bab2a496c073b60311",
              "IPY_MODEL_980d1069d2d54a74ac1037ab1c283874",
              "IPY_MODEL_2f1fd9497fb84f349c20232ae3a31333"
            ],
            "layout": "IPY_MODEL_02a5b50eba404656bb06f38e9c6bb5b0"
          }
        },
        "6c2c145685ab47bab2a496c073b60311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62332dff83cb4a2db8a4d6da5e953f86",
            "placeholder": "​",
            "style": "IPY_MODEL_231937a3002e40a5b0ba08b07ee08680",
            "value": "tokenizer.json: 100%"
          }
        },
        "980d1069d2d54a74ac1037ab1c283874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2183c5e31474c168d913389546b46b0",
            "max": 17518497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0a8db49b3c94c6b89b95ea132cd5aa3",
            "value": 17518497
          }
        },
        "2f1fd9497fb84f349c20232ae3a31333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_054df9bb60ed425487de64b055ec5476",
            "placeholder": "​",
            "style": "IPY_MODEL_9b607c0a24664ab0b7ef011b45bff427",
            "value": " 17.5M/17.5M [00:00&lt;00:00, 236MB/s]"
          }
        },
        "02a5b50eba404656bb06f38e9c6bb5b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62332dff83cb4a2db8a4d6da5e953f86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "231937a3002e40a5b0ba08b07ee08680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2183c5e31474c168d913389546b46b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0a8db49b3c94c6b89b95ea132cd5aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "054df9bb60ed425487de64b055ec5476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b607c0a24664ab0b7ef011b45bff427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3f5b78c50144db397adaf6058d3053b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c06c94fb370947a186f02d369d4e971c",
              "IPY_MODEL_08e189579df34388831ab9f5b511698b",
              "IPY_MODEL_34f66c76a0924f99b248f99b3cdc8acc"
            ],
            "layout": "IPY_MODEL_66d2cb6a0d204c7295af214273248623"
          }
        },
        "c06c94fb370947a186f02d369d4e971c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8282c9fd37424a05a84725e2abf878a7",
            "placeholder": "​",
            "style": "IPY_MODEL_6a38c2a48fb1479d830651ff4d4c0ae7",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "08e189579df34388831ab9f5b511698b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bbdeaa3b19f40598296ff4d5ccf1e74",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4e9a7ad2af341328a18f2cd2894dfe6",
            "value": 636
          }
        },
        "34f66c76a0924f99b248f99b3cdc8acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8481a589ef9458c8bd6cb83e9352ec9",
            "placeholder": "​",
            "style": "IPY_MODEL_7af9fd85c71847a1abb60fe3aeb8a4ce",
            "value": " 636/636 [00:00&lt;00:00, 68.4kB/s]"
          }
        },
        "66d2cb6a0d204c7295af214273248623": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8282c9fd37424a05a84725e2abf878a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a38c2a48fb1479d830651ff4d4c0ae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bbdeaa3b19f40598296ff4d5ccf1e74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4e9a7ad2af341328a18f2cd2894dfe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8481a589ef9458c8bd6cb83e9352ec9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7af9fd85c71847a1abb60fe3aeb8a4ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bda2c7a76a740c1820855ab0d1a6d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9814970fd654f698009b90ae316fbb8",
              "IPY_MODEL_7ef73aefdf394f7686c6f89cbeed8629",
              "IPY_MODEL_bc288460a8074588a0e986b152a3249c"
            ],
            "layout": "IPY_MODEL_8afaca580d4d485aa45fe43fe49960cf"
          }
        },
        "f9814970fd654f698009b90ae316fbb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d60d2bc0ce14b00a4b46e5351505a99",
            "placeholder": "​",
            "style": "IPY_MODEL_e4d84d8f132c444e854922e66e35c7b0",
            "value": "config.json: 100%"
          }
        },
        "7ef73aefdf394f7686c6f89cbeed8629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d20bd65f24b241bcabc0e7ebe18554d1",
            "max": 627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f356750cd9c497296ac26860c89f74f",
            "value": 627
          }
        },
        "bc288460a8074588a0e986b152a3249c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81fa0101b7ee4235a7a2d733835c0ff1",
            "placeholder": "​",
            "style": "IPY_MODEL_e6febb1102434988a2aed3cbecc217a1",
            "value": " 627/627 [00:00&lt;00:00, 43.5kB/s]"
          }
        },
        "8afaca580d4d485aa45fe43fe49960cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d60d2bc0ce14b00a4b46e5351505a99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4d84d8f132c444e854922e66e35c7b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d20bd65f24b241bcabc0e7ebe18554d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f356750cd9c497296ac26860c89f74f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81fa0101b7ee4235a7a2d733835c0ff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6febb1102434988a2aed3cbecc217a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa2c347c796a423eb0add6a6fff533ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6e8fad762a24a65844dfd16afc756c5",
              "IPY_MODEL_bb82065867c6474aa1e991aa06601248",
              "IPY_MODEL_8a0dced1f1e14f20aa9234737b1f0292"
            ],
            "layout": "IPY_MODEL_1ef8bdaa51e24daf83e0aeddfec9dddb"
          }
        },
        "c6e8fad762a24a65844dfd16afc756c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd828f950be44f1ebebdf91ffbd32af6",
            "placeholder": "​",
            "style": "IPY_MODEL_aa5c215e1c5b41e795b3a1d730540d54",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "bb82065867c6474aa1e991aa06601248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82805cc6ac6b499aad51ac738c865c48",
            "max": 13489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8aa1b2b2e7341d383b24825d497d623",
            "value": 13489
          }
        },
        "8a0dced1f1e14f20aa9234737b1f0292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d6d544ed2274115a3117276622c0c31",
            "placeholder": "​",
            "style": "IPY_MODEL_d223cbccd6e342159b55280b7261baaf",
            "value": " 13.5k/13.5k [00:00&lt;00:00, 895kB/s]"
          }
        },
        "1ef8bdaa51e24daf83e0aeddfec9dddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd828f950be44f1ebebdf91ffbd32af6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa5c215e1c5b41e795b3a1d730540d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82805cc6ac6b499aad51ac738c865c48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8aa1b2b2e7341d383b24825d497d623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d6d544ed2274115a3117276622c0c31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d223cbccd6e342159b55280b7261baaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8ca2f6eaf624a9594b9c0222ad64bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28dc1a3b996f49ce99a43ba7ea1bef0d",
              "IPY_MODEL_61836ee9f7e54c6095f4d4f1a711f108",
              "IPY_MODEL_6213ae0a5754471b97be7ca3bfc4bc2c"
            ],
            "layout": "IPY_MODEL_0f6221cb4a33421db2112b48ca80ee81"
          }
        },
        "28dc1a3b996f49ce99a43ba7ea1bef0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd174bc0905e4431b7e355605fc1929e",
            "placeholder": "​",
            "style": "IPY_MODEL_f10441d25c4c480e8eb61fba1696672e",
            "value": "Fetching 2 files: 100%"
          }
        },
        "61836ee9f7e54c6095f4d4f1a711f108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db426c13be804bbbb1ac6e58a0e2aa9c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e223c79986834125ac52c6ebf2877688",
            "value": 2
          }
        },
        "6213ae0a5754471b97be7ca3bfc4bc2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4762661c1d54e88be80c5cf47065a10",
            "placeholder": "​",
            "style": "IPY_MODEL_6dd37035888346338f3f506f5dc05bad",
            "value": " 2/2 [00:52&lt;00:00, 52.90s/it]"
          }
        },
        "0f6221cb4a33421db2112b48ca80ee81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd174bc0905e4431b7e355605fc1929e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f10441d25c4c480e8eb61fba1696672e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db426c13be804bbbb1ac6e58a0e2aa9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e223c79986834125ac52c6ebf2877688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4762661c1d54e88be80c5cf47065a10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dd37035888346338f3f506f5dc05bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f5470f3874f4239a902ea0feee05a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21c6a659253a4e8cac66035c45123285",
              "IPY_MODEL_88216c2df9174004a2563134574469ee",
              "IPY_MODEL_295fd1f6e3294d8098d6220c051e6f22"
            ],
            "layout": "IPY_MODEL_2ee247a4585742e9a0774cf34eeabcbe"
          }
        },
        "21c6a659253a4e8cac66035c45123285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fd595dc6c354516bd824a6ef6890223",
            "placeholder": "​",
            "style": "IPY_MODEL_ead590be321a47caa9bef1851d981948",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "88216c2df9174004a2563134574469ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_807199c5604a48b2bb602b7ec8679efb",
            "max": 67121608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3704506e5914de58dde941767a58dbc",
            "value": 67121608
          }
        },
        "295fd1f6e3294d8098d6220c051e6f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c376075f6a6f41e08cb334bb17e65aa0",
            "placeholder": "​",
            "style": "IPY_MODEL_465fcc4654014c43a63edb58d00b212b",
            "value": " 67.1M/67.1M [00:00&lt;00:00, 226MB/s]"
          }
        },
        "2ee247a4585742e9a0774cf34eeabcbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fd595dc6c354516bd824a6ef6890223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ead590be321a47caa9bef1851d981948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "807199c5604a48b2bb602b7ec8679efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3704506e5914de58dde941767a58dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c376075f6a6f41e08cb334bb17e65aa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "465fcc4654014c43a63edb58d00b212b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce5fab931b674271ba6a3011737ab667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a82b6acd0b34df19101b0bebd5a07e6",
              "IPY_MODEL_975e7fe97a494dfcb5d420f878e6d75c",
              "IPY_MODEL_ee81089d9d274a98805f254efb397f81"
            ],
            "layout": "IPY_MODEL_7cea138fd2f04b42b7ec67e503e50252"
          }
        },
        "6a82b6acd0b34df19101b0bebd5a07e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b71815dc35bf49f3a2e44da6d781097c",
            "placeholder": "​",
            "style": "IPY_MODEL_835841490a194cb8a8e4005de641f1b5",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "975e7fe97a494dfcb5d420f878e6d75c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b6bec8b6b7049f4b07beaca37a6b9ee",
            "max": 4945242264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a853735973604ce6af12b6665241eea0",
            "value": 4945242264
          }
        },
        "ee81089d9d274a98805f254efb397f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e92c26c989d4082a3c8f0c0db160632",
            "placeholder": "​",
            "style": "IPY_MODEL_2c525814f2ef42ec984ec835b55a6646",
            "value": " 4.95G/4.95G [00:52&lt;00:00, 244MB/s]"
          }
        },
        "7cea138fd2f04b42b7ec67e503e50252": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b71815dc35bf49f3a2e44da6d781097c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "835841490a194cb8a8e4005de641f1b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b6bec8b6b7049f4b07beaca37a6b9ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a853735973604ce6af12b6665241eea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e92c26c989d4082a3c8f0c0db160632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c525814f2ef42ec984ec835b55a6646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "990346f345b346cbaa728a38a219a5cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b95c9b9c21e4030980f9c817a9ee883",
              "IPY_MODEL_073cfed82b934b559ba9fc8bcae57ee4",
              "IPY_MODEL_b0627e4e7fe34f218abc9a0aa14c022f"
            ],
            "layout": "IPY_MODEL_cce558ef0430433f9bb7ea4568adb316"
          }
        },
        "7b95c9b9c21e4030980f9c817a9ee883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dfe2fef684c4a098ec767efca9d48eb",
            "placeholder": "​",
            "style": "IPY_MODEL_ce75883d900e4f7bbda730290c08898c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "073cfed82b934b559ba9fc8bcae57ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cebe12e6f99453b9bc61d5c9a1f144f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb0686a6d07e4d6d864ad7b1defea582",
            "value": 2
          }
        },
        "b0627e4e7fe34f218abc9a0aa14c022f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de776b70ec1d4b9899ab1ea8ca2cb01d",
            "placeholder": "​",
            "style": "IPY_MODEL_b4581b4f55de4e33bb3d67dbd3aa5826",
            "value": " 2/2 [00:27&lt;00:00, 11.47s/it]"
          }
        },
        "cce558ef0430433f9bb7ea4568adb316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dfe2fef684c4a098ec767efca9d48eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce75883d900e4f7bbda730290c08898c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cebe12e6f99453b9bc61d5c9a1f144f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb0686a6d07e4d6d864ad7b1defea582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de776b70ec1d4b9899ab1ea8ca2cb01d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4581b4f55de4e33bb3d67dbd3aa5826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca90efc14b2d4ad5af56661882a015e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_967bb44a7373442487b09e04972e6204",
              "IPY_MODEL_19cce7a1a0f94b4aaf40632e65d59c64",
              "IPY_MODEL_08d3eadea6804bba8447ff09cc86887b"
            ],
            "layout": "IPY_MODEL_4aea8c9d69a04fe780ab79f1d466b662"
          }
        },
        "967bb44a7373442487b09e04972e6204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a570924c34d42bea4153f233ee60e09",
            "placeholder": "​",
            "style": "IPY_MODEL_16d3c74426154373977e8ec04e0544db",
            "value": "generation_config.json: 100%"
          }
        },
        "19cce7a1a0f94b4aaf40632e65d59c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2297eca392f467caa486ddda8dc6ee1",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec0ed16ba20b42f0a8f799d135c7f01c",
            "value": 137
          }
        },
        "08d3eadea6804bba8447ff09cc86887b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cba9395f483f4a68ba037a7eb61f1ce5",
            "placeholder": "​",
            "style": "IPY_MODEL_b8229e8ca52941028f3441fb5022cf69",
            "value": " 137/137 [00:00&lt;00:00, 11.6kB/s]"
          }
        },
        "4aea8c9d69a04fe780ab79f1d466b662": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a570924c34d42bea4153f233ee60e09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d3c74426154373977e8ec04e0544db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2297eca392f467caa486ddda8dc6ee1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec0ed16ba20b42f0a8f799d135c7f01c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cba9395f483f4a68ba037a7eb61f1ce5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8229e8ca52941028f3441fb5022cf69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de092b7514b24d8c88e0c1d39d9f37f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a3d80a9d7bb47b9a90e37c14e7ed378",
              "IPY_MODEL_296c113db2c649d7ac0d75b52b8ebc15",
              "IPY_MODEL_4e7bbb8256a547b994f56b5aee857751"
            ],
            "layout": "IPY_MODEL_dbc3c46c680d441b901f2d9466b66bed"
          }
        },
        "6a3d80a9d7bb47b9a90e37c14e7ed378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d4fc5f45d694f9fac1baf6c02d507c1",
            "placeholder": "​",
            "style": "IPY_MODEL_63890ba768de43a69fd111a2b805c88f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "296c113db2c649d7ac0d75b52b8ebc15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cedc5741214f4541b292c01eb4045ec9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5688f57e2974e38b4d47431c7ff469c",
            "value": 2
          }
        },
        "4e7bbb8256a547b994f56b5aee857751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd9beb8203ab48649a8b4f97a68f93de",
            "placeholder": "​",
            "style": "IPY_MODEL_207f40f1be3746af88e5abd4c2dc9900",
            "value": " 2/2 [00:22&lt;00:00,  9.29s/it]"
          }
        },
        "dbc3c46c680d441b901f2d9466b66bed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d4fc5f45d694f9fac1baf6c02d507c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63890ba768de43a69fd111a2b805c88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cedc5741214f4541b292c01eb4045ec9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5688f57e2974e38b4d47431c7ff469c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd9beb8203ab48649a8b4f97a68f93de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "207f40f1be3746af88e5abd4c2dc9900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "436be3a820ee41d7917db35e531aeec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70ef2bef7cb84061995aa2b375097498",
              "IPY_MODEL_dfebb7d8deb247a687a8368601a56e75",
              "IPY_MODEL_ec4aa8c5a24841c3baa423fa0e565b0e"
            ],
            "layout": "IPY_MODEL_95f5669ca7644575b0dff80a333f6b2d"
          }
        },
        "70ef2bef7cb84061995aa2b375097498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fb727e27d5449bcbd5576edd186fbb8",
            "placeholder": "​",
            "style": "IPY_MODEL_91ddc9b9a2da41bc83d7cbf4cca2af61",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "dfebb7d8deb247a687a8368601a56e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6691dc6e4070444cbac061d2412b0226",
            "max": 34173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba9aa456c8ae4570a2e3b94d6192786d",
            "value": 34173
          }
        },
        "ec4aa8c5a24841c3baa423fa0e565b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0b67f68494c4cb69800e7ef71dbbd82",
            "placeholder": "​",
            "style": "IPY_MODEL_30d20d9ab9944eeba96b79ed3661bb33",
            "value": " 34.2k/34.2k [00:00&lt;00:00, 3.50MB/s]"
          }
        },
        "95f5669ca7644575b0dff80a333f6b2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb727e27d5449bcbd5576edd186fbb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91ddc9b9a2da41bc83d7cbf4cca2af61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6691dc6e4070444cbac061d2412b0226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba9aa456c8ae4570a2e3b94d6192786d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0b67f68494c4cb69800e7ef71dbbd82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30d20d9ab9944eeba96b79ed3661bb33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4d50fb712ad4a108758a337be04c800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58d0a15690f14939a9aa200f210c071a",
              "IPY_MODEL_e76f32d175eb48fa9343a27e0ba563da",
              "IPY_MODEL_f14cc66d9bfa4e5f8257104c033c3e95"
            ],
            "layout": "IPY_MODEL_77e0489e441148ae9bad26dfdbd6f023"
          }
        },
        "58d0a15690f14939a9aa200f210c071a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a279255add454a2d8c96691ef795780a",
            "placeholder": "​",
            "style": "IPY_MODEL_a5220ec979364099a7307e58fb525a60",
            "value": "tokenizer.model: 100%"
          }
        },
        "e76f32d175eb48fa9343a27e0ba563da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5050d4b91ed44f1a8634b6f399402b31",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d9e7ba141e84544be2c513c8aebba3c",
            "value": 4241003
          }
        },
        "f14cc66d9bfa4e5f8257104c033c3e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed7cf64cbe6c476a82c4625f2665cb65",
            "placeholder": "​",
            "style": "IPY_MODEL_61882815d5bb4babb6ed15430eb7af3e",
            "value": " 4.24M/4.24M [00:00&lt;00:00, 54.0MB/s]"
          }
        },
        "77e0489e441148ae9bad26dfdbd6f023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a279255add454a2d8c96691ef795780a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5220ec979364099a7307e58fb525a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5050d4b91ed44f1a8634b6f399402b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d9e7ba141e84544be2c513c8aebba3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed7cf64cbe6c476a82c4625f2665cb65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61882815d5bb4babb6ed15430eb7af3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8fc69bd699044a4be2a97d3c924c5e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb24380be0c24b108ca3d2c7d2d49c17",
              "IPY_MODEL_57658a00135e4f889c37d785e329f17f",
              "IPY_MODEL_01d173e1f9c8476d94c50f7ac11c805a"
            ],
            "layout": "IPY_MODEL_c1978eae54e742f9b9cb83c82177dbe3"
          }
        },
        "bb24380be0c24b108ca3d2c7d2d49c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50ea366caae24e68b54afc808e4208e6",
            "placeholder": "​",
            "style": "IPY_MODEL_cfc0f9eda7a149768f7f48ed9a172b3b",
            "value": "tokenizer.json: 100%"
          }
        },
        "57658a00135e4f889c37d785e329f17f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f430f4a437344632a78f578fd72e9afd",
            "max": 17518497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_642d02a84c7a4c19807fe7c49c8c114c",
            "value": 17518497
          }
        },
        "01d173e1f9c8476d94c50f7ac11c805a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a3dd531bf6b40cdb0fcf2559d496f42",
            "placeholder": "​",
            "style": "IPY_MODEL_ca23ae35eaaf4d8f8102592c08c460c0",
            "value": " 17.5M/17.5M [00:00&lt;00:00, 173MB/s]"
          }
        },
        "c1978eae54e742f9b9cb83c82177dbe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ea366caae24e68b54afc808e4208e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfc0f9eda7a149768f7f48ed9a172b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f430f4a437344632a78f578fd72e9afd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "642d02a84c7a4c19807fe7c49c8c114c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a3dd531bf6b40cdb0fcf2559d496f42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca23ae35eaaf4d8f8102592c08c460c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e0d11c75fcc4cffaac044a4491c4ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27f2c89ed84e4c869159e33899d3b1c3",
              "IPY_MODEL_b92820eadfd84260bb0ce7f58838bd5a",
              "IPY_MODEL_d8f3e876552d434db658d046dbd897e0"
            ],
            "layout": "IPY_MODEL_1e2211e0f4bc4cd589a9d30abbf7150d"
          }
        },
        "27f2c89ed84e4c869159e33899d3b1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_887449d18f6746a28f2bbc1cc9c95dd3",
            "placeholder": "​",
            "style": "IPY_MODEL_728cef1185fe4d098582995d38ba5300",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "b92820eadfd84260bb0ce7f58838bd5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff134eb536264e3cac7f44e328bda13f",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc0289462a294805b86648872333527d",
            "value": 636
          }
        },
        "d8f3e876552d434db658d046dbd897e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0693816d9ce540a5a661b23e2047166f",
            "placeholder": "​",
            "style": "IPY_MODEL_5e290d8b966744d9a832fa19812c815c",
            "value": " 636/636 [00:00&lt;00:00, 73.9kB/s]"
          }
        },
        "1e2211e0f4bc4cd589a9d30abbf7150d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "887449d18f6746a28f2bbc1cc9c95dd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "728cef1185fe4d098582995d38ba5300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff134eb536264e3cac7f44e328bda13f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc0289462a294805b86648872333527d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0693816d9ce540a5a661b23e2047166f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e290d8b966744d9a832fa19812c815c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4adfb6aa76db4c17bca719257b1293e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc738f2780a444cebeb0a4d9c9a00af8",
              "IPY_MODEL_983c5d8aa4d341e4afbe8229b01d5497",
              "IPY_MODEL_de57e431dd964b30b772f971963f5c25"
            ],
            "layout": "IPY_MODEL_d22b5956d6424487bdc0734e7b883f99"
          }
        },
        "fc738f2780a444cebeb0a4d9c9a00af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_313d1f98c5614ff9ad0c4544fe94d8f4",
            "placeholder": "​",
            "style": "IPY_MODEL_d95220c2d94d45d7aa0de84fbac4e80e",
            "value": "config.json: 100%"
          }
        },
        "983c5d8aa4d341e4afbe8229b01d5497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e999dc9a536142dab72525fc00734ad7",
            "max": 627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99a99b6a2dc6422ebb147b0881d7ee21",
            "value": 627
          }
        },
        "de57e431dd964b30b772f971963f5c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f94bcd239ac455ab5221634569e3f09",
            "placeholder": "​",
            "style": "IPY_MODEL_3cb7be72fb5c491c827f09db3e4ee4a0",
            "value": " 627/627 [00:00&lt;00:00, 47.8kB/s]"
          }
        },
        "d22b5956d6424487bdc0734e7b883f99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "313d1f98c5614ff9ad0c4544fe94d8f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95220c2d94d45d7aa0de84fbac4e80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e999dc9a536142dab72525fc00734ad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a99b6a2dc6422ebb147b0881d7ee21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f94bcd239ac455ab5221634569e3f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cb7be72fb5c491c827f09db3e4ee4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c91ad6f26af5420ca9914fdf839da990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd9bf18bb53d40aaab5e82b4c470fc64",
              "IPY_MODEL_ba2db9fee3f1480e95e426984eaddc13",
              "IPY_MODEL_e76971a26e8b4170a4d9ad0bb2adb7e0"
            ],
            "layout": "IPY_MODEL_20e5a6cfc27447a8b5e0b2172baf244a"
          }
        },
        "fd9bf18bb53d40aaab5e82b4c470fc64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b13661d0d844e29352fa90e97be065",
            "placeholder": "​",
            "style": "IPY_MODEL_3469b089b64c4b779b06ba0f78998187",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "ba2db9fee3f1480e95e426984eaddc13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7031a51f7594bff970f9b3747c339f5",
            "max": 13489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5489ce067d34f699225f7d59934af0d",
            "value": 13489
          }
        },
        "e76971a26e8b4170a4d9ad0bb2adb7e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef255162da4f4a708a26af8597a5cec2",
            "placeholder": "​",
            "style": "IPY_MODEL_0aaf086c2cb64af1bd0f7a2d90094b0b",
            "value": " 13.5k/13.5k [00:00&lt;00:00, 1.22MB/s]"
          }
        },
        "20e5a6cfc27447a8b5e0b2172baf244a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29b13661d0d844e29352fa90e97be065": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3469b089b64c4b779b06ba0f78998187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7031a51f7594bff970f9b3747c339f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5489ce067d34f699225f7d59934af0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef255162da4f4a708a26af8597a5cec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aaf086c2cb64af1bd0f7a2d90094b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c79bebe889e64e6f9c15192b38beb59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30b2c494d6484be8a7095620f44e7e43",
              "IPY_MODEL_f5cea509a8474089a3936e8127aeab79",
              "IPY_MODEL_c501f0f14dbb4404ac74f2886fda8406"
            ],
            "layout": "IPY_MODEL_3cec2e396ae9444485ffaa86b1aa2617"
          }
        },
        "30b2c494d6484be8a7095620f44e7e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2b576e39d804f5ebbb67c41780ef835",
            "placeholder": "​",
            "style": "IPY_MODEL_aaa7c482291f42478ac5e04dcc8fd80c",
            "value": "Fetching 2 files: 100%"
          }
        },
        "f5cea509a8474089a3936e8127aeab79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dad2b441f74e42eea8a6db829d0f085d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bef56f67a98a4cdbacadd54dfedaf4a6",
            "value": 2
          }
        },
        "c501f0f14dbb4404ac74f2886fda8406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0c3a75245f547a8bcd385516c79c701",
            "placeholder": "​",
            "style": "IPY_MODEL_0c9077b6a3bf471d9c8360eb30b7d283",
            "value": " 2/2 [00:29&lt;00:00, 29.78s/it]"
          }
        },
        "3cec2e396ae9444485ffaa86b1aa2617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2b576e39d804f5ebbb67c41780ef835": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaa7c482291f42478ac5e04dcc8fd80c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dad2b441f74e42eea8a6db829d0f085d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bef56f67a98a4cdbacadd54dfedaf4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0c3a75245f547a8bcd385516c79c701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c9077b6a3bf471d9c8360eb30b7d283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35abf6a5315e4da18b6847852e45ad5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42bdba9290a34b44b0bc1db642239427",
              "IPY_MODEL_47aef0f1957f4299a9e66a142f413140",
              "IPY_MODEL_c8c57c0155aa466fb532c18d4cb1983e"
            ],
            "layout": "IPY_MODEL_144bcbcb35e24bd6b8bd6b0fbd84d65b"
          }
        },
        "42bdba9290a34b44b0bc1db642239427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_725a0e91e22d4f27b2832cb6b3c4843c",
            "placeholder": "​",
            "style": "IPY_MODEL_527c956557694f2982d9d89c77b96a8e",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "47aef0f1957f4299a9e66a142f413140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9aae6059b1a44e86a5c837f9945069c9",
            "max": 4945242264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8472d26dd60a4628991db0e65ff29384",
            "value": 4945242264
          }
        },
        "c8c57c0155aa466fb532c18d4cb1983e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_439cb11e4ab64dbe9615bd14b40f81df",
            "placeholder": "​",
            "style": "IPY_MODEL_fc2124df06694090b88760c48f1c8ad2",
            "value": " 4.95G/4.95G [00:29&lt;00:00, 260MB/s]"
          }
        },
        "144bcbcb35e24bd6b8bd6b0fbd84d65b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "725a0e91e22d4f27b2832cb6b3c4843c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "527c956557694f2982d9d89c77b96a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9aae6059b1a44e86a5c837f9945069c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8472d26dd60a4628991db0e65ff29384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "439cb11e4ab64dbe9615bd14b40f81df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc2124df06694090b88760c48f1c8ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f841a353b9614ed0b05500a044aec6d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90e797c6937c443eaf27083feb65f498",
              "IPY_MODEL_b22daccf74444ac4841aac59fbb32398",
              "IPY_MODEL_e0bd0566e05345c1b770e71cffa71443"
            ],
            "layout": "IPY_MODEL_8ed15193e4364c659958c0f525d74f39"
          }
        },
        "90e797c6937c443eaf27083feb65f498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e0c25469d914073a32a8fd5d2e058d6",
            "placeholder": "​",
            "style": "IPY_MODEL_dbad72520ba34ec5be6cf280d9907313",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "b22daccf74444ac4841aac59fbb32398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09aad63dba4b45fb86f18b09df34c272",
            "max": 67121608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f3e615b968d4789831b5a1f9b90b2f4",
            "value": 67121608
          }
        },
        "e0bd0566e05345c1b770e71cffa71443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_190fc74176e14cf180538f95dce6db3d",
            "placeholder": "​",
            "style": "IPY_MODEL_4050330efc5c4095885cd075736ec438",
            "value": " 67.1M/67.1M [00:00&lt;00:00, 193MB/s]"
          }
        },
        "8ed15193e4364c659958c0f525d74f39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e0c25469d914073a32a8fd5d2e058d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbad72520ba34ec5be6cf280d9907313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09aad63dba4b45fb86f18b09df34c272": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f3e615b968d4789831b5a1f9b90b2f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "190fc74176e14cf180538f95dce6db3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4050330efc5c4095885cd075736ec438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87b9ee3e02784b6cbdddf63ac0466eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_285bae89558b470c94e4f4048e4f322a",
              "IPY_MODEL_06410c692523465caba78cc307528a28",
              "IPY_MODEL_70e7d9983d664ea0ada6b024e7877b03"
            ],
            "layout": "IPY_MODEL_68742bbd120c43ad8b1f8384ff2f4c3a"
          }
        },
        "285bae89558b470c94e4f4048e4f322a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00db6340e7274dfeaeff34bbc9e6d835",
            "placeholder": "​",
            "style": "IPY_MODEL_d2cfa20574ea4939a9feda1995a8cb2a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "06410c692523465caba78cc307528a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6aad2df267e544ba90e24de1e80acb63",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31525f2a87c544dc8ed101aeff5cde36",
            "value": 2
          }
        },
        "70e7d9983d664ea0ada6b024e7877b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dc39fa015c243fe96a30ef6a42aa918",
            "placeholder": "​",
            "style": "IPY_MODEL_53b5270bc85040b18fe22fa6aa418aca",
            "value": " 2/2 [00:28&lt;00:00, 11.97s/it]"
          }
        },
        "68742bbd120c43ad8b1f8384ff2f4c3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00db6340e7274dfeaeff34bbc9e6d835": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2cfa20574ea4939a9feda1995a8cb2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6aad2df267e544ba90e24de1e80acb63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31525f2a87c544dc8ed101aeff5cde36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5dc39fa015c243fe96a30ef6a42aa918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53b5270bc85040b18fe22fa6aa418aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddc87dbbcfca45fe8fe133f2c1540b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e09de585c75144cea4b79355051f9103",
              "IPY_MODEL_ed1b7fdc685b4a95873930dc7527b3b9",
              "IPY_MODEL_7d24b8a406214418af03a151788f273b"
            ],
            "layout": "IPY_MODEL_a41725045ffa424c9597d89d5844aad8"
          }
        },
        "e09de585c75144cea4b79355051f9103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7eb6ae9803e4ec18c61a90398793b8e",
            "placeholder": "​",
            "style": "IPY_MODEL_4781eccbf14a4f3e8ab4cedd42e9d2d5",
            "value": "generation_config.json: 100%"
          }
        },
        "ed1b7fdc685b4a95873930dc7527b3b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b25bd2ae3dea4cc892af7a09579901ec",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ed5bf7f00b1486e9009e2021fe87fe6",
            "value": 137
          }
        },
        "7d24b8a406214418af03a151788f273b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cf9249ee5464aaf88e2faa31589b969",
            "placeholder": "​",
            "style": "IPY_MODEL_abff27c5aa8a4ccab45757b1ff8eae3f",
            "value": " 137/137 [00:00&lt;00:00, 11.2kB/s]"
          }
        },
        "a41725045ffa424c9597d89d5844aad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7eb6ae9803e4ec18c61a90398793b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4781eccbf14a4f3e8ab4cedd42e9d2d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b25bd2ae3dea4cc892af7a09579901ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ed5bf7f00b1486e9009e2021fe87fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2cf9249ee5464aaf88e2faa31589b969": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abff27c5aa8a4ccab45757b1ff8eae3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}