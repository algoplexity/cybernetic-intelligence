{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "initial-notebook"
   },
   "source": [
    "# Modern Slavery Compliance Analysis Notebook\n",
    "## Implementation of Phases 1A and 1B\n",
    "Last updated: 08 Oct 2025, 09:50 AM AEDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "setup-output"
   },
   "outputs": [],
   "source": [
    "# Install required packages (if not present)\n",
    "!pip install polars loguru -q\n",
    "\n",
    "import polars as pl\n",
    "import os\n",
    "import json\n",
    "from loguru import logger\n",
    "from google.colab import drive\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    DRIVE_PATH = '/content/drive/MyDrive/ModernSlaveryProject/'\n",
    "    logger.info(\"Google Drive mounted successfully.\")\n",
    "except ImportError:\n",
    "    DRIVE_PATH = './'\n",
    "    logger.warning(\"Not in Google Colab. Using local directory.\")\n",
    "\n",
    "# File paths\n",
    "abr_bulk_path = os.path.join(DRIVE_PATH, 'abn_bulk_data.jsonl')\n",
    "asic_names_path = os.path.join(DRIVE_PATH, 'BUSINESS_NAMES_202510.csv')\n",
    "ato_tax_paths = sorted(glob.glob(os.path.join(DRIVE_PATH, 'CorporateTaxTransparency/*-corporate-report-of-entity-tax-information.xlsx')))\n",
    "asic_company_path = os.path.join(DRIVE_PATH, 'COMPANY_202509.csv')\n",
    "acnc_charity_path = os.path.join(DRIVE_PATH, 'acnc-registered-charities.csv')\n",
    "\n",
    "abr_intermediate_path = os.path.join(DRIVE_PATH, 'intermediate_abr_pairs.parquet')\n",
    "asic_intermediate_path = os.path.join(DRIVE_PATH, 'intermediate_asic_pairs.parquet')\n",
    "identity_universe_path = os.path.join(DRIVE_PATH, 'abn_name_lookup.parquet')  # Changed to Parquet\n",
    "obligation_universe_path = os.path.join(DRIVE_PATH, 'obligated_entities.parquet')  # Changed to Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phase-1a"
   },
   "outputs": [],
   "source": [
    "# Phase 1A: Build the Universe of Identity\n",
    "\n",
    "def validate_abn(abn_str):\n",
    "    if not (isinstance(abn_str, str) and len(abn_str) == 11 and abn_str.isdigit()):\n",
    "        return False\n",
    "    weights = [10, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
    "    digits = [int(d) for d in abn_str]\n",
    "    total = sum(w * d for w, d in zip(weights, digits))\n",
    "    return total % 89 == 0\n",
    "\n",
    "def build_abr_intermediate(source_path, output_path, force_rerun=False):\n",
    "    if os.path.exists(output_path) and not force_rerun:\n",
    "        logger.info(f\"Intermediate file '{os.path.basename(output_path)}' exists. Skipping.\")\n",
    "        return\n",
    "    logger.info(f\"Ingesting from '{os.path.basename(source_path)}'...\")\n",
    "    records = []\n",
    "    with open(source_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if (i + 1) % 2_000_000 == 0:\n",
    "                logger.info(f\"Processed {i + 1:,} lines\")\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                abn = record.get('ABN')\n",
    "                if not abn or not validate_abn(abn):\n",
    "                    continue\n",
    "                if record.get('MainEntity') and record['MainEntity'].get('NonIndividualName'):\n",
    "                    name = record['MainEntity']['NonIndividualName']['NonIndividualNameText']\n",
    "                    records.append({'ABN': abn, 'Name': name, 'CleanName': re.sub(r'[^A-Za-z0-9\\s]', '', name).strip().upper()})\n",
    "                if record.get('BusinessName'):\n",
    "                    for bn in record['BusinessName']:\n",
    "                        if bn.get('BusinessNameText'):\n",
    "                            name = bn['BusinessNameText']\n",
    "                            records.append({'ABN': abn, 'Name': name, 'CleanName': re.sub(r'[^A-Za-z0-9\\s]', '', name).strip().upper()})\n",
    "            except (json.JSONDecodeError, KeyError) as e:\n",
    "                logger.warning(f\"Line {i + 1}: JSON error - {e}\")\n",
    "    df = pl.DataFrame(records)\n",
    "    df.write_parquet(output_path)\n",
    "    logger.info(f\"Extracted {len(df):,} pairs to '{os.path.basename(output_path)}'\")\n",
    "\n",
    "def build_asic_intermediate(source_path, output_path, force_rerun=False):\n",
    "    if os.path.exists(output_path) and not force_rerun:\n",
    "        logger.info(f\"Intermediate file '{os.path.basename(output_path)}' exists. Skipping.\")\n",
    "        return\n",
    "    logger.info(f\"Ingesting from '{os.path.basename(source_path)}'...\")\n",
    "    df = pl.read_csv(source_path, separator='\\t', columns=['BN_NAME', 'BN_ABN'], dtypes={'BN_NAME': pl.Utf8, 'BN_ABN': pl.Utf8})\n",
    "    df = df.filter(pl.col('BN_ABN').is_not_null() & pl.col('BN_NAME').is_not_null()).rename({'BN_NAME': 'Name', 'BN_ABN': 'ABN'})\n",
    "    df = df.with_columns([\n",
    "        pl.col('ABN').apply(validate_abn).alias('is_valid_abn'),\n",
    "        pl.col('Name').str.replace_all(r'[^A-Za-z0-9\\s]', '').str.strip().str.to_upper().alias('CleanName')\n",
    "    ]).filter(pl.col('is_valid_abn')).drop('is_valid_abn')\n",
    "    df.write_parquet(output_path)\n",
    "    logger.info(f\"Extracted {len(df):,} pairs to '{os.path.basename(output_path)}'\")\n",
    "\n",
    "def combine_identity_universe(abr_path, asic_path, output_path):\n",
    "    logger.info(\"Combining Identity Universe...\")\n",
    "    df_abr = pl.read_parquet(abr_path)\n",
    "    df_asic = pl.read_parquet(asic_path)\n",
    "    df = pl.concat([df_abr, df_asic], how='vertical')\n",
    "    df = df.unique(subset=['ABN', 'Name', 'CleanName']).drop_nulls()\n",
    "    df.write_parquet(output_path)\n",
    "    logger.info(f\"Final Identity Universe saved with {len(df):,} unique pairs to '{os.path.basename(output_path)}'\")\n",
    "\n",
    "# Execute Phase 1A\n",
    "build_abr_intermediate(abr_bulk_path, abr_intermediate_path)\n",
    "build_asic_intermediate(asic_names_path, asic_intermediate_path)\n",
    "combine_identity_universe(abr_intermediate_path, asic_intermediate_path, identity_universe_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phase-1b"
   },
   "outputs": [],
   "source": [
    "# Phase 1B: Build the Universe of Obligation\n",
    "\n",
    "def get_threshold(row):\n",
    "    year_start = int(row['Year'].split('-')[0])\n",
    "    if year_start >= 2022:\n",
    "        return 100_000_000\n",
    "    else:\n",
    "        if row['ASIC_Type'] == 'APTY':\n",
    "            return 200_000_000\n",
    "        else:\n",
    "            return 100_000_000\n",
    "\n",
    "def build_obligation_universe(ato_paths, asic_path, acnc_path, identity_path, output_path):\n",
    "    logger.info(\"Building Universe of Obligation...\")\n",
    "\n",
    "    # Process ATO Corporate Tax Transparency Reports\n",
    "    all_corporate_df = pl.concat([\n",
    "        pl.read_excel(file, sheet_name='Income tax details', columns=['ABN', 'Year', 'Total income $'], dtypes={'ABN': pl.Utf8, 'Year': pl.Utf8, 'Total income $': pl.Float64})\n",
    "        .rename({'Total income $': 'TotalIncome'})\n",
    "        .filter(pl.col('TotalIncome').is_not_null() & (pl.col('TotalIncome') > 0))\n",
    "        for file in ato_paths\n",
    "    ])\n",
    "    all_corporate_df = all_corporate_df.with_columns([\n",
    "        pl.col('ABN').str.replace(r'\\.0$', '').str.zfill(11),\n",
    "        pl.col('TotalIncome').cast(pl.Int64)\n",
    "    ])\n",
    "\n",
    "    # Join with ASIC for company type\n",
    "    asic_df = pl.read_csv(asic_company_path, separator='\\t', columns=['ABN', 'Type'], dtypes={'ABN': pl.Utf8, 'Type': pl.Utf8})\n",
    "    asic_df = asic_df.rename({'Type': 'ASIC_Type'}).filter(pl.col('ABN').is_not_null())\n",
    "    corporate_df = all_corporate_df.join(asic_df, on='ABN', how='left')\n",
    "\n",
    "    # Apply thresholds\n",
    "    corporate_df = corporate_df.with_columns([\n",
    "        pl.col('TotalIncome').apply(get_threshold).alias('Threshold'),\n",
    "        (pl.col('TotalIncome') >= pl.col('Threshold')).alias('IsObligated')\n",
    "    ]).filter(pl.col('IsObligated'))\n",
    "\n",
    "    # Process ACNC Charities\n",
    "    acnc_df = pl.read_csv(acnc_charity_path, columns=['ABN', 'CharitySize'], dtypes={'ABN': pl.Utf8, 'CharitySize': pl.Utf8})\n",
    "    charity_df = acnc_df.filter(pl.col('CharitySize') == 'Large').select(['ABN'])\n",
    "\n",
    "    # Combine obligated entities\n",
    "    obligated_df = pl.concat([corporate_df.select(['ABN', 'Year']), charity_df], how='vertical').unique(subset=['ABN'])\n",
    "    obligated_df = obligated_df.join(pl.read_parquet(identity_path).select(['ABN', 'Name']), on='ABN', how='left')\n",
    "\n",
    "    # Pivot for year-specific flags\n",
    "    pivot_df = obligated_df.group_by('ABN').agg(\n",
    "        [pl.lit(1).alias(f'Obligated_{year}') for year in sorted(obligated_df['Year'].unique())]\n",
    "    ).fill_null(0)\n",
    "\n",
    "    # Save\n",
    "    pivot_df.write_parquet(output_path)\n",
    "    logger.info(f\"Universe of Obligation saved with {len(pivot_df):,} entities to '{os.path.basename(output_path)}'\")\n",
    "\n",
    "# Execute Phase 1B\n",
    "build_obligation_universe(ato_tax_paths, asic_company_path, acnc_charity_path, identity_universe_path, obligation_universe_path)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}