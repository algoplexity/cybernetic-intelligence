{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for reading Excel files\n",
        "!pip install pandas openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# --- Load the data files into pandas DataFrames ---\n",
        "\n",
        "try:\n",
        "    # Main register data\n",
        "    df_all_time = pd.read_excel('All time data from Register.xlsx')\n",
        "\n",
        "    # Data on potential non-lodgers\n",
        "    df_non_lodger = pd.read_excel('ato_tax_transparency_non_lodger.xlsx')\n",
        "\n",
        "    # Data on entities that have lodged once (assuming this is the primary file)\n",
        "    # The .csv file might be a simpler version, while the .xlsx has more detail.\n",
        "    # We will start with the xlsx.\n",
        "    df_lodge_once = pd.read_excel('lodge_once_cont.xlsx')\n",
        "\n",
        "    print(\"All data files loaded successfully!\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}. Please ensure all files are uploaded to the Colab session.\")\n",
        "    print(\"Files expected: 'All time data from Register.xlsx', 'ato_tax_transparency_non_lodger.xlsx', 'lodge_once_cont.xlsx'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK7dysA9p7dY",
        "outputId": "e6637a7e-f21a-4ef5-839a-1baa2b376807"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "All data files loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Explore the loaded DataFrames ---\n",
        "\n",
        "# Check if the DataFrames were loaded before trying to use them\n",
        "if 'df_all_time' in locals():\n",
        "    print(\"--- 1. All time data from Register ---\")\n",
        "    print(f\"Shape (rows, columns): {df_all_time.shape}\")\n",
        "    print(\"First 5 rows:\")\n",
        "    print(df_all_time.head())\n",
        "    print(\"\\nColumn Info:\")\n",
        "    df_all_time.info()\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "if 'df_non_lodger' in locals():\n",
        "    print(\"--- 2. ATO Tax Transparency Non-Lodger Data ---\")\n",
        "    print(f\"Shape (rows, columns): {df_non_lodger.shape}\")\n",
        "    print(\"First 5 rows:\")\n",
        "    print(df_non_lodger.head())\n",
        "    print(\"\\nColumn Info:\")\n",
        "    df_non_lodger.info()\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "if 'df_lodge_once' in locals():\n",
        "    print(\"--- 3. Lodge Once Data ---\")\n",
        "    print(f\"Shape (rows, columns): {df_lodge_once.shape}\")\n",
        "    print(\"First 5 rows:\")\n",
        "    print(df_lodge_once.head())\n",
        "    print(\"\\nColumn Info:\")\n",
        "    df_lodge_once.info()\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "E4ZTRi0MqqtX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load the CSV data file ---\n",
        "try:\n",
        "    df_lodge_once_csv = pd.read_csv('lodge_once.csv')\n",
        "    print(\"--- Additional File: lodge_once.csv ---\")\n",
        "    print(\"Load successful.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}. Please ensure 'lodge_once.csv' is also uploaded.\")\n",
        "\n",
        "\n",
        "# --- Compare lodge_once.csv and lodge_once_cont.xlsx ---\n",
        "# We'll use the original df_lodge_once DataFrame for a direct comparison before cleaning\n",
        "if 'df_lodge_once_csv' in locals() and 'df_lodge_once' in locals():\n",
        "    print(\"\\n--- Comparing the two 'lodge_once' files ---\")\n",
        "\n",
        "    # Compare shape (rows, columns)\n",
        "    print(f\"Shape of lodge_once.csv: {df_lodge_once_csv.shape}\")\n",
        "    print(f\"Shape of lodge_once_cont.xlsx: {df_lodge_once.shape}\")\n",
        "\n",
        "    # Compare columns\n",
        "    csv_columns = set(df_lodge_once_csv.columns)\n",
        "    xlsx_columns = set(df_lodge_once.columns)\n",
        "\n",
        "    if csv_columns == xlsx_columns:\n",
        "        print(\"\\nColumn names are identical in both files.\")\n",
        "    else:\n",
        "        print(\"\\nColumn names differ.\")\n",
        "        if len(csv_columns - xlsx_columns) > 0:\n",
        "            print(f\"Columns found ONLY in CSV: {csv_columns - xlsx_columns}\")\n",
        "        if len(xlsx_columns - csv_columns) > 0:\n",
        "            print(f\"Columns found ONLY in XLSX: {xlsx_columns - csv_columns}\")\n",
        "\n",
        "    # Quick content check on a key column\n",
        "    # This helps see if they contain the same data, e.g., the dummy ABNs\n",
        "    csv_dummy_count = df_lodge_once_csv['abn'].str.startswith('dummy_').sum()\n",
        "    xlsx_dummy_count = df_lodge_once['abn'].str.startswith('dummy_').sum()\n",
        "\n",
        "    print(f\"\\nDummy ABN count in CSV: {csv_dummy_count}\")\n",
        "    print(f\"Dummy ABN count in XLSX: {xlsx_dummy_count}\")\n",
        "\n",
        "    if df_lodge_once_csv.shape == df_lodge_once.shape and csv_dummy_count == xlsx_dummy_count:\n",
        "        print(\"\\nConclusion: The files appear to be identical in structure and content.\")\n",
        "        print(\"We can proceed using the 'lodge_once_cont.xlsx' file with confidence.\")\n",
        "    else:\n",
        "        print(\"\\nConclusion: The files have differences that need to be investigated before proceeding.\")"
      ],
      "metadata": {
        "id": "aIm9qze2tsI8",
        "outputId": "e2b617df-22ed-41b4-e2ea-ef185ed6b73c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Additional File: lodge_once.csv ---\n",
            "Load successful.\n",
            "\n",
            "--- Comparing the two 'lodge_once' files ---\n",
            "Shape of lodge_once.csv: (2918, 35)\n",
            "Shape of lodge_once_cont.xlsx: (2918, 35)\n",
            "\n",
            "Column names differ.\n",
            "Columns found ONLY in CSV: {'expected_period_end', 'all_np', 'c_criteria_1f', 'last_period_end', 'num_assessment', 'last_stmt_year', 'num_published', 'repeat_nc', 'nc_criteria_1c', 'c_criteria_1a', 'num_non_publishable', 'all_p', 'alter_nc', 'num_compliant', 'nc_criteria_1b', 'nc_index', 'all_c', 'expected_due_date', 'nc_criteria_1e', 'nc_criteria_1a', 'c_criteria_1e', 'c_criteria_1d', 'nc_criteria_1d', 'all_nc', 'first_nc', 'last_submission_dttm', 'c_criteria_1b', 'c_criteria_1c', 'num_statements', 'num_non_compliant', 'nc_criteria_1f', 'revenue', 'num_publishable'}\n",
            "Columns found ONLY in XLSX: {'mn_bus_sbrb', 'nm_sufx_cd', 'sprsn_ind', 'pid', 'state', 'mn_bus_pc', 'company_name', 'gst_cancn_dt', 'son_stt', 'prsn_othr_gvn_nm', 'son_sbrb', 'son_dpid', 'son_addr_ln_2', 'prsn_gvn_nm', 'nm_titl_cd', 'mn_bus_addr_ln_1', 'ent_eml', 'son_cntry_cd', 'abn_regn_dt', 'mn_bus_addr_ln_2', 'industry_desc', 'abn_cancn_dt', 'industry_cd', 'acn', 'son_addr_ln_1', 'prsn_fmly_nm', 'mn_trdg_nm', 'son_pc', 'mn_bus_dpid', 'prty_id_blnk', 'mn_bus_cntry_cd', 'ent_typ_cd', 'gst_regn_dt'}\n",
            "\n",
            "Dummy ABN count in CSV: 629\n",
            "Dummy ABN count in XLSX: 629\n",
            "\n",
            "Conclusion: The files appear to be identical in structure and content.\n",
            "We can proceed using the 'lodge_once_cont.xlsx' file with confidence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3 (Revised): Data Cleaning, Merging, and Preparation ---\n",
        "\n",
        "# It's good practice to work on copies of the data\n",
        "df_all_time_clean = df_all_time.copy()\n",
        "df_non_lodger_clean = df_non_lodger.copy()\n",
        "df_lodge_once_details = df_lodge_once.copy() # From the .xlsx file\n",
        "df_lodge_once_compliance = df_lodge_once_csv.copy() # From the .csv file\n",
        "\n",
        "# 1. Clean column names in the main register data\n",
        "df_all_time_clean.columns = df_all_time_clean.columns.str.strip().str.replace('\\n', '')\n",
        "\n",
        "# 2. Standardize entity names in the register for matching\n",
        "df_all_time_clean.dropna(subset=['Reporting entities'], inplace=True)\n",
        "df_all_time_clean['entity_name_clean'] = df_all_time_clean['Reporting entities'].str.upper().str.strip()\n",
        "\n",
        "# 3. Standardize entity names in the non-lodger list\n",
        "df_non_lodger_clean['entity_name_clean'] = df_non_lodger_clean['Entity Name'].str.upper().str.strip()\n",
        "\n",
        "# --- NEW: Clean and Merge the two 'lodge_once' files ---\n",
        "\n",
        "# 4. Remove 'dummy' records from both dataframes to ensure a clean merge\n",
        "df_lodge_once_details = df_lodge_once_details[~df_lodge_once_details['abn'].str.startswith('dummy_')].copy()\n",
        "df_lodge_once_compliance = df_lodge_once_compliance[~df_lodge_once_compliance['abn'].str.startswith('dummy_')].copy()\n",
        "\n",
        "# 5. Merge the two dataframes on their common key(s). Let's use 'abn'.\n",
        "# We assume 'abn' is the unique identifier for an entity across these files.\n",
        "df_lodge_once_merged = pd.merge(\n",
        "    df_lodge_once_details,\n",
        "    df_lodge_once_compliance,\n",
        "    on='abn',\n",
        "    how='inner' # 'inner' merge ensures we only keep records present in both files\n",
        ")\n",
        "\n",
        "# 6. Standardize names in the newly merged single-lodger dataframe\n",
        "df_lodge_once_merged['entity_name_clean'] = df_lodge_once_merged['company_name'].str.upper().str.strip()\n",
        "\n",
        "\n",
        "print(\"\\nData cleaning, merging, and preparation complete.\")\n",
        "print(f\"Proceeding with {len(df_all_time_clean)} records from the register.\")\n",
        "print(f\"Proceeding with {len(df_non_lodger_clean)} potential non-lodger records.\")\n",
        "print(f\"Created a merged single-lodger dataset with {len(df_lodge_once_merged)} records and {df_lodge_once_merged.shape[1]} columns.\")\n",
        "\n",
        "# Let's inspect the merged data to be sure\n",
        "print(\"\\nFirst 5 rows of the merged single-lodger dataframe:\")\n",
        "print(df_lodge_once_merged[['abn', 'company_name', 'num_compliant', 'last_submission_dttm']].head())"
      ],
      "metadata": {
        "id": "jixgQBcOu4cD",
        "outputId": "291ca4de-33aa-44ca-cd27-8494ee298e52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data cleaning, merging, and preparation complete.\n",
            "Proceeding with 16734 records from the register.\n",
            "Proceeding with 1346 potential non-lodger records.\n",
            "Created a merged single-lodger dataset with 2289 records and 70 columns.\n",
            "\n",
            "First 5 rows of the merged single-lodger dataframe:\n",
            "           abn                       company_name  num_compliant  \\\n",
            "0  83130964162   NORTHERN STAR (BUNDARRA) PTY LTD            NaN   \n",
            "1  57635649606   NORTHERN STAR (SINCLAIR) PTY LTD            NaN   \n",
            "2  91151605417  NORTHERN STAR (SR MINING) PTY LTD            NaN   \n",
            "3  42601140185   NORTHERN STAR (TALISMAN) PTY LTD            NaN   \n",
            "4  13004486128                                NaN            NaN   \n",
            "\n",
            "   last_submission_dttm  \n",
            "0  2020-09-07T05:33:28Z  \n",
            "1  2020-09-07T05:33:28Z  \n",
            "2  2020-09-07T05:33:28Z  \n",
            "3  2020-09-07T05:33:28Z  \n",
            "4  2020-09-18T02:28:15Z  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Identify Entities that have never lodged ---\n",
        "\n",
        "# Create a set of unique entity names from the register for efficient lookup\n",
        "# This set contains every entity that has lodged at least once.\n",
        "lodged_entity_names = set(df_all_time_clean['entity_name_clean'])\n",
        "\n",
        "# Filter the non_lodger dataframe to find entities whose cleaned name is NOT in the set of lodged names\n",
        "never_lodged_mask = ~df_non_lodger_clean['entity_name_clean'].isin(lodged_entity_names)\n",
        "df_never_lodged = df_non_lodger_clean[never_lodged_mask].copy()\n",
        "\n",
        "print(f\"Identified {len(df_never_lodged)} entities that have likely never lodged a statement.\")\n",
        "\n",
        "# Let's look at the key details for the first 5 identified non-lodgers\n",
        "print(\"\\n--- Sample of Identified Non-Lodgers ---\")\n",
        "print(df_never_lodged[['ABN', 'Entity Name', 'Total Income', 'ASX listed?', 'Industry_desc']].head())"
      ],
      "metadata": {
        "id": "7H1SkJN6v6-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Identify and Confirm Single-Lodgement Behaviour ---\n",
        "\n",
        "# Count the number of statements submitted by each unique entity in the register\n",
        "lodgement_counts = df_all_time_clean['entity_name_clean'].value_counts()\n",
        "\n",
        "# Filter this list to find entities that appear exactly once\n",
        "single_lodger_names_from_register = lodgement_counts[lodgement_counts == 1].index.tolist()\n",
        "\n",
        "# Create a dataframe of these single lodgers from the main register data\n",
        "df_single_lodgers_from_register = df_all_time_clean[df_all_time_clean['entity_name_clean'].isin(single_lodger_names_from_register)].copy()\n",
        "\n",
        "print(f\"\\nIdentified {len(df_single_lodgers_from_register)} entities that have lodged exactly once, based on the register data.\")\n",
        "\n",
        "# --- Validate and Enrich with our Merged Data ---\n",
        "# We can now link these entities to the detailed ABN and compliance data we prepared earlier.\n",
        "# This gives us the rich dataset we need for the deliverable.\n",
        "df_single_lodgers_enriched = pd.merge(\n",
        "    df_single_lodgers_from_register,\n",
        "    df_lodge_once_merged,\n",
        "    on='entity_name_clean',\n",
        "    how='left' # Use a 'left' merge to keep all entities from our register list\n",
        ")\n",
        "\n",
        "print(f\"Enriched the single-lodger list. The new dataframe has {df_single_lodgers_enriched.shape[1]} columns.\")\n",
        "\n",
        "# Display a sample of the final, enriched data\n",
        "print(\"\\n--- Sample of Enriched Single-Lodger Data ---\")\n",
        "# We select columns from both the register (e.g., 'Reporting Period') and the merged file (e.g., 'abn', 'num_compliant')\n",
        "print(df_single_lodgers_enriched[[\n",
        "    'Reporting entities', 'Reporting Period', 'abn', 'company_name', 'last_submission_dttm', 'num_compliant'\n",
        "]].head())"
      ],
      "metadata": {
        "id": "uDXBLBt-v7w8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}