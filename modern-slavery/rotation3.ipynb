{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# START - 26th Sept"
      ],
      "metadata": {
        "id": "b2f7TN_ayEHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings that may arise from older Excel file formats\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "print(\"--- Starting Automated Pipeline for Month 1 ---\")\n",
        "\n",
        "# --- 1. Configuration: Define File Paths and Sheet Names ---\n",
        "# This makes the script easy to update if file names change.\n",
        "file_paths = {\n",
        "    \"register\": \"All time data from Register.xlsx\",\n",
        "    \"non_lodger\": \"ato_tax_transparency_non_lodger.xlsx\",\n",
        "    \"lodge_once_comp\": \"lodge_once.csv\",\n",
        "    \"lodge_once_details\": \"lodge_once_cont.xlsx\"\n",
        "}\n",
        "sheet_names = {\n",
        "    \"register\": \"Statements\",\n",
        "    \"non_lodger\": \"Non-Lodger\",\n",
        "    \"lodge_once_details\": \"lodge_once\"\n",
        "}\n",
        "\n",
        "# --- 2. Data Loading ---\n",
        "try:\n",
        "    df_register = pd.read_excel(file_paths[\"register\"], sheet_name=sheet_names[\"register\"])\n",
        "    df_non_lodger = pd.read_excel(file_paths[\"non_lodger\"], sheet_name=sheet_names[\"non_lodger\"])\n",
        "    df_lodge_once_comp = pd.read_csv(file_paths[\"lodge_once_comp\"])\n",
        "    df_lodge_once_details = pd.read_excel(file_paths[\"lodge_once_details\"], sheet_name=sheet_names[\"lodge_once_details\"])\n",
        "    print(\"Step 1/5: All source files loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: A file could not be loaded. Please check paths and names. Details: {e}\")\n",
        "    # Stop execution if files can't be loaded\n",
        "    raise\n",
        "\n",
        "# --- 3. Data Preparation and Merging (Incorporating all fixes) ---\n",
        "# Merge the two 'lodge_once' files with robust ABN standardization\n",
        "df_lodge_once_details['abn'] = df_lodge_once_details['abn'].astype(str)\n",
        "df_lodge_once_comp['abn'] = df_lodge_once_comp['abn'].astype(str)\n",
        "df_lodge_once_merged = pd.merge(df_lodge_once_details, df_lodge_once_comp, on='abn', how='inner')\n",
        "df_lodge_once_merged = df_lodge_once_merged[~df_lodge_once_merged['abn'].str.startswith('dummy_')].copy()\n",
        "\n",
        "# Clean the master register\n",
        "df_register.columns = df_register.columns.str.strip().str.replace('\\n', '')\n",
        "df_register.dropna(subset=['Reporting Period'], inplace=True)\n",
        "df_register['entity_name_clean'] = df_register['Reporting entities'].astype(str).str.upper().str.strip()\n",
        "\n",
        "# Clean the non-lodger list\n",
        "df_non_lodger['entity_name_clean'] = df_non_lodger['Entity Name'].astype(str).str.upper().str.strip()\n",
        "print(\"Step 2/5: Data has been cleaned and prepared for analysis.\")\n",
        "\n",
        "# --- 4. Core Analysis: Identify Non-Lodgers and Single-Lodgers ---\n",
        "# Identify Non-Lodgers by cross-referencing\n",
        "lodged_entity_names = set(df_register['entity_name_clean'])\n",
        "df_never_lodged = df_non_lodger[~df_non_lodger['entity_name_clean'].isin(lodged_entity_names)].copy()\n",
        "\n",
        "# Identify Single-Lodgers by counting occurrences in the register\n",
        "lodgement_counts = df_register['entity_name_clean'].value_counts()\n",
        "single_lodger_names = lodgement_counts[lodgement_counts == 1].index\n",
        "df_single_lodgers_base = df_register[df_register['entity_name_clean'].isin(single_lodger_names)].copy()\n",
        "\n",
        "# Enrich Single-Lodger data by extracting ABN and merging\n",
        "def extract_abn(text):\n",
        "    if not isinstance(text, str): return None\n",
        "    match = re.search(r'(\\d{2}\\s*\\d{3}\\s*\\d{3}\\s*\\d{3})', text)\n",
        "    return re.sub(r'\\s+', '', match.group(1)) if match else None\n",
        "\n",
        "df_single_lodgers_base['extracted_abn'] = df_single_lodgers_base['Reporting entities'].apply(extract_abn)\n",
        "df_single_lodgers_enriched = pd.merge(\n",
        "    df_single_lodgers_base,\n",
        "    df_lodge_once_merged,\n",
        "    left_on='extracted_abn',\n",
        "    right_on='abn',\n",
        "    how='left'\n",
        ")\n",
        "print(\"Step 3/5: Cohort identification and data enrichment complete.\")\n",
        "\n",
        "# --- 5. Final Deliverable Preparation and Export ---\n",
        "# Define the columns for the final export, based on Month 1 objectives\n",
        "non_lodger_columns = [\n",
        "    'ABN', 'Entity Name', 'Total Income', 'Entity size', 'ASX listed?', 'ASX300',\n",
        "    'Industry_desc', 'Division_Description', 'State',\n",
        "    'Mn_bus_addr_ln_1', 'Mn_bus_sbrb', 'Mn_bus_pc', 'Ent_eml', 'ACN'\n",
        "]\n",
        "single_lodger_columns = [\n",
        "    'Reporting entities', 'extracted_abn', 'abn', 'company_name', 'Reporting Period', 'Submitted',\n",
        "    'Status', 'Voluntary?', 'Revenue', 'abn_regn_dt', 'abn_cancn_dt', 'industry_desc',\n",
        "    'last_submission_dttm', 'num_compliant', 'num_non_compliant', 'expected_due_date'\n",
        "]\n",
        "\n",
        "# Ensure only existing columns are selected to prevent errors\n",
        "final_non_lodger_cols = [col for col in non_lodger_columns if col in df_never_lodged.columns]\n",
        "final_single_lodger_cols = [col for col in single_lodger_columns if col in df_single_lodgers_enriched.columns]\n",
        "\n",
        "# Create final DataFrames and export to a single Excel file\n",
        "final_non_lodgers_df = df_never_lodged[final_non_lodger_cols]\n",
        "final_single_lodgers_df = df_single_lodgers_enriched[final_single_lodger_cols]\n",
        "\n",
        "output_filename = 'Month_1_Analysis_Deliverable_Automated.xlsx'\n",
        "with pd.ExcelWriter(output_filename) as writer:\n",
        "    final_non_lodgers_df.to_excel(writer, sheet_name='Never Lodged Entities', index=False)\n",
        "    final_single_lodgers_df.to_excel(writer, sheet_name='Single Lodgement Entities', index=False)\n",
        "print(f\"Step 4/5: Final datasets prepared and exported to '{output_filename}'.\")\n",
        "\n",
        "# --- 6. Final Summary Report ---\n",
        "print(\"\\n--- Month 1 Pipeline Execution Summary ---\")\n",
        "print(f\"Identified {len(final_non_lodgers_df)} potential non-lodger entities.\")\n",
        "print(f\"Identified {len(final_single_lodgers_df)} single-lodgement entities.\")\n",
        "print(\"The final deliverable includes all available supporting details.\")\n",
        "print(\"Key Finding: 'Responsible persons' data is not explicitly available but can be proxied by 'associates' data in later analysis.\")\n",
        "print(\"--- Pipeline Complete ---\")"
      ],
      "metadata": {
        "id": "E3TKzHmyyKcf",
        "outputId": "0cef0a00-1cc0-43d2-e4a4-149f2601f4af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Automated Pipeline for Month 1 ---\n",
            "Step 1/5: All source files loaded successfully.\n",
            "Step 2/5: Data has been cleaned and prepared for analysis.\n",
            "Step 3/5: Cohort identification and data enrichment complete.\n",
            "Step 4/5: Final datasets prepared and exported to 'Month_1_Analysis_Deliverable_Automated.xlsx'.\n",
            "\n",
            "--- Month 1 Pipeline Execution Summary ---\n",
            "Identified 0 potential non-lodger entities.\n",
            "Identified 0 single-lodgement entities.\n",
            "The final deliverable includes all available supporting details.\n",
            "Key Finding: 'Responsible persons' data is not explicitly available but can be proxied by 'associates' data in later analysis.\n",
            "--- Pipeline Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END\n"
      ],
      "metadata": {
        "id": "FBMVx26xyJG_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}