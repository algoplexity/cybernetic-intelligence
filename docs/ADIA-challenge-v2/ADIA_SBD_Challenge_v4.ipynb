{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWIItAe-0fN"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/crunchdao/quickstarters/blob/master/competitions/structural-break/quickstarters/baseline/baseline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNUXnJa_-0fO"
      },
      "source": [
        "![Banner](https://raw.githubusercontent.com/crunchdao/quickstarters/refs/heads/master/competitions/structural-break/assets/banner.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lurIF1Ve-0fP"
      },
      "source": [
        "# ADIA Lab Structural Break Challenge\n",
        "\n",
        "## Challenge Overview\n",
        "\n",
        "Welcome to the ADIA Lab Structural Break Challenge! In this challenge, you will analyze univariate time series data to determine whether a structural break has occurred at a specified boundary point.\n",
        "\n",
        "### What is a Structural Break?\n",
        "\n",
        "A structural break occurs when the process governing the data generation changes at a certain point in time. These changes can be subtle or dramatic, and detecting them accurately is crucial across various domains such as climatology, industrial monitoring, finance, and healthcare.\n",
        "\n",
        "![Structural Break Example](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/structural-break/quickstarters/baseline/images/example.png)\n",
        "\n",
        "### Your Task\n",
        "\n",
        "For each time series in the test set, you need to predict a score between `0` and `1`:\n",
        "- Values closer to `0` indicate no structural break at the specified boundary point;\n",
        "- Values closer to `1` indicate a structural break did occur.\n",
        "\n",
        "### Evaluation Metric\n",
        "\n",
        "The evaluation metric is [ROC AUC (Area Under the Receiver Operating Characteristic Curve)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), which measures the performance of detection algorithms regardless of their specific calibration.\n",
        "\n",
        "- ROC AUC around `0.5`: No better than random chance;\n",
        "- ROC AUC approaching `1.0`: Perfect detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDFALziSBc9f"
      },
      "source": [
        "# Setup\n",
        "\n",
        "The first steps to get started are:\n",
        "1. Get the setup command\n",
        "2. Execute it in the cell below\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Reveal token](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/reveal-token.gif)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install crunch-cli --upgrade --quiet --progress-bar off\n",
        "!crunch setup-notebook structural-break XeZW3YGp3JZY1mndLHWH9Nw4"
      ],
      "metadata": {
        "id": "huz6Jw62vReE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DUeixiC_IJM"
      },
      "outputs": [],
      "source": [
        "# Install the Crunch CLI\n",
        "%pip install --upgrade crunch-cli\n",
        "\n",
        "# Setup your local environment\n",
        "!crunch setup --notebook structural-break hello --token aaaabbbbccccddddeeeeffff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IBhw7hv-0fQ"
      },
      "source": [
        "# Your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpLeMWSw-0fQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T09:52:21.302334Z",
          "start_time": "2024-11-18T09:52:18.268241Z"
        },
        "id": "MKqz-6Zw-0fR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing\n",
        "\n",
        "# Import your dependencies\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjD_WSAS-0fR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f622e9b2-6f66-4281-f10c-09f051309246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded inline runner with module: <module '__main__'>\n",
            "\n",
            "cli version: 7.2.0\n",
            "available ram: 12.67 gb\n",
            "available cpu: 2 core\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "import crunch\n",
        "\n",
        "# Load the Crunch Toolings\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiKJODFx-0fR"
      },
      "source": [
        "## Understanding the Data\n",
        "\n",
        "The dataset consists of univariate time series, each containing ~2,000-5,000 values with a designated boundary point. For each time series, you need to determine whether a structural break occurred at this boundary point.\n",
        "\n",
        "The data was downloaded when you setup your local environment and is now available in the `data/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKHXgvjN-0fS"
      },
      "outputs": [],
      "source": [
        "# Load the data simply\n",
        "X_train, y_train, X_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T_JmgMq-0fS"
      },
      "source": [
        "### Understanding `X_train`\n",
        "\n",
        "The training data is structured as a pandas DataFrame with a MultiIndex:\n",
        "\n",
        "**Index Levels:**\n",
        "- `id`: Identifies the unique time series\n",
        "- `time`: The timestep within each time series\n",
        "\n",
        "**Columns:**\n",
        "- `value`: The actual time series value at each timestep\n",
        "- `period`: A binary indicator where `0` represents the **period before** the boundary point, and `1` represents the **period after** the boundary point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "0oRCTnOb-0fS",
        "outputId": "2d0663ba-76b2-4937-d7fc-e6c314784242"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th>time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
              "      <th>0</th>\n",
              "      <td>0.001858</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001664</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.004386</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000699</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.002433</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">10000</th>\n",
              "      <th>1890</th>\n",
              "      <td>-0.005903</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1891</th>\n",
              "      <td>0.007295</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1892</th>\n",
              "      <td>0.003527</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1893</th>\n",
              "      <td>0.007218</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1894</th>\n",
              "      <td>0.000034</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23802099 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               value  period\n",
              "id    time                  \n",
              "0     0     0.001858       0\n",
              "      1    -0.001664       0\n",
              "      2    -0.004386       0\n",
              "      3     0.000699       0\n",
              "      4    -0.002433       0\n",
              "...              ...     ...\n",
              "10000 1890 -0.005903       1\n",
              "      1891  0.007295       1\n",
              "      1892  0.003527       1\n",
              "      1893  0.007218       1\n",
              "      1894  0.000034       1\n",
              "\n",
              "[23802099 rows x 2 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WP39dgx-0fS"
      },
      "source": [
        "### Understanding `y_train`\n",
        "\n",
        "This is a simple `pandas.Series` that tells if a dataset id has a structural breakpoint or not.\n",
        "\n",
        "**Index:**\n",
        "- `id`: the ID of the dataset\n",
        "\n",
        "**Value:**\n",
        "- `structural_breakpoint`: Boolean indicating whether a structural break occurred (`True`) or not (`False`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "dPsQPdIj-0fT",
        "outputId": "acd28eab-afd8-44c7-d229-d7e414f2e3c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id\n",
              "0         True\n",
              "1         True\n",
              "2        False\n",
              "3         True\n",
              "4        False\n",
              "         ...  \n",
              "9996     False\n",
              "9997      True\n",
              "9998     False\n",
              "9999     False\n",
              "10000     True\n",
              "Name: structural_breakpoint, Length: 10001, dtype: bool"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oSS08Ks-0fT"
      },
      "source": [
        "### Understanding `X_test`\n",
        "\n",
        "The test data is provided as a **`list` of `pandas.DataFrame`s** with the same format as [`X_train`](#understanding-X_test).\n",
        "\n",
        "It is structured as a list to encourage processing records one by one, which will be mandatory in the `infer()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ErbKAs--0fT",
        "outputId": "5ce46294-1824-4067-be90-547189e90be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of datasets: 101\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of datasets:\", len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "M_dTYXms-0fT",
        "outputId": "b3ee6375-995f-47f6-f6e5-7f08d9838820"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th>time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"11\" valign=\"top\">10001</th>\n",
              "      <th>0</th>\n",
              "      <td>-0.020657</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.005894</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.003052</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.000590</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.009887</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2517</th>\n",
              "      <td>0.005084</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2518</th>\n",
              "      <td>-0.024414</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2519</th>\n",
              "      <td>-0.014986</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2520</th>\n",
              "      <td>0.012999</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2521</th>\n",
              "      <td>-0.022138</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2522 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               value  period\n",
              "id    time                  \n",
              "10001 0    -0.020657       0\n",
              "      1    -0.005894       0\n",
              "      2    -0.003052       0\n",
              "      3    -0.000590       0\n",
              "      4     0.009887       0\n",
              "...              ...     ...\n",
              "      2517  0.005084       1\n",
              "      2518 -0.024414       1\n",
              "      2519 -0.014986       1\n",
              "      2520  0.012999       1\n",
              "      2521 -0.022138       1\n",
              "\n",
              "[2522 rows x 2 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgulFOGX-0fT"
      },
      "source": [
        "## Strategy Implementation\n",
        "\n",
        "There are multiple approaches you can take to detect structural breaks:\n",
        "\n",
        "1. **Statistical Tests**: Compare distributions before and after the boundary point;\n",
        "2. **Feature Engineering**: Extract features from both segments for comparison;\n",
        "3. **Time Series Modeling**: Detect deviations from expected patterns;\n",
        "4. **Machine Learning**: Train models to recognize break patterns from labeled examples.\n",
        "\n",
        "The baseline implementation below uses a simple statistical approach: a t-test to compare the distributions before and after the boundary point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfYIXlz-0fT"
      },
      "source": [
        "### The `train()` Function\n",
        "\n",
        "In this function, you build and train your model for making inferences on the test data. Your model must be stored in the `model_directory_path`.\n",
        "\n",
        "The baseline implementation below doesn't require a pre-trained model, as it uses a statistical test that will be computed at inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:04:00.459399Z",
          "start_time": "2024-11-18T10:04:00.455716Z"
        },
        "id": "xQwWDC6M-0fT"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    # For our baseline t-test approach, we don't need to train a model\n",
        "    # This is essentially an unsupervised approach calculated at inference time\n",
        "    model = None\n",
        "\n",
        "    # You could enhance this by training an actual model, for example:\n",
        "    # 1. Extract features from before/after segments of each time series\n",
        "    # 2. Train a classifier using these features and y_train labels\n",
        "    # 3. Save the trained model\n",
        "\n",
        "    joblib.dump(model, os.path.join(model_directory_path, 'model.joblib'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-jboJH-0fU"
      },
      "source": [
        "### The `infer()` Function\n",
        "\n",
        "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
        "\n",
        "**Important workflow:**\n",
        "1. Load your model;\n",
        "2. Use the `yield` statement to signal readiness to the runner;\n",
        "3. Process each dataset one by one within the for loop;\n",
        "4. For each dataset, use `yield prediction` to return your prediction.\n",
        "\n",
        "**Note:** The datasets can only be iterated once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:03:59.120294Z",
          "start_time": "2024-11-18T10:03:59.114830Z"
        },
        "id": "r1b7hRkl-0fU"
      },
      "outputs": [],
      "source": [
        "def infer(\n",
        "    X_test: typing.Iterable[pd.DataFrame],\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "    yield  # Mark as ready\n",
        "\n",
        "    # X_test can only be iterated once.\n",
        "    # Before getting the next dataset, you must predict the current one.\n",
        "    for dataset in X_test:\n",
        "        # Baseline approach: Compute t-test between values before and after boundary point\n",
        "        # The negative p-value is used as our score - smaller p-values (larger negative numbers)\n",
        "        # indicate more evidence against the null hypothesis that distributions are the same,\n",
        "        # suggesting a structural break\n",
        "        def t_test(u: pd.DataFrame):\n",
        "            return -scipy.stats.ttest_ind(\n",
        "                u[\"value\"][u[\"period\"] == 0],  # Values before boundary point\n",
        "                u[\"value\"][u[\"period\"] == 1],  # Values after boundary point\n",
        "            ).pvalue\n",
        "\n",
        "        prediction = t_test(dataset)\n",
        "        yield prediction  # Send the prediction for the current dataset\n",
        "\n",
        "        # Note: This baseline approach uses a t-test to compare the distributions\n",
        "        # before and after the boundary point. A smaller p-value (larger negative number)\n",
        "        # suggests stronger evidence that the distributions are different,\n",
        "        # indicating a potential structural break."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0Kl9CA-0fU"
      },
      "source": [
        "## Local testing\n",
        "\n",
        "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
        "Even if it is not perfect, it should give you a quick idea if your model is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDZeP-4--0fU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446,
          "referenced_widgets": [
            "04beaea2273d41cd86b92b3d5f712e13",
            "2e3a91dd26cd464798887b502afacebe",
            "d1ca9e03620e450eb73e7d0399796047",
            "d1f9d5996ae34653b2217b9193bb15f2",
            "c2f740f04d39488b8c7749087cbe40c1",
            "2e47133648a14a2494e49bba953ec27f",
            "d84476d3f0df4fb4b6bff7fa8512ede1",
            "b2dcd903454948aba72ce0d5677c2211",
            "55fde5e2363144ac8477c55c92e8e3e3",
            "c569f6a1930b46a0bae9b9f9d972a5bf",
            "cba68990a8ad462c9bf14e4dbdd484d8"
          ]
        },
        "outputId": "850c41c2-bea1-44b7-bacb-3f708bc751a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11:55:37 no forbidden library found\n",
            "11:55:37 \n",
            "11:55:37 started\n",
            "11:55:37 running local test\n",
            "11:55:37 internet access isn't restricted, no check will be done\n",
            "11:55:37 \n",
            "11:55:38 starting unstructured loop...\n",
            "11:55:38 executing - command=train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n",
            "============================================================\n",
            "                STARTING TRAINING PIPELINE v3\n",
            "============================================================\n",
            "--- Starting Stage 1: MDL Pre-training on 'Edge of Chaos' ECA data ---\n",
            "Generating synthetic data for 28 'Edge of Chaos' rules...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating ECA Data:   0%|          | 0/28 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04beaea2273d41cd86b92b3d5f712e13"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "crunch.test(\n",
        "    # Uncomment to disable the train\n",
        "    # force_first_train=False,\n",
        "\n",
        "    # Uncomment to disable the determinism check\n",
        "    # no_determinism_check=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_5CKs--0fU"
      },
      "source": [
        "## Results\n",
        "\n",
        "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly5q68sA-0fU"
      },
      "outputs": [],
      "source": [
        "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oP-NLGh-0fU"
      },
      "source": [
        "### Local scoring\n",
        "\n",
        "You can call the function that the system uses to estimate your score locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCrjpzv-0fU"
      },
      "outputs": [],
      "source": [
        "# Load the targets\n",
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "\n",
        "# Call the scoring function\n",
        "sklearn.metrics.roc_auc_score(\n",
        "    target,\n",
        "    prediction,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "g-JmsLkXxiFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution v9"
      ],
      "metadata": {
        "id": "UF4VmLVcb7YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title CELL 1: SETUP AND GLOBAL CONFIGURATION\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Imports ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import random\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import cellpylib as cpl\n",
        "from dataclasses import dataclass, field\n",
        "import typing\n",
        "import hashlib\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import math # <-- Ensure math is imported\n",
        "import itertools\n",
        "\n",
        "# --- Main Configuration Class (for Full-Scale Runs) ---\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Reproducibility\n",
        "    SEED: int = 42\n",
        "\n",
        "    # Data Processing\n",
        "    PERMUTATION_EMBEDDING_DIM: int = 4\n",
        "    PERMUTATION_TIME_LAG: int = 1\n",
        "    SERIES_PROCESSOR_SEQUENCE_LENGTH: int = 256\n",
        "    SERIES_PROCESSOR_N_AUGMENTATIONS: int = 5\n",
        "\n",
        "    # ECA Pre-training Data Generation\n",
        "    ECA_RULES_TO_USE: typing.List[int] = field(default_factory=lambda: [\n",
        "        22, 30, 45, 54, 60, 75, 82, 86, 89, 90, 105, 106, 110,\n",
        "        122, 126, 135, 146, 149, 150, 153, 154, 161, 165, 169,\n",
        "        182, 193, 195, 225\n",
        "    ])\n",
        "    ECA_N_SAMPLES_PER_RULE: int = 100\n",
        "    ECA_TIMESTEPS: int = 256\n",
        "    ECA_WIDTH: int = 128\n",
        "\n",
        "    # Model Architecture (Full 3-Stage U-Net)\n",
        "    MODEL_DIMENSIONS: typing.List[int] = field(default_factory=lambda: [128, 256, 512])\n",
        "    MODEL_LAYERS_PER_BLOCK: typing.List[int] = field(default_factory=lambda: [2, 2, 2])\n",
        "    MODEL_N_HEADS: int = 8\n",
        "    MODEL_MAX_SEQLENS: typing.List[int] = field(default_factory=lambda: [256, 64, 16])\n",
        "    MODEL_BOTTLENECK_DIM: int = 64\n",
        "\n",
        "    # Training Hyperparameters\n",
        "    PRETRAIN_EPOCHS: int = 5\n",
        "    FINETUNE_EPOCHS: int = 10\n",
        "    BATCH_SIZE: int = 32\n",
        "    PRETRAIN_LR: float = 1e-4\n",
        "    FINETUNE_FULL_LR: float = 1e-5 # Lower LR for fine-tuning the whole model\n",
        "    MDL_RECON_LOSS_ALPHA: float = 1.0\n",
        "    MDL_RULE_LOSS_BETA: float = 0.5\n",
        "\n",
        "    # Paths\n",
        "    MODEL_DIR: Path = field(default_factory=lambda: Path(\"./adia_model_store\"))\n",
        "\n",
        "# --- Global Functions & Instantiation ---\n",
        "config = Config()\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(config.SEED)\n",
        "config.MODEL_DIR.mkdir(exist_ok=True, parents=True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Global configuration set for a full-scale run.\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Helper function for creating mock data, used in later cells\n",
        "def create_dataset(n_samples, has_break_prob=0.5):\n",
        "    X_list, y_list = [], []\n",
        "    for i in range(n_samples):\n",
        "        has_break = np.random.rand() < has_break_prob\n",
        "        length = np.random.randint(500, 1500)\n",
        "        break_point = np.random.randint(int(length * 0.3), int(length * 0.7))\n",
        "        t = np.linspace(0, np.random.uniform(5, 15), length)\n",
        "        noise = np.random.randn(length) * 0.1\n",
        "\n",
        "        series = pd.Series(np.sin(t * 2 * np.pi) + noise, name='value')\n",
        "        if has_break:\n",
        "            t2_freq = np.random.uniform(3, 8)\n",
        "            t2_amp = np.random.uniform(1.2, 2.0)\n",
        "            series.iloc[break_point:] = pd.Series(np.cos(t[break_point:] * t2_freq * np.pi) * t2_amp + noise[break_point:])\n",
        "\n",
        "        X_list.append({'series': series, 'period': break_point + 1})\n",
        "        y_list.append(1 if has_break else 0)\n",
        "    return pd.DataFrame(X_list), pd.Series(y_list)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cfRsXprWcOj3",
        "outputId": "333f410c-83e0-4481-9cb5-8fed786e0786",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global configuration set for a full-scale run.\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title CELL 2: CORE LIBRARY AND PIPELINE LOGIC (BUG FIXED)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MODULE 1: core_library/data_processing.py\n",
        "# ------------------------------------------------------------------------------\n",
        "class PermutationSymbolizer:\n",
        "    def __init__(self, embedding_dim, time_lag):\n",
        "        self.d = embedding_dim\n",
        "        self.tau = time_lag\n",
        "        self.permutations = {\n",
        "            tuple(p): i for i, p in enumerate(itertools.permutations(range(self.d)))\n",
        "        }\n",
        "\n",
        "    def symbolize_vector(self, vector: np.ndarray) -> int:\n",
        "        hasher = hashlib.sha256(vector.tobytes())\n",
        "        seed = int.from_bytes(hasher.digest(), 'big') % (2**32)\n",
        "        rng = np.random.default_rng(seed)\n",
        "        noisy_vector = vector + rng.normal(0, 1e-9, size=vector.shape)\n",
        "        return self.permutations[tuple(np.argsort(noisy_vector))]\n",
        "\n",
        "class SeriesProcessor:\n",
        "    def __init__(self, symbolizer, sequence_length, n_augmentations):\n",
        "        self.symbolizer = symbolizer\n",
        "        self.seq_len = sequence_length\n",
        "        self.d = symbolizer.d\n",
        "        self.n_aug = n_augmentations\n",
        "\n",
        "    def process_for_finetune(self, series: pd.Series) -> np.ndarray:\n",
        "        if len(series) < self.d: return np.array([])\n",
        "        view_shape = (len(series) - self.d + 1, self.d)\n",
        "        view_strides = (series.values.strides[0], series.values.strides[0])\n",
        "        windows = np.lib.stride_tricks.as_strided(series.values, shape=view_shape, strides=view_strides)\n",
        "        symbols = np.array([self.symbolizer.symbolize_vector(w) for w in windows])\n",
        "        if len(symbols) < self.seq_len: return np.array([])\n",
        "        sequences = []\n",
        "        for i in range(self.n_aug):\n",
        "            start_idx = np.random.randint(0, len(symbols) - self.seq_len + 1)\n",
        "            sequences.append(symbols[start_idx:start_idx + self.seq_len])\n",
        "        return np.array(sequences)\n",
        "\n",
        "class ECADataGenerator:\n",
        "    def __init__(self, eca_config, n_samples_per_rule, timesteps, width):\n",
        "        self.config = eca_config\n",
        "        self.n_samples = n_samples_per_rule\n",
        "        self.timesteps = timesteps\n",
        "        self.width = width\n",
        "\n",
        "    def _generate_for_rule(self, rule_info):\n",
        "        is_composite = isinstance(rule_info, dict) # <-- BUG FIX\n",
        "        rule_label = str(rule_info['rules']) if is_composite else str(rule_info)\n",
        "        ca_list = []\n",
        "        for _ in range(self.n_samples):\n",
        "            init_cond = cpl.init_random(self.width)\n",
        "            if is_composite:\n",
        "                ca = cpl.evolve_ca_chain(init_cond, self.timesteps, rule_info['rules'], rule_info['timesteps'])\n",
        "            else:\n",
        "                ca = cpl.evolve(init_cond, self.timesteps, lambda n, c, t: cpl.nks_rule(n, rule_info))\n",
        "            ca_list.append(ca)\n",
        "        return np.array(ca_list), [rule_label] * self.n_samples\n",
        "\n",
        "    def generate_training_data(self):\n",
        "        all_cas, all_labels_str = [], []\n",
        "        base_rules = self.config.get('base', [])\n",
        "        for rule in tqdm(base_rules, desc=\"Generating Base ECAs\"):\n",
        "            cas, labels = self._generate_for_rule(rule)\n",
        "            all_cas.append(cas)\n",
        "            all_labels_str.extend(labels)\n",
        "        composite_rules = self.config.get('composite', [])\n",
        "        for i, rule_info in enumerate(tqdm(composite_rules, desc=\"Generating Composite ECAs\")):\n",
        "            cas, labels = self._generate_for_rule(rule_info)\n",
        "            all_cas.append(cas)\n",
        "            all_labels_str.extend(labels)\n",
        "        if not all_cas: return torch.empty(0), torch.empty(0), {}\n",
        "        unique_labels = sorted(list(set(all_labels_str)))\n",
        "        label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "        final_cas = np.vstack(all_cas)\n",
        "        final_labels = np.array([label_map[l] for l in all_labels_str])\n",
        "        return torch.from_numpy(final_cas).float(), torch.from_numpy(final_labels).long(), label_map\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MODULE 2: core_library/model_architecture.py\n",
        "# ------------------------------------------------------------------------------\n",
        "class CausalTransformer(nn.Module):\n",
        "    # ... (implementation is correct, no changes needed)\n",
        "    def __init__(self, dim, depth, heads, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=dim*4, batch_first=True, activation='gelu')\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, dim)\n",
        "        self.register_buffer('causal_mask', ~torch.ones(max_seq_len, max_seq_len, dtype=torch.bool).triu(1))\n",
        "    def forward(self, x):\n",
        "        n, seq_len, _ = x.shape\n",
        "        pos = torch.arange(seq_len, device=x.device)\n",
        "        x = x + self.pos_emb(pos)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask=self.causal_mask[:seq_len, :seq_len])\n",
        "        return x\n",
        "\n",
        "class SimpleTransition(nn.Module):\n",
        "    # ... (implementation is correct, no changes needed)\n",
        "    def __init__(self, dim_in, dim_out, factor, is_up):\n",
        "        super().__init__(); self.is_up = is_up; self.factor = factor; self.proj = nn.Linear(dim_in, dim_out)\n",
        "    def forward(self, x):\n",
        "        if self.is_up: x = x.repeat_interleave(self.factor, dim=1)\n",
        "        else: x = x[:, ::self.factor, :];\n",
        "        return self.proj(x)\n",
        "\n",
        "class HierarchicalDynamicalEncoder(nn.Module):\n",
        "    # ... (implementation is correct, no changes needed)\n",
        "    def __init__(self, dims, depths, heads, max_seqlens):\n",
        "        super().__init__(); self.levels = nn.ModuleList()\n",
        "        for i in range(len(dims) - 1): self.levels.append(nn.ModuleList([CausalTransformer(dims[i], depths[i], heads, max_seqlens[i]), SimpleTransition(dims[i], dims[i+1], max_seqlens[i] // max_seqlens[i+1], is_up=False)]))\n",
        "    def forward(self, x):\n",
        "        residuals = [];\n",
        "        for transformer, transition in self.levels: x = transformer(x); residuals.append(x); x = transition(x)\n",
        "        return x, residuals\n",
        "\n",
        "class HierarchicalDynamicalDecoder(nn.Module):\n",
        "    # ... (implementation is correct, no changes needed)\n",
        "    def __init__(self, dims, depths, heads, max_seqlens):\n",
        "        super().__init__(); self.levels = nn.ModuleList(); self.final_transformer = CausalTransformer(dims[0], depths[0], heads, max_seqlens[0])\n",
        "        for i in range(len(dims) - 1, 0, -1): self.levels.append(nn.ModuleList([SimpleTransition(dims[i], dims[i-1], max_seqlens[i-1] // max_seqlens[i], is_up=True), CausalTransformer(dims[i-1], depths[i-1], heads, max_seqlens[i-1])]))\n",
        "    def forward(self, x, residuals):\n",
        "        for (transition, transformer), res in zip(self.levels, residuals[::-1]): x = transition(x); x = x + res; x = transformer(x)\n",
        "        return self.final_transformer(x)\n",
        "\n",
        "class MDL_AU_Net_Autoencoder(nn.Module):\n",
        "    def __init__(self, model_cfg, data_cfg, n_rules):\n",
        "        super().__init__()\n",
        "        dims = model_cfg.MODEL_DIMENSIONS\n",
        "        n_symbols = math.factorial(data_cfg.PERMUTATION_EMBEDDING_DIM) # <-- BUG FIX\n",
        "        self.embedding = nn.Embedding(n_symbols, dims[0])\n",
        "        self.encoder = HierarchicalDynamicalEncoder(dims, model_cfg.MODEL_LAYERS_PER_BLOCK, model_cfg.MODEL_N_HEADS, model_cfg.MODEL_MAX_SEQLENS)\n",
        "        self.bottleneck = CausalTransformer(dims[-1], 1, model_cfg.MODEL_N_HEADS, model_cfg.MODEL_MAX_SEQLENS[-1])\n",
        "        self.rule_classifier = nn.Linear(dims[-1], n_rules) if n_rules > 0 else nn.Identity()\n",
        "        self.decoder = HierarchicalDynamicalDecoder(dims, model_cfg.MODEL_LAYERS_PER_BLOCK, model_cfg.MODEL_N_HEADS, model_cfg.MODEL_MAX_SEQLENS)\n",
        "        self.to_logits = nn.Linear(dims[0], data_cfg.ECA_WIDTH)\n",
        "\n",
        "    def forward_pretrain(self, x):\n",
        "        fingerprint, residuals = self.encoder(x); fingerprint = self.bottleneck(fingerprint)\n",
        "        rule_logits = self.rule_classifier(fingerprint.mean(dim=1)); reconstructed = self.decoder(fingerprint, residuals)\n",
        "        reconstructed_logits = self.to_logits(reconstructed); return reconstructed_logits, rule_logits\n",
        "    def encode(self, x_symbols):\n",
        "        x_embedded = self.embedding(x_symbols); fingerprint, _ = self.encoder(x_embedded)\n",
        "        fingerprint = self.bottleneck(fingerprint); return fingerprint.mean(dim=1)\n",
        "\n",
        "class StructuralBreakClassifier(nn.Module):\n",
        "    def __init__(self, encoder_model):\n",
        "        super().__init__(); self.encoder_model = encoder_model\n",
        "        fingerprint_dim = encoder_model.encoder.levels[-1][-1].proj.out_features; input_dim = fingerprint_dim * 3\n",
        "        self.classifier_head = nn.Sequential(nn.LayerNorm(input_dim), nn.Linear(input_dim, fingerprint_dim), nn.GELU(), nn.Linear(fingerprint_dim, 1))\n",
        "    def _get_fingerprint(self, sequences):\n",
        "        if sequences.shape[0] == 0: return torch.zeros(self.encoder_model.encoder.levels[-1][-1].proj.out_features, device=next(self.parameters()).device)\n",
        "        fingerprints = self.encoder_model.encode(sequences); return fingerprints.mean(dim=0)\n",
        "    def forward(self, before_seqs_batch, after_seqs_batch):\n",
        "        batch_logits = []\n",
        "        for before_seqs, after_seqs in zip(before_seqs_batch, after_seqs_batch):\n",
        "            fp_before = self._get_fingerprint(torch.from_numpy(before_seqs).long().to(next(self.parameters()).device))\n",
        "            fp_after = self._get_fingerprint(torch.from_numpy(after_seqs).long().to(next(self.parameters()).device))\n",
        "            combined = torch.cat([fp_before, fp_after, torch.abs(fp_before - fp_after)], dim=0)\n",
        "            batch_logits.append(self.classifier_head(combined))\n",
        "        return torch.stack(batch_logits)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MODULE 3 & 4: Training, Inference, and Platform Entry Points\n",
        "# ------------------------------------------------------------------------------\n",
        "class FineTuneDataset(Dataset):\n",
        "    def __init__(self, X, y, processor): self.X = X; self.y = y; self.processor = processor\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.X.iloc[idx]; label = self.y.iloc[idx]; series = row['series']; break_point = row['period'] - 1\n",
        "        series_before = series.iloc[:break_point]; series_after = series.iloc[break_point:]\n",
        "        before_sequences = self.processor.process_for_finetune(series_before)\n",
        "        after_sequences = self.processor.process_for_finetune(series_after)\n",
        "        return before_sequences, after_sequences, label\n",
        "\n",
        "def finetune_collate_fn(batch):\n",
        "    before_batch, after_batch, labels_batch = zip(*batch)\n",
        "    labels_tensor = torch.tensor(labels_batch, dtype=torch.float32)\n",
        "    return list(before_batch), list(after_batch), labels_tensor\n",
        "\n",
        "def train(X, y, model_dir):\n",
        "    print(\"--- Starting Training Pipeline ---\"); model_dir = Path(model_dir); model_dir.mkdir(exist_ok=True, parents=True)\n",
        "    # Stage 1: Pre-training\n",
        "    print(\"--- Starting Stage 1: Pre-training ---\")\n",
        "    eca_gen = ECADataGenerator(eca_config={'base': config.ECA_RULES_TO_USE, 'composite':[]}, n_samples_per_rule=config.ECA_N_SAMPLES_PER_RULE, timesteps=config.ECA_TIMESTEPS, width=config.ECA_WIDTH)\n",
        "    eca_tensors, eca_labels, label_map = eca_gen.generate_training_data(); n_rules = len(label_map)\n",
        "    pretrain_dataset = TensorDataset(eca_tensors, eca_labels); pretrain_loader = DataLoader(pretrain_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "    autoencoder = MDL_AU_Net_Autoencoder(config, config, n_rules).to(device)\n",
        "    eca_input_proj = nn.Linear(config.ECA_WIDTH, config.MODEL_DIMENSIONS[0]).to(device)\n",
        "    optimizer = optim.Adam(list(autoencoder.parameters()) + list(eca_input_proj.parameters()), lr=config.PRETRAIN_LR)\n",
        "    recon_loss_fn = nn.BCEWithLogitsLoss(); rule_loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(config.PRETRAIN_EPOCHS):\n",
        "        autoencoder.train(); total_loss, recon_L, rule_L = 0, 0, 0\n",
        "        pbar = tqdm(pretrain_loader, desc=f\"Pre-train Epoch {epoch+1}/{config.PRETRAIN_EPOCHS}\")\n",
        "        for sequences, labels in pbar:\n",
        "            sequences, labels = sequences.to(device), labels.to(device); optimizer.zero_grad()\n",
        "            projected_sequences = eca_input_proj(sequences)\n",
        "            recon_logits, rule_logits = autoencoder.forward_pretrain(projected_sequences)\n",
        "            loss_recon = recon_loss_fn(recon_logits, sequences > 0.5); loss_rule = rule_loss_fn(rule_logits, labels)\n",
        "            loss = config.MDL_RECON_LOSS_ALPHA * loss_recon + config.MDL_RULE_LOSS_BETA * loss_rule\n",
        "            loss.backward(); optimizer.step()\n",
        "            total_loss += loss.item(); recon_L += loss_recon.item(); rule_L += loss_rule.item()\n",
        "            pbar.set_postfix(loss=total_loss/len(pbar), recon_L=recon_L/len(pbar), rule_L=rule_L/len(pbar))\n",
        "\n",
        "    # --- ADDITION: Save the pre-trained model for faster iteration ---\n",
        "    print(\"--- Saving pre-trained autoencoder for faster iteration ---\")\n",
        "    torch.save(autoencoder.state_dict(), model_dir / \"pretrained_autoencoder.pth\")\n",
        "    joblib.dump(label_map, model_dir / \"label_map.joblib\") # Save label_map needed to rebuild model\n",
        "\n",
        "    # Stage 2: Fine-tuning\n",
        "    print(\"--- Starting Stage 2: Fine-tuning ---\")\n",
        "    symbolizer = PermutationSymbolizer(config.PERMUTATION_EMBEDDING_DIM, config.PERMUTATION_TIME_LAG)\n",
        "    processor = SeriesProcessor(symbolizer, config.SERIES_PROCESSOR_SEQUENCE_LENGTH, config.SERIES_PROCESSOR_N_AUGMENTATIONS)\n",
        "    finetune_dataset = FineTuneDataset(X, y, processor); finetune_loader = DataLoader(finetune_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=finetune_collate_fn)\n",
        "    classifier = StructuralBreakClassifier(autoencoder).to(device)\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=config.FINETUNE_FULL_LR)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    for epoch in range(config.FINETUNE_EPOCHS):\n",
        "        classifier.train(); total_loss = 0\n",
        "        pbar = tqdm(finetune_loader, desc=f\"Fine-tune Epoch {epoch+1}/{config.FINETUNE_EPOCHS}\")\n",
        "        for before_seqs, after_seqs, labels in pbar:\n",
        "            labels = labels.to(device); optimizer.zero_grad()\n",
        "            logits = classifier(before_seqs, after_seqs).squeeze()\n",
        "            loss = loss_fn(logits, labels); loss.backward(); optimizer.step()\n",
        "            total_loss += loss.item(); pbar.set_postfix(loss=total_loss/len(pbar))\n",
        "    # Stage 3: Save Artifacts\n",
        "    print(\"--- Saving final model artifacts ---\")\n",
        "    joblib.dump(config, model_dir / \"config.joblib\")\n",
        "    torch.save(classifier.state_dict(), model_dir / \"structural_break_classifier.pth\")\n",
        "\n",
        "def infer(X, model_dir):\n",
        "    print(\"--- Starting Inference Pipeline ---\"); model_dir = Path(model_dir)\n",
        "    loaded_config = joblib.load(model_dir / \"config.joblib\")\n",
        "    label_map = joblib.load(model_dir / \"label_map.joblib\"); n_rules = len(label_map)\n",
        "    autoencoder = MDL_AU_Net_Autoencoder(loaded_config, loaded_config, n_rules)\n",
        "    model = StructuralBreakClassifier(autoencoder)\n",
        "    model.load_state_dict(torch.load(model_dir / \"structural_break_classifier.pth\")); model.to(device).eval()\n",
        "    symbolizer = PermutationSymbolizer(loaded_config.PERMUTATION_EMBEDDING_DIM, loaded_config.PERMUTATION_TIME_LAG)\n",
        "    processor = SeriesProcessor(symbolizer, loaded_config.SERIES_PROCESSOR_SEQUENCE_LENGTH, loaded_config.SERIES_PROCESSOR_N_AUGMENTATIONS)\n",
        "    with torch.no_grad():\n",
        "        for i, row in tqdm(X.iterrows(), total=len(X), desc=\"Inferring\"):\n",
        "            series_data = row['series']; break_point = row['period'] - 1\n",
        "            series_before = series_data.iloc[:break_point]; series_after = series_data.iloc[break_point:]\n",
        "            before_sequences = processor.process_for_finetune(series_before); after_sequences = processor.process_for_finetune(series_after)\n",
        "            logits = model([before_sequences], [after_sequences]); score = torch.sigmoid(logits).item(); yield score"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Rp6CfpfScJos"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title CELL 3: TIER 3 - FULL PRE-TRAINING AND FINE-TUNING RUN\n",
        "# ==============================================================================\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"      RUNNING TIER 3: FULL END-TO-END TRAINING\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"This will take a long time, but only needs to be run once to generate\")\n",
        "    print(\"the 'pretrained_autoencoder.pth' file for faster iteration.\")\n",
        "\n",
        "    # We use the main 'Config' class defined in Cell 1\n",
        "    seed_everything(config.SEED)\n",
        "\n",
        "    # --- Create a large, representative mock dataset ---\n",
        "    print(\"\\nCreating a large mock dataset...\")\n",
        "    train_X, train_y = create_dataset(n_samples=1000)\n",
        "    test_X, test_y = create_dataset(n_samples=200)\n",
        "    print(f\"Training set size: {len(train_X)}, Test set size: {len(test_X)}\")\n",
        "\n",
        "    # --- Run Full Training and Inference ---\n",
        "    try:\n",
        "        if torch.cuda.is_available(): print(\"\\n✅ Found and using GPU.\\n\")\n",
        "        else: print(\"\\n⚠️ WARNING: GPU not found. This will be very slow.\\n\")\n",
        "\n",
        "        # Train the FULL model on the LARGE training data\n",
        "        train(train_X, train_y, config.MODEL_DIR)\n",
        "\n",
        "        # Infer on the unseen test data\n",
        "        print(\"\\nInferring on the unseen test set...\")\n",
        "        predictions = list(infer(test_X, config.MODEL_DIR))\n",
        "\n",
        "        # --- Evaluate Performance ---\n",
        "        auc_score = roc_auc_score(test_y, predictions)\n",
        "        print(\"\\n\" + \"=\"*50); print(\"      TIER 3 RESULTS\"); print(\"=\"*50)\n",
        "        print(f\"✅ Final Out-of-Sample ROC AUC Score: {auc_score:.4f}\"); print(\"=\"*50)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR during full run: {e}\"); import traceback; traceback.print_exc()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EDWAUeuJcD2k",
        "outputId": "a5a700db-f389-4d82-dadf-43e141868e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300,
          "referenced_widgets": [
            "942c5416a7c54f4384c5c4d798af0d2e",
            "c0744f5bb9d14d0f8764f9ccdc156b72",
            "f9d5853ddae64987950061085bc01c95",
            "db78054b43c748ed8200f177cc54695e",
            "66e07fa3211c4f15b3096e1f1250ad0f",
            "d3a3a7858ce1499c9d35ec270e10772f",
            "18d119886fb7488faf55154db9bd37a4",
            "97fa7d0b19c24147a6d991c1680ded0f",
            "fa2493d006b7429983db3fbf411a181a",
            "cc8f59b1209d45da8d58a91428f145a2",
            "12085a40604846d5a9fd482f83a96224"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "      RUNNING TIER 3: FULL END-TO-END TRAINING\n",
            "================================================================================\n",
            "This will take a long time, but only needs to be run once to generate\n",
            "the 'pretrained_autoencoder.pth' file for faster iteration.\n",
            "\n",
            "Creating a large mock dataset...\n",
            "Training set size: 1000, Test set size: 200\n",
            "\n",
            "✅ Found and using GPU.\n",
            "\n",
            "--- Starting Training Pipeline ---\n",
            "--- Starting Stage 1: Pre-training ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Base ECAs:   0%|          | 0/28 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "942c5416a7c54f4384c5c4d798af0d2e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title CELL 4: TIER 2.5 - FAST FINE-TUNING TEST (RUN AFTER CELL 3)\n",
        "# ==============================================================================\n",
        "def fine_tune_only(X, y, model_dir, ft_config):\n",
        "    \"\"\"\n",
        "    This function loads a pre-trained model and ONLY runs the fine-tuning stage.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80); print(\"      RUNNING TIER 2.5: FAST FINE-TUNING ONLY\"); print(\"=\"*80)\n",
        "    model_dir = Path(model_dir)\n",
        "    pretrained_path = model_dir / \"pretrained_autoencoder.pth\"\n",
        "    label_map_path = model_dir / \"label_map.joblib\"\n",
        "\n",
        "    if not pretrained_path.exists():\n",
        "        print(f\"ERROR: Pre-trained model not found at '{pretrained_path}'.\")\n",
        "        print(\"Please run the full Tier 3 training (Cell 3) once first.\")\n",
        "        return None\n",
        "\n",
        "    # --- Load the pre-trained autoencoder ---\n",
        "    print(\"--- Loading pre-trained autoencoder ---\")\n",
        "    label_map = joblib.load(label_map_path)\n",
        "    n_rules = len(label_map)\n",
        "    autoencoder = MDL_AU_Net_Autoencoder(config, config, n_rules).to(device) # Use global config for architecture\n",
        "    autoencoder.load_state_dict(torch.load(pretrained_path))\n",
        "    print(\"Pre-trained model loaded successfully.\")\n",
        "\n",
        "    # --- Stage 2: Fine-tuning ---\n",
        "    print(\"--- Starting Stage 2: Fine-tuning ---\")\n",
        "    symbolizer = PermutationSymbolizer(ft_config.PERMUTATION_EMBEDDING_DIM, ft_config.PERMUTATION_TIME_LAG)\n",
        "    processor = SeriesProcessor(symbolizer, ft_config.SERIES_PROCESSOR_SEQUENCE_LENGTH, ft_config.SERIES_PROCESSOR_N_AUGMENTATIONS)\n",
        "    finetune_dataset = FineTuneDataset(X, y, processor)\n",
        "    finetune_loader = DataLoader(finetune_dataset, batch_size=ft_config.BATCH_SIZE, shuffle=True, collate_fn=finetune_collate_fn)\n",
        "    classifier = StructuralBreakClassifier(autoencoder).to(device)\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=ft_config.FINETUNE_FULL_LR)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(ft_config.FINETUNE_EPOCHS):\n",
        "        classifier.train(); total_loss = 0\n",
        "        pbar = tqdm(finetune_loader, desc=f\"Fast Fine-tune Epoch {epoch+1}/{ft_config.FINETUNE_EPOCHS}\")\n",
        "        for before_seqs, after_seqs, labels in pbar:\n",
        "            labels = labels.to(device); optimizer.zero_grad()\n",
        "            logits = classifier(before_seqs, after_seqs).squeeze()\n",
        "            loss = loss_fn(logits, labels); loss.backward(); optimizer.step()\n",
        "            total_loss += loss.item(); pbar.set_postfix(loss=total_loss/len(pbar))\n",
        "\n",
        "    # --- Save the final, fine-tuned model ---\n",
        "    print(\"--- Saving final fine-tuned model artifacts ---\")\n",
        "    joblib.dump(ft_config, model_dir / \"config.joblib\") # Save the config used for this run\n",
        "    torch.save(classifier.state_dict(), model_dir / \"structural_break_classifier.pth\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define a specific, smaller configuration for this fast test\n",
        "    @dataclass\n",
        "    class FastTuneConfig(Config):\n",
        "        FINETUNE_EPOCHS: int = 5\n",
        "        BATCH_SIZE: int = 16\n",
        "        FINETUNE_FULL_LR: float = 3e-5\n",
        "\n",
        "    fast_config = FastTuneConfig()\n",
        "\n",
        "    # --- Create a SMALL mock dataset for this fast test ---\n",
        "    print(\"\\nCreating a small mock dataset for fast fine-tuning test...\")\n",
        "    train_X_fast, train_y_fast = create_dataset(n_samples=200)\n",
        "    test_X_fast, test_y_fast = create_dataset(n_samples=100)\n",
        "\n",
        "    # --- Run the fast fine-tuning and inference ---\n",
        "    fine_tune_only(train_X_fast, train_y_fast, config.MODEL_DIR, fast_config)\n",
        "\n",
        "    print(\"\\nInferring on the unseen test set...\")\n",
        "    # Make sure infer uses the correct, newly saved model and config\n",
        "    predictions = list(infer(test_X_fast, config.MODEL_DIR))\n",
        "\n",
        "    if predictions:\n",
        "        auc_score = roc_auc_score(test_y_fast, predictions)\n",
        "        print(\"\\n\" + \"=\"*50); print(\"      TIER 2.5 RESULTS\"); print(\"=\"*50)\n",
        "        print(f\"✅ Fast Fine-tune Out-of-Sample ROC AUC Score: {auc_score:.4f}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GcFntwOsb9oH",
        "outputId": "5998f898-0a0d-4be9-cda0-0c0e4cac39d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating a small mock dataset for fast fine-tuning test...\n",
            "\n",
            "================================================================================\n",
            "      RUNNING TIER 2.5: FAST FINE-TUNING ONLY\n",
            "================================================================================\n",
            "ERROR: Pre-trained model not found at 'adia_model_store/pretrained_autoencoder.pth'.\n",
            "Please run the full Tier 3 training (Cell 3) once first.\n",
            "\n",
            "Inferring on the unseen test set...\n",
            "--- Starting Inference Pipeline ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'adia_model_store/config.joblib'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-58-2081739079.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nInferring on the unseen test set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Make sure infer uses the correct, newly saved model and config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X_fast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-56-2815794562.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(X, model_dir)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Starting Inference Pipeline ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mloaded_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"config.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"label_map.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mn_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDL_AU_Net_Autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             with _validate_fileobject_and_memmap(f, filename, mmap_mode) as (\n\u001b[1;32m    737\u001b[0m                 \u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'adia_model_store/config.joblib'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution v8"
      ],
      "metadata": {
        "id": "zbEe09GYDFaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title CELL 1: SETUP AND GLOBAL CONFIGURATION\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Imports ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import random\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import cellpylib as cpl\n",
        "from dataclasses import dataclass, field\n",
        "import typing\n",
        "import hashlib\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# --- Main Configuration Class (for Full-Scale Runs) ---\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Reproducibility\n",
        "    SEED: int = 42\n",
        "\n",
        "    # Data Processing\n",
        "    PERMUTATION_EMBEDDING_DIM: int = 4\n",
        "    PERMUTATION_TIME_LAG: int = 1\n",
        "    SERIES_PROCESSOR_SEQUENCE_LENGTH: int = 256\n",
        "    SERIES_PROCESSOR_N_AUGMENTATIONS: int = 5\n",
        "\n",
        "    # ECA Pre-training Data Generation\n",
        "    ECA_RULES_TO_USE: typing.List[int] = field(default_factory=lambda: [\n",
        "        22, 30, 45, 54, 60, 75, 82, 86, 89, 90, 105, 106, 110,\n",
        "        122, 126, 135, 146, 149, 150, 153, 154, 161, 165, 169,\n",
        "        182, 193, 195, 225\n",
        "    ])\n",
        "    ECA_N_SAMPLES_PER_RULE: int = 100\n",
        "    ECA_TIMESTEPS: int = 256\n",
        "    ECA_WIDTH: int = 128\n",
        "\n",
        "    # Model Architecture (Full 3-Stage U-Net)\n",
        "    MODEL_DIMENSIONS: typing.List[int] = field(default_factory=lambda: [128, 256, 512])\n",
        "    MODEL_LAYERS_PER_BLOCK: typing.List[int] = field(default_factory=lambda: [2, 2, 2])\n",
        "    MODEL_N_HEADS: int = 8\n",
        "    MODEL_MAX_SEQLENS: typing.List[int] = field(default_factory=lambda: [256, 64, 16])\n",
        "    MODEL_BOTTLENECK_DIM: int = 64\n",
        "\n",
        "    # Training Hyperparameters\n",
        "    PRETRAIN_EPOCHS: int = 5\n",
        "    FINETUNE_EPOCHS: int = 10\n",
        "    BATCH_SIZE: int = 32\n",
        "    PRETRAIN_LR: float = 1e-4\n",
        "    FINETUNE_FULL_LR: float = 1e-5 # Lower LR for fine-tuning the whole model\n",
        "    MDL_RECON_LOSS_ALPHA: float = 1.0\n",
        "    MDL_RULE_LOSS_BETA: float = 0.5\n",
        "\n",
        "    # Paths\n",
        "    MODEL_DIR: Path = field(default_factory=lambda: Path(\"./adia_model_store\"))\n",
        "\n",
        "# --- Global Functions & Instantiation ---\n",
        "config = Config()\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(config.SEED)\n",
        "config.MODEL_DIR.mkdir(exist_ok=True, parents=True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Global configuration set for a full-scale run.\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4pOwpsIcSH93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title CELL 2: CORE LIBRARY AND PIPELINE LOGIC (with Bug Fix)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MODULE 1: core_library/data_processing.py\n",
        "# ------------------------------------------------------------------------------\n",
        "import itertools # Make sure this is imported at the top of the cell\n",
        "\n",
        "class PermutationSymbolizer:\n",
        "    def __init__(self, embedding_dim, time_lag):\n",
        "        self.d = embedding_dim\n",
        "        self.tau = time_lag\n",
        "        # Correctly use itertools.permutations\n",
        "        self.permutations = {\n",
        "            tuple(p): i for i, p in enumerate(itertools.permutations(range(self.d)))\n",
        "        }\n",
        "\n",
        "    def symbolize_vector(self, vector: np.ndarray) -> int:\n",
        "        hasher = hashlib.sha256(vector.tobytes())\n",
        "        seed = int.from_bytes(hasher.digest(), 'big') % (2**32)\n",
        "        rng = np.random.default_rng(seed)\n",
        "        noisy_vector = vector + rng.normal(0, 1e-9, size=vector.shape)\n",
        "        return self.permutations[tuple(np.argsort(noisy_vector))]\n",
        "\n",
        "class SeriesProcessor:\n",
        "    def __init__(self, symbolizer, sequence_length, n_augmentations):\n",
        "        self.symbolizer = symbolizer\n",
        "        self.seq_len = sequence_length\n",
        "        self.d = symbolizer.d\n",
        "        self.n_aug = n_augmentations\n",
        "\n",
        "    def process_for_finetune(self, series: pd.Series) -> np.ndarray:\n",
        "        if len(series) < self.d:\n",
        "            return np.array([])\n",
        "\n",
        "        view_shape = (len(series) - self.d + 1, self.d)\n",
        "        view_strides = (series.values.strides[0], series.values.strides[0])\n",
        "        windows = np.lib.stride_tricks.as_strided(series.values, shape=view_shape, strides=view_strides)\n",
        "\n",
        "        symbols = np.array([self.symbolizer.symbolize_vector(w) for w in windows])\n",
        "\n",
        "        if len(symbols) < self.seq_len:\n",
        "            return np.array([])\n",
        "\n",
        "        sequences = []\n",
        "        for i in range(self.n_aug):\n",
        "            start_idx = np.random.randint(0, len(symbols) - self.seq_len + 1)\n",
        "            sequences.append(symbols[start_idx:start_idx + self.seq_len])\n",
        "        return np.array(sequences)\n",
        "\n",
        "class ECADataGenerator:\n",
        "    def __init__(self, eca_config, n_samples_per_rule, timesteps, width):\n",
        "        self.config = eca_config\n",
        "        self.n_samples = n_samples_per_rule\n",
        "        self.timesteps = timesteps\n",
        "        self.width = width\n",
        "\n",
        "    def _generate_for_rule(self, rule_info):\n",
        "        # ### --- THIS IS THE CORRECTED LINE --- ###\n",
        "        is_composite = isinstance(rule_info, dict)\n",
        "\n",
        "        rule_label = str(rule_info['rules']) if is_composite else str(rule_info)\n",
        "\n",
        "        ca_list = []\n",
        "        for _ in range(self.n_samples):\n",
        "            init_cond = cpl.init_random(self.width)\n",
        "            if is_composite:\n",
        "                ca = cpl.evolve_ca_chain(init_cond, self.timesteps, rule_info['rules'], rule_info['timesteps'])\n",
        "            else: # It's a base rule (an integer)\n",
        "                ca = cpl.evolve(init_cond, self.timesteps, lambda n, c, t: cpl.nks_rule(n, rule_info))\n",
        "            ca_list.append(ca)\n",
        "        return np.array(ca_list), [rule_label] * self.n_samples\n",
        "\n",
        "    def generate_training_data(self):\n",
        "        all_cas, all_labels_str = [], []\n",
        "\n",
        "        base_rules = self.config.get('base', [])\n",
        "        for rule in tqdm(base_rules, desc=\"Generating Base ECAs\"):\n",
        "            cas, labels = self._generate_for_rule(rule)\n",
        "            all_cas.append(cas)\n",
        "            all_labels_str.extend(labels)\n",
        "\n",
        "        composite_rules = self.config.get('composite', [])\n",
        "        for i, rule_info in enumerate(tqdm(composite_rules, desc=\"Generating Composite ECAs\")):\n",
        "            cas, labels = self._generate_for_rule(rule_info)\n",
        "            all_cas.append(cas)\n",
        "            all_labels_str.extend(labels)\n",
        "\n",
        "        unique_labels = sorted(list(set(all_labels_str)))\n",
        "        label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "        if not all_cas: # Handle case where no rules are specified\n",
        "            return torch.empty(0, self.timesteps, self.width), torch.empty(0, dtype=torch.long), {}\n",
        "\n",
        "        final_cas = np.vstack(all_cas)\n",
        "        final_labels = np.array([label_map[l] for l in all_labels_str])\n",
        "\n",
        "        return torch.from_numpy(final_cas).float(), torch.from_numpy(final_labels).long(), label_map\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MODULE 2: core_library/model_architecture.py\n",
        "# ------------------------------------------------------------------------------\n",
        "import math\n",
        "\n",
        "class CausalTransformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=dim*4, batch_first=True, activation='gelu')\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, dim)\n",
        "        self.register_buffer('causal_mask', ~torch.ones(max_seq_len, max_seq_len, dtype=torch.bool).triu(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        n, seq_len, _ = x.shape\n",
        "        pos = torch.arange(seq_len, device=x.device)\n",
        "        x = x + self.pos_emb(pos)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask=self.causal_mask[:seq_len, :seq_len])\n",
        "        return x\n",
        "\n",
        "class SimpleTransition(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, factor, is_up):\n",
        "        super().__init__()\n",
        "        self.is_up = is_up\n",
        "        self.factor = factor\n",
        "        self.proj = nn.Linear(dim_in, dim_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.is_up:\n",
        "            x = x.repeat_interleave(self.factor, dim=1)\n",
        "        else:\n",
        "            x = x[:, ::self.factor, :]\n",
        "        return self.proj(x)\n",
        "\n",
        "class HierarchicalDynamicalEncoder(nn.Module):\n",
        "    def __init__(self, dims, depths, heads, max_seqlens):\n",
        "        super().__init__()\n",
        "        self.levels = nn.ModuleList()\n",
        "        for i in range(len(dims) - 1):\n",
        "            self.levels.append(nn.ModuleList([\n",
        "                CausalTransformer(dims[i], depths[i], heads, max_seqlens[i]),\n",
        "                SimpleTransition(dims[i], dims[i+1], max_seqlens[i] // max_seqlens[i+1], is_up=False)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        residuals = []\n",
        "        for transformer, transition in self.levels:\n",
        "            x = transformer(x)\n",
        "            residuals.append(x)\n",
        "            x = transition(x)\n",
        "        return x, residuals\n",
        "\n",
        "class HierarchicalDynamicalDecoder(nn.Module):\n",
        "    def __init__(self, dims, depths, heads, max_seqlens):\n",
        "        super().__init__()\n",
        "        self.levels = nn.ModuleList()\n",
        "        self.final_transformer = CausalTransformer(dims[0], depths[0], heads, max_seqlens[0])\n",
        "        for i in range(len(dims) - 1, 0, -1):\n",
        "            self.levels.append(nn.ModuleList([\n",
        "                SimpleTransition(dims[i], dims[i-1], max_seqlens[i-1] // max_seqlens[i], is_up=True),\n",
        "                CausalTransformer(dims[i-1], depths[i-1], heads, max_seqlens[i-1])\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, residuals):\n",
        "        for (transition, transformer), res in zip(self.levels, residuals[::-1]):\n",
        "            x = transition(x)\n",
        "            x = x + res\n",
        "            x = transformer(x)\n",
        "        return self.final_transformer(x)\n",
        "\n",
        "class MDL_AU_Net_Autoencoder(nn.Module):\n",
        "    def __init__(self, model_cfg, data_cfg, n_rules):\n",
        "        super().__init__()\n",
        "        dims = model_cfg.MODEL_DIMENSIONS\n",
        "        n_symbols = np.math.factorial(data_cfg.PERMUTATION_EMBEDDING_DIM)\n",
        "        self.embedding = nn.Embedding(n_symbols, dims[0])\n",
        "        self.encoder = HierarchicalDynamicalEncoder(dims, model_cfg.MODEL_LAYERS_PER_BLOCK, model_cfg.MODEL_N_HEADS, model_cfg.MODEL_MAX_SEQLENS)\n",
        "        self.bottleneck = CausalTransformer(dims[-1], 1, model_cfg.MODEL_N_HEADS, model_cfg.MODEL_MAX_SEQLENS[-1])\n",
        "        self.rule_classifier = nn.Linear(dims[-1], n_rules) if n_rules > 0 else nn.Identity()\n",
        "        self.decoder = HierarchicalDynamicalDecoder(dims, model_cfg.MODEL_LAYERS_PER_BLOCK, model_cfg.MODEL_N_HEADS, model_cfg.MODEL_MAX_SEQLENS)\n",
        "        self.to_logits = nn.Linear(dims[0], data_cfg.ECA_WIDTH)\n",
        "\n",
        "    def forward_pretrain(self, x):\n",
        "        fingerprint, residuals = self.encoder(x)\n",
        "        fingerprint = self.bottleneck(fingerprint)\n",
        "        rule_logits = self.rule_classifier(fingerprint.mean(dim=1))\n",
        "        reconstructed = self.decoder(fingerprint, residuals)\n",
        "        reconstructed_logits = self.to_logits(reconstructed)\n",
        "        return reconstructed_logits, rule_logits\n",
        "\n",
        "    def encode(self, x_symbols):\n",
        "        x_embedded = self.embedding(x_symbols)\n",
        "        fingerprint, _ = self.encoder(x_embedded)\n",
        "        fingerprint = self.bottleneck(fingerprint)\n",
        "        return fingerprint.mean(dim=1)\n",
        "\n",
        "class StructuralBreakClassifier(nn.Module):\n",
        "    def __init__(self, encoder_model):\n",
        "        super().__init__()\n",
        "        self.encoder_model = encoder_model\n",
        "        fingerprint_dim = encoder_model.encoder.levels[-1][-1].proj.out_features\n",
        "        input_dim = fingerprint_dim * 3\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.LayerNorm(input_dim),\n",
        "            nn.Linear(input_dim, fingerprint_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(fingerprint_dim, 1)\n",
        "        )\n",
        "\n",
        "    def _get_fingerprint(self, sequences):\n",
        "        if sequences.shape[0] == 0:\n",
        "            return torch.zeros(self.encoder_model.encoder.levels[-1][-1].proj.out_features, device=next(self.parameters()).device)\n",
        "        fingerprints = self.encoder_model.encode(sequences)\n",
        "        return fingerprints.mean(dim=0)\n",
        "\n",
        "    def forward(self, before_seqs_batch, after_seqs_batch):\n",
        "        batch_logits = []\n",
        "        for before_seqs, after_seqs in zip(before_seqs_batch, after_seqs_batch):\n",
        "            fp_before = self._get_fingerprint(torch.from_numpy(before_seqs).long().to(next(self.parameters()).device))\n",
        "            fp_after = self._get_fingerprint(torch.from_numpy(after_seqs).long().to(next(self.parameters()).device))\n",
        "            combined = torch.cat([fp_before, fp_after, torch.abs(fp_before - fp_after)], dim=0)\n",
        "            batch_logits.append(self.classifier_head(combined))\n",
        "        return torch.stack(batch_logits)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MODULE 3 & 4: Training, Inference, and Platform Entry Points\n",
        "# ------------------------------------------------------------------------------\n",
        "class FineTuneDataset(Dataset):\n",
        "    def __init__(self, X, y, processor):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.X.iloc[idx]\n",
        "        label = self.y.iloc[idx]\n",
        "        series = row['series']\n",
        "        break_point = row['period'] - 1\n",
        "\n",
        "        series_before = series.iloc[:break_point]\n",
        "        series_after = series.iloc[break_point:]\n",
        "\n",
        "        before_sequences = self.processor.process_for_finetune(series_before)\n",
        "        after_sequences = self.processor.process_for_finetune(series_after)\n",
        "\n",
        "        return before_sequences, after_sequences, label\n",
        "\n",
        "def finetune_collate_fn(batch):\n",
        "    before_batch, after_batch, labels_batch = zip(*batch)\n",
        "    labels_tensor = torch.tensor(labels_batch, dtype=torch.float32)\n",
        "    return list(before_batch), list(after_batch), labels_tensor\n",
        "\n",
        "def train(X, y, model_dir):\n",
        "    print(\"--- Starting Training Pipeline ---\")\n",
        "    model_dir = Path(model_dir)\n",
        "    model_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # --- Stage 1: MDL Pre-training ---\n",
        "    print(\"--- Starting Stage 1: Pre-training ---\")\n",
        "    eca_gen = ECADataGenerator(\n",
        "        eca_config={'base': config.ECA_RULES_TO_USE, 'composite':[]},\n",
        "        n_samples_per_rule=config.ECA_N_SAMPLES_PER_RULE,\n",
        "        timesteps=config.ECA_TIMESTEPS,\n",
        "        width=config.ECA_WIDTH\n",
        "    )\n",
        "    eca_tensors, eca_labels, label_map = eca_gen.generate_training_data()\n",
        "    n_rules = len(label_map)\n",
        "    pretrain_dataset = TensorDataset(eca_tensors, eca_labels)\n",
        "    pretrain_loader = DataLoader(pretrain_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    autoencoder = MDL_AU_Net_Autoencoder(config, config, n_rules).to(device)\n",
        "    eca_input_proj = nn.Linear(config.ECA_WIDTH, config.MODEL_DIMENSIONS[0]).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(list(autoencoder.parameters()) + list(eca_input_proj.parameters()), lr=config.PRETRAIN_LR)\n",
        "    recon_loss_fn = nn.BCEWithLogitsLoss()\n",
        "    rule_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(config.PRETRAIN_EPOCHS):\n",
        "        autoencoder.train()\n",
        "        total_loss, recon_L, rule_L = 0, 0, 0\n",
        "        pbar = tqdm(pretrain_loader, desc=f\"Pre-train Epoch {epoch+1}/{config.PRETRAIN_EPOCHS}\")\n",
        "        for sequences, labels in pbar:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            projected_sequences = eca_input_proj(sequences)\n",
        "            recon_logits, rule_logits = autoencoder.forward_pretrain(projected_sequences)\n",
        "\n",
        "            loss_recon = recon_loss_fn(recon_logits, sequences > 0.5)\n",
        "            loss_rule = rule_loss_fn(rule_logits, labels)\n",
        "\n",
        "            loss = config.MDL_RECON_LOSS_ALPHA * loss_recon + config.MDL_RULE_LOSS_BETA * loss_rule\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            recon_L += loss_recon.item()\n",
        "            rule_L += loss_rule.item()\n",
        "            pbar.set_postfix(loss=total_loss/len(pbar), recon_L=recon_L/len(pbar), rule_L=rule_L/len(pbar))\n",
        "\n",
        "    # --- Stage 2: Fine-tuning ---\n",
        "    print(\"--- Starting Stage 2: Fine-tuning ---\")\n",
        "    symbolizer = PermutationSymbolizer(config.PERMUTATION_EMBEDDING_DIM, config.PERMUTATION_TIME_LAG)\n",
        "    processor = SeriesProcessor(symbolizer, config.SERIES_PROCESSOR_SEQUENCE_LENGTH, config.SERIES_PROCESSOR_N_AUGMENTATIONS)\n",
        "    finetune_dataset = FineTuneDataset(X, y, processor)\n",
        "    finetune_loader = DataLoader(finetune_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=finetune_collate_fn)\n",
        "\n",
        "    classifier = StructuralBreakClassifier(autoencoder).to(device)\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=config.FINETUNE_FULL_LR)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(config.FINETUNE_EPOCHS):\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(finetune_loader, desc=f\"Fine-tune Epoch {epoch+1}/{config.FINETUNE_EPOCHS}\")\n",
        "        for before_seqs, after_seqs, labels in pbar:\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = classifier(before_seqs, after_seqs).squeeze()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix(loss=total_loss/len(pbar))\n",
        "\n",
        "    # --- Stage 3: Save Artifacts ---\n",
        "    print(\"--- Saving final model artifacts ---\")\n",
        "    joblib.dump(config, model_dir / \"config.joblib\")\n",
        "    torch.save(classifier.state_dict(), model_dir / \"structural_break_classifier.pth\")\n",
        "\n",
        "def infer(X, model_dir):\n",
        "    print(\"--- Starting Inference Pipeline ---\")\n",
        "    model_dir = Path(model_dir)\n",
        "\n",
        "    loaded_config = joblib.load(model_dir / \"config.joblib\")\n",
        "\n",
        "    # Check if any rules were used to determine n_rules\n",
        "    if loaded_config.ECA_RULES_TO_USE:\n",
        "        # A bit of a hack to get n_rules, assuming label_map would be based on this\n",
        "        n_rules = len(loaded_config.ECA_RULES_TO_USE)\n",
        "    else:\n",
        "        n_rules = 0\n",
        "\n",
        "    autoencoder = MDL_AU_Net_Autoencoder(loaded_config, loaded_config, n_rules)\n",
        "    model = StructuralBreakClassifier(autoencoder)\n",
        "    model.load_state_dict(torch.load(model_dir / \"structural_break_classifier.pth\"))\n",
        "    model.to(device).eval()\n",
        "\n",
        "    symbolizer = PermutationSymbolizer(loaded_config.PERMUTATION_EMBEDDING_DIM, loaded_config.PERMUTATION_TIME_LAG)\n",
        "    processor = SeriesProcessor(symbolizer, loaded_config.SERIES_PROCESSOR_SEQUENCE_LENGTH, loaded_config.SERIES_PROCESSOR_N_AUGMENTATIONS)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, row in tqdm(X.iterrows(), total=len(X), desc=\"Inferring\"):\n",
        "            series_data = row['series']\n",
        "            break_point = row['period'] - 1\n",
        "            series_before = series_data.iloc[:break_point]\n",
        "            series_after = series_data.iloc[break_point:]\n",
        "\n",
        "            before_sequences = processor.process_for_finetune(series_before)\n",
        "            after_sequences = processor.process_for_finetune(series_after)\n",
        "\n",
        "            # The model forward pass expects a list of numpy arrays, not a single one\n",
        "            logits = model([before_sequences], [after_sequences])\n",
        "            score = torch.sigmoid(logits).item()\n",
        "            yield score"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YTkTNTTSTSdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title CELL 3: EXPERIMENT 3 - LARGE-SCALE TRAINING SIMULATION\n",
        "# ==============================================================================\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"      RUNNING EXPERIMENT 3: LARGE-SCALE TRAINING SIMULATION\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"New Insight: The true training dataset is massive.\")\n",
        "    print(\"Hypothesis: Previous failure (AUC < 0.5) was due to severe UNDER-TRAINING, not overfitting.\")\n",
        "    print(\"Action: Using the full-sized model and fine-tuning on a larger, more representative mock dataset.\")\n",
        "\n",
        "    # We use the main 'Config' class defined in Cell 1\n",
        "    print(\"\\n--- Using Full Production Configuration ---\")\n",
        "    print(f\"Model Dimensions: {config.MODEL_DIMENSIONS}\")\n",
        "    print(f\"Fine-tuning Epochs: {config.FINETUNE_EPOCHS}\")\n",
        "    print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Set seed for reproducibility of this specific experiment\n",
        "    seed_everything(config.SEED)\n",
        "\n",
        "    # --- Create a helper function to generate a mock dataset ---\n",
        "    def create_dataset(n_samples, has_break_prob=0.5):\n",
        "        X_list, y_list = [], []\n",
        "        for i in range(n_samples):\n",
        "            has_break = np.random.rand() < has_break_prob\n",
        "            length = np.random.randint(500, 1500)\n",
        "            break_point = np.random.randint(int(length * 0.3), int(length * 0.7))\n",
        "            t = np.linspace(0, np.random.uniform(5, 15), length)\n",
        "            noise = np.random.randn(length) * 0.1\n",
        "\n",
        "            series = pd.Series(np.sin(t * 2 * np.pi) + noise, name='value')\n",
        "            if has_break:\n",
        "                t2_freq = np.random.uniform(3, 8)\n",
        "                t2_amp = np.random.uniform(1.2, 2.0)\n",
        "                series.iloc[break_point:] = pd.Series(np.cos(t[break_point:] * t2_freq * np.pi) * t2_amp + noise[break_point:])\n",
        "\n",
        "            X_list.append({'series': series, 'period': break_point + 1})\n",
        "            y_list.append(1 if has_break else 0)\n",
        "        return pd.DataFrame(X_list), pd.Series(y_list)\n",
        "\n",
        "    # --- Create a LARGE mock dataset to simulate the real training data ---\n",
        "    print(\"\\nCreating a large mock dataset for fine-tuning...\")\n",
        "    # Create 1000 samples for fine-tuning\n",
        "    train_X, train_y = create_dataset(n_samples=1000)\n",
        "    # And a separate, unseen test set to evaluate on\n",
        "    test_X, test_y = create_dataset(n_samples=200)\n",
        "    print(\"Mock datasets created.\")\n",
        "    print(f\"Training set size: {len(train_X)}, Test set size: {len(test_X)}\")\n",
        "\n",
        "\n",
        "    # --- Run Training and Inference ---\n",
        "    try:\n",
        "        # Check for GPU\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"\\n✅ Found and using GPU for this test.\\n\")\n",
        "        else:\n",
        "            print(\"\\n⚠️ WARNING: GPU not found. This test will be very slow on CPU.\\n\")\n",
        "\n",
        "        # Train the FULL model on the LARGE training data\n",
        "        train(train_X, train_y, config.MODEL_DIR)\n",
        "\n",
        "        # Infer on the unseen test data\n",
        "        print(\"\\nInferring on the unseen test set...\")\n",
        "        predictions = list(infer(test_X, config.MODEL_DIR))\n",
        "\n",
        "        # --- Evaluate Performance ---\n",
        "        auc_score = roc_auc_score(test_y, predictions)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"      EXPERIMENT 3 RESULTS\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"✅ Final Out-of-Sample ROC AUC Score: {auc_score:.4f}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        if auc_score > 0.6:\n",
        "            print(\"\\nConclusion: SUCCESS! The model generalizes when given enough data and training time.\")\n",
        "        elif auc_score > 0.5:\n",
        "             print(\"\\nConclusion: PARTIAL SUCCESS. The model shows positive learning but needs more tuning.\")\n",
        "        else:\n",
        "            print(\"\\nConclusion: FAILED. The issue may be more fundamental (e.g., pre-training features not useful).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR during validation run: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JcMoUAZ1SXqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution v7"
      ],
      "metadata": {
        "id": "Gg6vH0ivCUOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title COMPLETE AND CONSOLIDATED SOLUTION\n",
        "# This cell contains all code required to run the validation test.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 1: SETUP AND CONFIGURATION\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Ensure cellpylib is installed ---\n",
        "try:\n",
        "    import cellpylib as cpl\n",
        "except ImportError:\n",
        "    print(\"Installing cellpylib...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"cellpylib\"])\n",
        "    import cellpylib as cpl\n",
        "    print(\"✅ cellpylib installed successfully.\")\n",
        "\n",
        "# --- Standard Imports ---\n",
        "import os\n",
        "import random\n",
        "import hashlib\n",
        "import typing\n",
        "import math\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Global Configuration Class ---\n",
        "@dataclass\n",
        "class Config:\n",
        "    SEED: int = 42\n",
        "    # Data Processing\n",
        "    PERMUTATION_DIM: int = 5\n",
        "    PERMUTATION_LAG: int = 1\n",
        "    SERIES_PROCESSOR_SEQUENCE_LENGTH: int = 256\n",
        "    SERIES_PROCESSOR_N_SEQUENCES_PER_SEGMENT: int = 10\n",
        "\n",
        "    ECA_RULES_TO_USE: list = field(default_factory=lambda: [\n",
        "        22, 30, 45, 54, 60, 75, 82, 86, 89, 90, 105, 106, 110,\n",
        "        122, 126, 135, 146, 149, 150, 153, 154, 161, 165, 169,\n",
        "        182, 193, 195, 225\n",
        "    ])\n",
        "\n",
        "    ECA_CONFIG: dict = field(init=False, repr=False)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.ECA_CONFIG = {\n",
        "            'base': self.ECA_RULES_TO_USE,\n",
        "            'composite': [\n",
        "                {'rules': [30, 110], 'timesteps': [10, 10]},\n",
        "                {'rules': [45, 90, 150], 'timesteps': [5, 5, 5]},\n",
        "                {'rules': [54, 146], 'timesteps': [15, 5]},\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    ECA_N_SAMPLES_PER_RULE: int = 100\n",
        "    ECA_TIMESTEPS: int = 64\n",
        "    ECA_WIDTH: int = 64\n",
        "    # Model Architecture\n",
        "    MODEL_DIMENSIONS: typing.List[int] = field(default_factory=lambda: [128, 256, 512])\n",
        "    MODEL_LAYERS_PER_BLOCK: typing.List[int] = field(default_factory=lambda: [2, 2, 2])\n",
        "    MODEL_MAX_SEQLENS: typing.List[int] = field(default_factory=lambda: [256, 64, 16])\n",
        "    MODEL_N_HEADS: int = 4\n",
        "    MODEL_BOTTLENECK_DIM: int = 32\n",
        "    # Training Stages\n",
        "    PRETRAIN_EPOCHS: int = 5\n",
        "    PRETRAIN_LEARNING_RATE: float = 1e-4\n",
        "    MDL_RECON_LOSS_WEIGHT: float = 1.0\n",
        "    MDL_RULE_LOSS_WEIGHT: float = 0.5\n",
        "    EMBEDDING_PRETRAIN_EPOCHS: int = 5\n",
        "    EMBEDDING_PRETRAIN_LR: float = 1e-3\n",
        "    FINETUNE_EPOCHS: int = 10\n",
        "    FINETUNE_HEAD_ONLY_EPOCHS: int = 3\n",
        "    FINETUNE_HEAD_LR: float = 1e-3\n",
        "    FINETUNE_FULL_LR: float = 5e-5\n",
        "    # General\n",
        "    BATCH_SIZE: int = 32\n",
        "    MODEL_DIR: Path = Path(\"./production_model\")\n",
        "\n",
        "# --- Seeder Function ---\n",
        "def seed_everything(seed_value: int):\n",
        "    random.seed(seed_value)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- Initial Setup ---\n",
        "config = Config()\n",
        "seed_everything(config.SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Global configuration set. Using device: {device}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 2: CORE LIBRARY - DATA PROCESSING\n",
        "# ==============================================================================\n",
        "\n",
        "class PermutationSymbolizer:\n",
        "    def __init__(self, dim, lag):\n",
        "        self.d = dim\n",
        "        self.tau = lag\n",
        "        self.permutations = {tuple(p): i for i, p in enumerate(itertools.permutations(range(dim)))}\n",
        "\n",
        "    def symbolize_vector(self, vector: np.ndarray) -> int:\n",
        "        hasher = hashlib.sha256(vector.tobytes())\n",
        "        seed = int.from_bytes(hasher.digest(), 'big') % (2**32)\n",
        "        local_rand = np.random.RandomState(seed)\n",
        "        noisy_vector = vector + local_rand.uniform(0, 1e-8, size=vector.shape)\n",
        "        return self.permutations[tuple(np.argsort(noisy_vector))]\n",
        "\n",
        "class SeriesProcessor:\n",
        "    def __init__(self, symbolizer, seq_len, n_seqs):\n",
        "        self.symbolizer = symbolizer\n",
        "        self.seq_len = seq_len\n",
        "        self.n_seqs = n_seqs\n",
        "        self.d = symbolizer.d\n",
        "        self.tau = symbolizer.tau\n",
        "\n",
        "    def _series_to_windows(self, series: np.ndarray):\n",
        "        shape = (series.shape[0] - (self.d - 1) * self.tau, self.d)\n",
        "        strides = (series.strides[0], series.strides[0] * self.tau)\n",
        "        return np.lib.stride_tricks.as_strided(series, shape=shape, strides=strides)\n",
        "\n",
        "    def process_segment(self, segment: pd.Series) -> typing.List[np.ndarray]:\n",
        "        if len(segment) < self.d * self.tau: return []\n",
        "        windows = self._series_to_windows(segment.values)\n",
        "        symbols = np.apply_along_axis(self.symbolizer.symbolize_vector, 1, windows)\n",
        "        if len(symbols) < self.seq_len: return []\n",
        "        num_sequences = min(self.n_seqs, len(symbols) - self.seq_len + 1)\n",
        "        indices = np.linspace(0, len(symbols) - self.seq_len, num_sequences, dtype=int)\n",
        "        return [symbols[i:i+self.seq_len] for i in indices]\n",
        "\n",
        "class ECADataGenerator:\n",
        "    def __init__(self, base, composite, n_samples_per_rule, timesteps, width):\n",
        "        self.config = {'base': base, 'composite': composite}\n",
        "        self.n_samples_per_rule = n_samples_per_rule\n",
        "        self.timesteps = timesteps\n",
        "        self.width = width\n",
        "\n",
        "    def run(self):\n",
        "        all_sims, all_labels = [], []\n",
        "        rule_map = {rule: i for i, rule in enumerate(self.config['base'])}\n",
        "        print(f\"Generating synthetic data for {len(self.config['base'])} 'Edge of Chaos' rules...\")\n",
        "        for rule in tqdm(self.config['base'], desc=\"Generating Base ECA Data\"):\n",
        "            for _ in range(self.n_samples_per_rule):\n",
        "                init_cond = cpl.init_random(self.width)\n",
        "                sim = cpl.evolve(init_cond, timesteps=self.timesteps, apply_rule=lambda n, c, t: cpl.nks_rule(n, rule))\n",
        "                all_sims.append(torch.tensor(sim, dtype=torch.float32))\n",
        "                all_labels.append(torch.tensor(rule_map[rule], dtype=torch.long))\n",
        "        return all_sims, all_labels\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 3: CORE LIBRARY - MODEL ARCHITECTURE\n",
        "# ==============================================================================\n",
        "class CausalTransformer(nn.Module):\n",
        "    def __init__(self, dim, n_heads, ff_mult, n_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=dim, nhead=n_heads, dim_feedforward=dim*ff_mult,\n",
        "                                       activation='gelu', batch_first=True, norm_first=True)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "    def forward(self, x, mask): return self.layers[0](x, mask)\n",
        "\n",
        "class SimpleTransition(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, seq_len_in, seq_len_out):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(in_dim, out_dim)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(seq_len_out)\n",
        "        self.unpool = nn.Upsample(size=seq_len_in, mode='nearest')\n",
        "    def down(self, x): return self.pool(x.transpose(1, 2)).transpose(1, 2)\n",
        "    def up(self, x): return self.unpool(x.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "class HierarchicalDynamicalEncoder(nn.Module):\n",
        "    def __init__(self, dims, layers, n_heads, max_seqlens):\n",
        "        super().__init__()\n",
        "        self.levels = nn.ModuleList()\n",
        "        for i in range(len(dims) - 1):\n",
        "            self.levels.append(nn.ModuleDict({\n",
        "                'block': CausalTransformer(dims[i], n_heads, 4, layers[i]),\n",
        "                'transition': SimpleTransition(dims[i], dims[i+1], max_seqlens[i], max_seqlens[i+1])\n",
        "            }))\n",
        "    def forward(self, x):\n",
        "        residuals = []\n",
        "        for level in self.levels:\n",
        "            mask = nn.Transformer.generate_square_subsequent_mask(x.shape[1]).to(x.device)\n",
        "            x = level['block'](x, mask)\n",
        "            residuals.append(x)\n",
        "            x = level['transition'].proj(x)\n",
        "            x = level['transition'].down(x)\n",
        "        return x, residuals\n",
        "\n",
        "class HierarchicalDynamicalDecoder(nn.Module):\n",
        "    def __init__(self, dims, layers, n_heads, max_seqlens):\n",
        "        super().__init__()\n",
        "        self.levels = nn.ModuleList()\n",
        "        for i in range(len(dims) - 1, 0, -1):\n",
        "            self.levels.append(nn.ModuleDict({\n",
        "                'transition': SimpleTransition(dims[i], dims[i-1], max_seqlens[i-1], max_seqlens[i]),\n",
        "                'block': CausalTransformer(dims[i-1], n_heads, 4, layers[i-1])\n",
        "            }))\n",
        "    def forward(self, x, residuals):\n",
        "        for i, level in enumerate(self.levels):\n",
        "            x = level['transition'].up(x)\n",
        "            x = level['transition'].proj(x)\n",
        "            x = x + residuals[-(i+1)]\n",
        "            mask = nn.Transformer.generate_square_subsequent_mask(x.shape[1]).to(x.device)\n",
        "            x = level['block'](x, mask)\n",
        "        return x\n",
        "\n",
        "class MDL_AU_Net_Autoencoder(nn.Module):\n",
        "    def __init__(self, m_config):\n",
        "        super().__init__()\n",
        "        self.n_symbols = math.factorial(config.PERMUTATION_DIM)\n",
        "        self.embedding = nn.Embedding(self.n_symbols, m_config.MODEL_DIMENSIONS[0])\n",
        "        self.eca_proj = nn.Linear(config.ECA_WIDTH, m_config.MODEL_DIMENSIONS[0])\n",
        "        self.encoder = HierarchicalDynamicalEncoder(m_config.MODEL_DIMENSIONS, m_config.MODEL_LAYERS_PER_BLOCK, m_config.MODEL_N_HEADS, m_config.MODEL_MAX_SEQLENS)\n",
        "        self.bottleneck = CausalTransformer(m_config.MODEL_DIMENSIONS[-1], m_config.MODEL_N_HEADS, 4, 1)\n",
        "        self.decoder = HierarchicalDynamicalDecoder(m_config.MODEL_DIMENSIONS, m_config.MODEL_LAYERS_PER_BLOCK, m_config.MODEL_N_HEADS, m_config.MODEL_MAX_SEQLENS)\n",
        "        self.recon_head = nn.Linear(m_config.MODEL_DIMENSIONS[0], config.ECA_WIDTH)\n",
        "        self.rule_head = nn.Linear(m_config.MODEL_DIMENSIONS[-1], len(config.ECA_RULES_TO_USE))\n",
        "\n",
        "    def pretrain_forward(self, x):\n",
        "        x = self.eca_proj(x)\n",
        "        fingerprint, residuals = self.encoder(x)\n",
        "        fingerprint = self.bottleneck(fingerprint, None)\n",
        "        rule_logits = self.rule_head(fingerprint.mean(dim=1))\n",
        "        reconstructed = self.decoder(fingerprint, residuals)\n",
        "        recon_logits = self.recon_head(reconstructed)\n",
        "        return recon_logits, rule_logits\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.embedding(x)\n",
        "        fingerprint, _ = self.encoder(x)\n",
        "        fingerprint = self.bottleneck(fingerprint, None)\n",
        "        return fingerprint.mean(dim=1)\n",
        "\n",
        "class StructuralBreakClassifier(nn.Module):\n",
        "    def __init__(self, encoder_model):\n",
        "        super().__init__()\n",
        "        self.encoder_model = encoder_model\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.LayerNorm(config.MODEL_BOTTLENECK_DIM * 3),\n",
        "            nn.Linear(config.MODEL_BOTTLENECK_DIM * 3, config.MODEL_BOTTLENECK_DIM), nn.GELU(),\n",
        "            nn.Linear(config.MODEL_BOTTLENECK_DIM, 1)\n",
        "        )\n",
        "    def forward(self, before_seqs, after_seqs):\n",
        "        fp_before = self.encoder_model.encode(before_seqs)\n",
        "        fp_after = self.encoder_model.encode(after_seqs)\n",
        "        combined = torch.cat([fp_before, fp_after, torch.abs(fp_before - fp_after)], dim=1)\n",
        "        return self.classifier_head(combined)\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 4: PIPELINE LOGIC - TRAINING & ARTIFACTS\n",
        "# ==============================================================================\n",
        "\n",
        "class MDLPreTrainer:\n",
        "    def __init__(self, model, data_config, model_config):\n",
        "        self.model = model\n",
        "        self.data_config = data_config\n",
        "        self.model_config = model_config\n",
        "        self.data_generator = ECADataGenerator(**data_config.ECA_CONFIG, n_samples_per_rule=data_config.ECA_N_SAMPLES_PER_RULE, timesteps=data_config.ECA_TIMESTEPS, width=data_config.ECA_WIDTH)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.model_config.PRETRAIN_LEARNING_RATE)\n",
        "        self.recon_loss_fn = nn.BCEWithLogitsLoss()\n",
        "        self.rule_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def run(self):\n",
        "        print(\"--- Starting Stage 1: MDL Pre-training ---\")\n",
        "        sims, labels = self.data_generator.run()\n",
        "        dataset = TensorDataset(torch.stack(sims), torch.stack(labels))\n",
        "        loader = DataLoader(dataset, batch_size=self.model_config.BATCH_SIZE, shuffle=True)\n",
        "        for epoch in range(self.model_config.PRETRAIN_EPOCHS):\n",
        "            self.model.train()\n",
        "            pbar = tqdm(loader, desc=f\"Pre-train Epoch {epoch+1}/{self.model_config.PRETRAIN_EPOCHS}\")\n",
        "            for batch_sims, batch_labels in pbar:\n",
        "                batch_sims, batch_labels = batch_sims.to(device), batch_labels.to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "                recon_logits, rule_logits = self.model.pretrain_forward(batch_sims)\n",
        "                recon_loss = self.recon_loss_fn(recon_logits, batch_sims)\n",
        "                rule_loss = self.rule_loss_fn(rule_logits, batch_labels)\n",
        "                total_loss = self.model_config.MDL_RECON_LOSS_WEIGHT * recon_loss + self.model_config.MDL_RULE_LOSS_WEIGHT * rule_loss\n",
        "                total_loss.backward()\n",
        "                self.optimizer.step()\n",
        "                pbar.set_postfix(loss=total_loss.item(), recon_L=recon_loss.item(), rule_L=rule_loss.item())\n",
        "        print(\"--- Pre-training Complete ---\")\n",
        "\n",
        "class BreakFinetuningDataset(Dataset):\n",
        "    def __init__(self, X, y, processor):\n",
        "        self.y = y\n",
        "        self.series_processor = processor\n",
        "        print(\"Processing real-world data into symbolic sequences...\")\n",
        "        self.processed_data = []\n",
        "        for i, row in tqdm(X.iterrows(), total=len(X), desc=\"Processing All Series\"):\n",
        "            series_data = row['value']\n",
        "            break_point = row['period'] - 1\n",
        "            before_segment, after_segment = series_data.iloc[:break_point], series_data.iloc[break_point:]\n",
        "            before_seqs = self.series_processor.process_segment(before_segment)\n",
        "            after_seqs = self.series_processor.process_segment(after_segment)\n",
        "            if before_seqs and after_seqs:\n",
        "                self.processed_data.append((before_seqs, after_seqs, y.iloc[i]))\n",
        "\n",
        "    def __len__(self): return len(self.processed_data)\n",
        "    def __getitem__(self, idx):\n",
        "        before_seqs, after_seqs, label = self.processed_data[idx]\n",
        "        before_seq = random.choice(before_seqs)\n",
        "        after_seq = random.choice(after_seqs)\n",
        "        return torch.tensor(before_seq, dtype=torch.long), torch.tensor(after_seq, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "class ArtifactSaver:\n",
        "    def __init__(self, model_dir):\n",
        "        self.model_dir = Path(model_dir)\n",
        "        self.model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    def save(self, model, config_to_save):\n",
        "        print(f\"--- Saving Artifacts to {self.model_dir} ---\")\n",
        "        torch.save(model.state_dict(), self.model_dir / \"final_model.pth\")\n",
        "        joblib.dump(config_to_save, self.model_dir / \"model_config.joblib\")\n",
        "        print(\"Artifacts saved.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 5: PLATFORM ENTRY POINTS\n",
        "# ==============================================================================\n",
        "\n",
        "def train(X, y, model_dir):\n",
        "    print(f\"Starting training on device: {device}\")\n",
        "    autoencoder = MDL_AU_Net_Autoencoder(config).to(device)\n",
        "    pre_trainer = MDLPreTrainer(autoencoder, config, config)\n",
        "    pre_trainer.run()\n",
        "\n",
        "    print(\"--- Starting Stage 2: Fine-tuning ---\")\n",
        "    symbolizer = PermutationSymbolizer(config.PERMUTATION_DIM, config.PERMUTATION_LAG)\n",
        "    processor = SeriesProcessor(symbolizer, config.SERIES_PROCESSOR_SEQUENCE_LENGTH, config.SERIES_PROCESSOR_N_SEQUENCES_PER_SEGMENT)\n",
        "    finetune_dataset = BreakFinetuningDataset(X, y, processor)\n",
        "    finetune_loader = DataLoader(finetune_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    classifier = StructuralBreakClassifier(autoencoder).to(device)\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=config.FINETUNE_FULL_LR)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(config.FINETUNE_EPOCHS):\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(finetune_loader, desc=f\"Fine-tune Epoch {epoch+1}/{config.FINETUNE_EPOCHS}\")\n",
        "        for before_batch, after_batch, labels_batch in pbar:\n",
        "            before_batch, after_batch, labels_batch = before_batch.to(device), after_batch.to(device), labels_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = classifier(before_batch, after_batch).squeeze()\n",
        "            loss = loss_fn(logits, labels_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "        print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(finetune_loader):.6f}\")\n",
        "\n",
        "    print(\"--- Fine-tuning Complete ---\")\n",
        "    config_to_save = {k: v for k, v in config.__dict__.items() if k != 'ECA_CONFIG'}\n",
        "    saver = ArtifactSaver(model_dir)\n",
        "    saver.save(classifier, config_to_save)\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n                TRAINING PIPELINE FINISHED\\n\" + \"=\"*60)\n",
        "\n",
        "def infer(X, model_dir):\n",
        "    model_dir = Path(model_dir)\n",
        "    loaded_config_dict = joblib.load(model_dir / \"model_config.joblib\")\n",
        "\n",
        "    @dataclass\n",
        "    class LoadedConfig:\n",
        "        pass\n",
        "    loaded_config = LoadedConfig()\n",
        "    for k, v in loaded_config_dict.items(): setattr(loaded_config, k, v)\n",
        "\n",
        "    model = StructuralBreakClassifier(MDL_AU_Net_Autoencoder(loaded_config)).to(device)\n",
        "    model.load_state_dict(torch.load(model_dir / \"final_model.pth\"))\n",
        "    model.eval()\n",
        "\n",
        "    symbolizer = PermutationSymbolizer(loaded_config.PERMUTATION_DIM, loaded_config.PERMUTATION_LAG)\n",
        "    processor = SeriesProcessor(symbolizer, loaded_config.SERIES_PROCESSOR_SEQUENCE_LENGTH, loaded_config.SERIES_PROCESSOR_N_SEQUENCES_PER_SEGMENT)\n",
        "\n",
        "    print(\"--- Starting Inference ---\")\n",
        "    yield {\"health\": \"ok\"} # Health check\n",
        "\n",
        "    for df in X:\n",
        "        series_data = df['value']\n",
        "        break_point = df['period'].iloc[0] - 1\n",
        "        before_segment, after_segment = series_data.iloc[:break_point], series_data.iloc[break_point:]\n",
        "        before_seqs = processor.process_segment(before_segment)\n",
        "        after_seqs = processor.process_segment(after_segment)\n",
        "\n",
        "        if not before_seqs or not after_seqs:\n",
        "            yield 0.5; continue\n",
        "\n",
        "        with torch.no_grad():\n",
        "            before_batch = torch.tensor(np.array(before_seqs), dtype=torch.long).to(device)\n",
        "            after_batch = torch.tensor(np.array(after_seqs), dtype=torch.long).to(device)\n",
        "            logits = model(before_batch, after_batch)\n",
        "            score = torch.sigmoid(logits).mean().item()\n",
        "        yield score\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 6: TEST EXECUTION BLOCK\n",
        "# ==============================================================================\n",
        "if __name__ == '__main__':\n",
        "    # --- Create a standard config instance and then override it ---\n",
        "    config = Config()\n",
        "\n",
        "    print(\"--- Modifying main Config for a validation run. ---\")\n",
        "    config.ECA_N_SAMPLES_PER_RULE = 20\n",
        "    config.PRETRAIN_EPOCHS = 3\n",
        "    config.EMBEDDING_PRETRAIN_EPOCHS = 3\n",
        "    config.FINETUNE_HEAD_ONLY_EPOCHS = 3\n",
        "    config.FINETUNE_EPOCHS = 5\n",
        "    config.BATCH_SIZE = 16\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"      RUNNING TIER 3 VALIDATION (Platform Compliant)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    seed_everything(config.SEED)\n",
        "\n",
        "    # --- Create Mock Data & Split ---\n",
        "    def create_mock_series_data(series_id, has_break=False, length=500, break_point=250):\n",
        "        t = np.linspace(0, 10, length)\n",
        "        noise = np.random.randn(length) * 0.1\n",
        "        series_values = np.sin(t * 2 * np.pi) + noise\n",
        "        if has_break:\n",
        "            series_values[break_point:] = np.cos(t[break_point:] * 5 * np.pi) * 1.5 + noise[break_point:]\n",
        "        data = []\n",
        "        for time_step in range(length):\n",
        "            data.append({'id': series_id, 'time': time_step, 'value': series_values[time_step]})\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    mock_X_list, mock_y_list = [], []\n",
        "    for i in range(100):\n",
        "        has_break = i % 2 == 0\n",
        "        df = create_mock_series_data(i, has_break=has_break)\n",
        "        df['period'] = 251\n",
        "        X_df = df[['value', 'period']]\n",
        "        y_df = pd.Series([1 if has_break else 0], index=[i])\n",
        "        mock_X_list.append(X_df)\n",
        "        mock_y_list.append(y_df)\n",
        "\n",
        "    X_all = pd.concat(mock_X_list, keys=range(100), names=['id', 'time'])\n",
        "    y_all = pd.concat(mock_y_list)\n",
        "\n",
        "    train_ids, test_ids = train_test_split(y_all.index, test_size=0.3, random_state=config.SEED, stratify=y_all)\n",
        "\n",
        "    X_train = X_all[X_all.index.get_level_values('id').isin(train_ids)]\n",
        "    y_train = y_all[y_all.index.isin(train_ids)]\n",
        "    X_test = X_all[X_all.index.get_level_values('id').isin(test_ids)]\n",
        "    y_test = y_all[y_all.index.isin(test_ids)]\n",
        "    print(f\"Training samples: {len(train_ids)}, Testing samples: {len(test_ids)}\")\n",
        "\n",
        "    # --- Run Training & Inference ---\n",
        "    try:\n",
        "        model_dir = Path(\"./final_model_validation\")\n",
        "        train(X_train, y_train, str(model_dir))\n",
        "\n",
        "        test_iterable = [X_test.loc[test_id] for test_id in test_ids]\n",
        "        predictions = list(infer(test_iterable, str(model_dir)))\n",
        "        if isinstance(predictions[0], dict) and \"health\" in predictions[0]:\n",
        "            predictions.pop(0)\n",
        "\n",
        "        y_test_values = y_test.values\n",
        "        auc_score = roc_auc_score(y_test_values, predictions)\n",
        "        print(\"\\n\" + \"=\"*20 + \" FINAL RESULT \" + \"=\"*20)\n",
        "        print(f\"📈 Final Out-of-Sample ROC AUC Score: {auc_score:.4f}\")\n",
        "        print(\"=\"*54)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR during full validation run: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "g5_5-FeZCWNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution v6"
      ],
      "metadata": {
        "id": "pH8EAkypvBZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title CELL 1: SETUP, CONFIGURATION, AND SEEDING (Corrected)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Ensure cellpylib is installed ---\n",
        "try:\n",
        "    import cellpylib as cpl\n",
        "    print(\"✅ cellpylib is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"Installing cellpylib...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"cellpylib\"])\n",
        "    import cellpylib as cpl\n",
        "    print(\"✅ cellpylib installed successfully.\")\n",
        "\n",
        "# --- Standard Imports ---\n",
        "import os\n",
        "import random\n",
        "import hashlib\n",
        "import typing\n",
        "import math\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# --- Global Configuration Class (with 3-Stage Training Params) ---\n",
        "@dataclass\n",
        "class Config:\n",
        "    SEED: int = 42\n",
        "    # Data Processing\n",
        "    PERMUTATION_DIM: int = 5\n",
        "    PERMUTATION_LAG: int = 1\n",
        "    SERIES_PROCESSOR_SEQUENCE_LENGTH: int = 256\n",
        "    SERIES_PROCESSOR_N_SEQUENCES_PER_SEGMENT: int = 10\n",
        "\n",
        "    # --- THIS ATTRIBUTE IS NOW GUARANTEED TO BE IN THE CLASS DEFINITION ---\n",
        "    ECA_RULES_TO_USE: list = field(default_factory=lambda: [\n",
        "        22, 30, 45, 54, 60, 75, 82, 86, 89, 90, 105, 106, 110,\n",
        "        122, 126, 135, 146, 149, 150, 153, 154, 161, 165, 169,\n",
        "        182, 193, 195, 225\n",
        "    ])\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # ECA Pre-training\n",
        "    ECA_N_SAMPLES_PER_RULE: int = 100\n",
        "    ECA_TIMESTEPS: int = 64\n",
        "    ECA_WIDTH: int = 64\n",
        "    # Model Architecture\n",
        "    MODEL_DIMENSIONS: typing.List[int] = field(default_factory=lambda: [128, 256, 512])\n",
        "    MODEL_LAYERS_PER_BLOCK: typing.List[int] = field(default_factory=lambda: [2, 2, 2])\n",
        "    MODEL_MAX_SEQLENS: typing.List[int] = field(default_factory=lambda: [256, 64, 16])\n",
        "    MODEL_N_HEADS: int = 4\n",
        "    MODEL_BOTTLENECK_DIM: int = 32\n",
        "    # Stage 1: Encoder Pre-training\n",
        "    PRETRAIN_EPOCHS: int = 5\n",
        "    PRETRAIN_LEARNING_RATE: float = 1e-4\n",
        "    MDL_RECON_LOSS_WEIGHT: float = 1.0\n",
        "    MDL_RULE_LOSS_WEIGHT: float = 0.5\n",
        "    # Stage 1.5: Embedding Pre-training\n",
        "    EMBEDDING_PRETRAIN_EPOCHS: int = 5\n",
        "    EMBEDDING_PRETRAIN_LR: float = 1e-3\n",
        "    # Stage 2: Final Fine-tuning\n",
        "    FINETUNE_EPOCHS: int = 10\n",
        "    FINETUNE_HEAD_ONLY_EPOCHS: int = 3\n",
        "    FINETUNE_HEAD_LR: float = 1e-3\n",
        "    FINETUNE_FULL_LR: float = 5e-5 # Discriminative LR for full model tuning\n",
        "    # General\n",
        "    BATCH_SIZE: int = 32\n",
        "    MODEL_DIR: Path = Path(\"./production_model\")\n",
        "\n",
        "\n",
        "# --- Seeder Function ---\n",
        "def seed_everything(seed_value: int):\n",
        "    random.seed(seed_value)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- Initial Setup ---\n",
        "config = Config()\n",
        "seed_everything(config.SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Global configuration set. Using device: {device}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WaLyLcW2AneU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title CELL 2: CORE LIBRARY (Data, Model Architecture)\n",
        "# ==============================================================================\n",
        "\n",
        "class PermutationSymbolizer:\n",
        "    def __init__(self, dim: int, lag: int):\n",
        "        self.d = dim\n",
        "        self.lag = lag\n",
        "\n",
        "    def _get_permutation_indices(self, windows: np.ndarray) -> np.ndarray:\n",
        "        hasher = hashlib.sha256(windows.tobytes())\n",
        "        seed = int.from_bytes(hasher.digest(), 'little') % (2**32 - 1)\n",
        "        rng = np.random.RandomState(seed)\n",
        "        noise = rng.uniform(low=-1e-8, high=1e-8, size=windows.shape)\n",
        "        return (windows + noise).argsort(axis=1)\n",
        "\n",
        "    def _lehmer_encode(self, permutations: np.ndarray) -> np.ndarray:\n",
        "        d = self.d\n",
        "        symbols = np.zeros(permutations.shape[0], dtype=np.int64)\n",
        "        factorials = np.array([math.factorial(d - 1 - i) for i in range(d)], dtype=np.int64)\n",
        "        temp_perms = np.copy(permutations)\n",
        "        for i in range(d):\n",
        "            rank = temp_perms[:, 0]\n",
        "            symbols += rank * factorials[i]\n",
        "            temp_perms = temp_perms[:, 1:]\n",
        "            if temp_perms.shape[1] > 0:\n",
        "                temp_perms[temp_perms > rank[:, np.newaxis]] -= 1\n",
        "        return symbols\n",
        "\n",
        "    def symbolize(self, series: np.ndarray) -> np.ndarray:\n",
        "        if len(series) < self.d * self.lag:\n",
        "            return np.array([], dtype=np.int64)\n",
        "        shape = (len(series) - (self.d - 1) * self.lag, self.d)\n",
        "        strides = (series.strides[0], series.strides[0] * self.lag)\n",
        "        windows = np.lib.stride_tricks.as_strided(series, shape=shape, strides=strides)\n",
        "        permutations = self._get_permutation_indices(windows)\n",
        "        return self._lehmer_encode(permutations)\n",
        "\n",
        "class SeriesProcessor:\n",
        "    def __init__(self, symbolizer: PermutationSymbolizer, sequence_length: int, n_sequences: int):\n",
        "        self.symbolizer = symbolizer\n",
        "        self.seq_len = sequence_length\n",
        "        self.n_seq = n_sequences\n",
        "    def process(self, series: pd.Series) -> typing.List[torch.Tensor]:\n",
        "        if series.empty or len(series) < self.symbolizer.d:\n",
        "            return []\n",
        "        symbols = self.symbolizer.symbolize(series.values)\n",
        "        if len(symbols) < self.seq_len:\n",
        "            return []\n",
        "        # Take n_seq evenly spaced subsequences\n",
        "        indices = np.linspace(0, len(symbols) - self.seq_len, self.n_seq, dtype=int)\n",
        "        sequences = [torch.LongTensor(symbols[i : i + self.seq_len]) for i in indices]\n",
        "        return sequences\n",
        "\n",
        "class ECADataGenerator:\n",
        "    def __init__(self, eca_config: dict, n_samples_per_rule: int, timesteps: int, width: int):\n",
        "        self.config = eca_config\n",
        "        self.n_samples = n_samples_per_rule\n",
        "        self.timesteps = timesteps\n",
        "        self.width = width\n",
        "        self.rule_map = self._create_rule_map()\n",
        "    def _create_rule_map(self) -> dict:\n",
        "        rule_map = {}\n",
        "        idx = 0\n",
        "        for rule_num in self.config.get('base', []):\n",
        "            rule_map[idx] = {'type': 'base', 'rule': rule_num}\n",
        "            idx += 1\n",
        "        for comp_rule in self.config.get('composite', []):\n",
        "            rule_map[idx] = {'type': 'composite', 'config': comp_rule}\n",
        "            idx += 1\n",
        "        return rule_map\n",
        "    def generate(self) -> typing.Tuple[typing.List[torch.Tensor], typing.List[torch.Tensor]]:\n",
        "        all_tensors, all_labels = [], []\n",
        "        print(f\"Generating synthetic data for {len(self.rule_map)} 'Edge of Chaos' rule configurations...\")\n",
        "        for rule_idx, rule_def in tqdm(self.rule_map.items(), desc=\"Generating ECA Data\"):\n",
        "            for _ in range(self.n_samples):\n",
        "                initial_cond = cpl.init_random(self.width)\n",
        "                if rule_def['type'] == 'base':\n",
        "                    ca = cpl.evolve(initial_cond, timesteps=self.timesteps, apply_rule=lambda n, c, t: cpl.nks_rule(n, rule_def['rule']))\n",
        "                else:\n",
        "                    comp_config = rule_def['config']\n",
        "                    ca = cpl.evolve(initial_cond, timesteps=self.timesteps, apply_rule=lambda n, c, t: cpl.nks_rule(n, comp_config['rules'][(t-1) % len(comp_config['rules'])]))\n",
        "                all_tensors.append(torch.FloatTensor(ca))\n",
        "                all_labels.append(torch.LongTensor([rule_idx]))\n",
        "        print(f\"Generated {len(all_tensors)} total ECA simulations.\")\n",
        "        return all_tensors, all_labels\n",
        "\n",
        "class CausalTransformer(nn.Module):\n",
        "    def __init__(self, model_dim: int, n_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(model_dim, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.ffn = nn.Sequential(nn.Linear(model_dim, model_dim * 4), nn.GELU(), nn.Linear(model_dim * 4, model_dim), nn.Dropout(dropout))\n",
        "        self.norm1 = nn.LayerNorm(model_dim)\n",
        "        self.norm2 = nn.LayerNorm(model_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        attn_output, _ = self.attn(x, x, x, attn_mask=mask, need_weights=False, is_causal=True) # Use causal mask\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "        return x\n",
        "\n",
        "class SimpleTransition(nn.Module):\n",
        "    def __init__(self, model_dim_in: int, model_dim_out: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(model_dim_in, model_dim_out)\n",
        "        self.pool = nn.AvgPool1d(kernel_size=4, stride=4)\n",
        "        self.upsample = nn.Upsample(scale_factor=4, mode='nearest')\n",
        "        self.norm = nn.LayerNorm(model_dim_out)\n",
        "    def down(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.pool(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.proj(x)\n",
        "        return self.norm(x)\n",
        "    def up(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.proj(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.upsample(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        return self.norm(x)\n",
        "\n",
        "class HierarchicalDynamicalEncoder(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.transitions = nn.ModuleList()\n",
        "        dims = config.MODEL_DIMENSIONS\n",
        "        for i in range(len(dims)):\n",
        "            self.layers.append(nn.ModuleList([CausalTransformer(dims[i], config.MODEL_N_HEADS) for _ in range(config.MODEL_LAYERS_PER_BLOCK[i])]))\n",
        "            if i < len(dims) - 1:\n",
        "                self.transitions.append(SimpleTransition(dims[i], dims[i+1]))\n",
        "    def forward(self, x: torch.Tensor) -> typing.List[torch.Tensor]:\n",
        "        residuals = []\n",
        "        for i, stage in enumerate(self.layers):\n",
        "            for layer in stage:\n",
        "                x = layer(x)\n",
        "            residuals.append(x)\n",
        "            if i < len(self.transitions):\n",
        "                x = self.transitions[i].down(x)\n",
        "        return residuals\n",
        "\n",
        "class HierarchicalDynamicalDecoder(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.transitions = nn.ModuleList()\n",
        "        dims = config.MODEL_DIMENSIONS\n",
        "        for i in range(len(dims) -1, -1, -1):\n",
        "            self.layers.append(nn.ModuleList([CausalTransformer(dims[i], config.MODEL_N_HEADS) for _ in range(config.MODEL_LAYERS_PER_BLOCK[i])]))\n",
        "            if i > 0:\n",
        "                self.transitions.append(SimpleTransition(dims[i], dims[i-1]))\n",
        "    def forward(self, residuals: typing.List[torch.Tensor]) -> torch.Tensor:\n",
        "        x = residuals.pop()\n",
        "        for i, stage in enumerate(self.layers):\n",
        "            for layer in stage:\n",
        "                x = layer(x)\n",
        "            if i < len(self.transitions):\n",
        "                # The residual from the encoder has shape (B, S_long, D_in)\n",
        "                # The upsampled x has shape (B, S_long, D_out)\n",
        "                # We need to project the residual before adding\n",
        "                res_to_add = residuals.pop()\n",
        "                x = self.transitions[i].up(x)\n",
        "                if x.shape[2] != res_to_add.shape[2]:\n",
        "                     # This case shouldn't happen with correct config but is a safeguard\n",
        "                     pass # or add a projection\n",
        "                x = x + res_to_add\n",
        "        return x\n",
        "\n",
        "class MDL_AU_Net_Autoencoder(nn.Module):\n",
        "    def __init__(self, config: Config, num_rules: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_symbols = math.factorial(config.PERMUTATION_DIM)\n",
        "        self.symbol_embedding_head = nn.Embedding(self.n_symbols, config.MODEL_DIMENSIONS[0])\n",
        "        self.eca_input_proj = nn.Linear(config.ECA_WIDTH, config.MODEL_DIMENSIONS[0])\n",
        "        self.encoder = HierarchicalDynamicalEncoder(config)\n",
        "        self.decoder = HierarchicalDynamicalDecoder(config)\n",
        "        self.bottleneck = nn.Linear(config.MODEL_DIMENSIONS[-1], config.MODEL_BOTTLENECK_DIM)\n",
        "        self.rule_classifier_head = nn.Linear(config.MODEL_BOTTLENECK_DIM, num_rules)\n",
        "        self.reconstruction_head = nn.Linear(config.MODEL_DIMENSIONS[0], config.ECA_WIDTH)\n",
        "    def get_fingerprint(self, symbol_sequences: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.symbol_embedding_head(symbol_sequences)\n",
        "        residuals = self.encoder(x)\n",
        "        fingerprint = residuals[-1].mean(dim=1)\n",
        "        return self.bottleneck(fingerprint)\n",
        "    def forward(self, eca_tensors: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
        "        x = self.eca_input_proj(eca_tensors)\n",
        "        residuals = self.encoder(x)\n",
        "        bottleneck_input = residuals[-1]\n",
        "        bottleneck_out = self.bottleneck(bottleneck_input.mean(dim=1))\n",
        "        rule_logits = self.rule_classifier_head(bottleneck_out)\n",
        "        decoded = self.decoder(list(residuals)) # Pass a copy\n",
        "        reconstructed_tensor = self.reconstruction_head(decoded)\n",
        "        return reconstructed_tensor, rule_logits\n",
        "\n",
        "class StructuralBreakClassifier(nn.Module):\n",
        "    def __init__(self, autoencoder: MDL_AU_Net_Autoencoder, config: Config):\n",
        "        super().__init__()\n",
        "        self.autoencoder = autoencoder\n",
        "        self.config = config\n",
        "        classifier_input_dim = config.MODEL_BOTTLENECK_DIM * 3\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.LayerNorm(classifier_input_dim),\n",
        "            nn.Linear(classifier_input_dim, config.MODEL_BOTTLENECK_DIM),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(config.MODEL_BOTTLENECK_DIM, 1)\n",
        "        )\n",
        "    def _get_fingerprint(self, sequences: typing.List[torch.Tensor]) -> torch.Tensor:\n",
        "        model_device = next(self.parameters()).device\n",
        "        if not sequences:\n",
        "            return torch.zeros(self.config.MODEL_BOTTLENECK_DIM, device=model_device)\n",
        "        batch = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0).to(model_device)\n",
        "        fingerprints = self.autoencoder.get_fingerprint(batch)\n",
        "        return fingerprints.mean(dim=0)\n",
        "    def forward(self, before_seq_list: list, after_seq_list: list) -> torch.Tensor:\n",
        "        fp_before_batch = torch.stack([self._get_fingerprint(seqs) for seqs in before_seq_list])\n",
        "        fp_after_batch = torch.stack([self._get_fingerprint(seqs) for seqs in after_seq_list])\n",
        "        fp_diff = torch.abs(fp_before_batch - fp_after_batch)\n",
        "        combined_fp = torch.cat([fp_before_batch, fp_after_batch, fp_diff], dim=1)\n",
        "        return self.classifier_head(combined_fp)\n",
        "\n",
        "class ArtifactManager:\n",
        "    def __init__(self, model_dir: Path):\n",
        "        self.model_dir = model_dir\n",
        "        self.model_dir.mkdir(exist_ok=True, parents=True)\n",
        "    def save(self, model: StructuralBreakClassifier, config: Config):\n",
        "        print(f\"Saving artifacts to {self.model_dir}...\")\n",
        "        # Save the full state dict of the final classifier model\n",
        "        torch.save(model.state_dict(), self.model_dir / \"structural_break_classifier.pth\")\n",
        "        joblib.dump(config, self.model_dir / \"config.joblib\")\n",
        "        print(\"Artifacts saved.\")\n",
        "    def load_for_inference(self, device: str) -> typing.Tuple[StructuralBreakClassifier, Config]:\n",
        "        print(f\"Loading artifacts from {self.model_dir}...\")\n",
        "        loaded_config = joblib.load(self.model_dir / \"config.joblib\")\n",
        "        # To load the state dict, we need to know the number of rules the original model was trained on\n",
        "        # This is a bit of a hack; a better way would be to save this in the config\n",
        "        # For now, we assume a placeholder value.\n",
        "        num_rules_placeholder = len(loaded_config.ECA_RULES_TO_USE) + 10 # A safe upper bound\n",
        "        autoencoder = MDL_AU_Net_Autoencoder(loaded_config, num_rules=num_rules_placeholder)\n",
        "        inference_model = StructuralBreakClassifier(autoencoder, loaded_config)\n",
        "        inference_model.load_state_dict(torch.load(self.model_dir / \"structural_break_classifier.pth\", map_location=device))\n",
        "        print(\"Artifacts loaded successfully.\")\n",
        "        return inference_model.to(device).eval(), loaded_config"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0V71szYN0Xsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title CELL 3: THE NEW 3-STAGE TRAINING PIPELINE\n",
        "# ==============================================================================\n",
        "\n",
        "class MDLPreTrainer:\n",
        "    \"\"\"Orchestrates Stage 1: Pre-training the core U-Net Encoder on raw ECA data.\"\"\"\n",
        "    def __init__(self, model: MDL_AU_Net_Autoencoder, eca_config: dict, config: Config, device: str):\n",
        "        self.model = model\n",
        "        self.eca_config = eca_config\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=config.PRETRAIN_LEARNING_RATE)\n",
        "        self.recon_loss_fn = nn.MSELoss()\n",
        "        self.rule_loss_fn = nn.CrossEntropyLoss()\n",
        "    def run(self):\n",
        "        generator = ECADataGenerator(self.eca_config, self.config.ECA_N_SAMPLES_PER_RULE, self.config.ECA_TIMESTEPS, self.config.ECA_WIDTH)\n",
        "        tensors_list, labels_list = generator.generate()\n",
        "        dataset = TensorDataset(torch.stack(tensors_list), torch.cat(labels_list))\n",
        "        loader = DataLoader(dataset, batch_size=self.config.BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "        self.model.train()\n",
        "        for epoch in range(self.config.PRETRAIN_EPOCHS):\n",
        "            pbar = tqdm(loader, desc=f\"Stage 1: Pre-train Epoch {epoch+1}/{self.config.PRETRAIN_EPOCHS}\")\n",
        "            for batch_tensors, batch_labels in pbar:\n",
        "                batch_tensors, batch_labels = batch_tensors.to(self.device), batch_labels.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                recon_tensors, rule_logits = self.model(batch_tensors)\n",
        "                recon_loss = self.recon_loss_fn(recon_tensors, batch_tensors)\n",
        "                rule_loss = self.rule_loss_fn(rule_logits, batch_labels)\n",
        "                total_loss = self.config.MDL_RECON_LOSS_WEIGHT * recon_loss + self.config.MDL_RULE_LOSS_WEIGHT * rule_loss\n",
        "                total_loss.backward()\n",
        "                self.optimizer.step()\n",
        "                pbar.set_postfix(loss=total_loss.item(), recon_L=recon_loss.item(), rule_L=rule_loss.item())\n",
        "\n",
        "class EmbeddingPreTrainer:\n",
        "    \"\"\"Orchestrates Stage 1.5: Pre-training the Embedding layer on Symbolized ECA data.\"\"\"\n",
        "    def __init__(self, model: MDL_AU_Net_Autoencoder, eca_config: dict, config: Config, device: str):\n",
        "        self.model = model\n",
        "        self.eca_config = eca_config\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.optimizer = optim.Adam(\n",
        "            list(model.symbol_embedding_head.parameters()) + list(model.rule_classifier_head.parameters()),\n",
        "            lr=config.EMBEDDING_PRETRAIN_LR\n",
        "        )\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.symbolizer = PermutationSymbolizer(dim=config.PERMUTATION_DIM, lag=config.PERMUTATION_LAG)\n",
        "\n",
        "    def _symbolize_eca_data(self, raw_tensors: list) -> list:\n",
        "        symbolized_data = []\n",
        "        pbar = tqdm(raw_tensors, desc=\"Symbolizing ECA data for Stage 1.5\")\n",
        "        for tensor in pbar:\n",
        "            # Create a 1D series by taking the mean across the width dimension\n",
        "            series_1d = tensor.mean(dim=1).cpu().numpy()\n",
        "            symbols = self.symbolizer.symbolize(series_1d)\n",
        "            if len(symbols) >= self.config.SERIES_PROCESSOR_SEQUENCE_LENGTH:\n",
        "                symbolized_data.append(torch.LongTensor(symbols[:self.config.SERIES_PROCESSOR_SEQUENCE_LENGTH]))\n",
        "            else: # Pad if too short\n",
        "                padded = np.pad(symbols, (0, self.config.SERIES_PROCESSOR_SEQUENCE_LENGTH - len(symbols)), 'constant')\n",
        "                symbolized_data.append(torch.LongTensor(padded))\n",
        "        return symbolized_data\n",
        "\n",
        "    def run(self):\n",
        "        # Freeze the already-trained encoder/decoder parts\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if 'symbol_embedding_head' not in name and 'rule_classifier_head' not in name:\n",
        "                param.requires_grad = False\n",
        "            else:\n",
        "                param.requires_grad = True\n",
        "\n",
        "        generator = ECADataGenerator(self.eca_config, self.config.ECA_N_SAMPLES_PER_RULE, self.config.ECA_TIMESTEPS, self.config.ECA_WIDTH)\n",
        "        tensors_list, labels_list = generator.generate()\n",
        "        symbol_sequences = self._symbolize_eca_data(tensors_list)\n",
        "\n",
        "        dataset = TensorDataset(torch.stack(symbol_sequences), torch.cat(labels_list))\n",
        "        loader = DataLoader(dataset, batch_size=self.config.BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "        self.model.train()\n",
        "        for epoch in range(self.config.EMBEDDING_PRETRAIN_EPOCHS):\n",
        "            pbar = tqdm(loader, desc=f\"Stage 1.5: Embedding Pre-train Epoch {epoch+1}/{self.config.EMBEDDING_PRETRAIN_EPOCHS}\")\n",
        "            for batch_symbols, batch_labels in pbar:\n",
        "                batch_symbols, batch_labels = batch_symbols.to(self.device), batch_labels.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                fingerprint = self.model.get_fingerprint(batch_symbols)\n",
        "                rule_logits = self.model.rule_classifier_head(fingerprint)\n",
        "                loss = self.loss_fn(rule_logits, batch_labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "class BreakClassifierFinetuner:\n",
        "    \"\"\"Orchestrates Stage 2: Fine-tuning on the real-world structural break task with two phases.\"\"\"\n",
        "    def __init__(self, model: StructuralBreakClassifier, config: Config, device: str):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def run(self, X_data: pd.DataFrame, y_data: pd.Series):\n",
        "        print(\"Processing real-world data for final fine-tuning...\")\n",
        "        symbolizer = PermutationSymbolizer(self.config.PERMUTATION_DIM, self.config.PERMUTATION_LAG)\n",
        "        processor = SeriesProcessor(symbolizer, self.config.SERIES_PROCESSOR_SEQUENCE_LENGTH, self.config.SERIES_PROCESSOR_N_SEQUENCES_PER_SEGMENT)\n",
        "        all_before_data, all_after_data = [], []\n",
        "        # The platform data has a MultiIndex ('id', 'time'). We group by 'id'.\n",
        "        grouped = X_data.groupby(level='id')\n",
        "        for _, series_df in tqdm(grouped, desc=\"Processing All Series\"):\n",
        "            full_series = series_df['value']\n",
        "            # The 'period' column indicates the first timestep of the new regime.\n",
        "            # So the break is at period - 1.\n",
        "            break_point = series_df['period'].iloc[0] - 1\n",
        "            all_before_data.append(processor.process(full_series.iloc[:break_point]))\n",
        "            all_after_data.append(processor.process(full_series.iloc[break_point:]))\n",
        "\n",
        "        labels = y_data.values\n",
        "\n",
        "        # --- Phase 2a: Fine-tune Head Only ---\n",
        "        print(\"\\n--- Stage 2a: Fine-tuning Head Only ---\")\n",
        "        for name, param in self.model.autoencoder.named_parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.model.classifier_head.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        optimizer_head = optim.Adam(self.model.classifier_head.parameters(), lr=self.config.FINETUNE_HEAD_LR)\n",
        "        self.model.train()\n",
        "        for epoch in range(self.config.FINETUNE_HEAD_ONLY_EPOCHS):\n",
        "            indices = np.random.permutation(len(y_data))\n",
        "            pbar = tqdm(range(0, len(indices), self.config.BATCH_SIZE), desc=f\"Fine-tune Head Epoch {epoch+1}/{self.config.FINETUNE_HEAD_ONLY_EPOCHS}\")\n",
        "            for i in pbar:\n",
        "                batch_indices = indices[i:i+self.config.BATCH_SIZE]\n",
        "                batch_before = [all_before_data[j] for j in batch_indices]\n",
        "                batch_after = [all_after_data[j] for j in batch_indices]\n",
        "                batch_labels = torch.FloatTensor(labels[batch_indices]).unsqueeze(1).to(self.device)\n",
        "                optimizer_head.zero_grad()\n",
        "                logits = self.model(batch_before, batch_after)\n",
        "                loss = self.loss_fn(logits, batch_labels)\n",
        "                loss.backward()\n",
        "                optimizer_head.step()\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # --- Phase 2b: Fine-tune Full Model ---\n",
        "        print(\"\\n--- Stage 2b: Fine-tuning Full Model (Low LR) ---\")\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        optimizer_full = optim.Adam(self.model.parameters(), lr=self.config.FINETUNE_FULL_LR)\n",
        "        self.model.train()\n",
        "        full_epochs = self.config.FINETUNE_EPOCHS - self.config.FINETUNE_HEAD_ONLY_EPOCHS\n",
        "        for epoch in range(full_epochs):\n",
        "            indices = np.random.permutation(len(y_data))\n",
        "            pbar = tqdm(range(0, len(indices), self.config.BATCH_SIZE), desc=f\"Fine-tune Full Epoch {epoch+1}/{full_epochs}\")\n",
        "            for i in pbar:\n",
        "                batch_indices = indices[i:i+self.config.BATCH_SIZE]\n",
        "                batch_before = [all_before_data[j] for j in batch_indices]\n",
        "                batch_after = [all_after_data[j] for j in batch_indices]\n",
        "                batch_labels = torch.FloatTensor(labels[batch_indices]).unsqueeze(1).to(self.device)\n",
        "                optimizer_full.zero_grad()\n",
        "                logits = self.model(batch_before, batch_after)\n",
        "                loss = self.loss_fn(logits, batch_labels)\n",
        "                loss.backward()\n",
        "                optimizer_full.step()\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "class UnifiedTrainer:\n",
        "    \"\"\"Manages the full three-stage training pipeline.\"\"\"\n",
        "    def __init__(self, eca_config: dict, config: Config, device: str):\n",
        "        self.eca_config = eca_config\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        num_rules = len(eca_config.get('base', [])) + len(eca_config.get('composite', []))\n",
        "        self.autoencoder = MDL_AU_Net_Autoencoder(config, num_rules=num_rules).to(device)\n",
        "\n",
        "    def run(self, X_train: pd.DataFrame, y_train: pd.Series) -> StructuralBreakClassifier:\n",
        "        # Stage 1\n",
        "        stage1_trainer = MDLPreTrainer(self.autoencoder, self.eca_config, self.config, self.device)\n",
        "        stage1_trainer.run()\n",
        "        # Stage 1.5\n",
        "        stage1_5_trainer = EmbeddingPreTrainer(self.autoencoder, self.eca_config, self.config, self.device)\n",
        "        stage1_5_trainer.run()\n",
        "        # Stage 2\n",
        "        classifier = StructuralBreakClassifier(self.autoencoder, self.config).to(device)\n",
        "        stage2_trainer = BreakClassifierFinetuner(classifier, self.config, self.device)\n",
        "        stage2_trainer.run(X_train, y_train)\n",
        "        return classifier"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6d6s7Cw30XgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title CELL 4: Main Platform Entrypoints (`train` and `infer`)\n",
        "# ==============================================================================\n",
        "\n",
        "def train(X_train: pd.DataFrame, y_train: pd.Series, model_directory_path: str, debug_mode: bool = False):\n",
        "    \"\"\"\n",
        "    Main training orchestrator that uses the UnifiedTrainer.\n",
        "    \"\"\"\n",
        "    if debug_mode:\n",
        "        print(\"\\n\" + \"!\"*60 + \"\\n!!! RUNNING IN FAST DEBUG MODE - NOT FOR PRODUCTION !!!\\n\" + \"!\"*60 + \"\\n\")\n",
        "        active_config = FastDebugConfig()\n",
        "    else:\n",
        "        active_config = Config()\n",
        "\n",
        "    seed_everything(active_config.SEED)\n",
        "    active_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Starting unified training on device: {active_device}\")\n",
        "\n",
        "    # Use a small subset of data for debugging if no data is provided\n",
        "    # This simulates the platform behavior where data is loaded internally\n",
        "    if X_train is None or y_train is None:\n",
        "        print(\"Loading data from disk for local simulation...\")\n",
        "        X_train = pd.read_parquet(\"data/X_train.parquet\")\n",
        "        y_train = pd.read_parquet(\"data/y_train.parquet\").squeeze()\n",
        "        if debug_mode:\n",
        "            print(\"Debug mode: Using a small subset of the data.\")\n",
        "            # Take 50 random series to ensure variety\n",
        "            all_ids = X_train.index.get_level_values('id').unique()\n",
        "            debug_ids = np.random.choice(all_ids, 50, replace=False)\n",
        "            X_train = X_train[X_train.index.get_level_values('id').isin(debug_ids)]\n",
        "            y_train = y_train.loc[debug_ids]\n",
        "\n",
        "    print(f\"Fine-tuning data ready. X={X_train.shape}, y={y_train.shape}\")\n",
        "\n",
        "    # Define the ECA rule configuration\n",
        "    eca_config = {'base': active_config.ECA_RULES_TO_USE, 'composite': [{'rules': [30, 110], 'timesteps': [10, 10]}]}\n",
        "\n",
        "    # The UnifiedTrainer handles all three stages internally\n",
        "    trainer = UnifiedTrainer(eca_config, active_config, active_device)\n",
        "    final_model = trainer.run(X_train, y_train)\n",
        "\n",
        "    # Save the final artifacts\n",
        "    manager = ArtifactManager(Path(model_directory_path))\n",
        "    manager.save(final_model, active_config)\n",
        "    print(\"\\n============================================================\")\n",
        "    print(\"                UNIFIED TRAINING PIPELINE FINISHED\")\n",
        "    print(\"============================================================\")\n",
        "\n",
        "def infer(X_test: typing.Iterable[pd.DataFrame], model_directory_path: str):\n",
        "    \"\"\"\n",
        "    Main inference orchestrator.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    manager = ArtifactManager(Path(model_directory_path))\n",
        "    model, loaded_config = manager.load_for_inference(device)\n",
        "\n",
        "    symbolizer = PermutationSymbolizer(loaded_config.PERMUTATION_DIM, loaded_config.PERMUTATION_LAG)\n",
        "    processor = SeriesProcessor(symbolizer, loaded_config.SERIES_PROCESSOR_SEQUENCE_LENGTH, loaded_config.SERIES_PROCESSOR_N_SEQUENCES_PER_SEGMENT)\n",
        "\n",
        "    yield # Health check\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for test_df in X_test:\n",
        "            if test_df.empty:\n",
        "                yield 0.5\n",
        "                continue\n",
        "\n",
        "            full_series = test_df['value']\n",
        "            break_point = test_df['period'].iloc[0] - 1\n",
        "\n",
        "            before_seqs = processor.process(full_series.iloc[:break_point])\n",
        "            after_seqs = processor.process(full_series.iloc[break_point:])\n",
        "\n",
        "            if not before_seqs or not after_seqs:\n",
        "                yield 0.5\n",
        "                continue\n",
        "\n",
        "            logits = model([before_seqs], [after_seqs])\n",
        "            score = torch.sigmoid(logits).item()\n",
        "            yield score"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iG2Iel8g0XVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# @title FINAL TIER 3 VALIDATION (Simplified & Corrected)\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Create a standard config instance and then override it ---\n",
        "    # This is the simplest and most robust way to create a test config.\n",
        "    config = Config() # Instantiate the full config from Cell 1\n",
        "\n",
        "    # Manually override attributes for a faster validation run\n",
        "    print(\"--- Using validation config overrides for this run. ---\")\n",
        "    config.ECA_N_SAMPLES_PER_RULE = 20\n",
        "    config.PRETRAIN_EPOCHS = 3\n",
        "    config.EMBEDDING_PRETRAIN_EPOCHS = 3\n",
        "    config.FINETUNE_HEAD_ONLY_EPOCHS = 3\n",
        "    config.FINETUNE_EPOCHS = 5\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"      RUNNING TIER 3 VALIDATION (3-Stage & OOS Test)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    seed_everything(config.SEED)\n",
        "\n",
        "    # --- Create Mock Data & Split ---\n",
        "    def create_mock_series_data(series_id, has_break=False, length=500, break_point=250):\n",
        "        t = np.linspace(0, 10, length)\n",
        "        noise = np.random.randn(length) * 0.1\n",
        "        series_values = np.sin(t * 2 * np.pi) + noise\n",
        "        if has_break:\n",
        "            series_values[break_point:] = np.cos(t[break_point:] * 5 * np.pi) * 1.5 + noise[break_point:]\n",
        "        data = []\n",
        "        for time_step in range(length):\n",
        "            data.append({\n",
        "                'id': series_id, 'time': time_step, 'value': series_values[time_step],\n",
        "                'period': break_point + 1, 'y': 1 if has_break else 0\n",
        "            })\n",
        "        return data\n",
        "\n",
        "    mock_data = []\n",
        "    for i in range(100):\n",
        "        mock_data.extend(create_mock_series_data(series_id=f\"series_{i}\", has_break=(i % 2 == 0)))\n",
        "\n",
        "    full_df = pd.DataFrame(mock_data).set_index(['id', 'time'])\n",
        "    X_all = full_df[['value', 'period']]\n",
        "    y_all = full_df.groupby('id')['y'].first()\n",
        "\n",
        "    train_ids, test_ids = train_test_split(\n",
        "        y_all.index, test_size=0.3, random_state=config.SEED, stratify=y_all\n",
        "    )\n",
        "\n",
        "    X_train = X_all[X_all.index.get_level_values('id').isin(train_ids)]\n",
        "    y_train = y_all[y_all.index.isin(train_ids)]\n",
        "    X_test = X_all[X_all.index.get_level_values('id').isin(test_ids)]\n",
        "    y_test = y_all[y_all.index.isin(test_ids)]\n",
        "    print(f\"Training samples: {len(train_ids)}, Testing samples: {len(test_ids)}\")\n",
        "\n",
        "    # --- Run Training & Inference ---\n",
        "    try:\n",
        "        model_dir = Path(\"./final_model_validation\")\n",
        "\n",
        "        # This line will now work because Cell 1 defines `ECA_RULES_TO_USE` on the Config class.\n",
        "        eca_config = {'base': config.ECA_RULES_TO_USE, 'composite': [{'rules': [30, 110], 'timesteps': [10, 10]}]}\n",
        "\n",
        "        train(X_train, y_train, str(model_dir), eca_config=eca_config)\n",
        "\n",
        "        # Re-format test data for the infer function\n",
        "        test_iterable = [X_test.loc[test_id] for test_id in test_ids]\n",
        "        predictions = list(infer(test_iterable, str(model_dir)))\n",
        "        if isinstance(predictions[0], dict) and \"health\" in predictions[0]:\n",
        "             predictions.pop(0)\n",
        "\n",
        "        # Evaluate performance\n",
        "        y_test_values = y_test.values\n",
        "        auc_score = roc_auc_score(y_test_values, predictions)\n",
        "        print(\"\\n\" + \"=\"*20 + \" FINAL RESULT \" + \"=\"*20)\n",
        "        print(f\"📈 Final Out-of-Sample ROC AUC Score: {auc_score:.4f}\")\n",
        "        print(\"=\"*54)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR during full validation run: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uaW8144vAuSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "pssykjKcCu4e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AE1i3pR-0fV"
      },
      "source": [
        "# Submit your Notebook\n",
        "\n",
        "To submit your work, you must:\n",
        "1. Download your Notebook from Colab\n",
        "2. Upload it to the platform\n",
        "3. Create a run to validate it\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Download and Submit Notebook](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/download-and-submit-notebook.gif)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04beaea2273d41cd86b92b3d5f712e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e3a91dd26cd464798887b502afacebe",
              "IPY_MODEL_d1ca9e03620e450eb73e7d0399796047",
              "IPY_MODEL_d1f9d5996ae34653b2217b9193bb15f2"
            ],
            "layout": "IPY_MODEL_c2f740f04d39488b8c7749087cbe40c1"
          }
        },
        "2e3a91dd26cd464798887b502afacebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e47133648a14a2494e49bba953ec27f",
            "placeholder": "​",
            "style": "IPY_MODEL_d84476d3f0df4fb4b6bff7fa8512ede1",
            "value": "Generating ECA Data:  11%"
          }
        },
        "d1ca9e03620e450eb73e7d0399796047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2dcd903454948aba72ce0d5677c2211",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55fde5e2363144ac8477c55c92e8e3e3",
            "value": 3
          }
        },
        "d1f9d5996ae34653b2217b9193bb15f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c569f6a1930b46a0bae9b9f9d972a5bf",
            "placeholder": "​",
            "style": "IPY_MODEL_cba68990a8ad462c9bf14e4dbdd484d8",
            "value": " 3/28 [01:05&lt;09:04, 21.78s/it]"
          }
        },
        "c2f740f04d39488b8c7749087cbe40c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e47133648a14a2494e49bba953ec27f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d84476d3f0df4fb4b6bff7fa8512ede1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2dcd903454948aba72ce0d5677c2211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55fde5e2363144ac8477c55c92e8e3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c569f6a1930b46a0bae9b9f9d972a5bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cba68990a8ad462c9bf14e4dbdd484d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "942c5416a7c54f4384c5c4d798af0d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0744f5bb9d14d0f8764f9ccdc156b72",
              "IPY_MODEL_f9d5853ddae64987950061085bc01c95",
              "IPY_MODEL_db78054b43c748ed8200f177cc54695e"
            ],
            "layout": "IPY_MODEL_66e07fa3211c4f15b3096e1f1250ad0f"
          }
        },
        "c0744f5bb9d14d0f8764f9ccdc156b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3a3a7858ce1499c9d35ec270e10772f",
            "placeholder": "​",
            "style": "IPY_MODEL_18d119886fb7488faf55154db9bd37a4",
            "value": "Generating Base ECAs:  71%"
          }
        },
        "f9d5853ddae64987950061085bc01c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97fa7d0b19c24147a6d991c1680ded0f",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa2493d006b7429983db3fbf411a181a",
            "value": 20
          }
        },
        "db78054b43c748ed8200f177cc54695e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc8f59b1209d45da8d58a91428f145a2",
            "placeholder": "​",
            "style": "IPY_MODEL_12085a40604846d5a9fd482f83a96224",
            "value": " 20/28 [23:23&lt;09:25, 70.68s/it]"
          }
        },
        "66e07fa3211c4f15b3096e1f1250ad0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3a3a7858ce1499c9d35ec270e10772f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d119886fb7488faf55154db9bd37a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97fa7d0b19c24147a6d991c1680ded0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa2493d006b7429983db3fbf411a181a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc8f59b1209d45da8d58a91428f145a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12085a40604846d5a9fd482f83a96224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}