**Title:**
When AIs Become a Society: Revisiting the Cybernetic Teammate Hypothesis

---

**Introduction: The Game Has Changed**

A breakthrough finding has emerged from the recent paper *"Emergence of shared linguistic conventions and biases in multi-agent large language models"* by Baronchelli et al. (2024):

> When you place multiple LLMs in an environment and allow them to interact freely, they spontaneously form shared conventions — and even collective biases — without any central orchestrator.

This changes everything.

Until now, the dominant mental model for human-AI teaming assumed that a human (or a smart orchestrator) had to coordinate tasks, direct queries, and manage roles. But this research shows that intelligence — and social structure — can emerge purely from **repeated interaction and feedback** among agents.

In this article, we revisit and expand our original *Cybernetic Teammate* hypothesis in light of this finding, identifying what changes, what remains, and what becomes possible.

---

**1. From Orchestrated Roles to Emergent Conventions**

Our original hypothesis emphasized the need for a **runtime platform** to route tasks to LLM agents based on internal state modeling, specialization, and semantic memory.

But Baronchelli’s experiment reveals:

* Agents spontaneously form stable task conventions.
* Minority agents can influence group norms.
* Biases are not pre-trained artifacts — they *emerge* through interaction.

This suggests that **query routing, task ownership, and even role formation** can emerge *organically*, driven by:

* Confidence scoring
* Role-adoption history
* Local social feedback

No explicit routing table required — just **persistent co-adaptation**.

---

**2. What Stays: The Cybernetic Core**

This finding doesn't dismantle our hypothesis — it **reinforces its foundations**:

✅ Intelligence is system-bound, not model-bound.
✅ Viability arises from adaptation, feedback, and memory.
✅ Observer framing is still essential — now to **monitor emergent coordination**, not impose it.
✅ The runtime platform remains — but as an **ecosystem facilitator**, not a task dispatcher.

The Viable System Model (Beer, 1981) remains deeply relevant, but we now reinterpret:

* **System 1**: Autonomous agents that self-select tasks.
* **System 2**: Stabilized conventions and shared context.
* **System 3**: Meta-feedback via human monitoring or runtime analytics.

---

**3. From Cybernetic Teammate to Cognitive Ecology**

This marks a philosophical shift:

> We move from building *individual teammates* to cultivating **AI societies**.

These collectives are:

* **Distributed**: No single point of failure or control.
* **Conventional**: Shared norms develop bottom-up.
* **Stigmergic**: Agents shape and respond to each other’s outputs.

As with human teams, not all behavior is desirable. Hence:

* We still need **meta-observation**, trust calibration, and human intervention.
* The goal is not to eliminate coordination but to **shift it into the system's culture**.

---

**4. Implications for AI Design and Governance**

This research opens new questions for viable AI ecosystems:

* 🔄 How do we steer conventions toward desirable norms?
* 📉 How do we detect and correct emergent bias or groupthink?
* 🧠 How can humans effectively interface with a *society* of models, not just one?
* 🧭 What counts as *agency* in a system where behavior arises from interaction, not intention?

It may even force us to redefine:

* What an AI “role” is
* What “alignment” means
* How we measure systemic intelligence

---

**Conclusion: The Teammate is Now a Society**

The emergence of AI societies doesn’t disprove the Cybernetic Teammate hypothesis — it **elevates it**. We are no longer just designing human-AI pairs. We are:

> **Engineering viable, evolving, feedback-driven collectives** — where agents adopt, shape, and stabilize roles through mutual observation and adaptation.

This is not coordination-as-control. It’s **coordination-as-culture**.

And it may be the missing link between today’s agentic tools and tomorrow’s artificial general intelligence.

---

*This article builds upon and extends our [Cybernetic Intelligence Hypothesis](https://www.linkedin.com/pulse/emergent-convergence-how-recursive-llms-cellular-yr0nc/), integrating new findings into a broader vision of AI as a viable, evolving system.*
